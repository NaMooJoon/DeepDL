Project,fix-commit,fix-shortMessage,fix-date,fix-author,patch
hadoop,3358fb5b09e3e34038b234a992ab151fe264a7f3,"HADOOP-9. Use roulette scheduling for temporary space when the size
is not known. (Ari Rabkin via omalley)


git-svn-id: https://svn.apache.org/repos/asf/hadoop/core/trunk@685320 13f79535-47bb-0310-9956-ffa450edef68
",2008-08-12 21:20:41,Owen O'Malley,"diff --git a/src/core/org/apache/hadoop/fs/LocalDirAllocator.java b/src/core/org/apache/hadoop/fs/LocalDirAllocator.java
index ff7ae0c..3263b097 100644
--- a/src/core/org/apache/hadoop/fs/LocalDirAllocator.java
+++ b/src/core/org/apache/hadoop/fs/LocalDirAllocator.java
@@ -267,18 +267,22 @@
     }
     
     /** Get a path from the local FS. This method should be used if the size of 
-     *  the file is not known apriori. We go round-robin over the set of disks
-     *  (via the configured dirs) and return the first complete path where
-     *  we could create the parent directory of the passed path. 
+     *  the file is not known a priori. 
+     *  
+     *  It will use roulette selection, picking directories
+     *  with probability proportional to their available space. 
      */
     public synchronized Path getLocalPathForWrite(String path, 
         Configuration conf) throws IOException {
       return getLocalPathForWrite(path, -1, conf);
     }
 
-    /** Get a path from the local FS. Pass size as -1 if not known apriori. We
+    /** Get a path from the local FS. If size is known, we go
      *  round-robin over the set of disks (via the configured dirs) and return
-     *  the first complete path which has enough space 
+     *  the first complete path which has enough space.
+     *  
+     *  If size is not known, use roulette selection -- pick directories
+     *  with probability proportional to their available space.
      */
     public synchronized Path getLocalPathForWrite(String pathStr, long size, 
         Configuration conf) throws IOException {
@@ -291,20 +295,38 @@
         pathStr = pathStr.substring(1);
       }
       Path returnPath = null;
-      while (numDirsSearched < numDirs && returnPath == null) {
-        if (size >= 0) {
+      
+      if(size == -1) {  //do roulette selection: pick dir with probability 
+                    //proportional to available size
+        long[] availableOnDisk = new long[dirDF.length];
+        long totalAvailable = 0;
+        
+            //build the ""roulette wheel""
+        for(int i =0; i < dirDF.length; ++i) {
+          availableOnDisk[i] = dirDF[i].getAvailable();
+          totalAvailable += availableOnDisk[i];
+        }
+            // ""roll the ball"" -- pick a directory
+        Random r = new java.util.Random();
+        long randomPosition = Math.abs(r.nextLong()) % totalAvailable;
+        int dir=0;
+        while(randomPosition > availableOnDisk[dir]) {
+          randomPosition -= availableOnDisk[dir];
+          dir++;
+        }
+        dirNumLastAccessed = dir;
+        returnPath = createPath(pathStr);
+      } else {
+        while (numDirsSearched < numDirs && returnPath == null) {
           long capacity = dirDF[dirNumLastAccessed].getAvailable();
           if (capacity > size) {
             returnPath = createPath(pathStr);
           }
-        } else {
-          returnPath = createPath(pathStr);
-        }
-        dirNumLastAccessed++;
-        dirNumLastAccessed = dirNumLastAccessed % numDirs; 
-        numDirsSearched++;
-      } 
-
+          dirNumLastAccessed++;
+          dirNumLastAccessed = dirNumLastAccessed % numDirs; 
+          numDirsSearched++;
+        } 
+      }
       if (returnPath != null) {
         return returnPath;
       }
"
hadoop,1231caef3bd7b19347249fcbdd938e4a423258d6,"HADOOP-236. JobTacker now refuses connection from a task tracker with a different version number. Contributed by Sharad Agarwal.

git-svn-id: https://svn.apache.org/repos/asf/hadoop/core/trunk@663829 13f79535-47bb-0310-9956-ffa450edef68
",2008-06-06 05:40:21,Devaraj Das,"diff --git a/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java b/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java
index fed3dd8..b4198f1 100644
--- a/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java
+++ b/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java
@@ -42,8 +42,9 @@
    * version 10 changes the TaskStatus representation for HADOOP-2208
    * version 11 changes string to JobID in getTaskCompletionEvents().
    * version 12 changes the counters representation for HADOOP-1915
+   * version 13 added call getBuildVersion() for HADOOP-236
    */
-  public static final long versionID = 12L;
+  public static final long versionID = 13L;
   
   public final static int TRACKERS_OK = 0;
   public final static int UNKNOWN_TASKTRACKER = 1;
@@ -102,6 +103,11 @@
   TaskCompletionEvent[] getTaskCompletionEvents(JobID jobid, int fromEventId
       , int maxEvents) throws IOException;
   
+  
+  /**
+   * Returns the buildVersion of the JobTracker 
+   */
+  public String getBuildVersion() throws IOException;
 }
 
 
"
hadoop,1231caef3bd7b19347249fcbdd938e4a423258d6,"HADOOP-236. JobTacker now refuses connection from a task tracker with a different version number. Contributed by Sharad Agarwal.

git-svn-id: https://svn.apache.org/repos/asf/hadoop/core/trunk@663829 13f79535-47bb-0310-9956-ffa450edef68
",2008-06-06 05:40:21,Devaraj Das,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index 39f6ad6..fd3282f 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -65,6 +65,7 @@
 import org.apache.hadoop.util.HostsFileReader;
 import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.hadoop.util.StringUtils;
+import org.apache.hadoop.util.VersionInfo;
 
 /*******************************************************
  * JobTracker is the central location for submitting and 
@@ -1224,6 +1225,10 @@
   ////////////////////////////////////////////////////
   // InterTrackerProtocol
   ////////////////////////////////////////////////////
+  
+  public String getBuildVersion() throws IOException{
+    return VersionInfo.getBuildVersion();
+  }
 
   /**
    * The periodic heartbeat mechanism between the {@link TaskTracker} and
"
hadoop,1231caef3bd7b19347249fcbdd938e4a423258d6,"HADOOP-236. JobTacker now refuses connection from a task tracker with a different version number. Contributed by Sharad Agarwal.

git-svn-id: https://svn.apache.org/repos/asf/hadoop/core/trunk@663829 13f79535-47bb-0310-9956-ffa450edef68
",2008-06-06 05:40:21,Devaraj Das,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 72b6dd1..9dcc420 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -75,6 +75,7 @@
 import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.hadoop.util.RunJar;
 import org.apache.hadoop.util.StringUtils;
+import org.apache.hadoop.util.VersionInfo;
 import org.apache.hadoop.util.DiskChecker.DiskErrorException;
 import org.apache.hadoop.util.Shell.ShellCommandExecutor;
 import org.apache.log4j.LogManager;
@@ -899,6 +900,23 @@
           }
         }
 
+        //verify the buildVersion if justStarted
+        if(justStarted){
+          String jobTrackerBV = jobClient.getBuildVersion();
+          if(!VersionInfo.getBuildVersion().equals(jobTrackerBV)) {
+            String msg = ""Shutting down. Incompatible buildVersion."" +
+            ""\nJobTracker's: "" + jobTrackerBV + 
+            ""\nTaskTracker's: ""+ VersionInfo.getBuildVersion();
+            LOG.error(msg);
+            try {
+              jobClient.reportTaskTrackerError(taskTrackerName, null, msg);
+            } catch(Exception e ) {
+              LOG.info(""Problem reporting to jobtracker: "" + e);
+            }
+            return State.DENIED;
+          }
+        }
+        
         // Send the heartbeat and process the jobtracker's directives
         HeartbeatResponse heartbeatResponse = transmitHeartBeat();
         TaskTrackerAction[] actions = heartbeatResponse.getActions();
"
hadoop,1231caef3bd7b19347249fcbdd938e4a423258d6,"HADOOP-236. JobTacker now refuses connection from a task tracker with a different version number. Contributed by Sharad Agarwal.

git-svn-id: https://svn.apache.org/repos/asf/hadoop/core/trunk@663829 13f79535-47bb-0310-9956-ffa450edef68
",2008-06-06 05:40:21,Devaraj Das,"diff --git a/src/java/org/apache/hadoop/util/VersionInfo.java b/src/java/org/apache/hadoop/util/VersionInfo.java
index 25adb40..6aee978 100644
--- a/src/java/org/apache/hadoop/util/VersionInfo.java
+++ b/src/java/org/apache/hadoop/util/VersionInfo.java
@@ -80,6 +80,17 @@
     return version != null ? version.url() : ""Unknown"";
   }
   
+  /**
+   * Returns the buildVersion which includes version, 
+   * revision, user and date. 
+   */
+  public static String getBuildVersion(){
+    return VersionInfo.getVersion() + 
+    "" from "" + VersionInfo.getRevision() +
+    "" by "" + VersionInfo.getUser() + 
+    "" on "" + VersionInfo.getDate();
+  }
+  
   public static void main(String[] args) {
     System.out.println(""Hadoop "" + getVersion());
     System.out.println(""Subversion "" + getUrl() + "" -r "" + getRevision());
"
hadoop,60c95c95f7ae1d0eb47a7eb198c7d9b41b740c88,"HADOOP-290. A DataNode log message now prints the target of a replication
request correctly. (dhruba)



git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@613058 13f79535-47bb-0310-9956-ffa450edef68
",2008-01-18 05:23:30,Dhruba Borthakur,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index 9b4d1f29..11e884b 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -833,8 +833,21 @@
                              errStr);
         break;
       }
-      if (xferTargets[i].length > 0) {
-        LOG.info(dnRegistration + "" Starting thread to transfer block "" + blocks[i] + "" to "" + xferTargets[i][0].getName());
+      int numTargets = xferTargets[i].length;
+      if (numTargets > 0) {
+        if (LOG.isInfoEnabled()) {
+          StringBuilder xfersBuilder = new StringBuilder();
+          for (int j = 0; j < numTargets; j++) {
+            DatanodeInfo nodeInfo = xferTargets[i][j];
+            xfersBuilder.append(nodeInfo.getName());
+            if (j < (numTargets - 1)) {
+              xfersBuilder.append("", "");
+            }
+          }
+          String xfersTo = xfersBuilder.toString();
+          LOG.info(dnRegistration + "" Starting thread to transfer block "" + 
+                   blocks[i] + "" to "" + xfersTo);                       
+        }
         new Daemon(new DataTransfer(xferTargets[i], blocks[i])).start();
       }
     }
"
hadoop,2051a4f1875454e98965993ee3a399331791bc32,"HADOOP-89.  A client can access file data even before the creator
has closed the file. Introduce a new dfs shell command ""tail"".
(Dhruba Borthakur via dhruba)



git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@577456 13f79535-47bb-0310-9956-ffa450edef68
",2007-09-19 22:12:49,Dhruba Borthakur,"diff --git a/src/java/org/apache/hadoop/dfs/FSDirectory.java b/src/java/org/apache/hadoop/dfs/FSDirectory.java
index eb81f77..68fa874 100644
--- a/src/java/org/apache/hadoop/dfs/FSDirectory.java
+++ b/src/java/org/apache/hadoop/dfs/FSDirectory.java
@@ -25,6 +25,7 @@
 import org.apache.hadoop.metrics.MetricsRecord;
 import org.apache.hadoop.metrics.MetricsUtil;
 import org.apache.hadoop.metrics.MetricsContext;
+import org.apache.hadoop.dfs.BlocksMap.BlockInfo;
 
 /*************************************************
  * FSDirectory stores the filesystem directory state.
@@ -114,31 +115,43 @@
   /**
    * Add the given filename to the fs.
    */
-  public boolean addFile(String path, Block[] blocks, short replication,
-                         long preferredBlockSize) {
+  INode addFile(String path, 
+                short replication,
+                long preferredBlockSize,
+                String clientName,
+                String clientMachine,
+                DatanodeDescriptor clientNode) 
+                throws IOException {
     waitForReady();
 
     // Always do an implicit mkdirs for parent directory tree.
     long modTime = FSNamesystem.now();
     if (!mkdirs(new Path(path).getParent().toString(), modTime)) {
-      return false;
+      return null;
     }
-    INodeFile newNode = (INodeFile)unprotectedAddFile(path, blocks, replication,
-                                                      modTime, 
-                                                      preferredBlockSize);
+    INodeFile newNode = new INodeFileUnderConstruction(replication, 
+                                 preferredBlockSize, modTime, clientName, 
+                                 clientMachine, clientNode);
+    synchronized (rootDir) {
+      try {
+        newNode = rootDir.addNode(path, newNode);
+      } catch (FileNotFoundException e) {
+        newNode = null;
+      }
+    }
     if (newNode == null) {
       NameNode.stateChangeLog.info(""DIR* FSDirectory.addFile: ""
-                                   +""failed to add ""+path+"" with ""
-                                   +blocks.length+"" blocks to the file system"");
-      return false;
+                                   +""failed to add ""+path
+                                   +"" to the file system"");
+      return null;
     }
     // add create file record to log
     fsImage.getEditLog().logCreateFile(path, newNode);
     NameNode.stateChangeLog.debug(""DIR* FSDirectory.addFile: ""
-                                  +path+"" with ""+blocks.length+"" blocks is added to the file system"");
-    return true;
+                                  +path+"" is added to the file system"");
+    return newNode;
   }
-    
+
   /**
    */
   INode unprotectedAddFile( String path, 
@@ -171,28 +184,35 @@
   }
 
   /**
-   * Add blocks to the file.
+   * Add a block to the file. Returns a reference to the added block.
    */
-  boolean addBlocks(String path, Block[] blocks) throws IOException {
+  Block addBlock(String path, INode file, Block block) throws IOException {
     waitForReady();
 
     synchronized (rootDir) {
-      INodeFile fileNode = this.getFileINode(path);
-      if (fileNode == null) {
-        throw new IOException(""Unknown file: "" + path);
-      }
-      if (fileNode.getBlocks() != null &&
-          fileNode.getBlocks().length != 0) {
-        throw new IOException(""Cannot add new blocks to "" +
-                              ""already existing file."");
-      }
+      INodeFile fileNode = (INodeFile) file;
 
       // associate the new list of blocks with this file
-      fileNode.allocateBlocks(blocks.length);
-      for (int i = 0; i < blocks.length; i++) {
-        fileNode.setBlock(i, 
-            namesystem.blocksMap.addINode(blocks[i], fileNode));
-      }
+      namesystem.blocksMap.addINode(block, fileNode);
+      BlockInfo blockInfo = namesystem.blocksMap.getStoredBlock(block);
+      fileNode.addBlock(blockInfo);
+
+      NameNode.stateChangeLog.debug(""DIR* FSDirectory.addFile: ""
+                                    + path + "" with "" + block
+                                    + "" block is added to the in-memory ""
+                                    + ""file system"");
+    }
+    return block;
+  }
+
+  /**
+   * Persist the block list for the inode.
+   */
+  void persistBlocks(String path, INode file) throws IOException {
+    waitForReady();
+
+    synchronized (rootDir) {
+      INodeFile fileNode = (INodeFile) file;
 
       // create two transactions. The first one deletes the empty
       // file and the second transaction recreates the same file
@@ -202,8 +222,37 @@
       // re-add create file record to log
       fsImage.getEditLog().logCreateFile(path, fileNode);
       NameNode.stateChangeLog.debug(""DIR* FSDirectory.addFile: ""
-                                    +path+"" with ""+blocks.length
-                                    +"" blocks is added to the file system"");
+                                    +path+"" with ""+ fileNode.getBlocks().length 
+                                    +"" blocks is persisted to the file system"");
+    }
+  }
+
+  /**
+   * Remove a block to the file.
+   */
+  boolean removeBlock(String path, INode file, Block block) throws IOException {
+    waitForReady();
+
+    synchronized (rootDir) {
+      INodeFile fileNode = (INodeFile) file;
+      if (fileNode == null) {
+        throw new IOException(""Unknown file: "" + path);
+      }
+
+      // modify file-> block and blocksMap
+      fileNode.removeBlock(block);
+      namesystem.blocksMap.removeINode(block);
+
+      // create two transactions. The first one deletes the empty
+      // file and the second transaction recreates the same file
+      // with the appropriate set of blocks.
+      fsImage.getEditLog().logDelete(path, fileNode.getModificationTime());
+
+      // re-add create file record to log
+      fsImage.getEditLog().logCreateFile(path, fileNode);
+      NameNode.stateChangeLog.debug(""DIR* FSDirectory.addFile: ""
+                                    +path+"" with ""+block
+                                    +"" block is added to the file system"");
     }
     return true;
   }
@@ -377,6 +426,28 @@
   }
 
   /**
+   * Replaces the specified inode with the specified one.
+   */
+  void replaceNode(String path, INodeFile oldnode, INodeFile newnode) 
+                      throws IOException {
+    synchronized (rootDir) {
+      //
+      // Remove the node from the namespace 
+      //
+      if (!oldnode.removeNode()) {
+        NameNode.stateChangeLog.warn(""DIR* FSDirectory.replaceNode: "" +
+                                     ""failed to remove "" + path);
+        throw new IOException(""FSDirectory.replaceNode: "" +
+                              ""failed to remove "" + path);
+      } 
+      rootDir.addNode(path, newnode); 
+      for (Block b : newnode.getBlocks()) {
+        namesystem.blocksMap.addINode(b, newnode);
+      }
+    }
+  }
+
+  /**
    * Get a listing of files given path 'src'
    *
    * This function is admittedly very inefficient right now.  We'll
"
hadoop,2051a4f1875454e98965993ee3a399331791bc32,"HADOOP-89.  A client can access file data even before the creator
has closed the file. Introduce a new dfs shell command ""tail"".
(Dhruba Borthakur via dhruba)



git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@577456 13f79535-47bb-0310-9956-ffa450edef68
",2007-09-19 22:12:49,Dhruba Borthakur,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index 850765b..9119edd 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -104,12 +104,6 @@
     new TreeMap<String, Collection<Block>>();
 
   //
-  // Keeps track of files that are being created, plus the
-  // blocks that make them up.
-  //
-  PendingCreates pendingCreates = new PendingCreates();
-
-  //
   // Stats on overall usage
   //
   long totalCapacity = 0L, totalUsed=0L, totalRemaining = 0L;
@@ -719,7 +713,15 @@
       throw new IOException( 
                             text + "" is less than the required minimum "" + minReplication);
   }
-    
+
+  void startFile(String src, String holder, String clientMachine, 
+                 boolean overwrite, short replication, long blockSize
+                ) throws IOException {
+    startFileInternal(src, holder, clientMachine, overwrite,
+                      replication, blockSize);
+    getEditLog().logSync();
+  }
+
   /**
    * The client would like to create a new block for the indicated
    * filename.  Return an array that consists of the block, plus a set 
@@ -731,7 +733,7 @@
    * @throws IOException if the filename is invalid
    *         {@link FSDirectory#isValidToCreate(String)}.
    */
-  synchronized void startFile(String src, 
+  synchronized void startFileInternal(String src, 
                                               String holder, 
                                               String clientMachine, 
                                               boolean overwrite,
@@ -746,10 +748,11 @@
       throw new IOException(""Invalid file name: "" + src);      	  
     }
     try {
-      FileUnderConstruction pendingFile = pendingCreates.get(src);
-      if (pendingFile != null) {
+      INode myFile = dir.getFileINode(src);
+      if (myFile != null && (myFile instanceof INodeFileUnderConstruction)) {
+        INodeFileUnderConstruction pendingFile = (INodeFileUnderConstruction) myFile;
         //
-        // If the file exists in pendingCreate, then it must be in our
+        // If the file is under construction , then it must be in our
         // leases. Find the appropriate lease record.
         //
         Lease lease = getLease(holder);
@@ -814,15 +817,6 @@
       DatanodeDescriptor clientNode = 
         host2DataNodeMap.getDatanodeByHost(clientMachine);
 
-      // Reserve space for this pending file
-      pendingCreates.put(src, 
-                         new FileUnderConstruction(replication, 
-                                                   blockSize,
-                                                   holder,
-                                                   clientMachine, 
-                                                   clientNode));
-      NameNode.stateChangeLog.debug(""DIR* NameSystem.startFile: ""
-                                    +""add ""+src+"" to pendingCreates for ""+holder);
       synchronized (leases) {
         Lease lease = getLease(holder);
         if (lease == null) {
@@ -836,20 +830,27 @@
         }
         lease.startedCreate(src);
       }
+
+      //
+      // Now we can add the name to the filesystem. This file has no
+      // blocks associated with it.
+      //
+      INode newNode = dir.addFile(src, replication, blockSize,
+                                  holder, 
+                                  clientMachine, 
+                                  clientNode);
+      if (newNode == null) {
+        throw new IOException(""DIR* NameSystem.startFile: "" +
+                              ""Unable to add file to namespace."");
+      }
     } catch (IOException ie) {
       NameNode.stateChangeLog.warn(""DIR* NameSystem.startFile: ""
                                    +ie.getMessage());
       throw ie;
     }
 
-    //
-    // Now we can add the name to the filesystem. This file has no
-    // blocks associated with it.
-    //
-    if (!dir.addFile(src, new Block[0], replication, blockSize)) {
-      throw new IOException(""DIR* NameSystem.startFile: "" +
-                            ""Unable to add file to namespace."");
-    }
+    NameNode.stateChangeLog.debug(""DIR* NameSystem.startFile: ""
+                                  +""add ""+src+"" to namespace for ""+holder);
   }
 
   /**
@@ -882,7 +883,7 @@
       //
       // make sure that we still have the lease on this file
       //
-      FileUnderConstruction pendingFile = pendingCreates.get(src);
+      INodeFileUnderConstruction pendingFile = (INodeFileUnderConstruction) dir.getFileINode(src);
       if (pendingFile == null) {
         throw new LeaseExpiredException(""No lease on "" + src);
       }
@@ -898,11 +899,11 @@
       if (!checkFileProgress(pendingFile, false)) {
         throw new NotReplicatedYetException(""Not replicated yet:"" + src);
       }
-      fileLength = pendingFile.computeFileLength();
-      blockSize = pendingFile.getBlockSize();
+      fileLength = pendingFile.computeContentsLength();
+      blockSize = pendingFile.getPreferredBlockSize();
       clientNode = pendingFile.getClientNode();
       replication = (int)pendingFile.getReplication();
-      newBlock = allocateBlock(src);
+      newBlock = allocateBlock(src, pendingFile);
     }
 
     DatanodeDescriptor targets[] = replicator.chooseTarget(replication,
@@ -928,13 +929,14 @@
     //
     NameNode.stateChangeLog.debug(""BLOCK* NameSystem.abandonBlock: ""
                                   +b.getBlockName()+""of file ""+src);
-    boolean status = pendingCreates.removeBlock(src, b);
-    if (status) {
-      NameNode.stateChangeLog.debug(""BLOCK* NameSystem.abandonBlock: ""
+    INode file = dir.getFileINode(src);
+    if (file != null) {
+      dir.removeBlock(src, file, b);
+    }
+    NameNode.stateChangeLog.debug(""BLOCK* NameSystem.abandonBlock: ""
                                     + b.getBlockName()
                                     + "" is removed from pendingCreates"");
-    }
-    return status;
+    return true;
   }
 
   /**
@@ -964,8 +966,7 @@
   }
 
   /**
-   * Finalize the created file and make it world-accessible.  The
-   * FSNamesystem will already know the blocks that make up the file.
+   * The FSNamesystem will already know the blocks that make up the file.
    * Before we return, we make sure that all the file's blocks have 
    * been reported by datanodes and are replicated correctly.
    */
@@ -980,11 +981,11 @@
     NameNode.stateChangeLog.debug(""DIR* NameSystem.completeFile: "" + src + "" for "" + holder);
     if (isInSafeMode())
       throw new SafeModeException(""Cannot complete file "" + src, safeMode);
-    FileUnderConstruction pendingFile = pendingCreates.get(src);
+    INodeFileUnderConstruction pendingFile = (INodeFileUnderConstruction) dir.getFileINode(src);
 
     Block[] fileBlocks =  dir.getFileBlocks(src);
-    if ((fileBlocks != null && fileBlocks.length > 0) ||
-         pendingFile == null) {    
+    if (fileBlocks == null || fileBlocks.length == 0 ||
+        pendingFile == null) {    
       NameNode.stateChangeLog.warn(""DIR* NameSystem.completeFile: ""
                                    + ""failed to complete "" + src
                                    + "" because dir.getFileBlocks() is "" + 
@@ -999,36 +1000,16 @@
       return STILL_WAITING;
     }
         
-    Collection<Block> blocks = pendingFile.getBlocks();
-    int nrBlocks = blocks.size();
-    Block pendingBlocks[] = new Block[nrBlocks];
+    // The file is no longer pending.
+    // Create permanent INode, update blockmap
+    INodeFile newFile = pendingFile.convertToInodeFile();
+    dir.replaceNode(src, pendingFile, newFile);
 
-    //
-    // We have the pending blocks, but they won't have
-    // length info in them (as they were allocated before
-    // data-write took place). Find the block stored in
-    // node descriptor.
-    //
-    int idx = 0;
-    for (Block b : blocks) {
-      Block storedBlock = blocksMap.getStoredBlock(b);
-      // according to checkFileProgress() every block is present & replicated
-      assert storedBlock != null : ""Missing block "" + b.getBlockName();
-      pendingBlocks[idx++] = storedBlock;
-    }
-        
-    //
-    // add blocks to the file
-    //
-    if (!dir.addBlocks(src, pendingBlocks)) {
-      return OPERATION_FAILED;
-    }
+    // persist block allocations for this file
+    dir.persistBlocks(src, newFile);
 
-    // The file is no longer pending
-    pendingCreates.remove(src);
-    NameNode.stateChangeLog.debug(
-                                  ""DIR* NameSystem.completeFile: "" + src
-                                  + "" is removed from pendingCreates"");
+    NameNode.stateChangeLog.debug(""DIR* NameSystem.completeFile: "" + src
+                                  + "" blocklist persisted"");
 
     synchronized (leases) {
       Lease lease = getLease(holder);
@@ -1051,6 +1032,8 @@
     // Now that the file is real, we need to be sure to replicate
     // the blocks.
     int numExpectedReplicas = pendingFile.getReplication();
+    Block[] pendingBlocks = pendingFile.getBlocks();
+    int nrBlocks = pendingBlocks.length;
     for (int i = 0; i < nrBlocks; i++) {
       // filter out containingNodes that are marked for decommission.
       NumberReplicas number = countNodes(pendingBlocks[i]);
@@ -1069,15 +1052,14 @@
   /**
    * Allocate a block at the given pending filename
    */
-  private Block allocateBlock(String src) throws IOException {
+  private Block allocateBlock(String src, INode file) throws IOException {
     Block b = null;
     do {
       b = new Block(FSNamesystem.randBlockId.nextLong(), 0);
     } while (isValidBlock(b));
-    pendingCreates.addBlock(src, b);
+    b = dir.addBlock(src, file, b);
     NameNode.stateChangeLog.info(""BLOCK* NameSystem.allocateBlock: ""
-                                 +src+ "". ""+b.getBlockName()+
-                                 "" is created and added to pendingCreates and pendingCreateBlocks"");      
+                                 +src+ "". ""+b.getBlockName());
     return b;
   }
 
@@ -1086,13 +1068,13 @@
    * replicated.  If not, return false. If checkall is true, then check
    * all blocks, otherwise check only penultimate block.
    */
-  synchronized boolean checkFileProgress(FileUnderConstruction v, boolean checkall) {
+  synchronized boolean checkFileProgress(INodeFile v, boolean checkall) {
     if (checkall) {
       //
       // check all blocks of the file.
       //
-      for (Iterator<Block> it = v.getBlocks().iterator(); it.hasNext();) {
-        if (blocksMap.numNodes(it.next()) < this.minReplication) {
+      for (Block block: v.getBlocks()) {
+        if (blocksMap.numNodes(block) < this.minReplication) {
           return false;
         }
       }
@@ -1490,21 +1472,36 @@
   }
 
   /**
-   * Release a pending file creation lock.
+   * Move a file that is being written to be immutable.
    * @param src The filename
    * @param holder The datanode that was creating the file
    */
   private void internalReleaseCreate(String src, String holder) throws IOException {
-    boolean status =  pendingCreates.remove(src);
-    if (status) {
-      NameNode.stateChangeLog.debug(""DIR* NameSystem.internalReleaseCreate: "" + src
-                                    + "" is removed from pendingCreates for ""
-                                    + holder + "" (failure)"");
-    } else {
-      NameNode.stateChangeLog.warn(""DIR* NameSystem.internalReleaseCreate: ""
-                                   + ""attempt to release a create lock on ""+ src
-                                   + "" that was not in pedingCreates"");
+    INodeFileUnderConstruction pendingFile = (INodeFileUnderConstruction) dir.getFileINode(src);
+
+    // The last block that was allocated migth not have been used by the
+    // client. In this case, the size of the last block would be 0. A fsck
+    // will report this block as a missing block because no datanodes have it.
+    // Delete this block.
+    Block[] blocks = pendingFile.getBlocks();
+    if (blocks != null && blocks.length > 1) {
+      Block last = blocks[blocks.length - 1];
+      if (last.getNumBytes() == 0) {
+          pendingFile.removeBlock(last);
+      }
     }
+
+    // The file is no longer pending.
+    // Create permanent INode, update blockmap
+    INodeFile newFile = pendingFile.convertToInodeFile();
+    dir.replaceNode(src, pendingFile, newFile);
+
+    // persist block allocations for this file
+    dir.persistBlocks(src, newFile);
+  
+    NameNode.stateChangeLog.debug(""DIR* NameSystem.internalReleaseCreate: "" + 
+                                  src + "" is no longer written to by "" + 
+                                  holder);
   }
 
   /**
@@ -2161,7 +2158,18 @@
     Block storedBlock = blocksMap.getStoredBlock(block); //extra look up!
     if (storedBlock != null && block != storedBlock) {
       if (block.getNumBytes() > 0) {
-        storedBlock.setNumBytes(block.getNumBytes());
+        long cursize = storedBlock.getNumBytes();
+        if (cursize == 0) {
+          storedBlock.setNumBytes(block.getNumBytes());
+        } else if (cursize != block.getNumBytes()) {
+          LOG.warn(""Inconsistent size for block "" + block + 
+                   "" reported from "" + node.getName() + 
+                   "" current size is "" + cursize +
+                   "" reported size is "" + block.getNumBytes());
+          // Accept this block even if there is a problem with its
+          // size. Clients should detect data corruption because of
+          // CRC mismatch.
+        }
       }
       block = storedBlock;
     }
@@ -2185,8 +2193,13 @@
                                    + block.getBlockName() + "" on "" + node.getName());
     }
 
-    if (fileINode == null)  // block does not belong to any file
+    //
+    // if file is being actively written to, then do not check 
+    // replication-factor here. It will be checked when the file is closed.
+    //
+    if (fileINode == null || fileINode instanceof INodeFileUnderConstruction) {
       return block;
+    }
         
     // filter out containingNodes that are marked for decommission.
     NumberReplicas num = countNodes(block);
@@ -3460,8 +3473,7 @@
    * Returns whether the given block is one pointed-to by a file.
    */
   private boolean isValidBlock(Block b) {
-    return (blocksMap.getINode(b) != null ||
-            pendingCreates.contains(b));
+    return (blocksMap.getINode(b) != null);
   }
 
   // Distributed upgrade manager
"
hadoop,2051a4f1875454e98965993ee3a399331791bc32,"HADOOP-89.  A client can access file data even before the creator
has closed the file. Introduce a new dfs shell command ""tail"".
(Dhruba Borthakur via dhruba)



git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@577456 13f79535-47bb-0310-9956-ffa450edef68
",2007-09-19 22:12:49,Dhruba Borthakur,"diff --git a/src/java/org/apache/hadoop/dfs/INode.java b/src/java/org/apache/hadoop/dfs/INode.java
index efec8aa..a33077e 100644
--- a/src/java/org/apache/hadoop/dfs/INode.java
+++ b/src/java/org/apache/hadoop/dfs/INode.java
@@ -23,6 +23,7 @@
 import java.util.Collections;
 import java.util.Arrays;
 import java.util.List;
+import java.io.IOException;
 
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.dfs.BlocksMap.BlockInfo;
@@ -472,6 +473,14 @@
     allocateBlocks(nrBlocks);
   }
 
+  protected INodeFile(BlockInfo[] blklist, short replication, long modificationTime,
+                      long preferredBlockSize) {
+    super(modificationTime);
+    this.blockReplication = replication;
+    this.preferredBlockSize = preferredBlockSize;
+    blocks = blklist;
+  }
+
   boolean isDirectory() {
     return false;
   }
@@ -492,7 +501,7 @@
    * Get file blocks 
    * @return file blocks
    */
-  Block[] getBlocks() {
+  BlockInfo[] getBlocks() {
     return this.blocks;
   }
 
@@ -505,6 +514,45 @@
   }
 
   /**
+   * add a block to the block list
+   */
+  void addBlock(BlockInfo newblock) {
+    if (this.blocks == null) {
+      this.blocks = new BlockInfo[1];
+      this.blocks[0] = newblock;
+    } else {
+      int size = this.blocks.length;
+      BlockInfo[] newlist = new BlockInfo[size + 1];
+      for (int i = 0; i < size; i++) {
+        newlist[i] = this.blocks[i];
+      }
+      newlist[size] = newblock;
+      this.blocks = newlist;
+    }
+  }
+
+  /**
+   * remove a block from the block list. This block should be
+   * the last one on the list.
+   */
+  void removeBlock(Block oldblock) throws IOException {
+    if (this.blocks == null) {
+      throw new IOException(""Trying to delete non-existant block "" +
+                            oldblock);
+    }
+    int size = this.blocks.length;
+    if (!this.blocks[size-1].equals(oldblock)) {
+      throw new IOException(""Trying to delete non-existant block "" +
+                            oldblock);
+    }
+    BlockInfo[] newlist = new BlockInfo[size - 1];
+    for (int i = 0; i < size-1; i++) {
+        newlist[i] = this.blocks[i];
+    }
+    this.blocks = newlist;
+  }
+
+  /**
    * Set file block
    */
   void setBlock(int idx, BlockInfo blk) {
@@ -537,4 +585,57 @@
   long getPreferredBlockSize() {
     return preferredBlockSize;
   }
+
+  /**
+   * Return the penultimate allocated block for this file.
+   */
+  Block getPenultimateBlock() {
+    if (blocks == null || blocks.length <= 1) {
+      return null;
+    }
+    return blocks[blocks.length - 2];
+  }
+}
+
+class INodeFileUnderConstruction extends INodeFile {
+  protected StringBytesWritable clientName;         // lease holder
+  protected StringBytesWritable clientMachine;
+  protected DatanodeDescriptor clientNode; // if client is a cluster node too.
+
+  INodeFileUnderConstruction(short replication,
+                             long preferredBlockSize,
+                             long modTime,
+                             String clientName,
+                             String clientMachine,
+                             DatanodeDescriptor clientNode) 
+                             throws IOException {
+    super(0, replication, modTime, preferredBlockSize);
+    this.clientName = new StringBytesWritable(clientName);
+    this.clientMachine = new StringBytesWritable(clientMachine);
+    this.clientNode = clientNode;
+  }
+
+  String getClientName() throws IOException {
+    return clientName.getString();
+  }
+
+  String getClientMachine() throws IOException {
+    return clientMachine.getString();
+  }
+
+  DatanodeDescriptor getClientNode() {
+    return clientNode;
+  }
+
+  //
+  // converts a INodeFileUnderConstruction into a INodeFile
+  //
+  INodeFile convertToInodeFile() {
+    INodeFile obj = new INodeFile(getBlocks(),
+                                  getReplication(),
+                                  getModificationTime(),
+                                  getPreferredBlockSize());
+    return obj;
+    
+  }
 }
"
hadoop,2051a4f1875454e98965993ee3a399331791bc32,"HADOOP-89.  A client can access file data even before the creator
has closed the file. Introduce a new dfs shell command ""tail"".
(Dhruba Borthakur via dhruba)



git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@577456 13f79535-47bb-0310-9956-ffa450edef68
",2007-09-19 22:12:49,Dhruba Borthakur,"diff --git a/src/java/org/apache/hadoop/fs/FsShell.java b/src/java/org/apache/hadoop/fs/FsShell.java
index e99b8d0..76f50ee 100644
--- a/src/java/org/apache/hadoop/fs/FsShell.java
+++ b/src/java/org/apache/hadoop/fs/FsShell.java
@@ -44,6 +44,7 @@
     modifFmt.setTimeZone(TimeZone.getTimeZone(""UTC""));
   }
   static final String SETREP_SHORT_USAGE=""-setrep [-R] [-w] <rep> <path/file>"";
+  static final String TAIL_USAGE=""-tail [-f] <file>"";
   private static final DecimalFormat decimalFormat = 
     new DecimalFormat(""#*0.0#*"");
 
@@ -881,6 +882,54 @@
   }
 
   /**
+   * Parse the incoming command string
+   * @param cmd
+   * @param pos ignore anything before this pos in cmd
+   * @throws IOException 
+   */
+  private void tail(String[] cmd, int pos) throws IOException {
+    CommandFormat c = new CommandFormat(""tail"", 1, 1, ""f"");
+    String src = null;
+    Path path = null;
+    short rep = 0;
+
+    try {
+      List<String> parameters = c.parse(cmd, pos);
+      src = parameters.get(0);
+    } catch(IllegalArgumentException iae) {
+      System.err.println(""Usage: java FsShell "" + TAIL_USAGE);
+      throw iae;
+    }
+    boolean foption = c.options.get(""f"") ? true: false;
+    path = new Path(src);
+
+    if (fs.isDirectory(path)) {
+      throw new IOException(""Source must be a file."");
+    }
+
+    long fileSize = fs.getFileStatus(path).getLen();
+    long offset = (fileSize > 1024) ? fileSize - 1024: 0;
+
+    while (true) {
+      FSDataInputStream in = fs.open(path);
+      in.seek(offset);
+      IOUtils.copyBytes(in, System.out, 1024, false);
+      offset = in.getPos();
+      in.close();
+      if (!foption) {
+        break;
+      }
+      fileSize = fs.getFileStatus(path).getLen();
+      offset = (fileSize > offset) ? offset: fileSize;
+      try {
+        Thread.sleep(5000);
+      } catch (InterruptedException e) {
+        break;
+      }
+    }
+  }
+
+  /**
    * Return an abbreviated English-language desc of the byte length
    */
   public static String byteDesc(long len) {
@@ -926,6 +975,7 @@
       ""[-copyToLocal <src><localdst>] [-moveToLocal <src> <localdst>]\n\t"" +
       ""[-mkdir <path>] [-report] ["" + SETREP_SHORT_USAGE + ""]\n\t"" +
       ""[-touchz <path>] [-test -[ezd] <path>] [-stat [format] <path>]\n\t"" +
+      ""[-tail [-f] <path>]\n\t"" +
       ""[-help [cmd]]\n"";
 
     String conf =""-conf <configuration file>:  Specify an application configuration file."";
@@ -1025,6 +1075,10 @@
       ""\t\tin the specified format. Format accepts filesize in blocks (%b), filename (%n),\n"" +
       ""\t\tblock size (%o), replication (%r), modification date (%y, %Y)\n"";
 
+    String tail = TAIL_USAGE
+      + "":  Show the last 1KB of the file. \n""
+      + ""\t\tThe -f option shows apended data as the file grows. \n"";
+
     String help = ""-help [cmd]: \tDisplays help for given command or all commands if none\n"" +
       ""\t\tis specified.\n"";
 
@@ -1078,6 +1132,8 @@
       System.out.println(test);
     } else if (""stat"".equals(cmd)) {
       System.out.println(stat);
+    } else if (""tail"".equals(cmd)) {
+      System.out.println(tail);
     } else if (""help"".equals(cmd)) {
       System.out.println(help);
     } else {
@@ -1214,6 +1270,8 @@
     } else if (""-stat"".equals(cmd)) {
       System.err.println(""Usage: java FsShell"" +
                          "" [-stat [format] <path>]"");
+    } else if (""-tail"".equals(cmd)) {
+      System.err.println(""Usage: java FsShell ["" + TAIL_USAGE + ""]"");
     } else {
       System.err.println(""Usage: java FsShell"");
       System.err.println(""           [-ls <path>]"");
@@ -1238,6 +1296,7 @@
       System.err.println(""           [-touchz <path>]"");
       System.err.println(""           [-test -[ezd] <path>]"");
       System.err.println(""           [-stat [format] <path>]"");
+      System.err.println(""           ["" + TAIL_USAGE + ""]"");
       System.err.println(""           [-help [cmd]]"");
       System.err.println();
       ToolRunner.printGenericCommandUsage(System.err);
@@ -1370,6 +1429,8 @@
         } else {
           printHelp("""");
         }
+      } else if (""-tail"".equals(cmd)) {
+        tail(argv, i);           
       } else {
         exitCode = -1;
         System.err.println(cmd.substring(1) + "": Unknown command"");
"
hadoop,73e3fa496ee807f5c723e9881bfe7f5583bc38bb,"HADOOP-120.  In ArrayWritable, prevent creation with null value class and improve documentation.  Contributed by Cameron Pope.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@577010 13f79535-47bb-0310-9956-ffa450edef68
",2007-09-18 18:25:56,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/ArrayWritable.java b/src/java/org/apache/hadoop/io/ArrayWritable.java
index 07632b9..630600f 100644
--- a/src/java/org/apache/hadoop/io/ArrayWritable.java
+++ b/src/java/org/apache/hadoop/io/ArrayWritable.java
@@ -21,16 +21,29 @@
 import java.io.*;
 import java.lang.reflect.Array;
 
-/** A Writable for arrays containing instances of a class. */
+/** 
+ * A Writable for arrays containing instances of a class. The elements of this
+ * writable must all be instances of the same class. If this writable will be
+ * the input for a Reducer, you will need to create a subclass that sets the
+ * value to be of the proper type.
+ *
+ * For example:
+ * <code>
+ * public class IntArrayWritable extends ArrayWritable {
+ *   public IntArrayWritable() { 
+ *     super(IntWritable.class); 
+ *   }	
+ * }
+ * </code>
+ */
 public class ArrayWritable implements Writable {
   private Class valueClass;
   private Writable[] values;
 
-  public ArrayWritable() {
-    this.valueClass = null;
-  }
-
   public ArrayWritable(Class valueClass) {
+    if (valueClass == null) { 
+      throw new IllegalArgumentException(""null valueClass""); 
+    }    
     this.valueClass = valueClass;
   }
 
@@ -46,13 +59,6 @@
     }
   }
 
-  public void setValueClass(Class valueClass) {
-    if (valueClass != this.valueClass) {
-      this.valueClass = valueClass;
-      this.values = null;
-    }
-  }
-  
   public Class getValueClass() {
     return valueClass;
   }
"
hadoop,3c22ef45df4e0b5118d9d8893198e429d43b85c0,"Merge -r 564740:564744 from trunk to 0.14 branch. Fixes HADOOP-71.


git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/branches/branch-0.14@564751 13f79535-47bb-0310-9956-ffa450edef68
",2007-08-10 20:25:50,Owen O'Malley,"diff --git a/src/java/org/apache/hadoop/mapred/LineRecordReader.java b/src/java/org/apache/hadoop/mapred/LineRecordReader.java
index e85a460..951a9a6 100644
--- a/src/java/org/apache/hadoop/mapred/LineRecordReader.java
+++ b/src/java/org/apache/hadoop/mapred/LineRecordReader.java
@@ -69,7 +69,7 @@
     final CompressionCodec codec = compressionCodecs.getCodec(file);
 
     // open the file and seek to the start of the split
-    FileSystem fs = FileSystem.get(job);
+    FileSystem fs = file.getFileSystem(job);
     FSDataInputStream fileIn = fs.open(split.getPath());
     InputStream in = fileIn;
     boolean skipFirstLine = false;
"
hadoop,3c22ef45df4e0b5118d9d8893198e429d43b85c0,"Merge -r 564740:564744 from trunk to 0.14 branch. Fixes HADOOP-71.


git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/branches/branch-0.14@564751 13f79535-47bb-0310-9956-ffa450edef68
",2007-08-10 20:25:50,Owen O'Malley,"diff --git a/src/java/org/apache/hadoop/mapred/MapTask.java b/src/java/org/apache/hadoop/mapred/MapTask.java
index f7e6950..fd513dc 100644
--- a/src/java/org/apache/hadoop/mapred/MapTask.java
+++ b/src/java/org/apache/hadoop/mapred/MapTask.java
@@ -210,14 +210,11 @@
 
     private Reporter reporter = null;
 
-    private JobConf job;
-
     public DirectMapOutputCollector(TaskUmbilicalProtocol umbilical,
         JobConf job, Reporter reporter) throws IOException {
-      this.job = job;
       this.reporter = reporter;
       String finalName = getOutputName(getPartition());
-      FileSystem fs = FileSystem.get(this.job);
+      FileSystem fs = FileSystem.get(job);
 
       out = job.getOutputFormat().getRecordWriter(fs, job, finalName, reporter);
     }
"
hadoop,3c22ef45df4e0b5118d9d8893198e429d43b85c0,"Merge -r 564740:564744 from trunk to 0.14 branch. Fixes HADOOP-71.


git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/branches/branch-0.14@564751 13f79535-47bb-0310-9956-ffa450edef68
",2007-08-10 20:25:50,Owen O'Malley,"diff --git a/src/java/org/apache/hadoop/mapred/SequenceFileRecordReader.java b/src/java/org/apache/hadoop/mapred/SequenceFileRecordReader.java
index d4f8c7a..1df55be 100644
--- a/src/java/org/apache/hadoop/mapred/SequenceFileRecordReader.java
+++ b/src/java/org/apache/hadoop/mapred/SequenceFileRecordReader.java
@@ -20,10 +20,11 @@
 
 import java.io.IOException;
 
-import org.apache.hadoop.fs.FileSystem;
 
-import org.apache.hadoop.io.*;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.*;
 import org.apache.hadoop.util.ReflectionUtils;
 
 /** An {@link RecordReader} for {@link SequenceFile}s. */
@@ -36,8 +37,9 @@
 
   public SequenceFileRecordReader(Configuration conf, FileSplit split)
     throws IOException {
-    FileSystem fs = FileSystem.get(conf);
-    this.in = new SequenceFile.Reader(fs, split.getPath(), conf);
+    Path path = split.getPath();
+    FileSystem fs = path.getFileSystem(conf);
+    this.in = new SequenceFile.Reader(fs, path, conf);
     this.end = split.getStart() + split.getLength();
     this.conf = conf;
 
"
hadoop,3c22ef45df4e0b5118d9d8893198e429d43b85c0,"Merge -r 564740:564744 from trunk to 0.14 branch. Fixes HADOOP-71.


git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/branches/branch-0.14@564751 13f79535-47bb-0310-9956-ffa450edef68
",2007-08-10 20:25:50,Owen O'Malley,"diff --git a/src/java/org/apache/hadoop/mapred/Task.java b/src/java/org/apache/hadoop/mapred/Task.java
index 5ffaeed..3381277 100644
--- a/src/java/org/apache/hadoop/mapred/Task.java
+++ b/src/java/org/apache/hadoop/mapred/Task.java
@@ -180,7 +180,8 @@
   private Path getTaskOutputPath(JobConf conf) {
     Path p = new Path(conf.getOutputPath(), (""_"" + taskId));
     try {
-      return p.makeQualified(FileSystem.get(conf));
+      FileSystem fs = p.getFileSystem(conf);
+      return p.makeQualified(fs);
     } catch (IOException ie) {
       LOG.warn(StringUtils.stringifyException(ie));
       return p;
@@ -390,21 +391,23 @@
    * @throws IOException
    */
   void saveTaskOutput() throws IOException {
-    FileSystem fs = FileSystem.get(conf);
 
-    if (taskOutputPath != null && fs.exists(taskOutputPath)) {
-      Path jobOutputPath = taskOutputPath.getParent();
+    if (taskOutputPath != null) {
+      FileSystem fs = taskOutputPath.getFileSystem(conf);
+      if (fs.exists(taskOutputPath)) {
+        Path jobOutputPath = taskOutputPath.getParent();
 
-      // Move the task outputs to their final place
-      moveTaskOutputs(fs, jobOutputPath, taskOutputPath);
+        // Move the task outputs to their final place
+        moveTaskOutputs(fs, jobOutputPath, taskOutputPath);
 
-      // Delete the temporary task-specific output directory
-      if (!fs.delete(taskOutputPath)) {
-        LOG.info(""Failed to delete the temporary output directory of task: "" + 
-                getTaskId() + "" - "" + taskOutputPath);
+        // Delete the temporary task-specific output directory
+        if (!fs.delete(taskOutputPath)) {
+          LOG.info(""Failed to delete the temporary output directory of task: "" + 
+                  getTaskId() + "" - "" + taskOutputPath);
+        }
+        
+        LOG.info(""Saved output of task '"" + getTaskId() + ""' to "" + jobOutputPath);
       }
-      
-      LOG.info(""Saved output of task '"" + getTaskId() + ""' to "" + jobOutputPath);
     }
   }
   
@@ -439,13 +442,14 @@
    * @throws IOException
    */
   void discardTaskOutput() throws IOException {
-    FileSystem fs = FileSystem.get(conf);
-
-    if (taskOutputPath != null && fs.exists(taskOutputPath)) {
-      // Delete the temporary task-specific output directory
-      FileUtil.fullyDelete(fs, taskOutputPath);
-      LOG.info(""Discarded output of task '"" + getTaskId() + ""' - "" 
-              + taskOutputPath);
+    if (taskOutputPath != null) {
+      FileSystem fs = taskOutputPath.getFileSystem(conf);
+      if (fs.exists(taskOutputPath)) {
+        // Delete the temporary task-specific output directory
+        FileUtil.fullyDelete(fs, taskOutputPath);
+        LOG.info(""Discarded output of task '"" + getTaskId() + ""' - "" 
+                + taskOutputPath);
+      }
     }
   }
 
"
hadoop,b38a0a5ddd8b0a4ad6dafa362e79fdce771dce42,"HADOOP-71.  Allow Text and SequenceFile Map/Reduce inputs from non-default
filesystems.


git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@564741 13f79535-47bb-0310-9956-ffa450edef68
",2007-08-10 20:09:47,Owen O'Malley,"diff --git a/src/java/org/apache/hadoop/mapred/LineRecordReader.java b/src/java/org/apache/hadoop/mapred/LineRecordReader.java
index e85a460..951a9a6 100644
--- a/src/java/org/apache/hadoop/mapred/LineRecordReader.java
+++ b/src/java/org/apache/hadoop/mapred/LineRecordReader.java
@@ -69,7 +69,7 @@
     final CompressionCodec codec = compressionCodecs.getCodec(file);
 
     // open the file and seek to the start of the split
-    FileSystem fs = FileSystem.get(job);
+    FileSystem fs = file.getFileSystem(job);
     FSDataInputStream fileIn = fs.open(split.getPath());
     InputStream in = fileIn;
     boolean skipFirstLine = false;
"
hadoop,b38a0a5ddd8b0a4ad6dafa362e79fdce771dce42,"HADOOP-71.  Allow Text and SequenceFile Map/Reduce inputs from non-default
filesystems.


git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@564741 13f79535-47bb-0310-9956-ffa450edef68
",2007-08-10 20:09:47,Owen O'Malley,"diff --git a/src/java/org/apache/hadoop/mapred/MapTask.java b/src/java/org/apache/hadoop/mapred/MapTask.java
index f7e6950..fd513dc 100644
--- a/src/java/org/apache/hadoop/mapred/MapTask.java
+++ b/src/java/org/apache/hadoop/mapred/MapTask.java
@@ -210,14 +210,11 @@
 
     private Reporter reporter = null;
 
-    private JobConf job;
-
     public DirectMapOutputCollector(TaskUmbilicalProtocol umbilical,
         JobConf job, Reporter reporter) throws IOException {
-      this.job = job;
       this.reporter = reporter;
       String finalName = getOutputName(getPartition());
-      FileSystem fs = FileSystem.get(this.job);
+      FileSystem fs = FileSystem.get(job);
 
       out = job.getOutputFormat().getRecordWriter(fs, job, finalName, reporter);
     }
"
hadoop,b38a0a5ddd8b0a4ad6dafa362e79fdce771dce42,"HADOOP-71.  Allow Text and SequenceFile Map/Reduce inputs from non-default
filesystems.


git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@564741 13f79535-47bb-0310-9956-ffa450edef68
",2007-08-10 20:09:47,Owen O'Malley,"diff --git a/src/java/org/apache/hadoop/mapred/SequenceFileRecordReader.java b/src/java/org/apache/hadoop/mapred/SequenceFileRecordReader.java
index d4f8c7a..1df55be 100644
--- a/src/java/org/apache/hadoop/mapred/SequenceFileRecordReader.java
+++ b/src/java/org/apache/hadoop/mapred/SequenceFileRecordReader.java
@@ -20,10 +20,11 @@
 
 import java.io.IOException;
 
-import org.apache.hadoop.fs.FileSystem;
 
-import org.apache.hadoop.io.*;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.*;
 import org.apache.hadoop.util.ReflectionUtils;
 
 /** An {@link RecordReader} for {@link SequenceFile}s. */
@@ -36,8 +37,9 @@
 
   public SequenceFileRecordReader(Configuration conf, FileSplit split)
     throws IOException {
-    FileSystem fs = FileSystem.get(conf);
-    this.in = new SequenceFile.Reader(fs, split.getPath(), conf);
+    Path path = split.getPath();
+    FileSystem fs = path.getFileSystem(conf);
+    this.in = new SequenceFile.Reader(fs, path, conf);
     this.end = split.getStart() + split.getLength();
     this.conf = conf;
 
"
hadoop,b38a0a5ddd8b0a4ad6dafa362e79fdce771dce42,"HADOOP-71.  Allow Text and SequenceFile Map/Reduce inputs from non-default
filesystems.


git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@564741 13f79535-47bb-0310-9956-ffa450edef68
",2007-08-10 20:09:47,Owen O'Malley,"diff --git a/src/java/org/apache/hadoop/mapred/Task.java b/src/java/org/apache/hadoop/mapred/Task.java
index 9cd894b..679f5df 100644
--- a/src/java/org/apache/hadoop/mapred/Task.java
+++ b/src/java/org/apache/hadoop/mapred/Task.java
@@ -180,7 +180,8 @@
   private Path getTaskOutputPath(JobConf conf) {
     Path p = new Path(conf.getOutputPath(), (""_"" + taskId));
     try {
-      return p.makeQualified(FileSystem.get(conf));
+      FileSystem fs = p.getFileSystem(conf);
+      return p.makeQualified(fs);
     } catch (IOException ie) {
       LOG.warn(StringUtils.stringifyException(ie));
       return p;
@@ -398,21 +399,23 @@
    * @throws IOException
    */
   void saveTaskOutput() throws IOException {
-    FileSystem fs = FileSystem.get(conf);
 
-    if (taskOutputPath != null && fs.exists(taskOutputPath)) {
-      Path jobOutputPath = taskOutputPath.getParent();
+    if (taskOutputPath != null) {
+      FileSystem fs = taskOutputPath.getFileSystem(conf);
+      if (fs.exists(taskOutputPath)) {
+        Path jobOutputPath = taskOutputPath.getParent();
 
-      // Move the task outputs to their final place
-      moveTaskOutputs(fs, jobOutputPath, taskOutputPath);
+        // Move the task outputs to their final place
+        moveTaskOutputs(fs, jobOutputPath, taskOutputPath);
 
-      // Delete the temporary task-specific output directory
-      if (!fs.delete(taskOutputPath)) {
-        LOG.info(""Failed to delete the temporary output directory of task: "" + 
-                getTaskId() + "" - "" + taskOutputPath);
+        // Delete the temporary task-specific output directory
+        if (!fs.delete(taskOutputPath)) {
+          LOG.info(""Failed to delete the temporary output directory of task: "" + 
+                  getTaskId() + "" - "" + taskOutputPath);
+        }
+        
+        LOG.info(""Saved output of task '"" + getTaskId() + ""' to "" + jobOutputPath);
       }
-      
-      LOG.info(""Saved output of task '"" + getTaskId() + ""' to "" + jobOutputPath);
     }
   }
   
@@ -447,13 +450,14 @@
    * @throws IOException
    */
   void discardTaskOutput() throws IOException {
-    FileSystem fs = FileSystem.get(conf);
-
-    if (taskOutputPath != null && fs.exists(taskOutputPath)) {
-      // Delete the temporary task-specific output directory
-      FileUtil.fullyDelete(fs, taskOutputPath);
-      LOG.info(""Discarded output of task '"" + getTaskId() + ""' - "" 
-              + taskOutputPath);
+    if (taskOutputPath != null) {
+      FileSystem fs = taskOutputPath.getFileSystem(conf);
+      if (fs.exists(taskOutputPath)) {
+        // Delete the temporary task-specific output directory
+        FileUtil.fullyDelete(fs, taskOutputPath);
+        LOG.info(""Discarded output of task '"" + getTaskId() + ""' - "" 
+                + taskOutputPath);
+      }
     }
   }
 
"
hadoop,c9b2c1e9610aaae4985378d51f00c9d080affa2a,"HADOOP-672.  Improve help for fs shell commands.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@524269 13f79535-47bb-0310-9956-ffa450edef68
",2007-03-30 21:00:52,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSAdmin.java b/src/java/org/apache/hadoop/dfs/DFSAdmin.java
index f2d4cbe..f19b22e 100644
--- a/src/java/org/apache/hadoop/dfs/DFSAdmin.java
+++ b/src/java/org/apache/hadoop/dfs/DFSAdmin.java
@@ -87,7 +87,7 @@
      */
     public void setSafeMode(String[] argv, int idx) throws IOException {
       if (!(fs instanceof DistributedFileSystem)) {
-        System.out.println(""FileSystem is "" + fs.getName());
+        System.err.println(""FileSystem is "" + fs.getName());
         return;
       }
       if (idx != argv.length - 1) {
@@ -141,7 +141,7 @@
       int exitCode = -1;
 
       if (!(fs instanceof DistributedFileSystem)) {
-        System.out.println(""FileSystem is "" + fs.getName());
+        System.err.println(""FileSystem is "" + fs.getName());
         return exitCode;
       }
 
@@ -152,6 +152,50 @@
       return exitCode;
     }
 
+    private void printHelp(String cmd) {
+        String summary = ""hadoop dfsadmin is the command to execute dfs administrative commands.\n"" +
+            ""The full syntax is: \n\n"" +
+            ""hadoop dfsadmin [-report] [-safemode <enter | leave | get | wait>]\n"" +
+            ""\t[-refreshNodes] [-help [cmd]]\n"";
+
+        String report =""-report: \tReports basic filesystem information and statistics.\n"";
+        
+        String safemode = ""-safemode <enter|leave|get|wait>:  Safemode maintenance command.\n"" + 
+            ""\t\tSafe mode is a name node state when it\n"" +
+            ""\t\t\t1.  does not accept changes to name space (read-only)\n"" +
+            ""\t\t\t2.  does not replicate or delete blocks.\n"" +
+            ""\t\tSafe mode is entered automatically at name node startup, and\n"" +
+            ""\t\tleaves safe mode automatically when the configured minimum\n"" +
+            ""\t\tpercentage of blocks satisfies the minimal replication\n"" +
+            ""\t\tcondition.  Safe mode can also be entered manually, but then\n"" +
+            ""\t\tcan only be turned off manually as well.\n"";
+
+        String refreshNodes = ""-refreshNodes: \tReread the hosts and exclude files to update the set\n"" +
+            ""\t\tof datanodes that are allowed to connect to the namenode\n"" +
+            ""\t\tand those that should be decommissioned/recommissioned.\n"";
+
+        String help = ""-help [cmd]: \tDisplays help for given command or all commands if none\n"" +
+            ""\t\tis specified.\n"";
+
+        if (""report"".equals(cmd)) {
+            System.out.println(report);
+        } else if (""safemode"".equals(cmd)) {
+            System.out.println(safemode);
+        } else if (""refreshNodes"".equals(cmd)) {
+            System.out.println(refreshNodes);
+        } else if (""help"".equals(cmd)) {
+            System.out.println(help);
+        } else {
+            System.out.println(summary);
+            System.out.println(report);
+            System.out.println(safemode);
+            System.out.println(refreshNodes);
+            System.out.println(help);
+        }
+
+    }
+
+
     /**
      * Displays format of commands.
      * @param cmd The command that is being executed.
@@ -171,6 +215,7 @@
             System.err.println(""           [-report]"");
             System.err.println(""           [-safemode enter | leave | get | wait]"");
             System.err.println(""           [-refreshNodes]"");
+            System.err.println(""           [-help [cmd]]"");
           }
     }
 
@@ -231,6 +276,12 @@
                 setSafeMode(argv, i);
             } else if (""-refreshNodes"".equals(cmd)) {
                 exitCode = refreshNodes();
+            } else if (""-help"".equals(cmd)) {
+                if (i < argv.length) {
+                    printHelp(argv[i]);
+                } else {
+                    printHelp("""");
+                }
             } else {
                 exitCode = -1;
                 System.err.println(cmd.substring(1) + "": Unknown command"");
"
hadoop,c9b2c1e9610aaae4985378d51f00c9d080affa2a,"HADOOP-672.  Improve help for fs shell commands.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@524269 13f79535-47bb-0310-9956-ffa450edef68
",2007-03-30 21:00:52,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FsShell.java b/src/java/org/apache/hadoop/fs/FsShell.java
index 10039c6..2171b8b 100644
--- a/src/java/org/apache/hadoop/fs/FsShell.java
+++ b/src/java/org/apache/hadoop/fs/FsShell.java
@@ -635,6 +635,181 @@
         return strVal;
     }
 
+    private void printHelp(String cmd) {
+        String summary = ""hadoop fs is the command to execute fs commands. "" +
+            ""The full syntax is: \n\n"" +
+            ""hadoop fs [-fs <local | file system URI>] [-conf <configuration file>]\n\t"" +
+            ""[-D <property=value>] [-ls <path>] [-lsr <path>] [-du <path>]\n\t"" + 
+            ""[-dus <path>] [-mv <src> <dst>] [-cp <src> <dst>] [-rm <src>]\n\t"" + 
+            ""[-rmr <src>] [-put <localsrc> <dst>] [-copyFromLocal <localsrc> <dst>]\n\t"" +
+            ""[-moveFromLocal <localsrc> <dst>] [-get <src> <localdst>]\n\t"" +
+            ""[-getmerge <src> <localdst> [addnl]] [-cat <src>]\n\t"" +
+            ""[-copyToLocal <src><localdst>] [-moveToLocal <src> <localdst>]\n\t"" +
+            ""[-mkdir <path>] [-report] [-setrep [-R] <rep> <path/file>]\n"" +
+            ""[-help [cmd]]\n""; 
+
+        String conf =""-conf <configuration file>:  Specify an application configuration file."";
+ 
+        String D = ""-D <property=value>:  Use value for given property."";
+  
+        String fs = ""-fs [local | <file system URI>]: \tSpecify the file system to use.\n"" + 
+            ""\t\tIf not specified, the current configuration is used, \n"" +
+            ""\t\ttaken from the following, in increasing precedence: \n"" + 
+            ""\t\t\thadoop-default.xml inside the hadoop jar file \n"" +
+            ""\t\t\thadoop-default.xml in $HADOOP_CONF_DIR \n"" +
+            ""\t\t\thadoop-site.xml in $HADOOP_CONF_DIR \n"" +
+            ""\t\t'local' means use the local file system as your DFS. \n"" +
+            ""\t\t<file system URI> specifies a particular file system to \n"" +
+            ""\t\tcontact. This argument is optional but if used must appear\n"" +
+            ""\t\tappear first on the command line.  Exactly one additional\n"" +
+            ""\t\targument must be specified. \n"";
+
+        
+        String ls = ""-ls <path>: \tList the contents that match the specified file pattern. If\n"" + 
+            ""\t\tpath is not specified, the contents of /user/<currentUser>\n"" +
+            ""\t\twill be listed. Directory entries are of the form \n"" +
+            ""\t\t\tdirName (full path) <dir> \n"" +
+            ""\t\tand file entries are of the form \n"" + 
+            ""\t\t\tfileName(full path) <r n> size \n"" +
+            ""\t\twhere n is the number of replicas specified for the file \n"" + 
+            ""\t\tand size is the size of the file, in bytes.\n"";
+
+        String lsr = ""-lsr <path>: \tRecursively list the contents that match the specified\n"" +
+            ""\t\tfile pattern.  Behaves very similarly to hadoop fs -ls,\n"" + 
+            ""\t\texcept that the data is shown for all the entries in the\n"" +
+            ""\t\tsubtree.\n"";
+
+        String du = ""-du <path>: \tShow the amount of space, in bytes, used by the files that \n"" +
+            ""\t\tmatch the specified file pattern.  Equivalent to the unix\n"" + 
+            ""\t\tcommand \""du -sb <path>/*\"" in case of a directory, \n"" +
+            ""\t\tand to \""du -b <path>\"" in case of a file.\n"" +
+            ""\t\tThe output is in the form \n"" + 
+            ""\t\t\tname(full path) size (in bytes)\n""; 
+
+        String dus = ""-dus <path>: \tShow the amount of space, in bytes, used by the files that \n"" +
+            ""\t\tmatch the specified file pattern.  Equivalent to the unix\n"" + 
+            ""\t\tcommand \""du -sb\""  The output is in the form \n"" + 
+            ""\t\t\tname(full path) size (in bytes)\n""; 
+    
+        String mv = ""-mv <src> <dst>:   Move files that match the specified file pattern <src>\n"" +
+            ""\t\tto a destination <dst>.  When moving multiple files, the \n"" +
+            ""\t\tdestination must be a directory. \n"";
+
+        String cp = ""-cp <src> <dst>:   Copy files that match the file pattern <src> to a \n"" +
+            ""\t\tdestination.  When copying multiple files, the destination\n"" +
+            ""\t\tmust be a directory. \n"";
+
+        String rm = ""-rm <src>: \tDelete all files that match the specified file pattern.\n"" +
+            ""\t\tEquivlent to the Unix command \""rm <src>\""\n"";
+
+        String rmr = ""-rmr <src>: \tRemove all directories which match the specified file \n"" +
+            ""\t\tpattern. Equivlent to the Unix command \""rm -rf <src>\""\n"";
+
+        String put = ""-put <localsrc> <dst>: \tCopy a single file from the local file system \n"" +
+            ""\t\tinto fs. \n"";
+
+        String copyFromLocal = ""-copyFromLocal <localsrc> <dst>:  Identical to the -put command.\n"";
+
+        String moveFromLocal = ""-moveFromLocal <localsrc> <dst>:  Same as -put, except that the source is\n"" +
+            ""\t\tdeleted after it's copied.\n""; 
+
+        String get = ""-get <src> <localdst>:  Copy files that match the file pattern <src> \n"" +
+            ""\t\tto the local name.  <src> is kept.  When copying mutiple, \n"" +
+            ""\t\tfiles, the destination must be a directory. \n"";
+
+        String getmerge = ""-getmerge <src> <localdst>:  Get all the files in the directories that \n"" +
+            ""\t\tmatch the source file pattern and merge and sort them to only\n"" +
+            ""\t\tone file on local fs. <src> is kept.\n"";
+
+        String cat = ""-cat <src>: \tFetch all files that match the file pattern <src> \n"" +
+            ""\t\tand display their content on stdout.\n"";
+        
+        String copyToLocal = ""-copyToLocal <src> <localdst>:  Identical to the -get command.\n"";
+
+        String moveToLocal = ""-moveToLocal <src> <localdst>:  Not implemented yet \n"";
+        
+        String mkdir = ""-mkdir <path>: \tCreate a directory in specified location. \n"";
+
+        String setrep = ""-setrep [-R] <rep> <path/file>:  Set the replication level of a file. \n"" +
+            ""\t\tThe -R flag requests a recursive change of replication level \n"" + 
+            ""\t\tfor an entire tree.\n""; 
+        
+        String help = ""-help [cmd]: \tDisplays help for given command or all commands if none\n"" +
+            ""\t\tis specified.\n"";
+
+        if (""fs"".equals(cmd)) {
+            System.out.println(fs);
+        } else if (""conf"".equals(cmd)) {
+            System.out.println(conf);
+        } else if (""D"".equals(cmd)) {
+            System.out.println(D);
+        } else if (""ls"".equals(cmd)) {
+            System.out.println(ls);
+        } else if (""lsr"".equals(cmd)) {
+            System.out.println(lsr);
+        } else if (""du"".equals(cmd)) {
+            System.out.println(du);
+        } else if (""dus"".equals(cmd)) {
+            System.out.println(dus);
+        } else if (""rm"".equals(cmd)) {
+            System.out.println(rm);
+        } else if (""rmr"".equals(cmd)) {
+            System.out.println(rmr);
+        } else if (""mkdir"".equals(cmd)) {
+            System.out.println(mkdir);
+        } else if (""mv"".equals(cmd)) {
+            System.out.println(mv);
+        } else if (""cp"".equals(cmd)) {
+            System.out.println(cp);
+        } else if (""put"".equals(cmd)) {
+            System.out.println(put);
+        } else if (""copyFromLocal"".equals(cmd)) {
+            System.out.println(copyFromLocal);
+        } else if (""moveFromLocal"".equals(cmd)) {
+            System.out.println(moveFromLocal);
+        } else if (""get"".equals(cmd)) {
+            System.out.println(get);
+        } else if (""getmerge"".equals(cmd)) {
+            System.out.println(getmerge);
+        } else if (""copyToLocal"".equals(cmd)) {
+            System.out.println(copyToLocal);
+        } else if (""moveToLocal"".equals(cmd)) {
+            System.out.println(moveToLocal);
+        } else if (""cat"".equals(cmd)) {
+            System.out.println(cat);
+        } else if (""get"".equals(cmd)) {
+            System.out.println(get);
+        } else if (""setrep"".equals(cmd)) {
+            System.out.println(setrep);
+        } else if (""help"".equals(cmd)) {
+            System.out.println(help);
+        } else {
+            System.out.println(summary);
+            System.out.println(fs);
+            System.out.println(ls);
+            System.out.println(lsr);
+            System.out.println(du);
+            System.out.println(dus);
+            System.out.println(mv);
+            System.out.println(cp);
+            System.out.println(rm);
+            System.out.println(rmr);
+            System.out.println(put);
+            System.out.println(copyFromLocal);
+            System.out.println(moveFromLocal);
+            System.out.println(get);
+            System.out.println(getmerge);
+            System.out.println(cat);
+            System.out.println(copyToLocal);
+            System.out.println(moveToLocal);
+            System.out.println(mkdir);
+            System.out.println(setrep);
+            System.out.println(help);
+        }        
+
+                           
+    }
+
     /**
      * Apply operation specified by 'cmd' on all parameters
      * starting from argv[startindex].
@@ -702,7 +877,7 @@
     public void printUsage(String cmd) {
           if (""-fs"".equals(cmd)) {
             System.err.println(""Usage: java FsShell"" + 
-                "" [-fs <local | namenode:port>]"");
+                "" [-fs <local | file system URI>]"");
           } else if (""-conf"".equals(cmd)) {
             System.err.println(""Usage: java FsShell"" + 
                 "" [-conf <configuration file>]"");
@@ -727,7 +902,7 @@
             System.err.println(""Usage: java FsShell"" + 
                 "" ["" + cmd + "" [-crc] <src> <localdst>]"");
           } else if (""-cat"".equals(cmd)) {
-            System.out.println(""Usage: java FsShell"" + 
+            System.err.println(""Usage: java FsShell"" + 
                 "" ["" + cmd + "" <src>]"");
           } else if (""-get"".equals(cmd)) {
             System.err.println(""Usage: java FsShell"" + 
@@ -737,7 +912,7 @@
                 "" [-setrep [-R] <rep> <path/file>]"");
           } else {
             System.err.println(""Usage: java FsShell"");
-            System.err.println(""           [-fs <local | namenode:port>]"");
+            System.err.println(""           [-fs <local | file system URI>]"");
             System.err.println(""           [-conf <configuration file>]"");
             System.err.println(""           [-D <[property=value>]"");
             System.err.println(""           [-ls <path>]"" );
@@ -759,6 +934,7 @@
             System.err.println(""           [-moveToLocal [-crc] <src> <localdst>]"");
             System.err.println(""           [-mkdir <path>]"");
             System.err.println(""           [-setrep [-R] <rep> <path/file>]"");
+            System.err.println(""           [-help [cmd]]"");
           }
     }
 
@@ -871,6 +1047,12 @@
               }         
             } else if (""-mkdir"".equals(cmd)) {
                 exitCode = doall(cmd, argv, conf, i);
+            } else if (""-help"".equals(cmd)) {
+                if (i < argv.length) {
+                    printHelp(argv[i]);
+                } else {
+                    printHelp("""");
+                }
             } else {
                 exitCode = -1;
                 System.err.println(cmd.substring(1) + "": Unknown command"");
"
hadoop,01b8b057bf6324207ba9fb55b8f33ae5dde14eb6,"HADOOP-654.  Stop assigning tasks to a tasktracker if it has failed more than a specified number in the job.  Contributed by Arun.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@510630 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-22 19:23:35,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobConf.java b/src/java/org/apache/hadoop/mapred/JobConf.java
index bd8aefb..3b756b7 100644
--- a/src/java/org/apache/hadoop/mapred/JobConf.java
+++ b/src/java/org/apache/hadoop/mapred/JobConf.java
@@ -532,6 +532,24 @@
     set(""mapred.job.name"", name);
   }
   
+  /**
+   * Set the maximum no. of failures of a given job per tasktracker.
+   * 
+   * @param noFailures maximum no. of failures of a given job per tasktracker.
+   */
+  public void setMaxTaskFailuresPerTracker(int noFailures) {
+    setInt(""mapred.max.tracker.failures"", noFailures);
+  }
+  
+  /**
+   * Get the maximum no. of failures of a given job per tasktracker.
+   * 
+   * @return the maximum no. of failures of a given job per tasktracker.
+   */
+  public int getMaxTaskFailuresPerTracker() {
+    return getInt(""mapred.max.tracker.failures"", 4); 
+  }
+  
   /** Find a jar that contains a class of the same name, if any.
    * It will return a jar file, even if that is not the first thing
    * on the class path that has a class with the same name.
"
hadoop,01b8b057bf6324207ba9fb55b8f33ae5dde14eb6,"HADOOP-654.  Stop assigning tasks to a tasktracker if it has failed more than a specified number in the job.  Contributed by Arun.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@510630 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-22 19:23:35,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobInProgress.java b/src/java/org/apache/hadoop/mapred/JobInProgress.java
index 8024e6e..6c4654b 100644
--- a/src/java/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/JobInProgress.java
@@ -38,7 +38,7 @@
 ///////////////////////////////////////////////////////
 class JobInProgress {
     private static final Log LOG = LogFactory.getLog(""org.apache.hadoop.mapred.JobInProgress"");
-
+    
     JobProfile profile;
     JobStatus status;
     Path localJobFile = null;
@@ -57,8 +57,15 @@
     JobTracker jobtracker = null;
     Map<String,List<TaskInProgress>> hostToMaps = new HashMap();
     private int taskCompletionEventTracker = 0 ; 
-    List<TaskCompletionEvent> taskCompletionEvents ; 
-
+    List<TaskCompletionEvent> taskCompletionEvents ;
+    
+    // The no. of tasktrackers where >= conf.getMaxTaskFailuresPerTracker()
+    // tasks have failed
+    private volatile int flakyTaskTrackers = 0;
+    // Map of trackerHostName -> no. of task failures
+    private Map<String, Integer> trackerToFailuresMap = 
+      new TreeMap<String, Integer>();
+    
     long startTime;
     long finishTime;
 
@@ -102,6 +109,7 @@
         this.numReduceTasks = conf.getNumReduceTasks();
         this.taskCompletionEvents = new ArrayList(
             numMapTasks + numReduceTasks + 10);
+        
         JobHistory.JobInfo.logSubmitted(jobid, conf.getJobName(), conf.getUser(), 
             System.currentTimeMillis(), jobFile); 
         
@@ -373,6 +381,61 @@
         return result;
     }
     
+    private String convertTrackerNameToHostName(String trackerName) {
+      // Ugly!
+      // Convert the trackerName to it's host name
+      int indexOfColon = trackerName.indexOf("":"");
+      String trackerHostName = (indexOfColon == -1) ? 
+                                trackerName : 
+                                trackerName.substring(0, indexOfColon);
+      return trackerHostName;
+    }
+    
+    private void addTrackerTaskFailure(String trackerName) {
+      String trackerHostName = convertTrackerNameToHostName(trackerName);
+      
+      Integer trackerFailures = trackerToFailuresMap.get(trackerHostName);
+      if (trackerFailures == null) {
+        trackerFailures = new Integer(0);
+      }
+      trackerToFailuresMap.put(trackerHostName, ++trackerFailures);
+      
+      // Check if this tasktracker has turned 'flaky'
+      if (trackerFailures.intValue() == conf.getMaxTaskFailuresPerTracker()) {
+        ++flakyTaskTrackers;
+        LOG.info(""TaskTracker at '"" + trackerHostName + ""' turned 'flaky'"");
+      }
+    }
+    
+    private int getTrackerTaskFailures(String trackerName) {
+      String trackerHostName = convertTrackerNameToHostName(trackerName);
+      Integer failedTasks = trackerToFailuresMap.get(trackerHostName);
+      return (failedTasks != null) ? failedTasks.intValue() : 0; 
+    }
+    
+    /**
+     * Get the no. of 'flaky' tasktrackers for a given job.
+     * 
+     * @return the no. of 'flaky' tasktrackers for a given job.
+     */
+    int getNoOfBlackListedTrackers() {
+      return flakyTaskTrackers;
+    }
+    
+    /**
+     * Get the information on tasktrackers and no. of errors which occurred
+     * on them for a given job. 
+     * 
+     * @return the map of tasktrackers and no. of errors which occurred
+     *         on them for a given job. 
+     */
+    synchronized Map<String, Integer> getTaskTrackerErrors() {
+      // Clone the 'trackerToFailuresMap' and return the copy
+      Map<String, Integer> trackerErrors = 
+        new TreeMap<String, Integer>(trackerToFailuresMap);
+      return trackerErrors;
+    }
+    
     /**
      * Find a new task to run.
      * @param tts The task tracker that is asking for a task
@@ -389,6 +452,25 @@
                             TaskInProgress[] tasks,
                             List cachedTasks) {
         String taskTracker = tts.getTrackerName();
+
+        //
+        // Check if too many tasks of this job have failed on this
+        // tasktracker prior to assigning it a new one.
+        //
+        int taskTrackerFailedTasks = getTrackerTaskFailures(taskTracker);
+        if (taskTrackerFailedTasks >= conf.getMaxTaskFailuresPerTracker()) {
+          String flakyTracker = convertTrackerNameToHostName(taskTracker); 
+          if (flakyTaskTrackers < clusterSize) {
+            LOG.debug(""Ignoring the black-listed tasktracker: '"" + flakyTracker 
+                    + ""' for assigning a new task"");
+            return -1;
+          } else {
+            LOG.warn(""Trying to assign a new task for black-listed tracker "" + 
+                    flakyTracker + "" since all task-trackers in the cluster are "" +
+                    ""'flaky' !"");
+          }
+        }
+        
         //
         // See if there is a split over a block that is stored on
         // the TaskTracker checking in.  That means the block
@@ -648,6 +730,11 @@
         }
             
         //
+        // Note down that a task has failed on this tasktracker
+        //
+        addTrackerTaskFailure(trackerName);
+        
+        //
         // Let the JobTracker know that this task has failed
         //
         jobtracker.markCompletedTaskAttempt(status.getTaskTracker(), taskid);
"
hadoop,1b0662b3b1b4db78a1996d5312b11fb07030a52a,"HADOOP-564.  Replace uses of dfs:// with hdfs://.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@510275 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-21 22:35:53,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/filecache/DistributedCache.java b/src/java/org/apache/hadoop/filecache/DistributedCache.java
index 4ec24eb..9185506 100644
--- a/src/java/org/apache/hadoop/filecache/DistributedCache.java
+++ b/src/java/org/apache/hadoop/filecache/DistributedCache.java
@@ -44,7 +44,7 @@
   /**
    * 
    * @param cache the cache to be localized, this should be specified as 
-   * new URI(dfs://hostname:port/absoulte_path_to_file#LINKNAME). If no schema 
+   * new URI(hdfs://hostname:port/absoulte_path_to_file#LINKNAME). If no schema 
    * or hostname:port is provided the file is assumed to be in the filesystem
    * being used in the Configuration
    * @param conf The Confguration file which contains the filesystem
@@ -137,7 +137,7 @@
   /*
    * Returns the relative path of the dir this cache will be localized in
    * relative path that this cache will be localized in. For
-   * dfs://hostname:port/absolute_path -- the relative path is
+   * hdfs://hostname:port/absolute_path -- the relative path is
    * hostname/absolute path -- if it is just /absolute_path -- then the
    * relative path is hostname of DFS this mapred cluster is running
    * on/absolute_path
@@ -147,7 +147,7 @@
     String fsname = cache.getScheme();
     String path;
     FileSystem dfs = FileSystem.get(conf);
-    if (""dfs"".equals(fsname)) {
+    if (""hdfs"".equals(fsname)) {
       path = cache.getHost() + cache.getPath();
     } else {
       String[] split = dfs.getName().split("":"");
@@ -348,7 +348,7 @@
   
   private static String getFileSysName(URI url) {
     String fsname = url.getScheme();
-    if (""dfs"".equals(fsname)) {
+    if (""hdfs"".equals(fsname)) {
       String host = url.getHost();
       int port = url.getPort();
       return (port == (-1)) ? host : (host + "":"" + port);
"
hadoop,1b0662b3b1b4db78a1996d5312b11fb07030a52a,"HADOOP-564.  Replace uses of dfs:// with hdfs://.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@510275 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-21 22:35:53,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/tools/Logalyzer.java b/src/java/org/apache/hadoop/tools/Logalyzer.java
index 30f29d1..ee0e996 100644
--- a/src/java/org/apache/hadoop/tools/Logalyzer.java
+++ b/src/java/org/apache/hadoop/tools/Logalyzer.java
@@ -177,7 +177,7 @@
   doArchive(String logListURI, String archiveDirectory)
   throws IOException
   {
-    String destURL = new String(""dfs://"" + fsConfig.get(""fs.default.name"", ""local"") + 
+    String destURL = new String(""hdfs://"" + fsConfig.get(""fs.default.name"", ""local"") + 
         archiveDirectory);
     CopyFiles.copy(fsConfig, logListURI, destURL, true, false);
   }
"
hadoop,1b0662b3b1b4db78a1996d5312b11fb07030a52a,"HADOOP-564.  Replace uses of dfs:// with hdfs://.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@510275 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-21 22:35:53,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/util/CopyFiles.java b/src/java/org/apache/hadoop/util/CopyFiles.java
index 7e370e2..c3e78a2 100644
--- a/src/java/org/apache/hadoop/util/CopyFiles.java
+++ b/src/java/org/apache/hadoop/util/CopyFiles.java
@@ -671,9 +671,8 @@
     ArrayList<String> protocolURIs = new ArrayList<String>(uris.length);
     
     for(int i=0; i < uris.length; ++i) {
-      // uri must start w/ protocol or if protocol is dfs, allow hdfs as alias.
-      if(uris[i].startsWith(protocol) || 
-          (protocol.equalsIgnoreCase(""dfs"") && uris[i].startsWith(""hdfs""))) {
+      // uri must start w/ protocol 
+      if(uris[i].startsWith(protocol)) {
         protocolURIs.add(uris[i]);
       }
     }
@@ -720,8 +719,8 @@
       //Source paths
       srcPaths = fetchSrcURIs(conf, srcURI);  
       
-      // Protocol - 'dfs://'
-      String[] dfsUrls = parseInputFile(""dfs"", srcPaths);
+      // Protocol - 'hdfs://'
+      String[] dfsUrls = parseInputFile(HDFS, srcPaths);
       if(dfsUrls != null) {
         for(int i=0; i < dfsUrls.length; ++i) {
           copy(conf, dfsUrls[i], destPath, false, ignoreReadFailures);
"
hadoop,1b0662b3b1b4db78a1996d5312b11fb07030a52a,"HADOOP-564.  Replace uses of dfs:// with hdfs://.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@510275 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-21 22:35:53,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/mapred/MRCaching.java b/src/test/org/apache/hadoop/mapred/MRCaching.java
index 85cc2e1..9b34a22 100644
--- a/src/test/org/apache/hadoop/mapred/MRCaching.java
+++ b/src/test/org/apache/hadoop/mapred/MRCaching.java
@@ -181,9 +181,9 @@
       archive2 = ""file://"" + cachePath + ""/test.zip"";
       file1 = ""file://"" + cachePath + ""/test.txt"";
     } else {
-      archive1 = ""dfs://"" + fileSys + cachePath + ""/test.jar"";
-      archive2 = ""dfs://"" + fileSys + cachePath + ""/test.zip"";
-      file1 = ""dfs://"" + fileSys + cachePath + ""/test.txt"";
+      archive1 = ""hdfs://"" + fileSys + cachePath + ""/test.jar"";
+      archive2 = ""hdfs://"" + fileSys + cachePath + ""/test.zip"";
+      file1 = ""hdfs://"" + fileSys + cachePath + ""/test.txt"";
     }
     URI uri1 = null;
     URI uri2 = null;
"
hadoop,3a7c2b0bd9c9c6d1321f560ab91fa9e744093542,"HADOOP-442.  Permit one to specify hosts allowed to connect to namenode and jobtracker with include and exclude files.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@510181 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-21 20:11:00,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/ClientProtocol.java b/src/java/org/apache/hadoop/dfs/ClientProtocol.java
index bd549c6..90bb053 100644
--- a/src/java/org/apache/hadoop/dfs/ClientProtocol.java
+++ b/src/java/org/apache/hadoop/dfs/ClientProtocol.java
@@ -29,7 +29,7 @@
  **********************************************************************/
 interface ClientProtocol extends VersionedProtocol {
 
-  public static final long versionID = 7L;  // periodic checkpoint added
+    public static final long versionID = 8L; // refreshNodes added
   
     ///////////////////////////////////////
     // File contents
@@ -313,7 +313,13 @@
      */
     public boolean setSafeMode( FSConstants.SafeModeAction action ) throws IOException;
 
-    public boolean decommission( FSConstants.DecommissionAction action, String[] nodenames) throws IOException;
+    /**
+     * Tells the namenode to reread the hosts and exclude files. 
+     * @return True if the call was successful, false otherwise.
+     * @throws IOException
+     */
+    public void refreshNodes() throws IOException;
+
 
     /**
      * Get the size of the current edit log (in bytes).
"
hadoop,3a7c2b0bd9c9c6d1321f560ab91fa9e744093542,"HADOOP-442.  Permit one to specify hosts allowed to connect to namenode and jobtracker with include and exclude files.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@510181 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-21 20:11:00,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSAdmin.java b/src/java/org/apache/hadoop/dfs/DFSAdmin.java
index 5d2f379..f2d4cbe 100644
--- a/src/java/org/apache/hadoop/dfs/DFSAdmin.java
+++ b/src/java/org/apache/hadoop/dfs/DFSAdmin.java
@@ -132,61 +132,23 @@
     }
 
     /**
-     * Command related to decommission of a datanode.
-     * Usage: java DFSAdmin -decommission [enter | leave | get]
-     * @param argv List of command line parameters. Each of these items
-              could be a hostname or a hostname:portname.
-     * @param idx The index of the command that is being processed.
-     * @exception IOException if the filesystem does not exist.
-     * @return 0 on success, non zero on error.
+     * Command to ask the namenode to reread the hosts and excluded hosts 
+     * file.
+     * Usage: java DFSAdmin -refreshNodes
+     * @exception IOException 
      */
-    public int decommission(String[] argv, int idx) throws IOException {
+    public int refreshNodes() throws IOException {
       int exitCode = -1;
 
       if (!(fs instanceof DistributedFileSystem)) {
         System.out.println(""FileSystem is "" + fs.getName());
         return exitCode;
       }
-      if (idx >= argv.length - 1) {
-        printUsage(""-decommission"");
-        return exitCode;
-      }
-      
-      //
-      // Copy all the datanode names to nodes[]
-      //
-      String[] nodes = new String[argv.length - idx - 1];
-      for (int i = idx + 1, j = 0; i < argv.length; i++, j++) {
-        nodes[j] = argv[i];
-      }
 
-      FSConstants.DecommissionAction action;
-
-      if (""set"".equalsIgnoreCase(argv[idx])) {
-        action = FSConstants.DecommissionAction.DECOMMISSION_SET;
-      } else if (""clear"".equalsIgnoreCase(argv[idx])) {
-        action = FSConstants.DecommissionAction.DECOMMISSION_CLEAR;
-      } else if (""get"".equalsIgnoreCase(argv[idx])) {
-        action = FSConstants.DecommissionAction.DECOMMISSION_GET;
-      } else {
-        printUsage(""-decommission"");
-        return exitCode;
-      }
       DistributedFileSystem dfs = (DistributedFileSystem) fs;
-      boolean mode = dfs.decommission(action, nodes);
-
-      if (action == FSConstants.DecommissionAction.DECOMMISSION_GET) {
-        if (mode) {
-          System.out.println(""Node(s) has finished decommission"");
-        }
-        else {
-          System.out.println(""Node(s) have not yet been decommissioned"");
-        }
-        return 0;
-      }
-      if (mode) {
-        return 0; // success
-      }
+      dfs.refreshNodes();
+      exitCode = 0;
+   
       return exitCode;
     }
 
@@ -197,19 +159,18 @@
     public void printUsage(String cmd) {
           if (""-report"".equals(cmd)) {
             System.err.println(""Usage: java DFSAdmin""
-                + "" [report]"");
+                + "" [-report]"");
           } else if (""-safemode"".equals(cmd)) {
             System.err.println(""Usage: java DFSAdmin""
                 + "" [-safemode enter | leave | get | wait]"");
-          } else if (""-decommission"".equals(cmd)) {
+          } else if (""-refreshNodes"".equals(cmd)) {
             System.err.println(""Usage: java DFSAdmin""
-                + "" [-decommission set | clear | get ""
-                + ""[datanode1[, datanode2..]]"");
+                + "" [-refreshNodes]"");
           } else {
             System.err.println(""Usage: java DFSAdmin"");
             System.err.println(""           [-report]"");
             System.err.println(""           [-safemode enter | leave | get | wait]"");
-            System.err.println(""           [-decommission set | clear | get]"");
+            System.err.println(""           [-refreshNodes]"");
           }
     }
 
@@ -242,13 +203,14 @@
                   printUsage(cmd);
                   return exitCode;
                 }
-        } else if (""-decommission"".equals(cmd)) {
-                if (argv.length < 2) {
+        } else if (""-refreshNodes"".equals(cmd)) {
+                if (argv.length != 1) {
                   printUsage(cmd);
                   return exitCode;
                 }
         }
 
+
         // initialize DFSAdmin
         try {
             init();
@@ -267,8 +229,8 @@
                 report();
             } else if (""-safemode"".equals(cmd)) {
                 setSafeMode(argv, i);
-            } else if (""-decommission"".equals(cmd)) {
-                exitCode = decommission(argv, i);
+            } else if (""-refreshNodes"".equals(cmd)) {
+                exitCode = refreshNodes();
             } else {
                 exitCode = -1;
                 System.err.println(cmd.substring(1) + "": Unknown command"");
"
hadoop,3a7c2b0bd9c9c6d1321f560ab91fa9e744093542,"HADOOP-442.  Permit one to specify hosts allowed to connect to namenode and jobtracker with include and exclude files.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@510181 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-21 20:11:00,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSClient.java b/src/java/org/apache/hadoop/dfs/DFSClient.java
index 8f83843..73e6a0a 100644
--- a/src/java/org/apache/hadoop/dfs/DFSClient.java
+++ b/src/java/org/apache/hadoop/dfs/DFSClient.java
@@ -370,17 +370,16 @@
     }
 
     /**
-     * Set, clear decommission state of datnode(s).
-     * See {@link ClientProtocol#decommission(FSConstants.DecommissionAction)} 
+     * Refresh the hosts and exclude files.  (Rereads them.)
+     * See {@link ClientProtocol#refreshNodes()} 
      * for more details.
      * 
-     * @see ClientProtocol#decommission(FSConstants.DecommissionAction)
+     * @see ClientProtocol#refreshNodes()
      */
-    public boolean decommission(DecommissionAction action, String[] nodes)
-                                throws IOException {
-      return namenode.decommission(action, nodes);
+    public void refreshNodes() throws IOException {
+      namenode.refreshNodes();
     }
-
+    
     /**
      */
     public boolean mkdirs(UTF8 src) throws IOException {
"
hadoop,3a7c2b0bd9c9c6d1321f560ab91fa9e744093542,"HADOOP-442.  Permit one to specify hosts allowed to connect to namenode and jobtracker with include and exclude files.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@510181 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-21 20:11:00,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index 56280d9..0861cc5 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -499,7 +499,8 @@
           return;
         } catch( RemoteException re ) {
           String reClass = re.getClassName();
-          if( UnregisteredDatanodeException.class.getName().equals( reClass )) {
+          if( UnregisteredDatanodeException.class.getName().equals( reClass ) ||
+              DisallowedDatanodeException.class.getName().equals( reClass )) {
             LOG.warn( ""DataNode is shutting down: "" + 
                       StringUtils.stringifyException(re));
             shutdown();
"
hadoop,3a7c2b0bd9c9c6d1321f560ab91fa9e744093542,"HADOOP-442.  Permit one to specify hosts allowed to connect to namenode and jobtracker with include and exclude files.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@510181 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-21 20:11:00,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DatanodeInfo.java b/src/java/org/apache/hadoop/dfs/DatanodeInfo.java
index a86651d..3ea1ce3 100644
--- a/src/java/org/apache/hadoop/dfs/DatanodeInfo.java
+++ b/src/java/org/apache/hadoop/dfs/DatanodeInfo.java
@@ -190,7 +190,6 @@
    * Sets the admin state to indicate that decommision is complete.
    */
    void setDecommissioned() {
-     assert isDecommissionInProgress();
      adminState = AdminStates.DECOMMISSIONED;
    }
 
"
hadoop,3a7c2b0bd9c9c6d1321f560ab91fa9e744093542,"HADOOP-442.  Permit one to specify hosts allowed to connect to namenode and jobtracker with include and exclude files.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@510181 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-21 20:11:00,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java b/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
index 613aa75..60569d5 100644
--- a/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
+++ b/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
@@ -343,12 +343,11 @@
       return dfs.setSafeMode( action );
     }
 
-    /**
-     * Set, clear decommission of a set of datanodes.
+    /*
+     * Refreshes the list of hosts and excluded hosts from the configured 
+     * files.  
      */
-    public boolean decommission(FSConstants.DecommissionAction action,
-                                String[] nodes)
-    throws IOException {
-      return dfs.decommission(action, nodes);
+    public void refreshNodes() throws IOException {
+      dfs.refreshNodes();
     }
 }
"
hadoop,3a7c2b0bd9c9c6d1321f560ab91fa9e744093542,"HADOOP-442.  Permit one to specify hosts allowed to connect to namenode and jobtracker with include and exclude files.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@510181 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-21 20:11:00,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSConstants.java b/src/java/org/apache/hadoop/dfs/FSConstants.java
index a6aa86a..edf07754 100644
--- a/src/java/org/apache/hadoop/dfs/FSConstants.java
+++ b/src/java/org/apache/hadoop/dfs/FSConstants.java
@@ -122,9 +122,6 @@
     // SafeMode actions
     public enum SafeModeAction{ SAFEMODE_LEAVE, SAFEMODE_ENTER, SAFEMODE_GET; }
 
-    // decommission administrative actions
-    public enum DecommissionAction{ DECOMMISSION_SET, DECOMMISSION_CLEAR, DECOMMISSION_GET; }
-
     // Version is reflected in the dfs image and edit log files.
     // Version is reflected in the data storage file.
     // Versions are negative.
"
hadoop,3a7c2b0bd9c9c6d1321f560ab91fa9e744093542,"HADOOP-442.  Permit one to specify hosts allowed to connect to namenode and jobtracker with include and exclude files.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@510181 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-21 20:11:00,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index 25a2685..c3d4cec 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -204,6 +204,9 @@
     // for block replicas placement
     Replicator replicator = new Replicator();
 
+    private HostsFileReader hostsReader; 
+    private Daemon dnthread = null;
+
     /**
      * dirs is a list oif directories where the filesystem directory state 
      * is stored
@@ -252,6 +255,11 @@
         replthread.start();
         this.systemStart = now();
         this.startTime = new Date(systemStart); 
+        
+        this.hostsReader = new HostsFileReader(conf.get(""dfs.hosts"",""""),
+                                               conf.get(""dfs.hosts.exclude"",""""));
+        this.dnthread = new Daemon(new DecommissionedMonitor());
+        dnthread.start();
 
         this.infoPort = conf.getInt(""dfs.info.port"", 50070);
         this.infoBindAddress = conf.get(""dfs.info.bindAddress"", ""0.0.0.0"");
@@ -292,6 +300,7 @@
             infoServer.stop();
             hbthread.join(3000);
             replthread.join(3000);
+            dnthread.join(3000);
         } catch (InterruptedException ie) {
         } finally {
           // using finally to ensure we also wait for lease daemon
@@ -1575,6 +1584,11 @@
           (System.currentTimeMillis() - heartbeatExpireInterval));
     }
     
+    void setDatanodeDead(DatanodeID nodeID) throws IOException {
+        DatanodeDescriptor node = getDatanode(nodeID);
+        node.setLastUpdate(0);
+    }
+
     /**
      * The given node has reported in.  This method should:
      * 1) Record the heartbeat, so the datanode isn't timed out
@@ -1606,6 +1620,12 @@
               return true;
           }
           
+          // Check if this datanode should actually be shutdown instead. 
+          if (shouldNodeShutdown(nodeinfo)) {
+              setDatanodeDead(nodeinfo);
+              throw new DisallowedDatanodeException(nodeinfo);
+          }
+
           if( !nodeinfo.isAlive ) {
               return true;
           } else {
@@ -1916,6 +1936,12 @@
           +""from ""+nodeID.getName()+"" ""+newReport.length+"" blocks"" );
         DatanodeDescriptor node = getDatanode( nodeID );
 
+        // Check if this datanode should actually be shutdown instead.
+        if (shouldNodeShutdown(node)) {
+          setDatanodeDead(node);
+          throw new DisallowedDatanodeException(node);
+        }
+
         //
         // Modify the (block-->datanode) map, according to the difference
         // between the old and new block report.
@@ -2198,8 +2224,16 @@
                 ""Unexpected exception.  Got blockReceived message from node "" 
                 + block.getBlockName() + "", but there is no info for it"");
         }
+
         NameNode.stateChangeLog.debug(""BLOCK* NameSystem.blockReceived: ""
                 +block.getBlockName()+"" is received from "" + nodeID.getName() );
+
+        // Check if this datanode should actually be shutdown instead.
+        if (shouldNodeShutdown(node)) {
+            setDatanodeDead(node);
+            throw new DisallowedDatanodeException(node);
+        }
+
         //
         // Modify the blocks->datanode map and node's map.
         // 
@@ -2260,100 +2294,33 @@
       }
     }
 
+
     /**
-     * Start decommissioning the specified datanodes. If a datanode is
-     * already being decommissioned, then this is a no-op.
+     * Start decommissioning the specified datanode. 
      */
-    public synchronized void startDecommission (String[] nodes) 
-                             throws IOException {
-      if (isInSafeMode()) {
-        throw new SafeModeException(""Cannot decommission node "", safeMode);
-      }
-      boolean isError = false;
-      String badnodes = """";
+    private void startDecommission (DatanodeDescriptor node) 
+        throws IOException {
 
-      synchronized (datanodeMap) {
-        for (int i = 0; i < nodes.length; i++) {
-          boolean found = false;
-          for (Iterator<DatanodeDescriptor> it = datanodeMap.values().iterator();
-               it.hasNext(); ) {
-            DatanodeDescriptor node = it.next();
-
-            //
-            // If this is a node that we are interested in, set its admin state.
-            //
-            if (node.getName().equals(nodes[i]) || 
-                node.getHost().equals(nodes[i])) {
-              found = true;
-              if (!node.isDecommissionInProgress() && !node.isDecommissioned()) {
-                LOG.info(""Start Decommissioning node "" + node.name);
-                node.startDecommission();
-                //
-                // all those blocks that resides on this node has to be 
-                // replicated.
-                Block decommissionBlocks[] = node.getBlocks();
-                for (int j = 0; j < decommissionBlocks.length; j++) {
-                    neededReplications.update(decommissionBlocks[j], -1, 0);
-                }
-              }
-              break;
-            }
-          }
-          //
-          // Record the fact that a specified node was not found
-          //
-          if (!found) {
-            badnodes += nodes[i] + "" "";
-            isError = true;
-          }
+      if (!node.isDecommissionInProgress() && !node.isDecommissioned()) {
+        LOG.info(""Start Decommissioning node "" + node.name);
+        node.startDecommission();
+        //
+        // all the blocks that reside on this node have to be 
+        // replicated.
+        Block decommissionBlocks[] = node.getBlocks();
+        for (int j = 0; j < decommissionBlocks.length; j++) {
+            neededReplications.update(decommissionBlocks[j], -1, 0);
         }
       }
-      if (isError) {
-        throw new IOException(""Nodes "" + badnodes + "" not found"");
-      }
     }
 
     /**
      * Stop decommissioning the specified datanodes.
      */
-    public synchronized void stopDecommission (String[] nodes) 
+    public void stopDecommission (DatanodeDescriptor node) 
                              throws IOException {
-      if (isInSafeMode()) {
-        throw new SafeModeException(""Cannot decommission node "", safeMode);
-      }
-      boolean isError = false;
-      String badnodes = """";
-
-      synchronized (datanodeMap) {
-        for (int i = 0; i < nodes.length; i++) {
-          boolean found = false;
-          for (Iterator<DatanodeDescriptor> it = datanodeMap.values().iterator();
-               it.hasNext(); ) {
-            DatanodeDescriptor node = it.next();
-
-            //
-            // If this is a node that we are interested in, set its admin state.
-            //
-            if (node.getName().equals(nodes[i]) || 
-                node.getHost().equals(nodes[i])) {
-              LOG.info(""Stop Decommissioning node "" + node.name);
-              found = true;
-              node.stopDecommission();
-              break;
-            }
-          }
-          //
-          // Record the fact that a specified node was not found
-          //
-          if (!found) {
-            badnodes += nodes[i] + "" "";
-            isError = true;
-          }
-        }
-      }
-      if (isError) {
-        throw new IOException(""Nodes "" + badnodes + "" not found"");
-      }
+      LOG.info(""Stop Decommissioning node "" + node.name);
+      node.stopDecommission();
     }
 
     /**
@@ -2620,7 +2587,7 @@
                 && (excessBlocks == null || ! excessBlocks.contains(block))) {
               // filter out containingNodes that are marked for decommission.
               List<DatanodeDescriptor> nodes = 
-                filterDecommissionedNodes(containingNodes);
+                  filterDecommissionedNodes(containingNodes);
               int numCurrentReplica = nodes.size();
               DatanodeDescriptor targets[] = replicator.chooseTarget(
                   Math.min( fileINode.getReplication() - numCurrentReplica,
@@ -3158,6 +3125,124 @@
     } //end of Replicator
 
 
+    // Keeps track of which datanodes are allowed to connect to the namenode.
+        
+    private boolean inHostsList(DatanodeID node) {
+      Set<String> hostsList = hostsReader.getHosts();
+      return (hostsList.isEmpty() || 
+              hostsList.contains(node.getName()) || 
+              hostsList.contains(node.getHost()));
+    }
+
+
+    private boolean inExcludedHostsList(DatanodeID node) {
+      Set<String> excludeList = hostsReader.getExcludedHosts();
+      return (excludeList.contains(node.getName()) ||
+              excludeList.contains(node.getHost()));
+    }
+
+    /**
+     * Rereads the files to update the hosts and exclude lists.  It
+     * checks if any of the hosts have changed states:
+     * 1. Added to hosts  --> no further work needed here.
+     * 2. Removed from hosts --> mark AdminState as decommissioned. 
+     * 3. Added to exclude --> start decommission.
+     * 4. Removed from exclude --> stop decommission.
+     */
+    void refreshNodes() throws IOException {
+      hostsReader.refresh();
+      synchronized (this) {
+        for (Iterator<DatanodeDescriptor> it = datanodeMap.values().iterator();
+             it.hasNext(); ) {
+          DatanodeDescriptor node = it.next();
+          // Check if not include.
+          if (!inHostsList(node)) {
+            node.setDecommissioned();  // case 2.
+          } else {
+            if (inExcludedHostsList(node)) {
+              if (!node.isDecommissionInProgress() && 
+                  !node.isDecommissioned()) {
+                startDecommission(node);   // case 3.
+              }
+            } else {
+              if (node.isDecommissionInProgress() || 
+                  node.isDecommissioned()) {
+                stopDecommission(node);   // case 4.
+              } 
+            }
+          }
+        }
+      } 
+      
+    }
+    
+
+    /**
+     * Checks if the node is not on the hosts list.  If it is not, then
+     * it will be ignored.  If the node is in the hosts list, but is also 
+     * on the exclude list, then it will be decommissioned.
+     * Returns FALSE if node is rejected for registration. 
+     * Returns TRUE if node is registered (including when it is on the 
+     * exclude list and is being decommissioned). 
+     */
+    public synchronized boolean verifyNodeRegistration(DatanodeRegistration nodeReg) 
+      throws IOException {
+      if (!inHostsList(nodeReg)) {
+        return false;    
+      }
+      if (inExcludedHostsList(nodeReg)) {
+        DatanodeDescriptor node = getDatanode(nodeReg);
+        if (!checkDecommissionStateInternal(node)) {
+          startDecommission(node);
+        }
+      } 
+      return true;
+    }
+    
+    /**
+     * Checks if the Admin state bit is DECOMMISSIONED.  If so, then 
+     * we should shut it down. 
+     * 
+     * Returns true if the node should be shutdown.
+     */
+    private boolean shouldNodeShutdown(DatanodeDescriptor node) {
+      return (node.isDecommissioned());
+    }
+
+    /**
+     * Check if any of the nodes being decommissioned has finished 
+     * moving all its datablocks to another replica. This is a loose
+     * heuristic to determine when a decommission is really over.
+     */
+    public synchronized void decommissionedDatanodeCheck() {
+      for (Iterator<DatanodeDescriptor> it = datanodeMap.values().iterator();
+           it.hasNext(); ) {
+        DatanodeDescriptor node = it.next();  
+        checkDecommissionStateInternal(node);
+      }
+    }
+    
+    /**
+     * Periodically calls decommissionedDatanodeCheck().
+     */
+    class DecommissionedMonitor implements Runnable {
+        
+      public void run() {
+        while (fsRunning) {
+          try {
+            decommissionedDatanodeCheck();
+          } catch (Exception e) {
+            FSNamesystem.LOG.info(StringUtils.stringifyException(e));
+          }
+          try {
+            Thread.sleep(1000 * 60 * 5);
+          } catch (InterruptedException ie) {
+          }
+        }
+      }
+    }
+    
+
     /**
      * Information about the file while it is being written to.
      * Note that at that time the file is not visible to the outside.
"
hadoop,3a7c2b0bd9c9c6d1321f560ab91fa9e744093542,"HADOOP-442.  Permit one to specify hosts allowed to connect to namenode and jobtracker with include and exclude files.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@510181 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-21 20:11:00,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/NameNode.java b/src/java/org/apache/hadoop/dfs/NameNode.java
index 52c3e79..82b8823 100644
--- a/src/java/org/apache/hadoop/dfs/NameNode.java
+++ b/src/java/org/apache/hadoop/dfs/NameNode.java
@@ -515,24 +515,12 @@
       return namesystem.isInSafeMode();
     }
 
-    /**
-     * Set administrative commands to decommission datanodes.
+    /*
+     * Refresh the list of datanodes that the namenode should allow to  
+     * connect.  Uses the files list in the configuration to update the list. 
      */
-    public boolean decommission(DecommissionAction action, String[] nodes)
-                                throws IOException {
-      boolean ret = true;
-      switch (action) {
-        case DECOMMISSION_SET: // decommission datanode(s)
-          namesystem.startDecommission(nodes);
-          break;
-        case DECOMMISSION_CLEAR: // remove decommission state of a datanode
-          namesystem.stopDecommission(nodes);
-          break;
-        case DECOMMISSION_GET: // are all the node decommissioned?
-          ret = namesystem.checkDecommissioned(nodes);
-          break;
-        }
-        return ret;
+    public void refreshNodes() throws IOException {
+      namesystem.refreshNodes();
     }
 
     /**
@@ -564,8 +552,12 @@
     public DatanodeRegistration register( DatanodeRegistration nodeReg,
                                           String networkLocation
                                         ) throws IOException {
+      if (!namesystem.verifyNodeRegistration(nodeReg)) {
+        throw new DisallowedDatanodeException( nodeReg );
+      }
       verifyVersion( nodeReg.getVersion() );
       namesystem.registerDatanode( nodeReg, networkLocation );
+      
       return nodeReg;
     }
     
@@ -650,7 +642,8 @@
     /** 
      * Verify request.
      * 
-     * Verifies correctness of the datanode version and registration ID.
+     * Verifies correctness of the datanode version, registration ID, and 
+     * if the datanode does not need to be shutdown.
      * 
      * @param nodeReg data node registration
      * @throws IOException
"
hadoop,3a7c2b0bd9c9c6d1321f560ab91fa9e744093542,"HADOOP-442.  Permit one to specify hosts allowed to connect to namenode and jobtracker with include and exclude files.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@510181 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-21 20:11:00,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index f34b55a..2b8097b 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -23,7 +23,7 @@
 import org.apache.hadoop.fs.*;
 import org.apache.hadoop.ipc.*;
 import org.apache.hadoop.conf.*;
-import org.apache.hadoop.util.StringUtils;
+import org.apache.hadoop.util.*;
 
 import java.io.*;
 import java.net.*;
@@ -449,6 +449,7 @@
     Random r = new Random();
 
     private int maxCurrentTasks;
+    private HostsFileReader hostsReader;
 
     //
     // Properties to maintain while running Jobs and Tasks:
@@ -572,6 +573,10 @@
         // Same with 'localDir' except it's always on the local disk.
         jobConf.deleteLocalFiles(SUBDIR);
 
+        // Read the hosts/exclude files to restrict access to the jobtracker.
+        this.hostsReader = new HostsFileReader(conf.get(""mapred.hosts"", """"),
+                                               conf.get(""mapred.hosts.exclude"", """"));
+                                           
         // Set ports, start RPC servers, etc.
         InetSocketAddress addr = getAddress(conf);
         this.localMachine = addr.getHostName();
@@ -962,7 +967,12 @@
               "" (initialContact: "" + initialContact + 
               "" acceptNewTasks: "" + acceptNewTasks + "")"" +
               "" with responseId: "" + responseId);
-      
+
+        // Make sure heartbeat is from a tasktracker allowed by the jobtracker.
+        if (!acceptTaskTracker(status)) {
+          throw new DisallowedTaskTrackerException(status);
+        }
+
         // First check if the last heartbeat response got through 
         String trackerName = status.getTrackerName();
         HeartbeatResponse prevHeartbeatResponse =
@@ -1036,6 +1046,32 @@
     }
     
     /**
+     * Return if the specified tasktracker is in the hosts list, 
+     * if one was configured.  If none was configured, then this 
+     * returns true.
+     */
+    private boolean inHostsList(TaskTrackerStatus status) {
+      Set<String> hostsList = hostsReader.getHosts();
+      return (hostsList.isEmpty() || hostsList.contains(status.getHost()));
+    }
+
+    /**
+     * Return if the specified tasktracker is in the exclude list.
+     */
+    private boolean inExcludedHostsList(TaskTrackerStatus status) {
+      Set<String> excludeList = hostsReader.getExcludedHosts();
+      return excludeList.contains(status.getHost());
+    }
+
+    /**
+     * Returns true if the tasktracker is in the hosts list and 
+     * not in the exclude list. 
+     */
+    private boolean acceptTaskTracker(TaskTrackerStatus status) {
+      return (inHostsList(status) && !inExcludedHostsList(status));
+    }
+    
+    /**
      * Update the last recorded status for the given task tracker.
      * It assumes that the taskTrackers are locked on entry.
      * @author Owen O'Malley
"
hadoop,3a7c2b0bd9c9c6d1321f560ab91fa9e744093542,"HADOOP-442.  Permit one to specify hosts allowed to connect to namenode and jobtracker with include and exclude files.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@510181 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-21 20:11:00,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index c55cb6b..9bad00d 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -55,7 +55,7 @@
     static final long WAIT_FOR_DONE = 3 * 1000;
     private int httpPort;
 
-    static enum State {NORMAL, STALE, INTERRUPTED}
+    static enum State {NORMAL, STALE, INTERRUPTED, DENIED}
 
     public static final Log LOG =
     LogFactory.getLog(""org.apache.hadoop.mapred.TaskTracker"");
@@ -529,6 +529,12 @@
             jobClient.reportTaskTrackerError(taskTrackerName, 
                     ""DiskErrorException"", msg);
             return State.STALE;
+          } catch (RemoteException re) {
+            String reClass = re.getClassName();
+            if (DisallowedTaskTrackerException.class.getName().equals(reClass)) {
+              LOG.info(""Tasktracker disallowed by JobTracker."");
+              return State.DENIED;
+            }
           } catch (Exception except) {
             String msg = ""Caught exception: "" + 
                          StringUtils.stringifyException(except);
@@ -855,14 +861,18 @@
      */
     public void run() {
         try {
-            while (running && !shuttingDown) {
+            boolean denied = false;
+            while (running && !shuttingDown && !denied) {
                 boolean staleState = false;
                 try {
                     // This while-loop attempts reconnects if we get network errors
-                    while (running && ! staleState && !shuttingDown ) {
+                    while (running && ! staleState && !shuttingDown && !denied) {
                         try {
-                            if (offerService() == State.STALE) {
+                            State osState = offerService();
+                            if (osState == State.STALE) {
                                 staleState = true;
+                            } else if (osState == State.DENIED) {
+                                denied = true;
                             }
                         } catch (Exception ex) {
                             if (!shuttingDown) {
@@ -882,6 +892,9 @@
                 LOG.warn(""Reinitializing local state"");
                 initialize();
             }
+            if (denied) {
+                shutdown();
+            }
         } catch (IOException iex) {
             LOG.error(""Got fatal exception while reinitializing TaskTracker: "" +
                       StringUtils.stringifyException(iex));
"
hadoop,a1b569d4c6b391a6633a3776de9d1b463cc56aea,"HADOOP-803.  Reduce memory use by HDFS namenode, phase I.  Contributed by Raghu.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@508595 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-16 21:37:03,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/Block.java b/src/java/org/apache/hadoop/dfs/Block.java
index 0492099..9b4b00f 100644
--- a/src/java/org/apache/hadoop/dfs/Block.java
+++ b/src/java/org/apache/hadoop/dfs/Block.java
@@ -122,17 +122,16 @@
     /////////////////////////////////////
     public int compareTo(Object o) {
         Block b = (Block) o;
-        if (getBlockId() < b.getBlockId()) {
+        if ( blkid < b.blkid ) {
             return -1;
-        } else if (getBlockId() == b.getBlockId()) {
+        } else if ( blkid == b.blkid ) {
             return 0;
         } else {
             return 1;
         }
     }
     public boolean equals(Object o) {
-        Block b = (Block) o;
-        return (this.compareTo(b) == 0);
+        return (this.compareTo(o) == 0);
     }
     
     public int hashCode() {
"
hadoop,a1b569d4c6b391a6633a3776de9d1b463cc56aea,"HADOOP-803.  Reduce memory use by HDFS namenode, phase I.  Contributed by Raghu.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@508595 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-16 21:37:03,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DatanodeDescriptor.java b/src/java/org/apache/hadoop/dfs/DatanodeDescriptor.java
index 41187f2..6dde722 100644
--- a/src/java/org/apache/hadoop/dfs/DatanodeDescriptor.java
+++ b/src/java/org/apache/hadoop/dfs/DatanodeDescriptor.java
@@ -37,7 +37,7 @@
  **************************************************/
 public class DatanodeDescriptor extends DatanodeInfo {
 
-  private volatile Collection<Block> blocks = new TreeSet<Block>();
+  private volatile SortedMap<Block, Block> blocks = new TreeMap<Block, Block>();
   // isAlive == heartbeats.contains(this)
   // This is an optimization, because contains takes O(n) time on Arraylist
   protected boolean isAlive = false;
@@ -118,17 +118,12 @@
 
   /**
    */
-  void updateBlocks(Block newBlocks[]) {
-    blocks.clear();
-    for (int i = 0; i < newBlocks.length; i++) {
-      blocks.add(newBlocks[i]);
-    }
-  }
-
-  /**
-   */
   void addBlock(Block b) {
-    blocks.add(b);
+      blocks.put(b, b);
+  }
+  
+  void removeBlock(Block b) {
+      blocks.remove(b);
   }
 
   void resetBlocks() {
@@ -152,13 +147,21 @@
   }
   
   Block[] getBlocks() {
-    return (Block[]) blocks.toArray(new Block[blocks.size()]);
+    return (Block[]) blocks.keySet().toArray(new Block[blocks.size()]);
   }
 
   Iterator<Block> getBlockIterator() {
-    return blocks.iterator();
+    return blocks.keySet().iterator();
   }
-
+  
+  Block getBlock(long blockId) {
+    return blocks.get( new Block(blockId, 0) );
+  }
+  
+  Block getBlock(Block b) {
+    return blocks.get(b);
+  }
+  
   /*
    * Store block replication work.
    */
"
hadoop,a1b569d4c6b391a6633a3776de9d1b463cc56aea,"HADOOP-803.  Reduce memory use by HDFS namenode, phase I.  Contributed by Raghu.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@508595 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-16 21:37:03,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSDirectory.java b/src/java/org/apache/hadoop/dfs/FSDirectory.java
index 812caab..d67bf86 100644
--- a/src/java/org/apache/hadoop/dfs/FSDirectory.java
+++ b/src/java/org/apache/hadoop/dfs/FSDirectory.java
@@ -49,7 +49,7 @@
     class INode {
         private String name;
         private INode parent;
-        private TreeMap children = new TreeMap();
+        private TreeMap<String, INode> children = null;
         private Block blocks[];
         private short blockReplication;
 
@@ -111,11 +111,19 @@
         }
 
         /**
-         * Get children 
-         * @return TreeMap of children
+         * Get children iterator
+         * @return Iterator of children
          */
-        TreeMap getChildren() {
-          return this.children;
+        Iterator<INode> getChildIterator() {
+          return ( children != null ) ?  children.values().iterator() : null;
+            // instead of null, we could return a static empty iterator.
+        }
+        
+        void addChild(String name, INode node) {
+          if ( children == null ) {
+            children = new TreeMap<String, INode>();
+          }
+          children.put(name, node);
         }
 
         /**
@@ -162,7 +170,7 @@
         }
         
         INode getChild( String name) {
-          return (INode) children.get( name );
+          return (children == null) ? null : children.get( name );
         }
 
         /**
@@ -197,7 +205,7 @@
             return null;
           }
           // insert into the parent children list
-          parentNode.children.put(name, newNode);
+          parentNode.addChild(name, newNode);
           newNode.parent = parentNode;
           return newNode;
         }
@@ -225,9 +233,9 @@
                 }
             }
             incrDeletedFileCount();
-            for (Iterator it = children.values().iterator(); it.hasNext(); ) {
-                INode child = (INode) it.next();
-                child.collectSubtreeBlocks(v);
+            for (Iterator<INode> it = getChildIterator(); it != null &&
+                                                          it.hasNext(); ) {
+                it.next().collectSubtreeBlocks(v);
             }
         }
 
@@ -235,9 +243,9 @@
          */
         int numItemsInTree() {
             int total = 0;
-            for (Iterator it = children.values().iterator(); it.hasNext(); ) {
-                INode child = (INode) it.next();
-                total += child.numItemsInTree();
+            for (Iterator<INode> it = getChildIterator(); it != null && 
+                                                          it.hasNext(); ) {
+                total += it.next().numItemsInTree();
             }
             return total + 1;
         }
@@ -268,9 +276,9 @@
          */
         long computeContentsLength() {
             long total = computeFileLength();
-            for (Iterator it = children.values().iterator(); it.hasNext(); ) {
-                INode child = (INode) it.next();
-                total += child.computeContentsLength();
+            for (Iterator<INode> it = getChildIterator(); it != null && 
+                                                          it.hasNext(); ) {
+                total += it.next().computeContentsLength();
             }
             return total;
         }
@@ -294,9 +302,9 @@
                 v.add(this);
             }
 
-            for (Iterator it = children.values().iterator(); it.hasNext(); ) {
-                INode child = (INode) it.next();
-                v.add(child);
+            for (Iterator<INode> it = getChildIterator(); it != null && 
+                                                          it.hasNext(); ) {
+                v.add(it.next());
             }
         }
     }
"
hadoop,a1b569d4c6b391a6633a3776de9d1b463cc56aea,"HADOOP-803.  Reduce memory use by HDFS namenode, phase I.  Contributed by Raghu.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@508595 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-16 21:37:03,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSImage.java b/src/java/org/apache/hadoop/dfs/FSImage.java
index 94cc3a7..a2b00c7 100644
--- a/src/java/org/apache/hadoop/dfs/FSImage.java
+++ b/src/java/org/apache/hadoop/dfs/FSImage.java
@@ -405,9 +405,9 @@
           root.getBlocks()[i].write(out);
       }
     }
-    for(Iterator it = root.getChildren().values().iterator(); it.hasNext(); ) {
-      INode child = (INode) it.next();
-      saveImage( fullName, child, out );
+    for(Iterator<INode> it = root.getChildIterator(); it != null &&
+                                                      it.hasNext(); ) {
+      saveImage( fullName, it.next(), out );
     }
   }
 
"
hadoop,a1b569d4c6b391a6633a3776de9d1b463cc56aea,"HADOOP-803.  Reduce memory use by HDFS namenode, phase I.  Contributed by Raghu.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@508595 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-16 21:37:03,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index dc4105e..87523e7 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -61,8 +61,8 @@
     // to client-sent information.
     // Mapping: Block -> TreeSet<DatanodeDescriptor>
     //
-    Map<Block, SortedSet<DatanodeDescriptor>> blocksMap = 
-                              new HashMap<Block, SortedSet<DatanodeDescriptor>>();
+    Map<Block, List<DatanodeDescriptor>> blocksMap = 
+                              new HashMap<Block, List<DatanodeDescriptor>>();
 
     /**
      * Stores the datanode -> block map.  
@@ -182,6 +182,8 @@
     private int maxReplicationStreams;
     // MIN_REPLICATION is how many copies we need in place or else we disallow the write
     private int minReplication;
+    // Default replication
+    private int defaultReplication;
     // heartbeatRecheckInterval is how often namenode checks for expired datanodes
     private long heartbeatRecheckInterval;
     // heartbeatExpireInterval is how long namenode waits for datanode to report
@@ -211,6 +213,7 @@
                         int port,
                         NameNode nn, Configuration conf) throws IOException {
         fsNamesystemObject = this;
+        this.defaultReplication = conf.getInt(""dfs.replication"", 3);
         this.maxReplication = conf.getInt(""dfs.replication.max"", 512);
         this.minReplication = conf.getInt(""dfs.replication.min"", 1);
         if( minReplication <= 0 )
@@ -524,7 +527,7 @@
             DatanodeDescriptor machineSets[][] = new DatanodeDescriptor[blocks.length][];
 
             for (int i = 0; i < blocks.length; i++) {
-              SortedSet<DatanodeDescriptor> containingNodes = blocksMap.get(blocks[i]);
+                Collection<DatanodeDescriptor> containingNodes = blocksMap.get(blocks[i]);
                 if (containingNodes == null) {
                     machineSets[i] = new DatanodeDescriptor[0];
                 } else {
@@ -889,22 +892,16 @@
         //
         // We have the pending blocks, but they won't have
         // length info in them (as they were allocated before
-        // data-write took place).  So we need to add the correct
-        // length info to each
-        //
-        // REMIND - mjc - this is very inefficient!  We should
-        // improve this!
+        // data-write took place). Find the block stored in
+        // node descriptor.
         //
         for (int i = 0; i < nrBlocks; i++) {
             Block b = pendingBlocks[i];
-            SortedSet<DatanodeDescriptor> containingNodes = blocksMap.get(b);
-            DatanodeDescriptor node = containingNodes.first();
-            for (Iterator<Block> it = node.getBlockIterator(); it.hasNext(); ) {
-                Block cur = it.next();
-                if (b.getBlockId() == cur.getBlockId()) {
-                    b.setNumBytes(cur.getNumBytes());
-                    break;
-                }
+            List<DatanodeDescriptor> containingNodes = blocksMap.get(b);
+            Block storedBlock = 
+                containingNodes.get(0).getBlock(b);
+            if ( storedBlock != null ) {
+                pendingBlocks[i] = storedBlock;
             }
         }
         
@@ -946,7 +943,7 @@
         // the blocks.
         int numExpectedReplicas = pendingFile.getReplication();
         for (int i = 0; i < nrBlocks; i++) {
-          SortedSet<DatanodeDescriptor> containingNodes = blocksMap.get(pendingBlocks[i]);
+          Collection<DatanodeDescriptor> containingNodes = blocksMap.get(pendingBlocks[i]);
           // filter out containingNodes that are marked for decommission.
           int numCurrentReplica = countContainingNodes(containingNodes);
 
@@ -986,7 +983,7 @@
 
         for (Iterator<Block> it = v.getBlocks().iterator(); it.hasNext(); ) {
             Block b = it.next();
-            SortedSet<DatanodeDescriptor> containingNodes = blocksMap.get(b);
+            Collection<DatanodeDescriptor> containingNodes = blocksMap.get(b);
             if (containingNodes == null || containingNodes.size() < this.minReplication) {
                 return false;
             }
@@ -1077,7 +1074,7 @@
             for (int i = 0; i < deletedBlocks.length; i++) {
                 Block b = deletedBlocks[i];
 
-                SortedSet<DatanodeDescriptor> containingNodes = blocksMap.get(b);
+                Collection<DatanodeDescriptor> containingNodes = blocksMap.get(b);
                 if (containingNodes != null) {
                     for (Iterator<DatanodeDescriptor> it = containingNodes.iterator(); it.hasNext(); ) {
                         DatanodeDescriptor node = it.next();
@@ -1201,7 +1198,7 @@
         } else {
           String hosts[][] = new String[(endBlock - startBlock) + 1][];
             for (int i = startBlock; i <= endBlock; i++) {
-              SortedSet<DatanodeDescriptor> containingNodes = blocksMap.get(blocks[i]);
+                Collection<DatanodeDescriptor> containingNodes = blocksMap.get(blocks[i]);
                 Collection<String> v = new ArrayList<String>();
                 if (containingNodes != null) {
                   for (Iterator<DatanodeDescriptor> it =containingNodes.iterator(); it.hasNext();) {
@@ -1924,12 +1921,16 @@
         // between the old and new block report.
         //
         int newPos = 0;
-        boolean modified = false;
         Iterator<Block> iter = node.getBlockIterator();
         Block oldblk = iter.hasNext() ? iter.next() : null;
         Block newblk = (newReport != null && newReport.length > 0) ? 
                         newReport[0]	: null;
 
+        // common case is that most of the blocks from the datanode
+        // matches blocks in datanode descriptor.                
+        Collection<Block> toRemove = new LinkedList<Block>();
+        Collection<Block> toAdd = new LinkedList<Block>();
+        
         while (oldblk != null || newblk != null) {
            
             int cmp = (oldblk == null) ? 1 : 
@@ -1943,25 +1944,27 @@
                          ? newReport[newPos] : null;
             } else if (cmp < 0) {
                 // The old report has a block the new one does not
-                removeStoredBlock(oldblk, node);
-                modified = true;
+                toRemove.add(oldblk);
                 oldblk = iter.hasNext() ? iter.next() : null;
             } else {
                 // The new report has a block the old one does not
-                addStoredBlock(newblk, node);
-                modified = true;
+                toAdd.add(newblk);
                 newPos++;
                 newblk = (newPos < newReport.length)
                          ? newReport[newPos] : null;
             }
         }
-        //
-        // Modify node so it has the new blockreport
-        //
-        if (modified) {
-            node.updateBlocks(newReport);
+        
+        for ( Iterator<Block> i = toRemove.iterator(); i.hasNext(); ) {
+            Block b = i.next();
+            removeStoredBlock( b, node );
+            node.removeBlock( b );
         }
-
+        for ( Iterator<Block> i = toAdd.iterator(); i.hasNext(); ) {
+            Block b = i.next();
+            node.addBlock( addStoredBlock(b, node) );
+        }
+        
         //
         // We've now completely updated the node's block report profile.
         // We now go through all its blocks and find which ones are invalid,
@@ -1990,12 +1993,27 @@
     /**
      * Modify (block-->datanode) map.  Remove block from set of 
      * needed replications if this takes care of the problem.
+     * @return the block that is stored in blockMap.
      */
-    synchronized void addStoredBlock(Block block, DatanodeDescriptor node) {
-      SortedSet<DatanodeDescriptor> containingNodes = blocksMap.get(block);
+    synchronized Block addStoredBlock(Block block, DatanodeDescriptor node) {
+        List<DatanodeDescriptor> containingNodes = blocksMap.get(block);
         if (containingNodes == null) {
-            containingNodes = new TreeSet<DatanodeDescriptor>();
+            //Create an arraylist with the current replication factor
+            FSDirectory.INode inode = dir.getFileByBlock(block);
+            int replication = (inode != null) ? 
+                              inode.getReplication() : defaultReplication;
+            containingNodes = new ArrayList<DatanodeDescriptor>(replication);
             blocksMap.put(block, containingNodes);
+        } else {
+            Block storedBlock = 
+                containingNodes.get(0).getBlock(block);
+            // update stored block's length.
+            if ( storedBlock != null ) {
+                if ( block.getNumBytes() > 0 ) {
+                    storedBlock.setNumBytes( block.getNumBytes() );
+                }
+                block = storedBlock;
+            }
         }
         int curReplicaDelta = 0;
         if (! containingNodes.contains(node)) {
@@ -2018,7 +2036,7 @@
 
         FSDirectory.INode fileINode = dir.getFileByBlock(block);
         if( fileINode == null )  // block does not belong to any file
-            return;
+            return block;
         
         // filter out containingNodes that are marked for decommission.
         int numCurrentReplica = countContainingNodes(containingNodes);
@@ -2036,6 +2054,7 @@
             pendingReplications.remove(block);
         }        
         proccessOverReplicatedBlock( block, fileReplication );
+        return block;
     }
     
     /**
@@ -2044,7 +2063,7 @@
      * mark them in the excessReplicateMap.
      */
     private void proccessOverReplicatedBlock( Block block, short replication ) {
-      SortedSet<DatanodeDescriptor> containingNodes = blocksMap.get(block);
+      Collection<DatanodeDescriptor> containingNodes = blocksMap.get(block);
       if( containingNodes == null )
         return;
       Collection<DatanodeDescriptor> nonExcess = new ArrayList<DatanodeDescriptor>();
@@ -2124,7 +2143,7 @@
     synchronized void removeStoredBlock(Block block, DatanodeDescriptor node) {
         NameNode.stateChangeLog.debug(""BLOCK* NameSystem.removeStoredBlock: ""
                 +block.getBlockName() + "" from ""+node.getName() );
-        SortedSet<DatanodeDescriptor> containingNodes = blocksMap.get(block);
+        Collection<DatanodeDescriptor> containingNodes = blocksMap.get(block);
         if (containingNodes == null || ! containingNodes.contains(node)) {
           NameNode.stateChangeLog.debug(""BLOCK* NameSystem.removeStoredBlock: ""
             +block.getBlockName()+"" has already been removed from node ""+node );
@@ -2182,14 +2201,9 @@
         NameNode.stateChangeLog.debug(""BLOCK* NameSystem.blockReceived: ""
                 +block.getBlockName()+"" is received from "" + nodeID.getName() );
         //
-        // Modify the blocks->datanode map
+        // Modify the blocks->datanode map and node's map.
         // 
-        addStoredBlock(block, node);
-
-        //
-        // Supplement node's blockreport
-        //
-        node.addBlock(block);
+        node.addBlock( addStoredBlock(block, node) );
     }
 
     /**
"
hadoop,fc86b40ce215a3ad38d513096be515c6ccc11a90,"HADOOP-649.  Fix so that jobs with no tasks are not lost.  Contributed by Thomas Friol.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@508583 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-16 21:18:27,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobInProgress.java b/src/java/org/apache/hadoop/mapred/JobInProgress.java
index d60f2c7..081fd69 100644
--- a/src/java/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/JobInProgress.java
@@ -146,6 +146,9 @@
         
         // if no split is returned, job is considered completed and successful
         if (numMapTasks == 0) {
+            // Finished time need to be setted here to prevent this job to be retired
+            // from the job tracker jobs at the next retire iteration.
+            this.finishTime = System.currentTimeMillis();
             this.status = new JobStatus(status.getJobId(), 1.0f, 1.0f, JobStatus.SUCCEEDED);
             tasksInited = true;
             return;
"
hadoop,62825831fa63e6ce32f17953f3ab19c2348b15a9,"HADOOP-476.  Rewrite contrib/streaming command-line processing, improving parameter validation.  Contributed by Sanjay.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@507163 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-13 19:11:46,Doug Cutting,"diff --git a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java
index 5a2167c..d475ac7 100644
--- a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java
+++ b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java
@@ -26,16 +26,33 @@
 import java.net.URLEncoder;
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashSet;
 import java.util.Iterator;
+import java.util.List;
+import java.util.ListIterator;
 import java.util.Map;
+import java.util.Set;
 import java.util.TreeMap;
 import java.util.TreeSet;
 
+import org.apache.commons.cli2.*; 
+import org.apache.commons.cli2.builder.ArgumentBuilder;
+import org.apache.commons.cli2.builder.DefaultOptionBuilder;
+import org.apache.commons.cli2.builder.GroupBuilder;
+import org.apache.commons.cli2.commandline.Parser;
+import org.apache.commons.cli2.option.PropertyOption;
+import org.apache.commons.cli2.resource.ResourceConstants;
+import org.apache.commons.cli2.util.HelpFormatter;
+import org.apache.commons.cli2.validation.FileValidator;
+import org.apache.commons.cli2.validation.InvalidArgumentException;
+import org.apache.commons.cli2.validation.Validator;
 import org.apache.commons.logging.*;
 
 import org.apache.hadoop.conf.Configuration;
 
 import org.apache.hadoop.io.Text;
+import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 
 import org.apache.hadoop.mapred.FileAlreadyExistsException;
@@ -45,6 +62,7 @@
 import org.apache.hadoop.mapred.RunningJob;
 import org.apache.hadoop.filecache.*;
 import org.apache.hadoop.util.*;
+import org.apache.log4j.helpers.OptionConverter;
 /** All the client-side work happens here.
  * (Jar packaging, MapRed job submission and monitoring)
  * @author Michel Tourn
@@ -55,7 +73,22 @@
   final static String REDUCE_NONE = ""NONE"";
   private boolean reducerNone_;
 
+  /** -----------Streaming CLI Implementation  **/
+  private DefaultOptionBuilder builder = 
+    new DefaultOptionBuilder(""-"",""-"", false);
+  private ArgumentBuilder argBuilder = new ArgumentBuilder(); 
+  private Parser parser = new Parser(); 
+  private Group allOptions ; 
+  HelpFormatter helpFormatter = new HelpFormatter(""  "", ""  "", ""  "", 900);
+  // need these two at class level to extract values later from 
+  // commons-cli command line
+  private MultiPropertyOption jobconf = new MultiPropertyOption(
+      ""-jobconf"", ""(n=v) Optional. Add or override a JobConf property."", 'D'); 
+  private MultiPropertyOption cmdenv = new MultiPropertyOption(
+      ""-cmdenv"", ""(n=v) Pass env.var to streaming commands."", 'E');  
+  
   public StreamJob(String[] argv, boolean mayExit) {
+    setupOptions();
     argv_ = argv;
     mayExit_ = mayExit;
   }
@@ -119,15 +152,6 @@
     redCmd_ = unqualifyIfLocalPath(redCmd_);
   }
 
-  String[] parseNameEqValue(String neqv) {
-    String[] nv = neqv.split(""="", 2);
-    if (nv.length < 2) {
-      fail(""Invalid name=value spec: "" + neqv);
-    }
-    msg(""Recording name=value: name="" + nv[0] + "" value="" + nv[1]);
-    return nv;
-  }
-
   String unqualifyIfLocalPath(String cmd) throws IOException {
     if (cmd == null) {
       //
@@ -168,142 +192,259 @@
     return new File(getHadoopClientHome() + ""/conf"", hadoopAliasConf_).getAbsolutePath();
   }
 
-  /**
-   * This method parses the command line args
-   * to a hadoop streaming job
-   */
-  void parseArgv() {
-    if (argv_.length == 0) {
-      exitUsage(false);
-    }
-    int i = 0;
-    while (i < argv_.length) {
-      String s;
-      if (argv_[i].equals(""-verbose"")) {
-        verbose_ = true;
-      } else if (argv_[i].equals(""-info"")) {
-        detailedUsage_ = true;
-      } else if (argv_[i].equals(""-debug"")) {
-        debug_++;
-      } else if ((s = optionArg(argv_, i, ""-input"", false)) != null) {
-        i++;
-        inputSpecs_.add(s);
-      } else if (argv_[i].equals(""-inputtagged"")) {
-        inputTagged_ = true;
-      } else if ((s = optionArg(argv_, i, ""-output"", output_ != null)) != null) {
-        i++;
-        output_ = s;
-      } else if ((s = optionArg(argv_, i, ""-mapsideoutput"", mapsideoutURI_ != null)) != null) {
-        i++;
-        mapsideoutURI_ = s;
-      } else if ((s = optionArg(argv_, i, ""-mapper"", mapCmd_ != null)) != null) {
-        i++;
-        mapCmd_ = s;
-      } else if ((s = optionArg(argv_, i, ""-combiner"", comCmd_ != null)) != null) {
-        i++;
-        comCmd_ = s;
-      } else if ((s = optionArg(argv_, i, ""-reducer"", redCmd_ != null)) != null) {
-        i++;
-        redCmd_ = s;
-      } else if ((s = optionArg(argv_, i, ""-file"", false)) != null) {
-        i++;
-        packageFiles_.add(s);
-      } else if ((s = optionArg(argv_, i, ""-cluster"", cluster_ != null)) != null) {
-        i++;
-        cluster_ = s;
-      } else if ((s = optionArg(argv_, i, ""-config"", false)) != null) {
-        i++;
-        configPath_.add(s);
-      } else if ((s = optionArg(argv_, i, ""-dfs"", false)) != null) {
-        i++;
-        userJobConfProps_.put(""fs.default.name"", s);
-      } else if ((s = optionArg(argv_, i, ""-jt"", false)) != null) {
-        i++;
-        userJobConfProps_.put(""mapred.job.tracker"", s);
-      } else if ((s = optionArg(argv_, i, ""-jobconf"", false)) != null) {
-        i++;
-        String[] nv = parseNameEqValue(s);
-        userJobConfProps_.put(nv[0], nv[1]);
-      } else if ((s = optionArg(argv_, i, ""-cmdenv"", false)) != null) {
-        i++;
-        parseNameEqValue(s);
-        if (addTaskEnvironment_.length() > 0) {
-          addTaskEnvironment_ += "" "";
-        }
-        addTaskEnvironment_ += s;
-      } else if ((s = optionArg(argv_, i, ""-inputreader"", inReaderSpec_ != null)) != null) {
-        i++;
-        inReaderSpec_ = s;
-      } else if((s = optionArg(argv_, i, ""-cacheArchive"", false)) != null) {
-    	  i++;
-    	  if (cacheArchives == null)
-    		  cacheArchives = s;
-    	  else
-    		  cacheArchives = cacheArchives + "","" + s;    	  
-      } else if((s = optionArg(argv_, i, ""-cacheFile"", false)) != null) {
-        i++;
-        System.out.println("" the val of s is "" + s);
-        if (cacheFiles == null)
-          cacheFiles = s;
-        else
-          cacheFiles = cacheFiles + "","" + s;
-        System.out.println("" the val of cachefiles is "" + cacheFiles);
-      }
-      else {
-        System.err.println(""Unexpected argument: "" + argv_[i]);
+  void parseArgv(){
+    CommandLine cmdLine = null ; 
+    try{
+       cmdLine = parser.parse(argv_);
+    }catch(Exception oe){
+      LOG.error(oe.getMessage());
+      if (detailedUsage_) {
+        exitUsage(true);
+      } else {
         exitUsage(false);
       }
-      i++;
     }
-    if (detailedUsage_) {
+    
+    if( cmdLine != null ){
+      verbose_ =  cmdLine.hasOption(""-verbose"") ;
+      detailedUsage_ = cmdLine.hasOption(""-info"") ;
+      debug_ = cmdLine.hasOption(""-debug"")? debug_ + 1 : debug_ ;
+      inputTagged_ = cmdLine.hasOption(""-inputtagged""); 
+      
+      inputSpecs_.addAll(cmdLine.getValues(""-input""));
+      output_ = (String) cmdLine.getValue(""-output""); 
+      mapsideoutURI_ = (String) cmdLine.getValue(""-mapsideoutput"");
+      
+      mapCmd_ = (String)cmdLine.getValue(""-mapper""); 
+      comCmd_ = (String)cmdLine.getValue(""-combiner""); 
+      redCmd_ = (String)cmdLine.getValue(""-reducer""); 
+      
+      packageFiles_.addAll(cmdLine.getValues(""-file""));
+      
+      cluster_ = (String)cmdLine.getValue(""-cluster"");
+      
+      configPath_.addAll(cmdLine.getValues(""-config""));
+      
+      String fsName = (String)cmdLine.getValue(""-dfs"");
+      if( null != fsName ){
+        userJobConfProps_.put(""fs.default.name"", fsName);        
+      }
+      
+      String jt = (String)cmdLine.getValue(""mapred.job.tracker"");
+      if( null != jt ){
+        userJobConfProps_.put(""fs.default.name"", jt);        
+      }
+      
+      inReaderSpec_ = (String)cmdLine.getValue(""-inputreader""); 
+      
+      List<String> car = cmdLine.getValues(""-cacheArchive""); 
+      if( null != car ){
+        for( String s : car ){
+          cacheArchives = (cacheArchives == null)?s :cacheArchives + "","" + s;  
+        }
+      }
+
+      List<String> caf = cmdLine.getValues(""-cacheFile""); 
+      if( null != caf ){
+        for( String s : caf ){
+          cacheFiles = (cacheFiles == null)?s :cacheFiles + "","" + s;  
+        }
+      }
+      
+      List<String> jobConfArgs = (List<String>)cmdLine.getValue(jobconf); 
+      List<String> envArgs = (List<String>)cmdLine.getValue(cmdenv); 
+      
+      if( null != jobConfArgs ){
+        for( String s : jobConfArgs){
+          String []parts = s.split(""=""); 
+          userJobConfProps_.put(parts[0], parts[1]);
+        }
+      }
+      if( null != envArgs ){
+        for( String s : envArgs ){
+          if (addTaskEnvironment_.length() > 0) {
+            addTaskEnvironment_ += "" "";
+          }
+          addTaskEnvironment_ += s;
+        }
+      }
+    }else if (detailedUsage_) {
       exitUsage(true);
     }
   }
 
-  String optionArg(String[] args, int index, String arg, boolean argSet) {
-    if (index >= args.length || !args[index].equals(arg)) {
-      return null;
-    }
-    if (argSet) {
-      throw new IllegalArgumentException(""Can only have one "" + arg + "" option"");
-    }
-    if (index >= args.length - 1) {
-      throw new IllegalArgumentException(""Expected argument after option "" + args[index]);
-    }
-    return args[index + 1];
-  }
-
   protected void msg(String msg) {
     if (verbose_) {
       System.out.println(""STREAM: "" + msg);
     }
   }
+  
+  private Option createOption(String name, String desc, 
+      String argName, int max, boolean required){
+    Argument argument = argBuilder.
+                      withName(argName).
+                      withMinimum(1).
+                      withMaximum(max).
+                      create();
+    return builder.
+              withLongName(name).
+              withArgument(argument).
+              withDescription(desc).
+              withRequired(required).
+              create();
+  }
+  
+  private Option createOption(String name, String desc, 
+      String argName, int max, boolean required, Validator validator){
+    
+    Argument argument = argBuilder.
+                              withName(argName).
+                              withMinimum(1).
+                              withMaximum(max).
+                              withValidator(validator).
+                              create() ;
+   
+    return builder.
+              withLongName(name).
+              withArgument(argument).
+              withDescription(desc).
+              withRequired(required).
+              create();
+  }  
+  
+  private Option createBoolOption(String name, String desc){
+    return builder.withLongName(name).withDescription(desc).create();
+  }
+  
+  private void setupOptions(){
+
+    final Validator fileValidator = new Validator(){
+      public void validate(final List values) throws InvalidArgumentException {
+        // Note : This code doesnt belong here, it should be changed to 
+        // an can exec check in java 6
+        for (String file : (List<String>)values) {
+          File f = new File(file);  
+          if ( ! f.exists() ) {
+            throw new InvalidArgumentException(""Argument : "" + 
+                f.getAbsolutePath() + "" doesn't exist.""); 
+          }
+          if ( ! f.isFile() ) {
+            throw new InvalidArgumentException(""Argument : "" + 
+                f.getAbsolutePath() + "" is not a file.""); 
+          }
+          if ( ! f.canRead() ) {
+            throw new InvalidArgumentException(""Argument : "" + 
+                f.getAbsolutePath() + "" is not accessible""); 
+          }
+        }
+      }      
+    }; 
+
+    // Note: not extending CLI2's FileValidator, that overwrites 
+    // the String arg into File and causes ClassCastException 
+    // in inheritance tree. 
+    final Validator execValidator = new Validator(){
+      public void validate(final List values) throws InvalidArgumentException {
+        // Note : This code doesnt belong here, it should be changed to 
+        // an can exec check in java 6
+        for (String file : (List<String>)values) {
+          try{
+            Runtime.getRuntime().exec(""chmod 0777 "" + (new File(file)).getAbsolutePath());
+          }catch(IOException ioe){
+            // ignore 
+          }
+        }
+        fileValidator.validate(values);
+    }      
+    }; 
+
+    Option input   = createOption(""input"", 
+        ""DFS input file(s) for the Map step"", 
+        ""path"", 
+        Integer.MAX_VALUE, 
+        true);  
+    
+    Option output  = createOption(""output"", 
+        ""DFS output directory for the Reduce step"", 
+        ""path"", 1, true); 
+    Option mapper  = createOption(""mapper"", 
+        ""The streaming command to run"", ""cmd"", 1, true);
+    Option combiner = createOption(""combiner"", 
+        ""The streaming command to run"", ""cmd"",1, false);
+    // reducer could be NONE 
+    Option reducer = createOption(""reducer"", 
+        ""The streaming command to run"", ""cmd"", 1, true); 
+    Option file = createOption(""file"", 
+        ""File/dir to be shipped in the Job jar file"", 
+        ""file"", Integer.MAX_VALUE, false, execValidator); 
+    Option dfs = createOption(""dfs"", 
+        ""Optional. Override DFS configuration"", ""<h:p>|local"", 1, false); 
+    Option jt = createOption(""jt"", 
+        ""Optional. Override JobTracker configuration"", ""<h:p>|local"",1, false);
+    Option inputreader = createOption(""inputreader"", 
+        ""Optional."", ""spec"",1, false );
+    Option cacheFile = createOption(""cacheFile"", 
+        ""File name URI"", ""fileNameURI"", 1, false);
+    Option cacheArchive = createOption(""cacheArchive"", 
+        ""File name URI"", ""fileNameURI"",1, false);
+    
+    // boolean properties
+    
+    Option verbose = createBoolOption(""verbose"", ""print verbose output""); 
+    Option info = createBoolOption(""info"", ""print verbose output""); 
+    Option help = createBoolOption(""help"", ""print this help message""); 
+    Option debug = createBoolOption(""debug"", ""print debug output""); 
+    Option inputtagged = createBoolOption(""inputtagged"", ""inputtagged""); 
+    
+    allOptions = new GroupBuilder().
+                          withOption(input).
+                          withOption(output).
+                          withOption(mapper).
+                          withOption(combiner).
+                          withOption(reducer).
+                          withOption(file).
+                          withOption(dfs).
+                          withOption(jt).
+                          withOption(inputreader).
+                          withOption(jobconf).
+                          withOption(cmdenv).
+                          withOption(cacheFile).
+                          withOption(cacheArchive).
+                          withOption(verbose).
+                          withOption(info).
+                          withOption(debug).
+                          withOption(inputtagged).
+                          withOption(help).
+                          create();
+    parser.setGroup(allOptions);
+    
+  }
 
   public void exitUsage(boolean detailed) {
     //         1         2         3         4         5         6         7
     //1234567890123456789012345678901234567890123456789012345678901234567890123456789
-    System.out.println(""Usage: $HADOOP_HOME/bin/hadoop [--config dir] jar \\"");
-    System.out.println(""          $HADOOP_HOME/hadoop-streaming.jar [options]"");
-    System.out.println(""Options:"");
-    System.out.println(""  -input    <path>     DFS input file(s) for the Map step"");
-    System.out.println(""  -output   <path>     DFS output directory for the Reduce step"");
-    System.out.println(""  -mapper   <cmd>      The streaming command to run"");
-    System.out.println(""  -combiner <cmd>      The streaming command to run"");
-    System.out.println(""  -reducer  <cmd>      The streaming command to run"");
-    System.out.println(""  -file     <file>     File/dir to be shipped in the Job jar file"");
-    //Only advertise the standard way: [--config dir] in our launcher 
-    //System.out.println(""  -cluster  <name>     Default uses hadoop-default.xml and hadoop-site.xml"");
-    //System.out.println(""  -config   <file>     Optional. One or more paths to xml config files"");
-    System.out.println(""  -dfs    <h:p>|local  Optional. Override DFS configuration"");
-    System.out.println(""  -jt     <h:p>|local  Optional. Override JobTracker configuration"");
-    System.out.println(""  -inputreader <spec>  Optional."");
-    System.out.println(""  -jobconf  <n>=<v>    Optional. Add or override a JobConf property"");
-    System.out.println(""  -cmdenv   <n>=<v>    Optional. Pass env.var to streaming commands"");
-    System.out.println(""  -cacheFile fileNameURI"");
-    System.out.println(""  -cacheArchive fileNameURI"");
-    System.out.println(""  -verbose"");
-    System.out.println();
     if (!detailed) {
+      System.out.println(""Usage: $HADOOP_HOME/bin/hadoop [--config dir] jar \\"");
+      System.out.println(""          $HADOOP_HOME/hadoop-streaming.jar [options]"");
+      System.out.println(""Options:"");
+      System.out.println(""  -input    <path>     DFS input file(s) for the Map step"");
+      System.out.println(""  -output   <path>     DFS output directory for the Reduce step"");
+      System.out.println(""  -mapper   <cmd>      The streaming command to run"");
+      System.out.println(""  -combiner <cmd>      The streaming command to run"");
+      System.out.println(""  -reducer  <cmd>      The streaming command to run"");
+      System.out.println(""  -file     <file>     File/dir to be shipped in the Job jar file"");
+      //Only advertise the standard way: [--config dir] in our launcher 
+      //System.out.println(""  -cluster  <name>     Default uses hadoop-default.xml and hadoop-site.xml"");
+      //System.out.println(""  -config   <file>     Optional. One or more paths to xml config files"");
+      System.out.println(""  -dfs    <h:p>|local  Optional. Override DFS configuration"");
+      System.out.println(""  -jt     <h:p>|local  Optional. Override JobTracker configuration"");
+      System.out.println(""  -inputreader <spec>  Optional."");
+      System.out.println(""  -jobconf  <n>=<v>    Optional. Add or override a JobConf property"");
+      System.out.println(""  -cmdenv   <n>=<v>    Optional. Pass env.var to streaming commands"");
+      System.out.println(""  -cacheFile fileNameURI"");
+      System.out.println(""  -cacheArchive fileNameURI"");
+      System.out.println(""  -verbose"");
+      System.out.println();      
       System.out.println(""For more details about these options:"");
       System.out.println(""Use $HADOOP_HOME/bin/hadoop jar build/hadoop-streaming.jar -info"");
       fail("""");
@@ -810,6 +951,56 @@
       jc_.close();
     }
   }
+  /** Support -jobconf x=y x1=y1 type options **/
+  class MultiPropertyOption extends PropertyOption{
+    private String optionString ; 
+    MultiPropertyOption(){
+      super(); 
+    }
+    
+    MultiPropertyOption(final String optionString,
+        final String description,
+        final int id){
+      super(optionString, description, id) ; 
+      this.optionString = optionString;
+    }
+
+    public boolean canProcess(final WriteableCommandLine commandLine,
+        final String argument) {
+        boolean ret = (argument != null) && argument.startsWith(optionString);
+        
+        return ret;
+    }    
+    public void process(final WriteableCommandLine commandLine,
+        final ListIterator arguments) throws OptionException {
+      final String arg = (String) arguments.next();
+
+      if (!canProcess(commandLine, arg)) {
+          throw new OptionException(this, 
+              ResourceConstants.UNEXPECTED_TOKEN, arg);
+      }
+      
+      ArrayList properties = new ArrayList(); 
+      String next = """" ; 
+      while( arguments.hasNext()){
+        next = (String) arguments.next();
+        if( ! next.startsWith(""-"") ){
+          properties.add(next);
+        }else{
+          arguments.previous();
+          break; 
+        }
+      } 
+
+      // add to any existing values ( support specifying args multiple times)
+      List<String> oldVal = (List<String>)commandLine.getValue(this) ; 
+      if( oldVal == null ){
+        commandLine.addValue(this, properties);
+      }else{
+        oldVal.addAll(properties); 
+      }
+    }
+  }
 
   protected boolean mayExit_;
   protected String[] argv_;
"
hadoop,15a78e06d76d80bdba30d2cbaff389a1054c64b0,"HADOOP-761.  Change unit tests to not use /tmp.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@505512 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-09 21:43:30,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/mapred/MRCaching.java b/src/test/org/apache/hadoop/mapred/MRCaching.java
index 2a63178..e7c2616 100644
--- a/src/test/org/apache/hadoop/mapred/MRCaching.java
+++ b/src/test/org/apache/hadoop/mapred/MRCaching.java
@@ -62,8 +62,9 @@
         Path[] localFiles = DistributedCache.getLocalCacheFiles(conf);
         FileSystem fs = FileSystem.get(conf);
         // read the cached files (unzipped, unjarred and text)
-        // and put it into a single file /tmp/test.txt
-        Path file = new Path(""/tmp"");
+        // and put it into a single file TEST_ROOT_DIR/test.txt
+        String TEST_ROOT_DIR = jconf.get(""test.build.data"",""/tmp"");
+        Path file = new Path(TEST_ROOT_DIR);
         if (!fs.mkdirs(file)) {
           throw new IOException(""Mkdirs failed to create "" + file.toString());
         }
@@ -127,6 +128,9 @@
   public static boolean launchMRCache(String indir,
       String outdir, JobConf conf, String input)
       throws IOException {
+    String TEST_ROOT_DIR = new Path(System.getProperty(""test.build.data"",""/tmp""))
+      .toString().replace(' ', '+');
+    conf.set(""test.build.data"",TEST_ROOT_DIR);
     final Path inDir = new Path(indir);
     final Path outDir = new Path(outdir);
     FileSystem fs = FileSystem.get(conf);
@@ -158,7 +162,7 @@
     Path txtPath = new Path(localPath, new Path(""test.txt""));
     Path jarPath = new Path(localPath, new Path(""test.jar""));
     Path zipPath = new Path(localPath, new Path(""test.zip""));
-    Path cacheTest = new Path(""/tmp/cachedir"");
+    Path cacheTest = new Path(TEST_ROOT_DIR + ""/cachedir"");
     fs.delete(cacheTest);
     if (!fs.mkdirs(cacheTest)) {
       throw new IOException(""Mkdirs failed to create "" + cacheTest.toString());
@@ -168,9 +172,9 @@
     fs.copyFromLocalFile(zipPath, cacheTest);
     // setting the cached archives to zip, jar and simple text files
     String fileSys = fs.getName();
-    String archive1 = ""dfs://"" + fileSys + ""/tmp/cachedir/test.jar"";
-    String archive2 = ""dfs://"" + fileSys + ""/tmp/cachedir/test.zip""; 
-    String file1 = ""dfs://"" + fileSys + ""/tmp/cachedir/test.txt"";
+    String archive1 = ""dfs://"" + fileSys + TEST_ROOT_DIR + ""/cachedir/test.jar"";
+    String archive2 = ""dfs://"" + fileSys + TEST_ROOT_DIR + ""/cachedir/test.zip"";
+    String file1 = ""dfs://"" + fileSys + TEST_ROOT_DIR + ""/cachedir/test.txt"";
     URI uri1 = null;
     URI uri2 = null;
     URI uri3 = null;
@@ -187,7 +191,7 @@
     int count = 0;
     // after the job ran check to see if the the input from the localized cache
     // match the real string. check if there are 3 instances or not.
-    Path result = new Path(""/tmp/test.txt"");
+    Path result = new Path(TEST_ROOT_DIR + ""/test.txt"");
     {
       BufferedReader file = new BufferedReader(new InputStreamReader(fs
           .open(result)));
"
hadoop,15a78e06d76d80bdba30d2cbaff389a1054c64b0,"HADOOP-761.  Change unit tests to not use /tmp.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@505512 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-09 21:43:30,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/mapred/PiEstimator.java b/src/test/org/apache/hadoop/mapred/PiEstimator.java
index 88f25d6..5fd007c 100644
--- a/src/test/org/apache/hadoop/mapred/PiEstimator.java
+++ b/src/test/org/apache/hadoop/mapred/PiEstimator.java
@@ -31,7 +31,7 @@
 import org.apache.hadoop.io.SequenceFile.CompressionType;
 
 /**
- * A Map-reduce program to estimaate the valu eof Pi using monte-carlo
+ * A Map-reduce program to estimate the value of Pi using monte-carlo
  * method.
  *
  * @author Milind Bhandarkar
"
hadoop,104c9e53c3957abce387c40be535f8c4dd5233b2,"HADOOP-309.  Fix some NullPointerExceptions in the StatusHttpServer.  Contributed by navychen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@502749 13f79535-47bb-0310-9956-ffa450edef68
",2007-02-02 21:06:25,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/StatusHttpServer.java b/src/java/org/apache/hadoop/mapred/StatusHttpServer.java
index 37a4df4..3093dbc 100644
--- a/src/java/org/apache/hadoop/mapred/StatusHttpServer.java
+++ b/src/java/org/apache/hadoop/mapred/StatusHttpServer.java
@@ -70,13 +70,15 @@
     listener.setHost(bindAddress);
     webServer.addListener(listener);
 
-    // set up the context for ""/logs/""
-    HttpContext logContext = new HttpContext();
-    logContext.setContextPath(""/logs/*"");
+    // set up the context for ""/logs/"" if ""hadoop.log.dir"" property is defined. 
     String logDir = System.getProperty(""hadoop.log.dir"");
-    logContext.setResourceBase(logDir);
-    logContext.addHandler(new ResourceHandler());
-    webServer.addContext(logContext);
+    if( logDir != null ) {
+      HttpContext logContext = new HttpContext();
+      logContext.setContextPath(""/logs/*"");
+      logContext.setResourceBase(logDir);
+      logContext.addHandler(new ResourceHandler());
+      webServer.addContext(logContext);
+    }
 
     // set up the context for ""/static/*""
     String appDir = getWebAppsPath();
@@ -151,6 +153,8 @@
    */
   private static String getWebAppsPath() throws IOException {
     URL url = StatusHttpServer.class.getClassLoader().getResource(""webapps"");
+    if( url == null ) 
+      throw new IOException(""webapps not found in CLASSPATH""); 
     String path = url.getPath();
     if (isWindows && path.startsWith(""/"")) {
       path = path.substring(1);
"
hadoop,d470548bb524c48a3017a8c0ff875296780bab50,"HADOOP-549.  Fix a NullPointerException in TaskReport's serialization.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@502029 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-31 22:43:44,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java b/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java
index 57297d9..686601b 100644
--- a/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java
+++ b/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java
@@ -31,8 +31,9 @@
    * version 3 introduced to replace 
    * emitHearbeat/pollForNewTask/pollForTaskWithClosedJob with
    * {@link #heartbeat(TaskTrackerStatus, boolean, boolean, short)}
+   * version 4 changed TaskReport for HADOOP-549.
    */
-  public static final long versionID = 3L;
+  public static final long versionID = 4L;
   
   public final static int TRACKERS_OK = 0;
   public final static int UNKNOWN_TASKTRACKER = 1;
"
hadoop,d470548bb524c48a3017a8c0ff875296780bab50,"HADOOP-549.  Fix a NullPointerException in TaskReport's serialization.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@502029 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-31 22:43:44,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobClient.java b/src/java/org/apache/hadoop/mapred/JobClient.java
index 7c16ae1..5470c79 100644
--- a/src/java/org/apache/hadoop/mapred/JobClient.java
+++ b/src/java/org/apache/hadoop/mapred/JobClient.java
@@ -349,12 +349,30 @@
         }
     }
 
+    /**
+     * Get the information of the current state of the map tasks of a job.
+     * @param jobId the job to query
+     * @return the list of all of the map tips
+     */
+    public TaskReport[] getMapTaskReports(String jobId) throws IOException {
+      return jobSubmitClient.getMapTaskReports(jobId);
+    }
+    
+    /**
+     * Get the information of the current state of the reduce tasks of a job.
+     * @param jobId the job to query
+     * @return the list of all of the map tips
+     */    
+    public TaskReport[] getReduceTaskReports(String jobId) throws IOException {
+      return jobSubmitClient.getReduceTaskReports(jobId);
+    }
+    
     public ClusterStatus getClusterStatus() throws IOException {
       return jobSubmitClient.getClusterStatus();
     }
     
     public JobStatus[] jobsToComplete() throws IOException {
-	return jobSubmitClient.jobsToComplete();
+      return jobSubmitClient.jobsToComplete();
     }
     
     /** Utility that submits a job, then polls for progress until the job is
"
hadoop,d470548bb524c48a3017a8c0ff875296780bab50,"HADOOP-549.  Fix a NullPointerException in TaskReport's serialization.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@502029 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-31 22:43:44,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskReport.java b/src/java/org/apache/hadoop/mapred/TaskReport.java
index 4c1e165..867ea60 100644
--- a/src/java/org/apache/hadoop/mapred/TaskReport.java
+++ b/src/java/org/apache/hadoop/mapred/TaskReport.java
@@ -84,23 +84,21 @@
   // Writable
   //////////////////////////////////////////////
   public void write(DataOutput out) throws IOException {
-    UTF8.writeString(out, taskid);
+    Text.writeString(out, taskid);
     out.writeFloat(progress);
-    UTF8.writeString(out, state);
+    Text.writeString(out, state);
     out.writeLong(startTime);
     out.writeLong(finishTime);
-    new ObjectWritable(diagnostics).write(out);
+    WritableUtils.writeStringArray(out, diagnostics);
   }
 
   public void readFields(DataInput in) throws IOException {
-    this.taskid = UTF8.readString(in);
+    this.taskid = Text.readString(in);
     this.progress = in.readFloat();
-    this.state = UTF8.readString(in);
+    this.state = Text.readString(in);
     this.startTime = in.readLong(); 
     this.finishTime = in.readLong() ;
     
-    ObjectWritable wrapper = new ObjectWritable();
-    wrapper.readFields(in);
-    diagnostics = (String[])wrapper.get();
+    diagnostics = WritableUtils.readStringArray(in);
   }
 }
"
hadoop,5dcbd87600f51c02de14652b93ed9da05ac584fe,"HADOOP-731.  When a checksum error is encountered on a file stored in HDFS, try to find another replica.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@500370 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-26 21:49:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSClient.java b/src/java/org/apache/hadoop/dfs/DFSClient.java
index 283abec..d7e2522 100644
--- a/src/java/org/apache/hadoop/dfs/DFSClient.java
+++ b/src/java/org/apache/hadoop/dfs/DFSClient.java
@@ -618,7 +618,7 @@
                 DNAddrPair retval = chooseDataNode(targetBlock, deadNodes);
                 chosenNode = retval.info;
                 InetSocketAddress targetAddr = retval.addr;
-            
+
                 try {
                     s = new Socket();
                     s.connect(targetAddr, READ_TIMEOUT);
@@ -764,7 +764,7 @@
               if (nodes[blockId] == null || nodes[blockId].length == 0) {
                 LOG.info(""No node available for block: "" + blockInfo);
               }
-              LOG.info(""Could not obtain block from any node:  "" + ie);
+              LOG.info(""Could not obtain block "" + blockId + "" from any node:  "" + ie);
               try {
                 Thread.sleep(3000);
               } catch (InterruptedException iex) {
@@ -890,6 +890,24 @@
         }
 
         /**
+         * Seek to given position on a node other than the current node.  If
+         * a node other than the current node is found, then returns true. 
+         * If another node could not be found, then returns false.
+         */
+        public synchronized boolean seekToNewSource(long targetPos) throws IOException {
+            TreeSet excludeNodes = new TreeSet();       
+            excludeNodes.add(currentNode);
+            String oldNodeID = currentNode.getStorageID();
+            DatanodeInfo newNode = blockSeekTo(targetPos, excludeNodes); 
+            if (!oldNodeID.equals(newNode.getStorageID())) {
+                currentNode = newNode;
+                return true;
+            } else {
+                return false;
+            }
+        }
+        
+        /**
          */
         public synchronized long getPos() throws IOException {
             return pos;
"
hadoop,5dcbd87600f51c02de14652b93ed9da05ac584fe,"HADOOP-731.  When a checksum error is encountered on a file stored in HDFS, try to find another replica.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@500370 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-26 21:49:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FSDataInputStream.java b/src/java/org/apache/hadoop/fs/FSDataInputStream.java
index c80e183..9134ae4 100644
--- a/src/java/org/apache/hadoop/fs/FSDataInputStream.java
+++ b/src/java/org/apache/hadoop/fs/FSDataInputStream.java
@@ -90,34 +90,61 @@
     }
     
     public int read(byte b[], int off, int len) throws IOException {
-      int read = in.read(b, off, len);
+      int read;
+      boolean retry;
+      int retriesLeft = 3;
+      long oldPos = getPos();
+      do {
+        retriesLeft--;
+        retry = false;
 
-      if (sums != null) {
-        int summed = 0;
-        while (summed < read) {
-          
-          int goal = bytesPerSum - inSum;
-          int inBuf = read - summed;
-          int toSum = inBuf <= goal ? inBuf : goal;
-          
+        read = in.read(b, off, len);
+        
+        if (sums != null) {
+          long oldSumsPos = sums.getPos();
           try {
-            sum.update(b, off+summed, toSum);
-          } catch (ArrayIndexOutOfBoundsException e) {
-            throw new RuntimeException(""Summer buffer overflow b.len="" + 
-                                       b.length + "", off="" + off + 
-                                       "", summed="" + summed + "", read="" + 
-                                       read + "", bytesPerSum="" + bytesPerSum +
-                                       "", inSum="" + inSum, e);
-          }
-          summed += toSum;
+            int summed = 0;
+            while (summed < read) {
+              int goal = bytesPerSum - inSum;
+              int inBuf = read - summed;
+              int toSum = inBuf <= goal ? inBuf : goal;
           
-          inSum += toSum;
-          if (inSum == bytesPerSum) {
-            verifySum(read-(summed-bytesPerSum));
+              try {
+                sum.update(b, off+summed, toSum);
+              } catch (ArrayIndexOutOfBoundsException e) {
+                throw new RuntimeException(""Summer buffer overflow b.len="" + 
+                                           b.length + "", off="" + off + 
+                                           "", summed="" + summed + "", read="" + 
+                                           read + "", bytesPerSum="" + bytesPerSum +
+                                           "", inSum="" + inSum, e);
+              }
+              summed += toSum;
+          
+              inSum += toSum;
+              if (inSum == bytesPerSum) {
+                verifySum(read-(summed-bytesPerSum));
+              }
+            }
+          } catch (ChecksumException ce) {
+            LOG.info(""Found checksum error: "" + StringUtils.stringifyException(ce));
+            if (retriesLeft == 0) {
+              throw ce;
+            }
+            sums.seek(oldSumsPos);
+            if (!((FSInputStream)in).seekToNewSource(oldPos) ||
+                !((FSInputStream)sumsIn).seekToNewSource(oldSumsPos)) {
+              // Neither the data stream nor the checksum stream are being read from
+              // different sources, meaning we'll still get a checksum error if we 
+              // try to do the read again.  We throw an exception instead.
+              throw ce;
+            } else {
+              // Since at least one of the sources is different, the read might succeed,
+              // so we'll retry.
+              retry = true;
+            }
           }
         }
-      }
-        
+      } while (retry);
       return read;
     }
 
@@ -270,7 +297,11 @@
   public FSDataInputStream(FileSystem fs, Path file, int bufferSize, Configuration conf)
       throws IOException {
     super(null);
-    this.in = new Buffer(new PositionCache(new Checker(fs, file, conf)), bufferSize);
+    Checker chkr = new Checker(fs, file, conf);  // sets bytesPerSum
+    if (bufferSize % bytesPerSum != 0) {
+      throw new IOException(""Buffer size must be multiple of "" + bytesPerSum);
+    }
+    this.in = new Buffer(new PositionCache(chkr), bufferSize);
   }
   
   
@@ -278,7 +309,11 @@
     throws IOException {
     super(null);
     int bufferSize = conf.getInt(""io.file.buffer.size"", 4096);
-    this.in = new Buffer(new PositionCache(new Checker(fs, file, conf)), bufferSize);
+    Checker chkr = new Checker(fs, file, conf);
+    if (bufferSize % bytesPerSum != 0) {
+      throw new IOException(""Buffer size must be multiple of "" + bytesPerSum);
+    }
+    this.in = new Buffer(new PositionCache(chkr), bufferSize);
   }
     
   /** Construct without checksums. */
"
hadoop,5dcbd87600f51c02de14652b93ed9da05ac584fe,"HADOOP-731.  When a checksum error is encountered on a file stored in HDFS, try to find another replica.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@500370 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-26 21:49:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FSInputStream.java b/src/java/org/apache/hadoop/fs/FSInputStream.java
index b1e65c2..b1b58ee 100644
--- a/src/java/org/apache/hadoop/fs/FSInputStream.java
+++ b/src/java/org/apache/hadoop/fs/FSInputStream.java
@@ -38,7 +38,13 @@
      * Return the current offset from the start of the file
      */
     public abstract long getPos() throws IOException;
-    
+
+    /**
+     * Seeks a different copy of the data.  Returns true if 
+     * found a new source, false otherwise.
+     */
+    public abstract boolean seekToNewSource(long targetPos) throws IOException;
+
     public int read(long position, byte[] buffer, int offset, int length)
     throws IOException {
       synchronized (this) {
"
hadoop,5dcbd87600f51c02de14652b93ed9da05ac584fe,"HADOOP-731.  When a checksum error is encountered on a file stored in HDFS, try to find another replica.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@500370 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-26 21:49:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/InMemoryFileSystem.java b/src/java/org/apache/hadoop/fs/InMemoryFileSystem.java
index 2408f51..0a209dd 100644
--- a/src/java/org/apache/hadoop/fs/InMemoryFileSystem.java
+++ b/src/java/org/apache/hadoop/fs/InMemoryFileSystem.java
@@ -114,6 +114,10 @@
       din.reset(fAttr.data, (int)pos, fAttr.size - (int)pos);
     }
     
+    public boolean seekToNewSource(long targetPos) throws IOException {
+      return false;
+    }
+
     public int available() throws IOException {
       return din.available(); 
     }
"
hadoop,5dcbd87600f51c02de14652b93ed9da05ac584fe,"HADOOP-731.  When a checksum error is encountered on a file stored in HDFS, try to find another replica.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@500370 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-26 21:49:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/LocalFileSystem.java b/src/java/org/apache/hadoop/fs/LocalFileSystem.java
index a4d3bd0..890aaf4 100644
--- a/src/java/org/apache/hadoop/fs/LocalFileSystem.java
+++ b/src/java/org/apache/hadoop/fs/LocalFileSystem.java
@@ -95,6 +95,10 @@
           return fis.getChannel().position();
         }
 
+        public boolean seekToNewSource(long targetPos) throws IOException {
+          return false;
+        }
+
         /*
          * Just forward to the fis
          */
"
hadoop,5dcbd87600f51c02de14652b93ed9da05ac584fe,"HADOOP-731.  When a checksum error is encountered on a file stored in HDFS, try to find another replica.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@500370 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-26 21:49:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/s3/S3InputStream.java b/src/java/org/apache/hadoop/fs/s3/S3InputStream.java
index 5127ac2..ed1b02d 100644
--- a/src/java/org/apache/hadoop/fs/s3/S3InputStream.java
+++ b/src/java/org/apache/hadoop/fs/s3/S3InputStream.java
@@ -61,6 +61,11 @@
   }
 
   @Override
+  public synchronized boolean seekToNewSource(long targetPos) throws IOException {
+    return false;
+  }
+
+  @Override
   public synchronized int read() throws IOException {
     if (closed) {
       throw new IOException(""Stream closed"");
"
hadoop,807756bcc409265e5fbbce35ca88c8364cef2b37,"HADOOP-758.  Fix exception handling during reduce so that root exceptions are not masked by exceptions in cleanups.  Contributed by Raghu.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@499998 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-25 21:28:46,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/ReduceTask.java b/src/java/org/apache/hadoop/mapred/ReduceTask.java
index ba0d69d..270b57b 100644
--- a/src/java/org/apache/hadoop/mapred/ReduceTask.java
+++ b/src/java/org/apache/hadoop/mapred/ReduceTask.java
@@ -326,12 +326,24 @@
         values.informReduceProgress();
       }
 
-    } finally {
+      //Clean up: repeated in catch block below
       reducer.close();
       out.close(reporter);
+      //End of clean up.
+      
       if( runSpeculative ){
         ((PhasedFileSystem)fs).commit(); 
-       }
+      }
+    } catch ( IOException ioe ) {
+      try {
+        reducer.close();
+      } catch ( IOException ignored ) {}
+        
+      try {
+        out.close(reporter);
+      } catch ( IOException ignored ) {}
+      
+      throw ioe;
     }
     done(umbilical);
   }
"
hadoop,ec7abfae9dd26a2ed5ec3faef85a1e6316c93e91,"HADOOP-735.  Switch generated record code to use BytesWritable to represent fields of type 'buffer'.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@497624 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-18 23:03:42,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/BinaryInputArchive.java b/src/java/org/apache/hadoop/record/BinaryInputArchive.java
index c531d8e..3ca48db 100644
--- a/src/java/org/apache/hadoop/record/BinaryInputArchive.java
+++ b/src/java/org/apache/hadoop/record/BinaryInputArchive.java
@@ -20,9 +20,9 @@
 
 import java.io.DataInput;
 import java.io.IOException;
-import java.io.ByteArrayOutputStream;
 import java.io.DataInputStream;
 import java.io.InputStream;
+import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.Text;
 
 import org.apache.hadoop.io.WritableUtils;
@@ -87,13 +87,11 @@
         return text;
     }
     
-    public ByteArrayOutputStream readBuffer(String tag) throws IOException {
-        int len = readInt(tag);
-        ByteArrayOutputStream buf = new ByteArrayOutputStream(len);
-        byte[] arr = new byte[len];
-        in.readFully(arr);
-        buf.write(arr, 0, len);
-        return buf;
+    public BytesWritable readBuffer(String tag) throws IOException {
+      int len = WritableUtils.readVInt(in);
+      byte[] barr = new byte[len];
+      in.readFully(barr);
+      return new BytesWritable(barr);
     }
     
     public void readRecord(Record r, String tag) throws IOException {
"
hadoop,ec7abfae9dd26a2ed5ec3faef85a1e6316c93e91,"HADOOP-735.  Switch generated record code to use BytesWritable to represent fields of type 'buffer'.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@497624 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-18 23:03:42,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/BinaryOutputArchive.java b/src/java/org/apache/hadoop/record/BinaryOutputArchive.java
index a72f200..6a2d678 100644
--- a/src/java/org/apache/hadoop/record/BinaryOutputArchive.java
+++ b/src/java/org/apache/hadoop/record/BinaryOutputArchive.java
@@ -19,12 +19,12 @@
 package org.apache.hadoop.record;
 
 import java.io.IOException;
-import java.io.ByteArrayOutputStream;
 import java.util.TreeMap;
 import java.util.ArrayList;
 import java.io.DataOutput;
 import java.io.DataOutputStream;
 import java.io.OutputStream;
+import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.Text;
 
 import org.apache.hadoop.io.WritableUtils;
@@ -74,11 +74,12 @@
         s.write(out);
     }
     
-    public void writeBuffer(ByteArrayOutputStream buf, String tag)
+    public void writeBuffer(BytesWritable buf, String tag)
     throws IOException {
-        byte[] barr = buf.toByteArray();
-        writeInt(barr.length, tag);
-        out.write(barr);
+      byte[] barr = buf.get();
+      int len = buf.getSize();
+      WritableUtils.writeVInt(out, len);
+      out.write(barr, 0, len);
     }
     
     public void writeRecord(Record r, String tag) throws IOException {
"
hadoop,ec7abfae9dd26a2ed5ec3faef85a1e6316c93e91,"HADOOP-735.  Switch generated record code to use BytesWritable to represent fields of type 'buffer'.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@497624 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-18 23:03:42,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/CsvInputArchive.java b/src/java/org/apache/hadoop/record/CsvInputArchive.java
index 3fd3eb7..200b361 100644
--- a/src/java/org/apache/hadoop/record/CsvInputArchive.java
+++ b/src/java/org/apache/hadoop/record/CsvInputArchive.java
@@ -21,9 +21,9 @@
 import java.io.InputStreamReader;
 import java.io.InputStream;
 import java.io.IOException;
-import java.io.ByteArrayOutputStream;
 import java.io.PushbackReader;
 import java.io.UnsupportedEncodingException;
+import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.Text;
 
 /**
@@ -127,7 +127,7 @@
         
     }
     
-    public ByteArrayOutputStream readBuffer(String tag) throws IOException {
+    public BytesWritable readBuffer(String tag) throws IOException {
         String sval = readField(tag);
         return Utils.fromCSVBuffer(sval);
     }
"
hadoop,ec7abfae9dd26a2ed5ec3faef85a1e6316c93e91,"HADOOP-735.  Switch generated record code to use BytesWritable to represent fields of type 'buffer'.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@497624 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-18 23:03:42,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/CsvOutputArchive.java b/src/java/org/apache/hadoop/record/CsvOutputArchive.java
index ce4a4ad..3788c44 100644
--- a/src/java/org/apache/hadoop/record/CsvOutputArchive.java
+++ b/src/java/org/apache/hadoop/record/CsvOutputArchive.java
@@ -19,12 +19,12 @@
 package org.apache.hadoop.record;
 
 import java.io.IOException;
-import java.io.ByteArrayOutputStream;
 import java.util.TreeMap;
 import java.util.ArrayList;
 import java.io.PrintStream;
 import java.io.OutputStream;
 import java.io.UnsupportedEncodingException;
+import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.Text;
 
 /**
@@ -97,7 +97,7 @@
         throwExceptionOnError(tag);
     }
     
-    public void writeBuffer(ByteArrayOutputStream buf, String tag)
+    public void writeBuffer(BytesWritable buf, String tag)
     throws IOException {
         printCommaUnlessFirst();
         stream.print(Utils.toCSVBuffer(buf));
"
hadoop,ec7abfae9dd26a2ed5ec3faef85a1e6316c93e91,"HADOOP-735.  Switch generated record code to use BytesWritable to represent fields of type 'buffer'.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@497624 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-18 23:03:42,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/InputArchive.java b/src/java/org/apache/hadoop/record/InputArchive.java
index d7f718d..cd4dcfe 100644
--- a/src/java/org/apache/hadoop/record/InputArchive.java
+++ b/src/java/org/apache/hadoop/record/InputArchive.java
@@ -19,7 +19,7 @@
 package org.apache.hadoop.record;
 
 import java.io.IOException;
-import java.io.ByteArrayOutputStream;
+import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.Text;
 
 /**
@@ -35,7 +35,7 @@
     public float readFloat(String tag) throws IOException;
     public double readDouble(String tag) throws IOException;
     public Text readString(String tag) throws IOException;
-    public ByteArrayOutputStream readBuffer(String tag) throws IOException;
+    public BytesWritable readBuffer(String tag) throws IOException;
     public void readRecord(Record r, String tag) throws IOException;
     public void startRecord(String tag) throws IOException;
     public void endRecord(String tag) throws IOException;
"
hadoop,ec7abfae9dd26a2ed5ec3faef85a1e6316c93e91,"HADOOP-735.  Switch generated record code to use BytesWritable to represent fields of type 'buffer'.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@497624 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-18 23:03:42,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/OutputArchive.java b/src/java/org/apache/hadoop/record/OutputArchive.java
index 75e965e..5ef05be 100644
--- a/src/java/org/apache/hadoop/record/OutputArchive.java
+++ b/src/java/org/apache/hadoop/record/OutputArchive.java
@@ -19,9 +19,9 @@
 package org.apache.hadoop.record;
 
 import java.io.IOException;
-import java.io.ByteArrayOutputStream;
 import java.util.TreeMap;
 import java.util.ArrayList;
+import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.Text;
 
 /**
@@ -37,7 +37,7 @@
     public void writeFloat(float f, String tag) throws IOException;
     public void writeDouble(double d, String tag) throws IOException;
     public void writeString(Text s, String tag) throws IOException;
-    public void writeBuffer(ByteArrayOutputStream buf, String tag)
+    public void writeBuffer(BytesWritable buf, String tag)
         throws IOException;
     public void writeRecord(Record r, String tag) throws IOException;
     public void startRecord(Record r, String tag) throws IOException;
"
hadoop,ec7abfae9dd26a2ed5ec3faef85a1e6316c93e91,"HADOOP-735.  Switch generated record code to use BytesWritable to represent fields of type 'buffer'.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@497624 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-18 23:03:42,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/Utils.java b/src/java/org/apache/hadoop/record/Utils.java
index f6d5246..d3382df 100644
--- a/src/java/org/apache/hadoop/record/Utils.java
+++ b/src/java/org/apache/hadoop/record/Utils.java
@@ -19,11 +19,8 @@
 package org.apache.hadoop.record;
 
 import java.io.ByteArrayOutputStream;
-import java.io.DataInput;
-import java.io.DataOutput;
 import java.io.IOException;
-import java.io.UnsupportedEncodingException;
-import java.nio.charset.CharacterCodingException;
+import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.Text;
 
 /**
@@ -36,32 +33,6 @@
     private Utils() {
     }
    
-    /**
-     * equals function that actually compares two buffers.
-     *
-     * @param one First buffer
-     * @param two Second buffer
-     * @return true if one and two contain exactly the same content, else false.
-     */
-    public static boolean bufEquals(ByteArrayOutputStream one,
-            ByteArrayOutputStream two) {
-        if (one == two) {
-            return true;
-        }
-        byte[] onearray = one.toByteArray();
-        byte[] twoarray = two.toByteArray();
-        boolean ret = (onearray.length == twoarray.length);
-        if (!ret) {
-            return ret;
-        }
-        for (int idx = 0; idx < onearray.length; idx++) {
-            if (onearray[idx] != twoarray[idx]) {
-                return false;
-            }
-        }
-        return true;
-    }
-    
     public static final char[] hexchars = { '0', '1', '2', '3', '4', '5',
                                             '6', '7', '8', '9', 'A', 'B',
                                             'C', 'D', 'E', 'F' };
@@ -200,10 +171,11 @@
      * @param s 
      * @return 
      */
-    static String toXMLBuffer(ByteArrayOutputStream s) {
-        byte[] barr = s.toByteArray();
-        StringBuffer sb = new StringBuffer(2*barr.length);
-        for (int idx = 0; idx < barr.length; idx++) {
+    static String toXMLBuffer(BytesWritable s) {
+        byte[] barr = s.get();
+        int bsize = s.getSize();
+        StringBuffer sb = new StringBuffer(2*bsize);
+        for (int idx = 0; idx < bsize; idx++) {
             sb.append(Integer.toHexString((int)barr[idx]));
         }
         return sb.toString();
@@ -215,10 +187,9 @@
      * @throws java.io.IOException 
      * @return 
      */
-    static ByteArrayOutputStream fromXMLBuffer(String s)
+    static BytesWritable fromXMLBuffer(String s)
     throws IOException {
-        ByteArrayOutputStream stream =  new ByteArrayOutputStream();
-        if (s.length() == 0) { return stream; }
+        if (s.length() == 0) { return new BytesWritable(); }
         int blen = s.length()/2;
         byte[] barr = new byte[blen];
         for (int idx = 0; idx < blen; idx++) {
@@ -226,8 +197,7 @@
             char c2 = s.charAt(2*idx+1);
             barr[idx] = Byte.parseByte(""""+c1+c2, 16);
         }
-        stream.write(barr);
-        return stream;
+        return new BytesWritable(barr);
     }
     
     /**
@@ -235,11 +205,12 @@
      * @param buf 
      * @return 
      */
-    static String toCSVBuffer(ByteArrayOutputStream buf) {
-        byte[] barr = buf.toByteArray();
-        StringBuffer sb = new StringBuffer(barr.length+1);
+    static String toCSVBuffer(BytesWritable buf) {
+        byte[] barr = buf.get();
+        int bsize = buf.getSize();
+        StringBuffer sb = new StringBuffer(bsize+1);
         sb.append('#');
-        for(int idx = 0; idx < barr.length; idx++) {
+        for(int idx = 0; idx < bsize; idx++) {
             sb.append(Integer.toHexString((int)barr[idx]));
         }
         return sb.toString();
@@ -247,18 +218,17 @@
     
     /**
      * Converts a CSV-serialized representation of buffer to a new
-     * ByteArrayOutputStream.
+     * BytesWritable.
      * @param s CSV-serialized representation of buffer
      * @throws java.io.IOException 
-     * @return Deserialized ByteArrayOutputStream
+     * @return Deserialized BytesWritable
      */
-    static ByteArrayOutputStream fromCSVBuffer(String s)
+    static BytesWritable fromCSVBuffer(String s)
     throws IOException {
         if (s.charAt(0) != '#') {
             throw new IOException(""Error deserializing buffer."");
         }
-        ByteArrayOutputStream stream =  new ByteArrayOutputStream();
-        if (s.length() == 1) { return stream; }
+        if (s.length() == 1) { return new BytesWritable(); }
         int blen = (s.length()-1)/2;
         byte[] barr = new byte[blen];
         for (int idx = 0; idx < blen; idx++) {
@@ -266,7 +236,6 @@
             char c2 = s.charAt(2*idx+2);
             barr[idx] = Byte.parseByte(""""+c1+c2, 16);
         }
-        stream.write(barr);
-        return stream;
+        return new BytesWritable(barr);
     }
 }
"
hadoop,ec7abfae9dd26a2ed5ec3faef85a1e6316c93e91,"HADOOP-735.  Switch generated record code to use BytesWritable to represent fields of type 'buffer'.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@497624 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-18 23:03:42,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/XmlInputArchive.java b/src/java/org/apache/hadoop/record/XmlInputArchive.java
index 8f5ba16..3d0891c 100644
--- a/src/java/org/apache/hadoop/record/XmlInputArchive.java
+++ b/src/java/org/apache/hadoop/record/XmlInputArchive.java
@@ -20,7 +20,6 @@
 
 import java.io.InputStream;
 import java.io.IOException;
-import java.io.ByteArrayOutputStream;
 import java.util.ArrayList;
 
 import org.xml.sax.*;
@@ -28,6 +27,7 @@
 import javax.xml.parsers.SAXParserFactory;
 import javax.xml.parsers.ParserConfigurationException;
 import javax.xml.parsers.SAXParser;
+import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.Text;
 /**
  *
@@ -207,7 +207,7 @@
         return Utils.fromXMLString(v.getValue());
     }
     
-    public ByteArrayOutputStream readBuffer(String tag) throws IOException {
+    public BytesWritable readBuffer(String tag) throws IOException {
         Value v = next();
         if (!""string"".equals(v.getType())) {
             throw new IOException(""Error deserializing ""+tag+""."");
"
hadoop,ec7abfae9dd26a2ed5ec3faef85a1e6316c93e91,"HADOOP-735.  Switch generated record code to use BytesWritable to represent fields of type 'buffer'.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@497624 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-18 23:03:42,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/XmlOutputArchive.java b/src/java/org/apache/hadoop/record/XmlOutputArchive.java
index 4221228..5f6e269 100644
--- a/src/java/org/apache/hadoop/record/XmlOutputArchive.java
+++ b/src/java/org/apache/hadoop/record/XmlOutputArchive.java
@@ -19,12 +19,12 @@
 package org.apache.hadoop.record;
 
 import java.io.IOException;
-import java.io.ByteArrayOutputStream;
 import java.util.TreeMap;
 import java.util.ArrayList;
 import java.io.PrintStream;
 import java.io.OutputStream;
 import java.util.Stack;
+import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.Text;
 
 /**
@@ -199,7 +199,7 @@
         printEndEnvelope(tag);
     }
     
-    public void writeBuffer(ByteArrayOutputStream buf, String tag)
+    public void writeBuffer(BytesWritable buf, String tag)
     throws IOException {
         printBeginEnvelope(tag);
         stream.print(""<string>"");
"
hadoop,ec7abfae9dd26a2ed5ec3faef85a1e6316c93e91,"HADOOP-735.  Switch generated record code to use BytesWritable to represent fields of type 'buffer'.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@497624 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-18 23:03:42,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/JBuffer.java b/src/java/org/apache/hadoop/record/compiler/JBuffer.java
index 1628efe..bec1ccd 100644
--- a/src/java/org/apache/hadoop/record/compiler/JBuffer.java
+++ b/src/java/org/apache/hadoop/record/compiler/JBuffer.java
@@ -26,7 +26,7 @@
     
     /** Creates a new instance of JBuffer */
     public JBuffer() {
-        super("" ::std::string"", ""java.io.ByteArrayOutputStream"", ""Buffer"", ""java.io.ByteArrayOutputStream"");
+        super("" ::std::string"", ""BytesWritable"", ""Buffer"", ""BytesWritable"");
     }
     
     public String genCppGetSet(String fname, int fIdx) {
@@ -34,7 +34,7 @@
         cgetFunc += ""    return m""+fname+"";\n"";
         cgetFunc += ""  }\n"";
         String getFunc = ""  virtual ""+getCppType()+""& get""+fname+""() {\n"";
-        getFunc += ""    bs_.set(""+fIdx+"");return m""+fname+"";\n"";
+        getFunc += ""    return m""+fname+"";\n"";
         getFunc += ""  }\n"";
         return cgetFunc + getFunc;
     }
@@ -46,7 +46,7 @@
     public String genJavaReadWrapper(String fname, String tag, boolean decl) {
         String ret = """";
         if (decl) {
-            ret = ""    java.io.ByteArrayOutputStream ""+fname+"";\n"";
+            ret = ""    BytesWritable ""+fname+"";\n"";
         }
         return ret + ""        ""+fname+""=a_.readBuffer(\""""+tag+""\"");\n"";
     }
@@ -58,9 +58,10 @@
     public String genJavaCompareTo(String fname, String other) {
       StringBuffer sb = new StringBuffer();
       sb.append(""    {\n"");
-      sb.append(""      byte[] my = ""+fname+"".toByteArray();\n"");
-      sb.append(""      byte[] ur = ""+other+"".toByteArray();\n"");
-      sb.append(""      ret = WritableComparator.compareBytes(my,0,my.length,ur,0,ur.length);\n"");
+      sb.append(""      byte[] my = ""+fname+"".get();\n"");
+      sb.append(""      byte[] ur = ""+other+"".get();\n"");
+      sb.append(""      ret = WritableComparator.compareBytes(my,0,""+
+          fname+"".getSize(),ur,0,""+other+"".getSize());\n"");
       sb.append(""    }\n"");
       return sb.toString();
     }
@@ -70,11 +71,11 @@
     }
     
     public String genJavaEquals(String fname, String peer) {
-        return ""    ret = org.apache.hadoop.record.Utils.bufEquals(""+fname+"",""+peer+"");\n"";
+        return ""    ret = ""+fname+"".equals(""+peer+"");\n"";
     }
     
     public String genJavaHashCode(String fname) {
-        return ""    ret = ""+fname+"".toString().hashCode();\n"";
+        return ""    ret = ""+fname+"".hashCode();\n"";
     }
     
     public String genJavaSlurpBytes(String b, String s, String l) {
"
hadoop,ec7abfae9dd26a2ed5ec3faef85a1e6316c93e91,"HADOOP-735.  Switch generated record code to use BytesWritable to represent fields of type 'buffer'.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@497624 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-18 23:03:42,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/JCompType.java b/src/java/org/apache/hadoop/record/compiler/JCompType.java
index 986a7cc..73bf9c7 100644
--- a/src/java/org/apache/hadoop/record/compiler/JCompType.java
+++ b/src/java/org/apache/hadoop/record/compiler/JCompType.java
@@ -36,7 +36,7 @@
         cgetFunc += ""    return m""+fname+"";\n"";
         cgetFunc += ""  }\n"";
         String getFunc = ""  virtual ""+getCppType()+""& get""+fname+""() {\n"";
-        getFunc += ""    bs_.set(""+fIdx+"");return m""+fname+"";\n"";
+        getFunc += ""    return m""+fname+"";\n"";
         getFunc += ""  }\n"";
         return cgetFunc + getFunc;
     }
"
hadoop,ec7abfae9dd26a2ed5ec3faef85a1e6316c93e91,"HADOOP-735.  Switch generated record code to use BytesWritable to represent fields of type 'buffer'.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@497624 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-18 23:03:42,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/JRecord.java b/src/java/org/apache/hadoop/record/compiler/JRecord.java
index 3b249d93..a9b391f 100644
--- a/src/java/org/apache/hadoop/record/compiler/JRecord.java
+++ b/src/java/org/apache/hadoop/record/compiler/JRecord.java
@@ -139,13 +139,11 @@
             JField jf = (JField) i.next();
             hh.write(jf.genCppDecl());
         }
-        hh.write(""  mutable std::bitset<""+mFields.size()+""> bs_;\n"");
         hh.write(""public:\n"");
         hh.write(""  virtual void serialize(::hadoop::OArchive& a_, const char* tag) const;\n"");
         hh.write(""  virtual void deserialize(::hadoop::IArchive& a_, const char* tag);\n"");
         hh.write(""  virtual const ::std::string& type() const;\n"");
         hh.write(""  virtual const ::std::string& signature() const;\n"");
-        hh.write(""  virtual bool validate() const;\n"");
         hh.write(""  virtual bool operator<(const ""+getName()+""& peer_) const;\n"");
         hh.write(""  virtual bool operator==(const ""+getName()+""& peer_) const;\n"");
         hh.write(""  virtual ~""+getName()+""() {};\n"");
@@ -159,7 +157,6 @@
             hh.write(""} // end namespace ""+ns[i]+""\n"");
         }
         cc.write(""void ""+getCppFQName()+""::serialize(::hadoop::OArchive& a_, const char* tag) const {\n"");
-        cc.write(""  if (!validate()) throw new ::hadoop::IOException(\""All fields not set.\"");\n"");
         cc.write(""  a_.startRecord(*this,tag);\n"");
         fIdx = 0;
         for (Iterator i = mFields.iterator(); i.hasNext(); fIdx++) {
@@ -170,7 +167,6 @@
             } else {
                 cc.write(""  a_.serialize(""+name+"",\""""+jf.getTag()+""\"");\n"");
             }
-            cc.write(""  bs_.reset(""+fIdx+"");\n"");
         }
         cc.write(""  a_.endRecord(*this,tag);\n"");
         cc.write(""  return;\n"");
@@ -187,23 +183,11 @@
             } else {
                 cc.write(""  a_.deserialize(""+name+"",\""""+jf.getTag()+""\"");\n"");
             }
-            cc.write(""  bs_.set(""+fIdx+"");\n"");
         }
         cc.write(""  a_.endRecord(*this,tag);\n"");
         cc.write(""  return;\n"");
         cc.write(""}\n"");
         
-        cc.write(""bool ""+getCppFQName()+""::validate() const {\n"");
-        cc.write(""  if (bs_.size() != bs_.count()) return false;\n"");
-        for (Iterator i = mFields.iterator(); i.hasNext(); fIdx++) {
-            JField jf = (JField) i.next();
-            JType type = jf.getType();
-            if (type instanceof JRecord) {
-                cc.write(""  if (!""+jf.getName()+"".validate()) return false;\n"");
-            }
-        }
-        cc.write(""  return true;\n"");
-        cc.write(""}\n"");
         
         cc.write(""bool ""+getCppFQName()+""::operator< (const ""+getCppFQName()+""& peer_) const {\n"");
         cc.write(""  return (1\n"");
@@ -261,6 +245,7 @@
         jj.write(""import org.apache.hadoop.io.WritableComparator;\n"");
         jj.write(""import org.apache.hadoop.io.WritableComparable;\n"");
         jj.write(""import org.apache.hadoop.io.WritableUtils;\n"");
+        jj.write(""import org.apache.hadoop.io.BytesWritable;\n"");
         jj.write(""import org.apache.hadoop.io.Text;\n\n"");
         jj.write(""public class ""+getName()+"" implements org.apache.hadoop.record.Record, WritableComparable {\n"");
         jj.write(""  private static final Log LOG= LogFactory.getLog(\""""+
@@ -269,23 +254,17 @@
             JField jf = (JField) i.next();
             jj.write(jf.genJavaDecl());
         }
-        jj.write(""  private java.util.BitSet bs_;\n"");
-        jj.write(""  public ""+getName()+""() {\n"");
-        jj.write(""    bs_ = new java.util.BitSet(""+(mFields.size()+1)+"");\n"");
-        jj.write(""    bs_.set(""+mFields.size()+"");\n"");
-        jj.write(""  }\n"");
+        jj.write(""  public ""+getName()+""() { }\n"");
+        
         
         jj.write(""  public ""+getName()+""(\n"");
         int fIdx = 0;
-        int fLen = mFields.size();
         for (Iterator i = mFields.iterator(); i.hasNext(); fIdx++) {
             JField jf = (JField) i.next();
             jj.write(jf.genJavaConstructorParam(fIdx));
-            jj.write((fLen-1 == fIdx)?"""":"",\n"");
+            jj.write((!i.hasNext())?"""":"",\n"");
         }
         jj.write("") {\n"");
-        jj.write(""    bs_ = new java.util.BitSet(""+(mFields.size()+1)+"");\n"");
-        jj.write(""    bs_.set(""+mFields.size()+"");\n"");
         fIdx = 0;
         for (Iterator i = mFields.iterator(); i.hasNext(); fIdx++) {
             JField jf = (JField) i.next();
@@ -298,24 +277,19 @@
             jj.write(jf.genJavaGetSet(fIdx));
         }
         jj.write(""  public void serialize(org.apache.hadoop.record.OutputArchive a_, String tag) throws java.io.IOException {\n"");
-        jj.write(""    if (!validate()) throw new java.io.IOException(\""All fields not set:\"");\n"");
         jj.write(""    a_.startRecord(this,tag);\n"");
-        fIdx = 0;
-        for (Iterator i = mFields.iterator(); i.hasNext(); fIdx++) {
+        for (Iterator i = mFields.iterator(); i.hasNext();) {
             JField jf = (JField) i.next();
             jj.write(jf.genJavaWriteMethodName());
-            jj.write(""    bs_.clear(""+fIdx+"");\n"");
         }
         jj.write(""    a_.endRecord(this,tag);\n"");
         jj.write(""  }\n"");
         
         jj.write(""  public void deserialize(org.apache.hadoop.record.InputArchive a_, String tag) throws java.io.IOException {\n"");
         jj.write(""    a_.startRecord(tag);\n"");
-        fIdx = 0;
-        for (Iterator i = mFields.iterator(); i.hasNext(); fIdx++) {
+        for (Iterator i = mFields.iterator(); i.hasNext();) {
             JField jf = (JField) i.next();
             jj.write(jf.genJavaReadMethodName());
-            jj.write(""    bs_.set(""+fIdx+"");\n"");
         }
         jj.write(""    a_.endRecord(tag);\n"");
         jj.write(""}\n"");
@@ -335,9 +309,8 @@
         jj.write(""      a_.endRecord(this,\""\"");\n"");
         jj.write(""      return new String(s.toByteArray(), \""UTF-8\"");\n"");
         jj.write(""    } catch (Throwable ex) {\n"");
-        jj.write(""      ex.printStackTrace();\n"");
+        jj.write(""      throw new RuntimeException(ex);\n"");
         jj.write(""    }\n"");
-        jj.write(""    return \""ERROR\"";\n"");
         jj.write(""  }\n"");
         
         jj.write(""  public void write(java.io.DataOutput out) throws java.io.IOException {\n"");
@@ -350,18 +323,6 @@
         jj.write(""    deserialize(archive, \""\"");\n"");
         jj.write(""  }\n"");
         
-        jj.write(""  public boolean validate() {\n"");
-        jj.write(""    if (bs_.cardinality() != bs_.length()) return false;\n"");
-        for (Iterator i = mFields.iterator(); i.hasNext(); fIdx++) {
-            JField jf = (JField) i.next();
-            JType type = jf.getType();
-            if (type instanceof JRecord) {
-                jj.write(""    if (!""+jf.getName()+"".validate()) return false;\n"");
-            }
-        }
-        jj.write(""    return true;\n"");
-        jj.write(""}\n"");
-        
         jj.write(""  public int compareTo (Object peer_) throws ClassCastException {\n"");
         jj.write(""    if (!(peer_ instanceof ""+getName()+"")) {\n"");
         jj.write(""      throw new ClassCastException(\""Comparing different types of records.\"");\n"");
"
hadoop,ec7abfae9dd26a2ed5ec3faef85a1e6316c93e91,"HADOOP-735.  Switch generated record code to use BytesWritable to represent fields of type 'buffer'.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@497624 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-18 23:03:42,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/JType.java b/src/java/org/apache/hadoop/record/compiler/JType.java
index 174eb92..39f8623 100644
--- a/src/java/org/apache/hadoop/record/compiler/JType.java
+++ b/src/java/org/apache/hadoop/record/compiler/JType.java
@@ -61,7 +61,7 @@
         getFunc += ""    return m""+fname+"";\n"";
         getFunc += ""  }\n"";
         String setFunc = ""  virtual void set""+fname+""(""+mCppName+"" m_) {\n"";
-        setFunc += ""    m""+fname+""=m_; bs_.set(""+fIdx+"");\n"";
+        setFunc += ""    m""+fname+""=m_;\n"";
         setFunc += ""  }\n"";
         return getFunc+setFunc;
     }
@@ -71,7 +71,7 @@
         getFunc += ""    return m""+fname+"";\n"";
         getFunc += ""  }\n"";
         String setFunc = ""  public void set""+fname+""(""+mJavaName+"" m_) {\n"";
-        setFunc += ""    m""+fname+""=m_; bs_.set(""+fIdx+"");\n"";
+        setFunc += ""    m""+fname+""=m_;\n"";
         setFunc += ""  }\n"";
         return getFunc+setFunc;
     }
@@ -143,6 +143,6 @@
     }
 
     String genJavaConstructorSet(String fname, int fIdx) {
-        return ""    m""+fname+""=m""+fIdx+""; bs_.set(""+fIdx+"");\n"";
+        return ""    m""+fname+""=m""+fIdx+"";\n"";
     }
 }
"
hadoop,ec7abfae9dd26a2ed5ec3faef85a1e6316c93e91,"HADOOP-735.  Switch generated record code to use BytesWritable to represent fields of type 'buffer'.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@497624 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-18 23:03:42,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java b/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java
index c21508d..3c07617 100644
--- a/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java
+++ b/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java
@@ -52,7 +52,7 @@
   private String language = ""java"";
   private File src;
   private File dest = new File(""."");
-  private final Vector<FileSet> filesets = new Vector();
+  private final Vector<FileSet> filesets = new Vector<FileSet>();
   private boolean failOnError = true;
   
   /** Creates a new instance of RccTask */
"
hadoop,ec7abfae9dd26a2ed5ec3faef85a1e6316c93e91,"HADOOP-735.  Switch generated record code to use BytesWritable to represent fields of type 'buffer'.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@497624 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-18 23:03:42,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/record/test/FromCpp.java b/src/test/org/apache/hadoop/record/test/FromCpp.java
index b4ceacb..b076591 100644
--- a/src/test/org/apache/hadoop/record/test/FromCpp.java
+++ b/src/test/org/apache/hadoop/record/test/FromCpp.java
@@ -26,6 +26,7 @@
 import java.util.ArrayList;
 import java.util.TreeMap;
 import junit.framework.*;
+import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.Text;
 
 /**
@@ -56,7 +57,7 @@
             r1.setIntVal(4567);
             r1.setLongVal(0x5a5a5a5a5a5aL);
             r1.setStringVal(new Text(""random text""));
-            r1.setBufferVal(new ByteArrayOutputStream(20));
+            r1.setBufferVal(new BytesWritable());
             r1.setVectorVal(new ArrayList());
             r1.setMapVal(new TreeMap());
             FileInputStream istream = new FileInputStream(tmpfile);
@@ -82,7 +83,7 @@
             r1.setIntVal(4567);
             r1.setLongVal(0x5a5a5a5a5a5aL);
             r1.setStringVal(new Text(""random text""));
-            r1.setBufferVal(new ByteArrayOutputStream(20));
+            r1.setBufferVal(new BytesWritable());
             r1.setVectorVal(new ArrayList());
             r1.setMapVal(new TreeMap());
             FileInputStream istream = new FileInputStream(tmpfile);
@@ -108,7 +109,7 @@
             r1.setIntVal(4567);
             r1.setLongVal(0x5a5a5a5a5a5aL);
             r1.setStringVal(new Text(""random text""));
-            r1.setBufferVal(new ByteArrayOutputStream(20));
+            r1.setBufferVal(new BytesWritable());
             r1.setVectorVal(new ArrayList());
             r1.setMapVal(new TreeMap());
             FileInputStream istream = new FileInputStream(tmpfile);
"
hadoop,ec7abfae9dd26a2ed5ec3faef85a1e6316c93e91,"HADOOP-735.  Switch generated record code to use BytesWritable to represent fields of type 'buffer'.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@497624 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-18 23:03:42,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/record/test/ToCpp.java b/src/test/org/apache/hadoop/record/test/ToCpp.java
index 3f7596a..3e7d483 100644
--- a/src/test/org/apache/hadoop/record/test/ToCpp.java
+++ b/src/test/org/apache/hadoop/record/test/ToCpp.java
@@ -26,6 +26,7 @@
 import java.io.FileOutputStream;
 import java.util.ArrayList;
 import java.util.TreeMap;
+import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.Text;
 
 /**
@@ -58,7 +59,7 @@
             r1.setIntVal(4567);
             r1.setLongVal(0x5a5a5a5a5a5aL);
             r1.setStringVal(new Text(""random text""));
-            r1.setBufferVal(new ByteArrayOutputStream(20));
+            r1.setBufferVal(new BytesWritable());
             r1.setVectorVal(new ArrayList());
             r1.setMapVal(new TreeMap());
             out.write(r1);
@@ -82,7 +83,7 @@
             r1.setIntVal(4567);
             r1.setLongVal(0x5a5a5a5a5a5aL);
             r1.setStringVal(new Text(""random text""));
-            r1.setBufferVal(new ByteArrayOutputStream(20));
+            r1.setBufferVal(new BytesWritable());
             r1.setVectorVal(new ArrayList());
             r1.setMapVal(new TreeMap());
             out.write(r1);
@@ -106,7 +107,7 @@
             r1.setIntVal(4567);
             r1.setLongVal(0x5a5a5a5a5a5aL);
             r1.setStringVal(new Text(""random text""));
-            r1.setBufferVal(new ByteArrayOutputStream(20));
+            r1.setBufferVal(new BytesWritable());
             r1.setVectorVal(new ArrayList());
             r1.setMapVal(new TreeMap());
             out.write(r1);
"
hadoop,c3618d9b49d63870f4e91ff45bf3a771e7e7556b,"HADOOP-803.  Reduce memory footprint of HDFS namenode by replacing the TreeSet of block locations with an ArrayList.  Contributed by Raghu.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@496845 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-16 20:11:42,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/Block.java b/src/java/org/apache/hadoop/dfs/Block.java
index 0492099..960efec 100644
--- a/src/java/org/apache/hadoop/dfs/Block.java
+++ b/src/java/org/apache/hadoop/dfs/Block.java
@@ -121,18 +121,11 @@
     // Comparable
     /////////////////////////////////////
     public int compareTo(Object o) {
-        Block b = (Block) o;
-        if (getBlockId() < b.getBlockId()) {
-            return -1;
-        } else if (getBlockId() == b.getBlockId()) {
-            return 0;
-        } else {
-            return 1;
-        }
+        long diff = getBlockId() - ((Block)o).getBlockId();
+        return ( diff < 0 ) ? -1 : ( ( diff > 0 ) ? 1 : 0 );
     }
     public boolean equals(Object o) {
-        Block b = (Block) o;
-        return (this.compareTo(b) == 0);
+        return (this.compareTo(o) == 0);
     }
     
     public int hashCode() {
"
hadoop,c3618d9b49d63870f4e91ff45bf3a771e7e7556b,"HADOOP-803.  Reduce memory footprint of HDFS namenode by replacing the TreeSet of block locations with an ArrayList.  Contributed by Raghu.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@496845 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-16 20:11:42,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DatanodeDescriptor.java b/src/java/org/apache/hadoop/dfs/DatanodeDescriptor.java
index 0af5130..c96cb6f 100644
--- a/src/java/org/apache/hadoop/dfs/DatanodeDescriptor.java
+++ b/src/java/org/apache/hadoop/dfs/DatanodeDescriptor.java
@@ -34,7 +34,7 @@
  **************************************************/
 class DatanodeDescriptor extends DatanodeInfo {
 
-  private volatile Collection<Block> blocks = new TreeSet<Block>();
+  private volatile SortedMap<Block, Block> blocks = new TreeMap<Block, Block>();
   // isAlive == heartbeats.contains(this)
   // This is an optimization, because contains takes O(n) time on Arraylist
   protected boolean isAlive = false;
@@ -60,17 +60,12 @@
 
   /**
    */
-  void updateBlocks(Block newBlocks[]) {
-    blocks.clear();
-    for (int i = 0; i < newBlocks.length; i++) {
-      blocks.add(newBlocks[i]);
-    }
-  }
-
-  /**
-   */
   void addBlock(Block b) {
-    blocks.add(b);
+      blocks.put(b, b);
+  }
+  
+  void removeBlock(Block b) {
+      blocks.remove(b);
   }
 
   void resetBlocks() {
@@ -94,10 +89,14 @@
   }
   
   Block[] getBlocks() {
-    return (Block[]) blocks.toArray(new Block[blocks.size()]);
+    return blocks.keySet().toArray(new Block[blocks.size()]);
   }
 
   Iterator<Block> getBlockIterator() {
-    return blocks.iterator();
+    return blocks.keySet().iterator();
+  }
+  
+  Block getBlock(long blockId) {
+      return blocks.get( new Block(blockId, 0) );
   }
 }
"
hadoop,c3618d9b49d63870f4e91ff45bf3a771e7e7556b,"HADOOP-803.  Reduce memory footprint of HDFS namenode by replacing the TreeSet of block locations with an ArrayList.  Contributed by Raghu.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@496845 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-16 20:11:42,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index c48d028..c5fd4e1 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -59,8 +59,8 @@
     // to client-sent information.
     // Mapping: Block -> TreeSet<DatanodeDescriptor>
     //
-    Map<Block, SortedSet<DatanodeDescriptor>> blocksMap = 
-                              new HashMap<Block, SortedSet<DatanodeDescriptor>>();
+    Map<Block, List<DatanodeDescriptor>> blocksMap = 
+                              new HashMap<Block, List<DatanodeDescriptor>>();
 
     /**
      * Stores the datanode -> block map.  
@@ -179,6 +179,8 @@
     private int maxReplicationStreams;
     // MIN_REPLICATION is how many copies we need in place or else we disallow the write
     private int minReplication;
+    // Default replication
+    private int defaultReplication;
     // heartbeatRecheckInterval is how often namenode checks for expired datanodes
     private long heartbeatRecheckInterval;
     // heartbeatExpireInterval is how long namenode waits for datanode to report
@@ -199,6 +201,7 @@
                         int port,
                         NameNode nn, Configuration conf) throws IOException {
         fsNamesystemObject = this;
+        this.defaultReplication = conf.getInt(""dfs.replication"", 3);
         this.maxReplication = conf.getInt(""dfs.replication.max"", 512);
         this.minReplication = conf.getInt(""dfs.replication.min"", 1);
         if( minReplication <= 0 )
@@ -299,7 +302,7 @@
             DatanodeDescriptor machineSets[][] = new DatanodeDescriptor[blocks.length][];
 
             for (int i = 0; i < blocks.length; i++) {
-              SortedSet<DatanodeDescriptor> containingNodes = blocksMap.get(blocks[i]);
+                List<DatanodeDescriptor> containingNodes = blocksMap.get(blocks[i]);
                 if (containingNodes == null) {
                     machineSets[i] = new DatanodeDescriptor[0];
                 } else {
@@ -660,22 +663,16 @@
         //
         // We have the pending blocks, but they won't have
         // length info in them (as they were allocated before
-        // data-write took place).  So we need to add the correct
-        // length info to each
-        //
-        // REMIND - mjc - this is very inefficient!  We should
-        // improve this!
+        // data-write took place). Find the block stored in
+        // node descriptor.
         //
         for (int i = 0; i < nrBlocks; i++) {
             Block b = pendingBlocks[i];
-            SortedSet<DatanodeDescriptor> containingNodes = blocksMap.get(b);
-            DatanodeDescriptor node = containingNodes.first();
-            for (Iterator<Block> it = node.getBlockIterator(); it.hasNext(); ) {
-                Block cur = it.next();
-                if (b.getBlockId() == cur.getBlockId()) {
-                    b.setNumBytes(cur.getNumBytes());
-                    break;
-                }
+            List<DatanodeDescriptor> containingNodes = blocksMap.get(b);
+            Block storedBlock = 
+                containingNodes.get(0).getBlock(b.getBlockId());
+            if ( storedBlock != null ) {
+                pendingBlocks[i] = storedBlock;
             }
         }
         
@@ -716,7 +713,7 @@
         // Now that the file is real, we need to be sure to replicate
         // the blocks.
         for (int i = 0; i < nrBlocks; i++) {
-          SortedSet<DatanodeDescriptor> containingNodes = blocksMap.get(pendingBlocks[i]);
+          List<DatanodeDescriptor> containingNodes = blocksMap.get(pendingBlocks[i]);
           // filter out containingNodes that are marked for decommission.
           int numCurrentReplica = countContainingNodes(containingNodes);
 
@@ -761,7 +758,7 @@
 
         for (Iterator<Block> it = v.getBlocks().iterator(); it.hasNext(); ) {
             Block b = it.next();
-            SortedSet<DatanodeDescriptor> containingNodes = blocksMap.get(b);
+            List<DatanodeDescriptor> containingNodes = blocksMap.get(b);
             if (containingNodes == null || containingNodes.size() < this.minReplication) {
                 return false;
             }
@@ -806,7 +803,7 @@
             for (int i = 0; i < deletedBlocks.length; i++) {
                 Block b = deletedBlocks[i];
 
-                SortedSet<DatanodeDescriptor> containingNodes = blocksMap.get(b);
+                List<DatanodeDescriptor> containingNodes = blocksMap.get(b);
                 if (containingNodes != null) {
                     for (Iterator<DatanodeDescriptor> it = containingNodes.iterator(); it.hasNext(); ) {
                         DatanodeDescriptor node = it.next();
@@ -935,7 +932,7 @@
         } else {
           String hosts[][] = new String[(endBlock - startBlock) + 1][];
             for (int i = startBlock; i <= endBlock; i++) {
-              SortedSet<DatanodeDescriptor> containingNodes = blocksMap.get(blocks[i]);
+                List<DatanodeDescriptor> containingNodes = blocksMap.get(blocks[i]);
                 Collection<String> v = new ArrayList<String>();
                 if (containingNodes != null) {
                   for (Iterator<DatanodeDescriptor> it =containingNodes.iterator(); it.hasNext();) {
@@ -1494,12 +1491,16 @@
         // between the old and new block report.
         //
         int newPos = 0;
-        boolean modified = false;
         Iterator<Block> iter = node.getBlockIterator();
         Block oldblk = iter.hasNext() ? iter.next() : null;
         Block newblk = (newReport != null && newReport.length > 0) ? 
                         newReport[0]	: null;
 
+        // common case is that most of the blocks from the datanode
+        // matches blocks in datanode descriptor.                
+        Collection<Block> toRemove = new LinkedList<Block>();
+        Collection<Block> toAdd = new LinkedList<Block>();
+        
         while (oldblk != null || newblk != null) {
            
             int cmp = (oldblk == null) ? 1 : 
@@ -1513,25 +1514,25 @@
                          ? newReport[newPos] : null;
             } else if (cmp < 0) {
                 // The old report has a block the new one does not
+                toRemove.add(oldblk);
                 removeStoredBlock(oldblk, node);
-                modified = true;
                 oldblk = iter.hasNext() ? iter.next() : null;
             } else {
                 // The new report has a block the old one does not
-                addStoredBlock(newblk, node);
-                modified = true;
+                toAdd.add(addStoredBlock(newblk, node));
                 newPos++;
                 newblk = (newPos < newReport.length)
                          ? newReport[newPos] : null;
             }
         }
-        //
-        // Modify node so it has the new blockreport
-        //
-        if (modified) {
-            node.updateBlocks(newReport);
+        
+        for ( Iterator<Block> i = toRemove.iterator(); i.hasNext(); ) {
+            node.removeBlock( i.next() );
         }
-
+        for ( Iterator<Block> i = toAdd.iterator(); i.hasNext(); ) {
+            node.addBlock( i.next() );
+        }
+        
         //
         // We've now completely updated the node's block report profile.
         // We now go through all its blocks and find which ones are invalid,
@@ -1560,12 +1561,25 @@
     /**
      * Modify (block-->datanode) map.  Remove block from set of 
      * needed replications if this takes care of the problem.
+     * @return the block that is stored in blockMap.
      */
-    synchronized void addStoredBlock(Block block, DatanodeDescriptor node) {
-      SortedSet<DatanodeDescriptor> containingNodes = blocksMap.get(block);
+    synchronized Block addStoredBlock(Block block, DatanodeDescriptor node) {
+        List<DatanodeDescriptor> containingNodes = blocksMap.get(block);
         if (containingNodes == null) {
-            containingNodes = new TreeSet<DatanodeDescriptor>();
+            //Create an arraylist with the current replication factor
+            FSDirectory.INode inode = dir.getFileByBlock(block);
+            int replication = (inode != null) ? 
+                              inode.getReplication() : defaultReplication;
+            containingNodes = new ArrayList<DatanodeDescriptor>(replication);
             blocksMap.put(block, containingNodes);
+        } else {
+            Block storedBlock = 
+                containingNodes.get(0).getBlock(block.getBlockId());
+            // update stored block's length.
+            if ( block.getNumBytes() > 0 ) {
+                storedBlock.setNumBytes( block.getNumBytes() );
+            }
+            block = storedBlock;
         }
         if (! containingNodes.contains(node)) {
             containingNodes.add(node);
@@ -1587,7 +1601,7 @@
         synchronized (neededReplications) {
             FSDirectory.INode fileINode = dir.getFileByBlock(block);
             if( fileINode == null )  // block does not belong to any file
-                return;
+                return block;
 
             // filter out containingNodes that are marked for decommission.
             int numCurrentReplica = countContainingNodes(containingNodes);
@@ -1612,6 +1626,7 @@
 
             proccessOverReplicatedBlock( block, fileReplication );
         }
+        return block;
     }
     
     /**
@@ -1620,7 +1635,7 @@
      * mark them in the excessReplicateMap.
      */
     private void proccessOverReplicatedBlock( Block block, short replication ) {
-      SortedSet<DatanodeDescriptor> containingNodes = blocksMap.get(block);
+      List<DatanodeDescriptor> containingNodes = blocksMap.get(block);
       if( containingNodes == null )
         return;
       Collection<DatanodeDescriptor> nonExcess = new ArrayList<DatanodeDescriptor>();
@@ -1700,7 +1715,7 @@
     synchronized void removeStoredBlock(Block block, DatanodeDescriptor node) {
         NameNode.stateChangeLog.debug(""BLOCK* NameSystem.removeStoredBlock: ""
                 +block.getBlockName() + "" from ""+node.getName() );
-        SortedSet<DatanodeDescriptor> containingNodes = blocksMap.get(block);
+        List<DatanodeDescriptor> containingNodes = blocksMap.get(block);
         if (containingNodes == null || ! containingNodes.contains(node)) {
           NameNode.stateChangeLog.debug(""BLOCK* NameSystem.removeStoredBlock: ""
             +block.getBlockName()+"" has already been removed from node ""+node );
@@ -1759,14 +1774,9 @@
         NameNode.stateChangeLog.debug(""BLOCK* NameSystem.blockReceived: ""
                 +block.getBlockName()+"" is received from "" + nodeID.getName() );
         //
-        // Modify the blocks->datanode map
+        // Modify the blocks->datanode map and node's map.
         // 
-        addStoredBlock(block, node);
-
-        //
-        // Supplement node's blockreport
-        //
-        node.addBlock(block);
+        node.addBlock( addStoredBlock(block, node) );
     }
 
     /**
"
hadoop,362a5a7288770f6891ebc416e13d2aa5a794b704,"HADOOP-757.  Fix 'Bad File Descriptor' exception in HDFS client when an output file is closed twice.  Contributed by Raghu.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@496844 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-16 20:02:10,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSClient.java b/src/java/org/apache/hadoop/dfs/DFSClient.java
index e42a513..b641975 100644
--- a/src/java/org/apache/hadoop/dfs/DFSClient.java
+++ b/src/java/org/apache/hadoop/dfs/DFSClient.java
@@ -936,6 +936,25 @@
             }
         }
 
+        /* Wrapper for closing backupStream. This sets backupStream to null so
+         * that we do not attempt to write to backupStream that could be
+         * invalid in subsequent writes. Otherwise we might end trying to write
+         * filedescriptor that we don't own.
+         */
+        private void closeBackupStream() throws IOException {
+          OutputStream stream = backupStream;
+          backupStream = null;
+          stream.close();
+        }
+        /* Similar to closeBackupStream(). Theoritically deleting a file
+         * twice could result in deleting a file that we should not.
+         */
+        private void deleteBackupFile() {
+          File file = backupFile;
+          backupFile = null;
+          file.delete();
+        }
+        
         private File newBackupFile() throws IOException {
           File result = conf.getFile(""dfs.client.buffer.dir"",
                                      ""tmp""+File.separator+
@@ -1147,6 +1166,10 @@
             int workingPos = Math.min(pos, maxPos);
             
             if (workingPos > 0) {
+                if ( backupStream == null ) {
+                    throw new IOException( ""Trying to write to backupStream "" +
+                                           ""but it already closed or not open"");
+                }
                 //
                 // To the local block backup, write just the bytes
                 //
@@ -1168,7 +1191,7 @@
             //
             // Done with local copy
             //
-            backupStream.close();
+            closeBackupStream();
 
             //
             // Send it to datanode
@@ -1204,10 +1227,11 @@
             //
             // Delete local backup, start new one
             //
-            backupFile.delete();
-            backupFile = newBackupFile();
-            backupStream = new FileOutputStream(backupFile);
+            deleteBackupFile();
+            File tmpFile = newBackupFile();
             bytesWrittenToBlock = 0;
+            backupStream = new FileOutputStream(tmpFile);
+            backupFile = tmpFile;
         }
 
         /**
@@ -1273,8 +1297,12 @@
               }
             }
             
-            backupStream.close();
-            backupFile.delete();
+            if ( backupStream != null ) {
+              closeBackupStream();
+            }
+            if ( backupFile != null ) {
+              deleteBackupFile();
+            }
 
             if (s != null) {
                 s.close();
"
hadoop,282f31b3453e78bcadb0c5a562dcc3698c996315,"HADOOP-781.  Remove methods deprecated in 0.10 that are no longer widely used.  (cutting)

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@496823 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-16 19:13:01,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FileSystem.java b/src/java/org/apache/hadoop/fs/FileSystem.java
index e47bb56..6aa2f80 100644
--- a/src/java/org/apache/hadoop/fs/FileSystem.java
+++ b/src/java/org/apache/hadoop/fs/FileSystem.java
@@ -73,7 +73,7 @@
             fs = new DistributedFileSystem(addr, conf);
         } else if (""-local"".equals(cmd)) {
             i++;
-            fs = new LocalFileSystem(conf);
+            fs = FileSystem.getLocal(conf);
         } else {
             fs = get(conf);                          // using default
             LOG.info(""No FS indicated, using default:""+fs.getName());
@@ -210,11 +210,6 @@
     // FileSystem
     ///////////////////////////////////////////////////////////////
 
-    /** @deprecated */
-    protected FileSystem(Configuration conf) {
-      super(conf);
-    }
-
     protected FileSystem() {
       super(null);
     }
"
hadoop,282f31b3453e78bcadb0c5a562dcc3698c996315,"HADOOP-781.  Remove methods deprecated in 0.10 that are no longer widely used.  (cutting)

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@496823 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-16 19:13:01,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/LocalFileSystem.java b/src/java/org/apache/hadoop/fs/LocalFileSystem.java
index 4771acf..f9b0dde 100644
--- a/src/java/org/apache/hadoop/fs/LocalFileSystem.java
+++ b/src/java/org/apache/hadoop/fs/LocalFileSystem.java
@@ -44,11 +44,6 @@
     
     public LocalFileSystem() {}
 
-    /** @deprecated */
-    public LocalFileSystem(Configuration conf) throws IOException {
-      initialize(NAME, conf);
-    }
-
     /**
      * Return 1x1 'localhost' cell if the file exists.
      * Return null if otherwise.
"
hadoop,282f31b3453e78bcadb0c5a562dcc3698c996315,"HADOOP-781.  Remove methods deprecated in 0.10 that are no longer widely used.  (cutting)

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@496823 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-16 19:13:01,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/MapFile.java b/src/java/org/apache/hadoop/io/MapFile.java
index d65f389..efb0005 100644
--- a/src/java/org/apache/hadoop/io/MapFile.java
+++ b/src/java/org/apache/hadoop/io/MapFile.java
@@ -67,16 +67,6 @@
     private WritableComparable lastKey;
 
 
-    /** Create the named map for keys of the named class.
-     * @deprecated
-     */
-    public Writer(FileSystem fs, String dirName,
-                  Class keyClass, Class valClass)
-      throws IOException {
-      this(new Configuration(), fs, dirName,
-           WritableComparator.get(keyClass), valClass);
-    }
-
     /** Create the named map for keys of the named class. */
     public Writer(Configuration conf, FileSystem fs, String dirName,
                   Class keyClass, Class valClass)
@@ -508,7 +498,7 @@
 
     Configuration conf = new Configuration();
     int ioFileBufferSize = conf.getInt(""io.file.buffer.size"", 4096);
-    FileSystem fs = new LocalFileSystem(conf);
+    FileSystem fs = FileSystem.getLocal(conf);
     MapFile.Reader reader = new MapFile.Reader(fs, in, conf);
     MapFile.Writer writer =
       new MapFile.Writer(conf, fs, out, reader.getKeyClass(), reader.getValueClass());
"
hadoop,282f31b3453e78bcadb0c5a562dcc3698c996315,"HADOOP-781.  Remove methods deprecated in 0.10 that are no longer widely used.  (cutting)

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@496823 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-16 19:13:01,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/SetFile.java b/src/java/org/apache/hadoop/io/SetFile.java
index 371d8d7..e34ed1a 100644
--- a/src/java/org/apache/hadoop/io/SetFile.java
+++ b/src/java/org/apache/hadoop/io/SetFile.java
@@ -29,20 +29,14 @@
 
   protected SetFile() {}                            // no public ctor
 
-  /** Write a new set file. */
+  /** Write a new set file.
+   * @deprecated pass a Configuration too
+   */
   public static class Writer extends MapFile.Writer {
 
     /** Create the named set for keys of the named class. */
     public Writer(FileSystem fs, String dirName, Class keyClass) throws IOException {
-      super(fs, dirName, keyClass, NullWritable.class);
-    }
-
-    /** Create the named set using the named key comparator.
-     * @deprecated
-     */
-    public Writer(FileSystem fs, String dirName, WritableComparator comparator)
-      throws IOException {
-      super(new Configuration(), fs, dirName, comparator, NullWritable.class);
+      super(new Configuration(), fs, dirName, keyClass, NullWritable.class);
     }
 
     /** Create a set naming the element class and compression type. */
"
hadoop,282f31b3453e78bcadb0c5a562dcc3698c996315,"HADOOP-781.  Remove methods deprecated in 0.10 that are no longer widely used.  (cutting)

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@496823 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-16 19:13:01,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/PhasedFileSystem.java b/src/java/org/apache/hadoop/mapred/PhasedFileSystem.java
index ba31cce..16bda88 100644
--- a/src/java/org/apache/hadoop/mapred/PhasedFileSystem.java
+++ b/src/java/org/apache/hadoop/mapred/PhasedFileSystem.java
@@ -50,8 +50,6 @@
    */
   public PhasedFileSystem(FileSystem fs, String jobid, 
       String tipid, String taskid) {
-    super(fs.getConf()); // not used
-    
     this.baseFS = fs ; 
     this.jobid = jobid; 
     this.tipid = tipid ; 
@@ -66,8 +64,6 @@
    * @param conf JobConf
    */
   public PhasedFileSystem(FileSystem fs, JobConf conf) {
-    super(fs.getConf()); // not used
-    
     this.baseFS = fs ; 
     this.jobid = conf.get(""mapred.job.id""); 
     this.tipid = conf.get(""mapred.tip.id""); 
@@ -80,7 +76,6 @@
    * @param conf
    */
   protected PhasedFileSystem(Configuration conf){
-    super(conf);
     throw new UnsupportedOperationException(""Operation not supported""); 
   }
   
"
hadoop,282f31b3453e78bcadb0c5a562dcc3698c996315,"HADOOP-781.  Remove methods deprecated in 0.10 that are no longer widely used.  (cutting)

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@496823 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-16 19:13:01,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTrackerStatus.java b/src/java/org/apache/hadoop/mapred/TaskTrackerStatus.java
index 9430ee3..336f908 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTrackerStatus.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTrackerStatus.java
@@ -95,17 +95,6 @@
     }
     
     /**
-     * All current tasks at the TaskTracker.  
-     *
-     * Tasks are tracked by a TaskStatus object.
-     * 
-     * @deprecated use {@link #getTaskReports()} instead
-     */
-    public Iterator taskReports() {
-        return taskReports.iterator();
-    }
-
-    /**
      * Get the current tasks at the TaskTracker.
      * Tasks are tracked by a {@link TaskStatus} object.
      * 
"
hadoop,31a595da863422040fde6b7996cff4c57f5c5539,"HADOOP-600.  Fix a race condition in the JobTracker.  Contributed by Arun.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@494172 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-08 19:42:24,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index 3767767..9e5385d 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -221,36 +221,45 @@
                 //
                 // Loop through all expired items in the queue
                 //
-                synchronized (taskTrackers) {
+                // Need to lock the JobTracker here since we are
+                // manipulating it's data-structures via
+                // ExpireTrackers.run -> JobTracker.lostTaskTracker ->
+                // JobInProgress.failedTask -> JobTracker.markCompleteTaskAttempt
+                // Also need to lock JobTracker before locking 'taskTracker' &
+                // 'trackerExpiryQueue' to prevent deadlock:
+                // @see {@link JobTracker.processHeartbeat(TaskTrackerStatus, boolean)} 
+                synchronized (JobTracker.this) {
+                  synchronized (taskTrackers) {
                     synchronized (trackerExpiryQueue) {
-                        long now = System.currentTimeMillis();
-                        TaskTrackerStatus leastRecent = null;
-                        while ((trackerExpiryQueue.size() > 0) &&
-                               ((leastRecent = (TaskTrackerStatus) trackerExpiryQueue.first()) != null) &&
-                               (now - leastRecent.getLastSeen() > TASKTRACKER_EXPIRY_INTERVAL)) {
-
-                            // Remove profile from head of queue
-                            trackerExpiryQueue.remove(leastRecent);
-                            String trackerName = leastRecent.getTrackerName();
-
-                            // Figure out if last-seen time should be updated, or if tracker is dead
-                            TaskTrackerStatus newProfile = (TaskTrackerStatus) taskTrackers.get(leastRecent.getTrackerName());
-                            // Items might leave the taskTracker set through other means; the
-                            // status stored in 'taskTrackers' might be null, which means the
-                            // tracker has already been destroyed.
-                            if (newProfile != null) {
-                                if (now - newProfile.getLastSeen() > TASKTRACKER_EXPIRY_INTERVAL) {
-                                    // Remove completely
-                                    updateTaskTrackerStatus(trackerName, null);
-                                    lostTaskTracker(leastRecent.getTrackerName(),
-                                                    leastRecent.getHost());
-                                } else {
-                                    // Update time by inserting latest profile
-                                    trackerExpiryQueue.add(newProfile);
-                                }
-                            }
+                      long now = System.currentTimeMillis();
+                      TaskTrackerStatus leastRecent = null;
+                      while ((trackerExpiryQueue.size() > 0) &&
+                              ((leastRecent = (TaskTrackerStatus) trackerExpiryQueue.first()) != null) &&
+                              (now - leastRecent.getLastSeen() > TASKTRACKER_EXPIRY_INTERVAL)) {
+                        
+                        // Remove profile from head of queue
+                        trackerExpiryQueue.remove(leastRecent);
+                        String trackerName = leastRecent.getTrackerName();
+                        
+                        // Figure out if last-seen time should be updated, or if tracker is dead
+                        TaskTrackerStatus newProfile = (TaskTrackerStatus) taskTrackers.get(leastRecent.getTrackerName());
+                        // Items might leave the taskTracker set through other means; the
+                        // status stored in 'taskTrackers' might be null, which means the
+                        // tracker has already been destroyed.
+                        if (newProfile != null) {
+                          if (now - newProfile.getLastSeen() > TASKTRACKER_EXPIRY_INTERVAL) {
+                            // Remove completely
+                            updateTaskTrackerStatus(trackerName, null);
+                            lostTaskTracker(leastRecent.getTrackerName(),
+                                    leastRecent.getHost());
+                          } else {
+                            // Update time by inserting latest profile
+                            trackerExpiryQueue.add(newProfile);
+                          }
                         }
+                      }
                     }
+                  }
                 }
               } catch (Exception t) {
                 LOG.error(""Tracker Expiry Thread got exception: "" +
"
hadoop,4427f66fbb98c1bf15c6200164264ada9ddb5260,"HADOOP-628.  Fix a problem with 'fs -cat' command.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@492276 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-03 20:06:25,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FsShell.java b/src/java/org/apache/hadoop/fs/FsShell.java
index 771c800..52c8a88 100644
--- a/src/java/org/apache/hadoop/fs/FsShell.java
+++ b/src/java/org/apache/hadoop/fs/FsShell.java
@@ -40,6 +40,22 @@
     }
 
     /**
+     * Copies from one stream to another.
+     */
+    private void copyBytes(InputStream in, OutputStream out) throws IOException {
+      PrintStream ps = out instanceof PrintStream ? (PrintStream)out : null;
+      byte buf[] = new byte[conf.getInt(""io.file.buffer.size"", 4096)];
+      int bytesRead = in.read(buf);
+      while (bytesRead >= 0) {
+        out.write(buf, 0, bytesRead);
+        if ((ps != null) && ps.checkError()) {
+          throw new IOException(""Unable to write to output stream."");
+        }
+        bytesRead = in.read(buf);
+      }
+    }
+      
+    /**
      * Copies from stdin to the indicated file.
      */
     private void copyFromStdin(Path dst) throws IOException {
@@ -50,18 +66,29 @@
         throw new IOException(""Target "" + dst.toString() + "" already exists."");
       }
       FSDataOutputStream out = fs.create(dst); 
-      byte buf[] = new byte[conf.getInt(""io.file.buffer.size"", 4096)];
       try {
-        int bytesRead = System.in.read(buf);
-        while (bytesRead >= 0) {
-          out.write(buf, 0, bytesRead);
-          bytesRead = System.in.read(buf);
-        }
+        copyBytes(System.in, out);
       } finally {
         out.close();
       }
     }
 
+    /** 
+     * Print from src to stdout.
+     */
+    private void printToStdout(Path src) throws IOException {
+      if (fs.isDirectory(src)) {
+        throw new IOException(""Source must be a file."");
+      }
+      FSDataInputStream in = fs.open(src);
+      try {
+        copyBytes(in, System.out);
+      } finally {
+        in.close();
+      }
+
+    }
+
     /**
      * Add a local file to the indicated FileSystem name. src is kept.
      */
@@ -178,38 +205,10 @@
     void cat(String srcf) throws IOException {
       Path [] srcs = fs.globPaths( new Path( srcf ) );
       for( int i=0; i<srcs.length; i++ ) {
-        cat(srcs[i]);
+        printToStdout(srcs[i]);
       }
     }
     
-
-    /* print the content of src to screen */
-    private void cat(Path src) throws IOException {
-      FSDataInputStream in = fs.open(src);
-      try {
-        BufferedReader din = new BufferedReader(new InputStreamReader(in));
-        String line;
-        int checkFactor = 0;
-        while((line = din.readLine()) != null) {
-          System.out.println(line);      
-
-          //
-          // Peridically check if the output encountered an error. This can
-          // happen if the output stream has been disconnected
-          //
-          if (checkFactor == 0) {
-            if (System.out.checkError()) {
-              throw new IOException(""Unable to write to output stream"");
-            }
-            checkFactor = 10000;
-          }
-          checkFactor--;
-        }
-      } finally {
-        in.close();
-      }
-    }
-
     /**
      * Parse the incoming command string
      * @param cmd
"
hadoop,2c1fda8a6107220bb5b0fb82566bb30d904426a5,"HADOOP-745.  Fix a synchronization bug in the HDFS namenode.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@492248 13f79535-47bb-0310-9956-ffa450edef68
",2007-01-03 18:47:48,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index fb7e48e..0f0a76c 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -780,7 +780,7 @@
     /**
      * Change the indicated filename.
      */
-    public boolean renameTo(UTF8 src, UTF8 dst) throws IOException {
+    public synchronized boolean renameTo(UTF8 src, UTF8 dst) throws IOException {
         NameNode.stateChangeLog.debug(""DIR* NameSystem.renameTo: "" + src + "" to "" + dst );
         if( isInSafeMode() )
           throw new SafeModeException( ""Cannot rename "" + src, safeMode );
@@ -869,7 +869,7 @@
     /**
      * Create all the necessary directories
      */
-    public boolean mkdirs( String src ) throws IOException {
+    public synchronized boolean mkdirs( String src ) throws IOException {
         boolean    success;
         NameNode.stateChangeLog.debug(""DIR* NameSystem.mkdirs: "" + src );
         if( isInSafeMode() )
"
hadoop,1d54b0c2e33a91c021e00c922452c0c67b5849f9,"HADOOP-738.  Extend FsShell '-put' and '-get' commands to accept standard input and output, respectively.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@489175 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-20 19:28:26,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FsShell.java b/src/java/org/apache/hadoop/fs/FsShell.java
index e300814..d563138 100644
--- a/src/java/org/apache/hadoop/fs/FsShell.java
+++ b/src/java/org/apache/hadoop/fs/FsShell.java
@@ -38,11 +38,39 @@
         conf.setQuietMode(true);
         this.fs = FileSystem.get(conf);
     }
+
+    /**
+     * Copies from stdin to the indicated file.
+     */
+    private void copyFromStdin(Path dst) throws IOException {
+      if (fs.isDirectory(dst)) {
+        throw new IOException(""When source is stdin, destination must be a file."");
+      }
+      if (fs.exists(dst)) {
+        throw new IOException(""Target "" + dst.toString() + "" already exists."");
+      }
+      FSDataOutputStream out = fs.create(dst); 
+      byte buf[] = new byte[conf.getInt(""io.file.buffer.size"", 4096)];
+      try {
+        int bytesRead = System.in.read(buf);
+        while (bytesRead >= 0) {
+          out.write(buf, 0, bytesRead);
+          bytesRead = System.in.read(buf);
+        }
+      } finally {
+        out.close();
+      }
+    }
+
     /**
      * Add a local file to the indicated FileSystem name. src is kept.
      */
     void copyFromLocal(Path src, String dstf) throws IOException {
+      if (src.toString().equals(""-"")) {
+        copyFromStdin(new Path(dstf));
+      } else {
         fs.copyFromLocalFile(src, new Path(dstf));
+      }
     }
 
     /**
@@ -74,14 +102,21 @@
       }
       String srcf = argv[pos++];
       String dstf = argv[pos++];
-      Path [] srcs = fs.globPaths( new Path(srcf) );
-      if( srcs.length > 1 && !new File( dstf ).isDirectory()) {
-        throw new IOException( ""When copy multiple files, "" 
-            + ""destination should be a directory."" );
-      }
-      Path dst = new Path( dstf );
-      for( int i=0; i<srcs.length; i++ ) {
-        fs.copyToLocalFile( srcs[i], dst, copyCrc );
+      if( dstf.equals(""-"")) {
+        if (copyCrc) {
+          System.err.println(""-crc option is not valid when destination is stdout."");
+        }
+        cat(srcf);
+      } else {
+        Path [] srcs = fs.globPaths( new Path(srcf) );
+        if( srcs.length > 1 && !new File( dstf ).isDirectory()) {
+          throw new IOException( ""When copying multiple files, "" 
+                                 + ""destination should be a directory."" );
+        }
+        Path dst = new Path( dstf );
+        for( int i=0; i<srcs.length; i++ ) {
+          fs.copyToLocalFile( srcs[i], dst, copyCrc );
+        }
       }
     }
     
"
hadoop,93bfdff596fe4a083275639e3e7a04fb3555bf90,"HADOOP-596.  Fix a bug in phase reporting during reduce.  Contributed by Sanjay.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@488402 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-18 20:10:44,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 3653da3..ae5acdb 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -1040,6 +1040,7 @@
               }else if( newPhase == TaskStatus.Phase.REDUCE){
                 this.taskStatus.setSortFinishTime(System.currentTimeMillis());
               }
+              this.taskStatus.setPhase(newPhase);
             }
             this.taskStatus.setStateString(state);
         }
"
hadoop,56b23dc7b0f5f3519037b803f3d208cbd21a5b83,"Merging changes from HADOOP-639, HADOOP-791 and HADOOP-827 into 0.9 branch, preparing for 0.9.2 release.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/branches/branch-0.9@487697 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-15 22:35:51,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java b/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java
index 9fe06fd..1f91e64 100644
--- a/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java
+++ b/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java
@@ -27,31 +27,40 @@
  * The JobTracker is the Server, which implements this protocol.
  */ 
 interface InterTrackerProtocol extends VersionedProtocol {
-  // version 2 introduced to replace TaskStatus.State with an enum
-  public static final long versionID = 2L;
+  /**
+   * version 3 introduced to replace 
+   * emitHearbeat/pollForNewTask/pollForTaskWithClosedJob with
+   * {@link #heartbeat(TaskTrackerStatus, boolean, boolean, short)}
+   */
+  public static final long versionID = 3L;
   
   public final static int TRACKERS_OK = 0;
   public final static int UNKNOWN_TASKTRACKER = 1;
 
-  /** 
-   * Called regularly by the task tracker to update the status of its tasks
-   * within the job tracker.  JobTracker responds with a code that tells the 
-   * TaskTracker whether all is well.
-   *
-   * TaskTracker must also indicate whether this is the first interaction
-   * (since state refresh)
+  /**
+   * Called regularly by the {@link TaskTracker} to update the status of its 
+   * tasks within the job tracker. {@link JobTracker} responds with a 
+   * {@link HeartbeatResponse} that directs the 
+   * {@link TaskTracker} to undertake a series of 'actions' 
+   * (see {@link org.apache.hadoop.mapred.TaskTrackerAction.ActionType}).  
+   * 
+   * {@link TaskTracker} must also indicate whether this is the first 
+   * interaction (since state refresh) and acknowledge the last response
+   * it recieved from the {@link JobTracker} 
+   * 
+   * @param status the status update
+   * @param initialContact <code>true</code> if this is first interaction since
+   *                       'refresh', <code>false</code> otherwise.
+   * @param acceptNewTasks <code>true</code> if the {@link TaskTracker} is
+   *                       ready to accept new tasks to run.                 
+   * @param responseId the last responseId successfully acted upon by the
+   *                   {@link TaskTracker}.
+   * @return a {@link org.apache.hadoop.mapred.HeartbeatResponse} with 
+   *         fresh instructions.
    */
-  int emitHeartbeat(TaskTrackerStatus status, 
-                    boolean initialContact) throws IOException;
-
-  /** Called to get new tasks from from the job tracker for this tracker.*/
-  Task pollForNewTask(String trackerName) throws IOException;
-
-  /** Called to find which tasks that have been run by this tracker should now
-   * be closed because their job is complete.  This is used to, e.g., 
-   * notify a map task that its output is no longer needed and may 
-   * be removed. */
-  String[] pollForTaskWithClosedJob(String trackerName) throws IOException;
+  HeartbeatResponse heartbeat(TaskTrackerStatus status, 
+          boolean initialContact, boolean acceptNewTasks, short responseId)
+  throws IOException;
 
   /** Called by a reduce task to find which map tasks are completed.
    *
"
hadoop,56b23dc7b0f5f3519037b803f3d208cbd21a5b83,"Merging changes from HADOOP-639, HADOOP-791 and HADOOP-827 into 0.9 branch, preparing for 0.9.2 release.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/branches/branch-0.9@487697 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-15 22:35:51,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index 109eccd..5e03d2d 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -424,6 +424,9 @@
     // (trackerID->TreeSet of taskids running at that tracker)
     TreeMap trackerToTaskMap = new TreeMap();
 
+    // (trackerID --> last sent HeartBeatResponseID)
+    Map<String, Short> trackerToHeartbeatResponseIDMap = new TreeMap();
+    
     //
     // Watch and expire TaskTracker objects using these structures.
     // We can map from Name->TaskTrackerStatus, or we can expire by time.
@@ -720,6 +723,74 @@
     ////////////////////////////////////////////////////
 
     /**
+     * The periodic heartbeat mechanism between the {@link TaskTracker} and
+     * the {@link JobTracker}.
+     * 
+     * The {@link JobTracker} processes the status information sent by the 
+     * {@link TaskTracker} and responds with instructions to start/stop 
+     * tasks or jobs, and also 'reset' instructions during contingencies. 
+     */
+    public synchronized HeartbeatResponse heartbeat(TaskTrackerStatus status, 
+            boolean initialContact, boolean acceptNewTasks, short responseId) 
+    throws IOException {
+      LOG.debug(""Got heartbeat from: "" + status.getTrackerName() + 
+              "" (initialContact: "" + initialContact + 
+              "" acceptNewTasks: "" + acceptNewTasks + "")"" +
+              "" with responseId: "" + responseId);
+      
+        // First check if the last heartbeat response got through 
+        String trackerName = status.getTrackerName();
+        Short oldResponseId = trackerToHeartbeatResponseIDMap.get(trackerName);
+      
+        short newResponseId = (short)(responseId + 1);
+        if (!initialContact && oldResponseId != null && 
+                oldResponseId.shortValue() != responseId) {
+            newResponseId = oldResponseId.shortValue();
+        }
+      
+        // Process this heartbeat 
+        if (!processHeartbeat(status, initialContact, 
+                (newResponseId != responseId))) {
+            if (oldResponseId != null) {
+                trackerToHeartbeatResponseIDMap.remove(trackerName);
+            }
+
+            return new HeartbeatResponse(newResponseId, 
+                  new TaskTrackerAction[] {new ReinitTrackerAction()});
+        }
+      
+        // Initialize the response to be sent for the heartbeat
+        HeartbeatResponse response = new HeartbeatResponse(newResponseId, null);
+        List<TaskTrackerAction> actions = new ArrayList();
+      
+        // Check for new tasks to be executed on the tasktracker
+        if (acceptNewTasks) {
+        Task task = getNewTaskForTaskTracker(trackerName);
+            if (task != null) {
+                LOG.debug(trackerName + "" -> LaunchTask: "" + task.getTaskId());
+                actions.add(new LaunchTaskAction(task));
+            }
+        }
+      
+        // Check for tasks to be killed
+        List<TaskTrackerAction> killTasksList = getTasksToKill(trackerName);
+        if (killTasksList != null) {
+            actions.addAll(killTasksList);
+        }
+     
+        response.setActions(
+                actions.toArray(new TaskTrackerAction[actions.size()]));
+        
+        // Update the trackerToHeartbeatResponseIDMap
+        if (newResponseId != responseId) {
+            trackerToHeartbeatResponseIDMap.put(trackerName, 
+                    new Short(newResponseId));
+        }
+
+        return response;
+    }
+    
+    /**
      * Update the last recorded status for the given task tracker.
      * It assumes that the taskTrackers are locked on entry.
      * @author Owen O'Malley
@@ -749,16 +820,21 @@
     /**
      * Process incoming heartbeat messages from the task trackers.
      */
-    public synchronized int emitHeartbeat(TaskTrackerStatus trackerStatus, boolean initialContact) {
+    private synchronized boolean processHeartbeat(
+            TaskTrackerStatus trackerStatus, 
+            boolean initialContact, boolean updateStatusTimestamp) {
         String trackerName = trackerStatus.getTrackerName();
-        trackerStatus.setLastSeen(System.currentTimeMillis());
+        if (initialContact || updateStatusTimestamp) {
+          trackerStatus.setLastSeen(System.currentTimeMillis());
+        }
 
         synchronized (taskTrackers) {
             synchronized (trackerExpiryQueue) {
                 boolean seenBefore = updateTaskTrackerStatus(trackerName,
                                                              trackerStatus);
                 if (initialContact) {
-                    // If it's first contact, then clear out any state hanging around
+                    // If it's first contact, then clear out 
+                    // any state hanging around
                     if (seenBefore) {
                         lostTaskTracker(trackerName, trackerStatus.getHost());
                     }
@@ -767,7 +843,7 @@
                     if (!seenBefore) {
                         LOG.warn(""Status from unknown Tracker : "" + trackerName);
                         taskTrackers.remove(trackerName); 
-                        return InterTrackerProtocol.UNKNOWN_TASKTRACKER;
+                        return false;
                     }
                 }
 
@@ -779,18 +855,17 @@
 
         updateTaskStatuses(trackerStatus);
         //LOG.info(""Got heartbeat from ""+trackerName);
-        return InterTrackerProtocol.TRACKERS_OK;
+        return true;
     }
 
     /**
-     * A tracker wants to know if there's a Task to run.  Returns
-     * a task we'd like the TaskTracker to execute right now.
+     * Returns a task we'd like the TaskTracker to execute right now.
      *
      * Eventually this function should compute load on the various TaskTrackers,
      * and incorporate knowledge of DFS file placement.  But for right now, it
      * just grabs a single item out of the pending task list and hands it back.
      */
-    public synchronized Task pollForNewTask(String taskTracker) {
+    private synchronized Task getNewTaskForTaskTracker(String taskTracker) {
         //
         // Compute average map and reduce task numbers across pool
         //
@@ -933,23 +1008,36 @@
      * A tracker wants to know if any of its Tasks have been
      * closed (because the job completed, whether successfully or not)
      */
-    public synchronized String[] pollForTaskWithClosedJob(String taskTracker) {
-        TreeSet taskIds = (TreeSet) trackerToTaskMap.get(taskTracker);
+    private synchronized List getTasksToKill(String taskTracker) {
+        Set<String> taskIds = (TreeSet) trackerToTaskMap.get(taskTracker);
         if (taskIds != null) {
-            ArrayList list = new ArrayList();
-            for (Iterator it = taskIds.iterator(); it.hasNext(); ) {
-                String taskId = (String) it.next();
-                TaskInProgress tip = (TaskInProgress) taskidToTIPMap.get(taskId);
-                if (tip.shouldCloseForClosedJob(taskId)) {
+            List<TaskTrackerAction> killList = new ArrayList();
+            Set<String> killJobIds = new TreeSet(); 
+            for (String killTaskId : taskIds ) {
+                TaskInProgress tip = (TaskInProgress) taskidToTIPMap.get(killTaskId);
+                if (tip.shouldCloseForClosedJob(killTaskId)) {
                     // 
                     // This is how the JobTracker ends a task at the TaskTracker.
                     // It may be successfully completed, or may be killed in
                     // mid-execution.
                     //
-                   list.add(taskId);
+                    if (tip.getJob().getStatus().getRunState() == JobStatus.RUNNING) {
+                        killList.add(new KillTaskAction(killTaskId));
+                        LOG.debug(taskTracker + "" -> KillTaskAction: "" + killTaskId);
+                    } else {
+                      //killTasksList.add(new KillJobAction(taskId));
+                        String killJobId = tip.getJob().getStatus().getJobId(); 
+                        killJobIds.add(killJobId);
+                    }
                 }
             }
-            return (String[]) list.toArray(new String[list.size()]);
+            
+            for (String killJobId : killJobIds) {
+                killList.add(new KillJobAction(killJobId));
+                LOG.debug(taskTracker + "" -> KillJobAction: "" + killJobId);
+            }
+
+            return killList;
         }
         return null;
     }
"
hadoop,56b23dc7b0f5f3519037b803f3d208cbd21a5b83,"Merging changes from HADOOP-639, HADOOP-791 and HADOOP-827 into 0.9 branch, preparing for 0.9.2 release.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/branches/branch-0.9@487697 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-15 22:35:51,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskInProgress.java b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
index b1a5bca..2cc276a 100644
--- a/src/java/org/apache/hadoop/mapred/TaskInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
@@ -228,7 +228,12 @@
             (job.getStatus().getRunState() != JobStatus.RUNNING)) {
             tasksReportedClosed.add(taskid);
             return true;
-        } else {
+        } else if( !isMapTask() && isComplete() && 
+                ! tasksReportedClosed.contains(taskid) ){
+            tasksReportedClosed.add(taskid);
+            return true; 
+        }
+        else {
             return false;
         }
     }
"
hadoop,56b23dc7b0f5f3519037b803f3d208cbd21a5b83,"Merging changes from HADOOP-639, HADOOP-791 and HADOOP-827 into 0.9 branch, preparing for 0.9.2 release.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/branches/branch-0.9@487697 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-15 22:35:51,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 74081e7..df27090 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -68,6 +68,9 @@
 
     Server taskReportServer = null;
     InterTrackerProtocol jobClient;
+    
+    // last heartbeat response recieved
+    short heartbeatResponseId = -1;
 
     StatusHttpServer server = null;
     
@@ -187,7 +190,7 @@
         }
       }
     }
-    
+
     static String getCacheSubdir() {
       return TaskTracker.SUBDIR + Path.SEPARATOR + TaskTracker.CACHEDIR;
     }
@@ -451,15 +454,23 @@
               }
             }
 
-            if (!transmitHeartBeat()) {
+            // Send the heartbeat and process the jobtracker's directives
+            HeartbeatResponse heartbeatResponse = transmitHeartBeat();
+            TaskTrackerAction[] actions = heartbeatResponse.getActions();
+            LOG.debug(""Got heartbeatResponse from JobTracker with responseId: "" + 
+                    heartbeatResponse.getResponseId() + "" and "" + 
+                    ((actions != null) ? actions.length : 0) + "" actions"");
+            
+            if (reinitTaskTracker(actions)) {
               return State.STALE;
             }
+            
             lastHeartbeat = now;
             justStarted = false;
 
-            checkForNewTasks();
+            checkAndStartNewTasks(actions);
             markUnresponsiveTasks();
-            closeCompletedTasks();
+            closeCompletedTasks(actions);
             killOverflowingTasks();
             
             //we've cleaned up, resume normal operation
@@ -491,56 +502,94 @@
      * @return false if the tracker was unknown
      * @throws IOException
      */
-    private boolean transmitHeartBeat() throws IOException {
+    private HeartbeatResponse transmitHeartBeat() throws IOException {
       //
       // Build the heartbeat information for the JobTracker
       //
-      List<TaskStatus> taskReports = new ArrayList(runningTasks.size());
+      List<TaskStatus> taskReports = 
+        new ArrayList<TaskStatus>(runningTasks.size());
       synchronized (this) {
-          for (TaskInProgress tip: runningTasks.values()) {
-              taskReports.add(tip.createStatus());
-          }
+        for (TaskInProgress tip: runningTasks.values()) {
+          taskReports.add(tip.createStatus());
+        }
       }
       TaskTrackerStatus status = 
         new TaskTrackerStatus(taskTrackerName, localHostname, 
-                              httpPort, taskReports, 
-                              failures); 
-
+                httpPort, taskReports, 
+                failures); 
+      
+      //
+      // Check if we should ask for a new Task
+      //
+      boolean askForNewTask = false; 
+      if ((mapTotal < maxCurrentTasks || reduceTotal < maxCurrentTasks) &&
+              acceptNewTasks) {
+        checkLocalDirs(fConf.getLocalDirs());
+        
+        if (enoughFreeSpace(minSpaceStart)) {
+          askForNewTask = true;
+        }
+      }
+      
       //
       // Xmit the heartbeat
       //
+      HeartbeatResponse heartbeatResponse = jobClient.heartbeat(status, 
+              justStarted, askForNewTask, 
+              heartbeatResponseId);
+      heartbeatResponseId = heartbeatResponse.getResponseId();
       
-      int resultCode = jobClient.emitHeartbeat(status, justStarted);
       synchronized (this) {
-        for (TaskStatus taskStatus: taskReports) {
-            if (taskStatus.getRunState() != TaskStatus.State.RUNNING) {
-                if (taskStatus.getIsMap()) {
-                    mapTotal--;
-                } else {
-                    reduceTotal--;
-                }
-                myMetrics.completeTask();
-                runningTasks.remove(taskStatus.getTaskId());
+        for (TaskStatus taskStatus : taskReports) {
+          if (taskStatus.getRunState() != TaskStatus.State.RUNNING) {
+            if (taskStatus.getIsMap()) {
+              mapTotal--;
+            } else {
+              reduceTotal--;
             }
+            myMetrics.completeTask();
+            runningTasks.remove(taskStatus.getTaskId());
+          }
         }
       }
-      return resultCode != InterTrackerProtocol.UNKNOWN_TASKTRACKER;
+      return heartbeatResponse;
     }
 
     /**
+     * Check if the jobtracker directed a 'reset' of the tasktracker.
+     * 
+     * @param actions the directives of the jobtracker for the tasktracker.
+     * @return <code>true</code> if tasktracker is to be reset, 
+     *         <code>false</code> otherwise.
+     */
+    private boolean reinitTaskTracker(TaskTrackerAction[] actions) {
+      if (actions != null) {
+        for (TaskTrackerAction action : actions) {
+          if (action.getActionId() == 
+            TaskTrackerAction.ActionType.REINIT_TRACKER) {
+            LOG.info(""Recieved RenitTrackerAction from JobTracker"");
+            return true;
+          }
+        }
+      }
+      return false;
+    }
+    
+    /**
      * Check to see if there are any new tasks that we should run.
      * @throws IOException
      */
-    private void checkForNewTasks() throws IOException {
-      //
-      // Check if we should ask for a new Task
-      //
-      if ((mapTotal < maxCurrentTasks || reduceTotal < maxCurrentTasks) &&
-          acceptNewTasks) {
-        checkLocalDirs(fConf.getLocalDirs());
-        
-        if (enoughFreeSpace(minSpaceStart)) {
-          Task t = jobClient.pollForNewTask(taskTrackerName);
+    private void checkAndStartNewTasks(TaskTrackerAction[] actions) 
+    throws IOException {
+      if (actions == null) {
+        return;
+      }
+      
+      for (TaskTrackerAction action : actions) {
+        if (action.getActionId() == 
+          TaskTrackerAction.ActionType.LAUNCH_TASK) {
+          Task t = ((LaunchTaskAction)(action)).getTask();
+          LOG.info(""LaunchTaskAction: "" + t.getTaskId());
           if (t != null) {
             startNewTask(t);
           }
@@ -573,24 +622,73 @@
      * Ask the JobTracker if there are any tasks that we should clean up,
      * either because we don't need them any more or because the job is done.
      */
-    private void closeCompletedTasks() throws IOException {
-      String[] toCloseIds = jobClient.pollForTaskWithClosedJob(taskTrackerName);
-      if (toCloseIds != null) {
-        synchronized (this) {
-          for (int i = 0; i < toCloseIds.length; i++) {
-            TaskInProgress tip = tasks.get(toCloseIds[i]);
-            if (tip != null) {
-              // remove the task from running jobs, removing the job if 
-              // it is the last task
-              removeTaskFromJob(tip.getTask().getJobId(), tip);
-              tasksToCleanup.put(tip);
+    private void closeCompletedTasks(TaskTrackerAction[] actions) 
+    throws IOException {
+      if (actions == null) {
+        return;
+      }
+      
+      for (TaskTrackerAction action : actions) {
+        TaskTrackerAction.ActionType actionType = action.getActionId();
+        
+        if (actionType == TaskTrackerAction.ActionType.KILL_JOB) {
+          String jobId = ((KillJobAction)action).getJobId();
+          LOG.info(""Received 'KillJobAction' for job: "" + jobId);
+          synchronized (runningJobs) {
+            RunningJob rjob = runningJobs.get(jobId);
+            if (rjob == null) {
+              LOG.warn(""Unknown job "" + jobId + "" being deleted."");
             } else {
-              LOG.info(""Attempt to cleanup unknown tip "" + toCloseIds[i]);
+              synchronized (rjob) {
+                int noJobTasks = rjob.tasks.size(); 
+                int taskCtr = 0;
+                
+                // Add this tips of this job to queue of tasks to be purged 
+                for (TaskInProgress tip : rjob.tasks) {
+                  // Purge the job files for the last element in rjob.tasks
+                  if (++taskCtr == noJobTasks) {
+                    tip.setPurgeJobFiles(true);
+                  }
+
+                  tasksToCleanup.put(tip);
+                }
+                
+                // Remove this job 
+                rjob.tasks.clear();
+                runningJobs.remove(jobId);
+              }
             }
           }
+        } else if(actionType == TaskTrackerAction.ActionType.KILL_TASK) {
+          String taskId = ((KillTaskAction)action).getTaskId();
+          LOG.info(""Received KillTaskAction for task: "" + taskId);
+          purgeTask(tasks.get(taskId), false);
         }
       }
     }
+    
+    /**
+     * Remove the tip and update all relevant state.
+     * 
+     * @param tip {@link TaskInProgress} to be removed.
+     * @param purgeJobFiles <code>true</code> if the job files are to be
+     *                      purged, <code>false</code> otherwise.
+     */
+    private void purgeTask(TaskInProgress tip, boolean purgeJobFiles) {
+      if (tip != null) {
+        LOG.info(""About to purge task: "" + tip.getTask().getTaskId());
+        
+        // Cleanup the job files? 
+        tip.setPurgeJobFiles(purgeJobFiles);
+        
+        // Remove the task from running jobs, 
+        // removing the job if it's the last task
+        removeTaskFromJob(tip.getTask().getJobId(), tip);
+        
+        // Add this tip to queue of tasks to be purged 
+        tasksToCleanup.put(tip);
+      }
+    }
 
     /** Check if we're dangerously low on disk space
      * If so, kill jobs to free up space and make sure
@@ -822,6 +920,9 @@
         private boolean alwaysKeepTaskFiles;
         private TaskStatus taskStatus ; 
         private boolean keepJobFiles;
+        
+        /** Cleanup the job files when the job is complete (done/failed) */
+        private boolean purgeJobFiles = false;
 
         /**
          */
@@ -886,6 +987,10 @@
             keepFailedTaskFiles = localJobConf.getKeepFailedTaskFiles();
         }
         
+        public void setPurgeJobFiles(boolean purgeJobFiles) {
+          this.purgeJobFiles = purgeJobFiles;
+        }
+        
         /**
          */
         public synchronized TaskStatus createStatus() {
@@ -1017,32 +1122,39 @@
          * We no longer need anything from this task, as the job has
          * finished.  If the task is still running, kill it (and clean up
          */
-        public synchronized void jobHasFinished() throws IOException {
-        	 
-            if (getRunState() == TaskStatus.State.RUNNING) {
+        public void jobHasFinished() throws IOException {
+          boolean killTask = false;  
+          synchronized(this){
+              killTask = (getRunState() == TaskStatus.State.RUNNING);
+              if (killTask) {
                 killAndCleanup(false);
-            } else {
-                cleanup();
-            }
-            if (keepJobFiles)
-              return;
-            
-            // Delete temp directory in case any task used PhasedFileSystem.
-            try{
-              String systemDir = task.getConf().get(""mapred.system.dir"");
-              Path taskTempDir = new Path(systemDir + ""/"" + 
-                  task.getJobId() + ""/"" + task.getTipId());
-              if( fs.exists(taskTempDir)){
-                fs.delete(taskTempDir) ;
               }
-            }catch(IOException e){
-              LOG.warn(""Error in deleting reduce temporary output"",e); 
+          }
+          if (!killTask) {
+            cleanup();
+          }
+          if (keepJobFiles)
+            return;
+              
+          synchronized(this){
+              // Delete temp directory in case any task used PhasedFileSystem.
+              try{
+                String systemDir = task.getConf().get(""mapred.system.dir"");
+                Path taskTempDir = new Path(systemDir + ""/"" + 
+                    task.getJobId() + ""/"" + task.getTipId() + ""/"" + task.getTaskId());
+                if( fs.exists(taskTempDir)){
+                  fs.delete(taskTempDir) ;
+                }
+              }catch(IOException e){
+                LOG.warn(""Error in deleting reduce temporary output"",e); 
+              }
             }
-            
-            // delete the job diretory for this task 
-            // since the job is done/failed
-            this.defaultJobConf.deleteLocalFiles(SUBDIR + Path.SEPARATOR + 
-                    JOBCACHE + Path.SEPARATOR +  task.getJobId());
+            // Delete the job directory for this  
+            // task if the job is done/failed
+            if (purgeJobFiles) {
+              this.defaultJobConf.deleteLocalFiles(SUBDIR + Path.SEPARATOR + 
+                      JOBCACHE + Path.SEPARATOR +  task.getJobId());
+            }
         }
 
         /**
@@ -1090,6 +1202,9 @@
          * We no longer need anything from this task.  Either the 
          * controlling job is all done and the files have been copied
          * away, or the task failed and we don't need the remains.
+         * Any calls to cleanup should not lock the tip first.
+         * cleanup does the right thing- updates tasks in Tasktracker
+         * by locking tasktracker first and then locks the tip.
          */
         void cleanup() throws IOException {
             String taskId = task.getTaskId();
"
hadoop,6e51b11b2cee2f315e04256e4f03ce19f9a072d6,"HADOOP-791. Fix a deadlock in the task tracker.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@487691 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-15 22:11:46,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 2b5de33..3653da3 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -1130,28 +1130,33 @@
          * We no longer need anything from this task, as the job has
          * finished.  If the task is still running, kill it (and clean up
          */
-        public synchronized void jobHasFinished() throws IOException {
-        	 
-            if (getRunState() == TaskStatus.State.RUNNING) {
+        public void jobHasFinished() throws IOException {
+          boolean killTask = false;  
+          synchronized(this){
+              killTask = (getRunState() == TaskStatus.State.RUNNING);
+              if (killTask) {
                 killAndCleanup(false);
-            } else {
-                cleanup();
-            }
-            if (keepJobFiles)
-              return;
-            
-            // Delete temp directory in case any task used PhasedFileSystem.
-            try{
-              String systemDir = task.getConf().get(""mapred.system.dir"");
-              Path taskTempDir = new Path(systemDir + ""/"" + 
-                  task.getJobId() + ""/"" + task.getTipId());
-              if( fs.exists(taskTempDir)){
-                fs.delete(taskTempDir) ;
               }
-            }catch(IOException e){
-              LOG.warn(""Error in deleting reduce temporary output"",e); 
+          }
+          if (!killTask) {
+            cleanup();
+          }
+          if (keepJobFiles)
+            return;
+              
+          synchronized(this){
+              // Delete temp directory in case any task used PhasedFileSystem.
+              try{
+                String systemDir = task.getConf().get(""mapred.system.dir"");
+                Path taskTempDir = new Path(systemDir + ""/"" + 
+                    task.getJobId() + ""/"" + task.getTipId() + ""/"" + task.getTaskId());
+                if( fs.exists(taskTempDir)){
+                  fs.delete(taskTempDir) ;
+                }
+              }catch(IOException e){
+                LOG.warn(""Error in deleting reduce temporary output"",e); 
+              }
             }
-            
             // Delete the job directory for this  
             // task if the job is done/failed
             if (purgeJobFiles) {
@@ -1205,6 +1210,9 @@
          * We no longer need anything from this task.  Either the 
          * controlling job is all done and the files have been copied
          * away, or the task failed and we don't need the remains.
+         * Any calls to cleanup should not lock the tip first.
+         * cleanup does the right thing- updates tasks in Tasktracker
+         * by locking tasktracker first and then locks the tip.
          */
         void cleanup() throws IOException {
             String taskId = task.getTaskId();
"
hadoop,3adf7101039db336ec754076b3e9a4a584399107,"HADOOP-794.  Fix a divide-by-zero exception when a job specifies zero map tasks.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@486810 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-13 19:31:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/InputFormatBase.java b/src/java/org/apache/hadoop/mapred/InputFormatBase.java
index 01c4bf5..0d087ef 100644
--- a/src/java/org/apache/hadoop/mapred/InputFormatBase.java
+++ b/src/java/org/apache/hadoop/mapred/InputFormatBase.java
@@ -130,8 +130,7 @@
       totalSize += fs.getLength(files[i]);
     }
 
-    long goalSize = totalSize / numSplits;   // start w/ desired num splits
-
+    long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits);
     long minSize = Math.max(job.getLong(""mapred.min.split.size"", 1),
                             minSplitSize);
 
@@ -159,7 +158,7 @@
         }
       }
     }
-    //LOG.info( ""Total # of splits: "" + splits.size() );
+    LOG.debug( ""Total # of splits: "" + splits.size() );
     return (FileSplit[])splits.toArray(new FileSplit[splits.size()]);
   }
 
"
hadoop,3c187e06cc1e8e2d1da2c0b5647e1e465712569f,"HADOOP-673.  Give each task its own working directory again.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@486372 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-12 22:24:09,Doug Cutting,"diff --git a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java
index 9cd1b7c..814b575 100644
--- a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java
+++ b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java
@@ -234,10 +234,12 @@
       String[] argvSplit = splitArgs(argv);
       String prog = argvSplit[0];
       String userdir = System.getProperty(""user.dir"");
+      File currentDir = new File(""."").getAbsoluteFile();
+      File jobCacheDir = new File(currentDir.getParentFile().getParent(), ""work"");
       if (new File(prog).isAbsolute()) {
         // we don't own it. Hope it is executable
       } else {
-        new MustangFile(prog).setExecutable(true, true);
+        new MustangFile(new File(jobCacheDir, prog).toString()).setExecutable(true, true);
       }
 
       if (job_.getInputValueClass().equals(BytesWritable.class)) {
@@ -282,7 +284,7 @@
       //
       if (!new File(argvSplit[0]).isAbsolute()) {
           PathFinder finder = new PathFinder(""PATH"");
-          finder.prependPathComponent(""."");
+          finder.prependPathComponent(jobCacheDir.toString());
           File f = finder.getAbsolutePath(argvSplit[0]);
           if (f != null) {
               argvSplit[0] = f.getAbsolutePath();
"
hadoop,3c187e06cc1e8e2d1da2c0b5647e1e465712569f,"HADOOP-673.  Give each task its own working directory again.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@486372 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-12 22:24:09,Doug Cutting,"diff --git a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java
index 19b2e87..3c814a5 100644
--- a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java
+++ b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java
@@ -620,8 +620,8 @@
       boolean b = DistributedCache.checkURIs(fileURIs, archiveURIs);
       if (!b)
         fail(LINK_URI);
-      DistributedCache.createSymlink(jobConf_);
     }
+    DistributedCache.createSymlink(jobConf_);
     // set the jobconf for the caching parameters
     if (cacheArchives != null)
       DistributedCache.setCacheArchives(archiveURIs, jobConf_);
"
hadoop,3c187e06cc1e8e2d1da2c0b5647e1e465712569f,"HADOOP-673.  Give each task its own working directory again.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@486372 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-12 22:24:09,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/filecache/DistributedCache.java b/src/java/org/apache/hadoop/filecache/DistributedCache.java
index 3966b68..93a3740 100644
--- a/src/java/org/apache/hadoop/filecache/DistributedCache.java
+++ b/src/java/org/apache/hadoop/filecache/DistributedCache.java
@@ -24,6 +24,7 @@
 import org.apache.hadoop.conf.*;
 import org.apache.hadoop.util.*;
 import org.apache.hadoop.fs.*;
+
 import java.security.MessageDigest;
 import java.security.NoSuchAlgorithmException;
 import java.net.URI;
@@ -108,6 +109,8 @@
     String cacheId = makeRelative(cache, conf);
     synchronized (cachedArchives) {
       CacheStatus lcacheStatus = (CacheStatus) cachedArchives.get(cacheId);
+      if (lcacheStatus == null)
+        return;
       synchronized (lcacheStatus) {
         lcacheStatus.refcount--;
       }
@@ -320,7 +323,29 @@
 
     return digest;
   }
-
+  
+  /**
+   * This method create symlinks for all files in a given dir in another directory
+   * @param conf the configuration
+   * @param jobCacheDir the target directory for creating symlinks
+   * @param workDir the directory in which the symlinks are created
+   * @throws IOException
+   */
+  public static void createAllSymlink(Configuration conf, File jobCacheDir, File workDir)
+  throws IOException{
+    if ((!jobCacheDir.isDirectory()) || (!workDir.isDirectory())){
+      return;
+    }
+    boolean createSymlink = getSymlink(conf);
+     if (createSymlink){
+       File[] list = jobCacheDir.listFiles();
+       for (int i=0; i < list.length; i++){
+         FileUtil.symLink(list[i].getAbsolutePath(),
+             new File(workDir, list[i].getName()).toString());
+       }
+     }  
+  }
+  
   private static String getFileSysName(URI url) {
     String fsname = url.getScheme();
     if (""dfs"".equals(fsname)) {
"
hadoop,3c187e06cc1e8e2d1da2c0b5647e1e465712569f,"HADOOP-673.  Give each task its own working directory again.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@486372 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-12 22:24:09,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FileUtil.java b/src/java/org/apache/hadoop/fs/FileUtil.java
index c659d51..5a81edb 100644
--- a/src/java/org/apache/hadoop/fs/FileUtil.java
+++ b/src/java/org/apache/hadoop/fs/FileUtil.java
@@ -43,6 +43,16 @@
             return false;
           }
         } else {
+          //try deleting the directory
+          // this might be a symlink
+          boolean b = false;
+          b = contents[i].delete();
+          if (b){
+            //this was indeed a symlink or an empty directory
+            continue;
+          }
+          // if not an empty directory or symlink let
+          // fullydelete handle it.
           if (! fullyDelete(contents[i])) {
             return false;
           }
"
hadoop,3c187e06cc1e8e2d1da2c0b5647e1e465712569f,"HADOOP-673.  Give each task its own working directory again.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@486372 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-12 22:24:09,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskRunner.java b/src/java/org/apache/hadoop/mapred/TaskRunner.java
index 9c8921f..3db3250 100644
--- a/src/java/org/apache/hadoop/mapred/TaskRunner.java
+++ b/src/java/org/apache/hadoop/mapred/TaskRunner.java
@@ -21,6 +21,7 @@
 
 import org.apache.hadoop.fs.*;
 import org.apache.hadoop.filecache.*;
+import org.apache.hadoop.util.*;
 import java.io.*;
 import java.util.Vector;
 import java.net.URI;
@@ -82,7 +83,8 @@
       
       //before preparing the job localize 
       //all the archives
-      File workDir = new File(new File(t.getJobFile()).getParentFile().getParent(), ""work"");
+      File workDir = new File(t.getJobFile()).getParentFile();
+      File jobCacheDir = new File(workDir.getParent(), ""work"");
       URI[] archives = DistributedCache.getCacheArchives(conf);
       URI[] files = DistributedCache.getCacheFiles(conf);
       if ((archives != null) || (files != null)) {
@@ -104,8 +106,6 @@
           }
           DistributedCache.setLocalFiles(conf, stringifyPathArray(p));
         }
-        
-        // sets the paths to local archives and paths
         Path localTaskFile = new Path(t.getJobFile());
         FileSystem localFs = FileSystem.getNamed(""local"", conf);
         localFs.delete(localTaskFile);
@@ -116,6 +116,16 @@
           out.close();
         }
       }
+    
+      // create symlinks for all the files in job cache dir in current
+      // workingdir for streaming
+      try{
+        DistributedCache.createAllSymlink(conf, jobCacheDir, 
+            workDir);
+      } catch(IOException ie){
+        // Do not exit even if symlinks have not been created.
+        LOG.warn(StringUtils.stringifyException(ie));
+      }
       
       if (! prepare()) {
         return;
@@ -135,7 +145,7 @@
       String jar = conf.getJar();
       if (jar != null) {       
     	  // if jar exists, it into workDir
-        File[] libs = new File(workDir, ""lib"").listFiles();
+        File[] libs = new File(jobCacheDir, ""lib"").listFiles();
         if (libs != null) {
           for (int i = 0; i < libs.length; i++) {
             classPath.append(sep);            // add libs from jar to classpath
@@ -143,11 +153,13 @@
           }
         }
         classPath.append(sep);
-        classPath.append(new File(workDir, ""classes""));
+        classPath.append(new File(jobCacheDir, ""classes""));
         classPath.append(sep);
-        classPath.append(workDir);
+        classPath.append(jobCacheDir);
+       
       }
-
+      classPath.append(sep);
+      classPath.append(workDir);
       //  Build exec child jmv args.
       Vector vargs = new Vector(8);
       File jvm =                                  // use same jvm as parent
"
hadoop,7269dc98ee0e96f6fd4c03aad93d0c25ca304315,"HADOOP-792.  Fix 'dfs -mv' to return correct status.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@486364 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-12 22:12:15,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSShell.java b/src/java/org/apache/hadoop/dfs/DFSShell.java
index 0857b36..81224d7 100644
--- a/src/java/org/apache/hadoop/dfs/DFSShell.java
+++ b/src/java/org/apache/hadoop/dfs/DFSShell.java
@@ -359,7 +359,7 @@
         if (fs.rename(srcs[i], dst)) {
             System.out.println(""Renamed "" + srcs[i] + "" to "" + dstf);
         } else {
-            System.out.println(""Rename failed "" + srcs[i]);
+            throw new IOException(""Rename failed "" + srcs[i]);
         }
       }
     }
"
hadoop,40ce05e5219e576e86772f855cf82de93d6e410f,"HADOOP-782.  Fully remove killed tasks.  Contributed by Arun.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@486342 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-12 21:50:14,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 87577b4..2b5de33 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -621,7 +621,7 @@
                 LOG.info(tip.getTask().getTaskId() + "": "" + msg);
                 ReflectionUtils.logThreadInfo(LOG, ""lost task"", 30);
                 tip.reportDiagnosticInfo(msg);
-                tasksToCleanup.put(tip);
+                purgeTask(tip, false);
             }
         }
     }
@@ -718,7 +718,7 @@
                          "" Killing task."";
             LOG.info(killMe.getTask().getTaskId() + "": "" + msg);
             killMe.reportDiagnosticInfo(msg);
-            tasksToCleanup.put(killMe);
+            purgeTask(killMe, false);
           }
         }
       }
"
hadoop,2ba3c3c783e100db87fc7bf36a5d81e32ad1e4cf,"HADOOP-764.  Reduce memory allocations in namenode some.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@486312 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-12 21:13:46,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index 21f418b..4320ad0 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -1378,10 +1378,9 @@
         nodeInfo.isAlive = false;
       }
 
-      Block deadblocks[] = nodeInfo.getBlocks();
-      if( deadblocks != null )
-        for( int i = 0; i < deadblocks.length; i++ )
-          removeStoredBlock(deadblocks[i], nodeInfo);
+      for (Iterator<Block> it = nodeInfo.getBlockIterator(); it.hasNext(); ) {
+          removeStoredBlock(it.next(), nodeInfo);
+      }
       unprotectedRemoveDatanode(nodeInfo);
     }
 
@@ -1464,40 +1463,44 @@
         // Modify the (block-->datanode) map, according to the difference
         // between the old and new block report.
         //
-        int oldPos = 0, newPos = 0;
-        Block oldReport[] = node.getBlocks();
-        while (oldReport != null && newReport != null && oldPos < oldReport.length && newPos < newReport.length) {
-            int cmp = oldReport[oldPos].compareTo(newReport[newPos]);
-            
+        int newPos = 0;
+        boolean modified = false;
+        Iterator<Block> iter = node.getBlockIterator();
+        Block oldblk = iter.hasNext() ? iter.next() : null;
+        Block newblk = (newReport != null && newReport.length > 0) ? 
+                        newReport[0]	: null;
+
+        while (oldblk != null || newblk != null) {
+           
+            int cmp = (oldblk == null) ? 1 : 
+                       ((newblk == null) ? -1 : oldblk.compareTo(newblk));
+
             if (cmp == 0) {
                 // Do nothing, blocks are the same
-                oldPos++;
                 newPos++;
+                oldblk = iter.hasNext() ? iter.next() : null;
+                newblk = (newPos < newReport.length)
+                         ? newReport[newPos] : null;
             } else if (cmp < 0) {
                 // The old report has a block the new one does not
-                removeStoredBlock(oldReport[oldPos], node);
-                oldPos++;
+                removeStoredBlock(oldblk, node);
+                modified = true;
+                oldblk = iter.hasNext() ? iter.next() : null;
             } else {
                 // The new report has a block the old one does not
-                addStoredBlock(newReport[newPos], node);
+                addStoredBlock(newblk, node);
+                modified = true;
                 newPos++;
+                newblk = (newPos < newReport.length)
+                         ? newReport[newPos] : null;
             }
         }
-        while (oldReport != null && oldPos < oldReport.length) {
-            // The old report has a block the new one does not
-            removeStoredBlock(oldReport[oldPos], node);
-            oldPos++;
-        }
-        while (newReport != null && newPos < newReport.length) {
-            // The new report has a block the old one does not
-            addStoredBlock(newReport[newPos], node);
-            newPos++;
-        }
-
         //
         // Modify node so it has the new blockreport
         //
-        node.updateBlocks(newReport);
+        if (modified) {
+            node.updateBlocks(newReport);
+        }
 
         //
         // We've now completely updated the node's block report profile.
"
hadoop,3d5aa681c3fe12045c653a90b9c1c673488a20a4,"HADOOP-786.  Log common exception at debug level.  Contributed by Sanjay.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@486302 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-12 20:36:25,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/PhasedFileSystem.java b/src/java/org/apache/hadoop/mapred/PhasedFileSystem.java
index 040cc79..d464444 100644
--- a/src/java/org/apache/hadoop/mapred/PhasedFileSystem.java
+++ b/src/java/org/apache/hadoop/mapred/PhasedFileSystem.java
@@ -163,7 +163,7 @@
       fInfo.getOpenFileStream().close();
     }catch(IOException ioe){
       // ignore if already closed
-      ioe.printStackTrace();
+      LOG.debug(""Ignoring exception : "" + ioe.getMessage());
     }
     Path tempPath = fInfo.getTempPath(); 
     // ignore .crc files 
"
hadoop,13f9885213bfaf9869f88b7a5a0686fb8a7f96cf,"HADOOP-639.  Restructure InterTrackerProtocol to make task accounting more reliable.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@483651 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-07 20:38:17,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java b/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java
index 9fe06fd..1f91e64 100644
--- a/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java
+++ b/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java
@@ -27,31 +27,40 @@
  * The JobTracker is the Server, which implements this protocol.
  */ 
 interface InterTrackerProtocol extends VersionedProtocol {
-  // version 2 introduced to replace TaskStatus.State with an enum
-  public static final long versionID = 2L;
+  /**
+   * version 3 introduced to replace 
+   * emitHearbeat/pollForNewTask/pollForTaskWithClosedJob with
+   * {@link #heartbeat(TaskTrackerStatus, boolean, boolean, short)}
+   */
+  public static final long versionID = 3L;
   
   public final static int TRACKERS_OK = 0;
   public final static int UNKNOWN_TASKTRACKER = 1;
 
-  /** 
-   * Called regularly by the task tracker to update the status of its tasks
-   * within the job tracker.  JobTracker responds with a code that tells the 
-   * TaskTracker whether all is well.
-   *
-   * TaskTracker must also indicate whether this is the first interaction
-   * (since state refresh)
+  /**
+   * Called regularly by the {@link TaskTracker} to update the status of its 
+   * tasks within the job tracker. {@link JobTracker} responds with a 
+   * {@link HeartbeatResponse} that directs the 
+   * {@link TaskTracker} to undertake a series of 'actions' 
+   * (see {@link org.apache.hadoop.mapred.TaskTrackerAction.ActionType}).  
+   * 
+   * {@link TaskTracker} must also indicate whether this is the first 
+   * interaction (since state refresh) and acknowledge the last response
+   * it recieved from the {@link JobTracker} 
+   * 
+   * @param status the status update
+   * @param initialContact <code>true</code> if this is first interaction since
+   *                       'refresh', <code>false</code> otherwise.
+   * @param acceptNewTasks <code>true</code> if the {@link TaskTracker} is
+   *                       ready to accept new tasks to run.                 
+   * @param responseId the last responseId successfully acted upon by the
+   *                   {@link TaskTracker}.
+   * @return a {@link org.apache.hadoop.mapred.HeartbeatResponse} with 
+   *         fresh instructions.
    */
-  int emitHeartbeat(TaskTrackerStatus status, 
-                    boolean initialContact) throws IOException;
-
-  /** Called to get new tasks from from the job tracker for this tracker.*/
-  Task pollForNewTask(String trackerName) throws IOException;
-
-  /** Called to find which tasks that have been run by this tracker should now
-   * be closed because their job is complete.  This is used to, e.g., 
-   * notify a map task that its output is no longer needed and may 
-   * be removed. */
-  String[] pollForTaskWithClosedJob(String trackerName) throws IOException;
+  HeartbeatResponse heartbeat(TaskTrackerStatus status, 
+          boolean initialContact, boolean acceptNewTasks, short responseId)
+  throws IOException;
 
   /** Called by a reduce task to find which map tasks are completed.
    *
"
hadoop,13f9885213bfaf9869f88b7a5a0686fb8a7f96cf,"HADOOP-639.  Restructure InterTrackerProtocol to make task accounting more reliable.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@483651 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-07 20:38:17,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index 9774855..3def4f0 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -427,6 +427,9 @@
     // (trackerID->TreeSet of taskids running at that tracker)
     TreeMap trackerToTaskMap = new TreeMap();
 
+    // (trackerID --> last sent HeartBeatResponseID)
+    Map<String, Short> trackerToHeartbeatResponseIDMap = new TreeMap();
+    
     //
     // Watch and expire TaskTracker objects using these structures.
     // We can map from Name->TaskTrackerStatus, or we can expire by time.
@@ -723,6 +726,74 @@
     ////////////////////////////////////////////////////
 
     /**
+     * The periodic heartbeat mechanism between the {@link TaskTracker} and
+     * the {@link JobTracker}.
+     * 
+     * The {@link JobTracker} processes the status information sent by the 
+     * {@link TaskTracker} and responds with instructions to start/stop 
+     * tasks or jobs, and also 'reset' instructions during contingencies. 
+     */
+    public synchronized HeartbeatResponse heartbeat(TaskTrackerStatus status, 
+            boolean initialContact, boolean acceptNewTasks, short responseId) 
+    throws IOException {
+      LOG.debug(""Got heartbeat from: "" + status.getTrackerName() + 
+              "" (initialContact: "" + initialContact + 
+              "" acceptNewTasks: "" + acceptNewTasks + "")"" +
+              "" with responseId: "" + responseId);
+      
+        // First check if the last heartbeat response got through 
+        String trackerName = status.getTrackerName();
+        Short oldResponseId = trackerToHeartbeatResponseIDMap.get(trackerName);
+      
+        short newResponseId = (short)(responseId + 1);
+        if (!initialContact && oldResponseId != null && 
+                oldResponseId.shortValue() != responseId) {
+            newResponseId = oldResponseId.shortValue();
+        }
+      
+        // Process this heartbeat 
+        if (!processHeartbeat(status, initialContact, 
+                (newResponseId != responseId))) {
+            if (oldResponseId != null) {
+                trackerToHeartbeatResponseIDMap.remove(trackerName);
+            }
+
+            return new HeartbeatResponse(newResponseId, 
+                  new TaskTrackerAction[] {new ReinitTrackerAction()});
+        }
+      
+        // Initialize the response to be sent for the heartbeat
+        HeartbeatResponse response = new HeartbeatResponse(newResponseId, null);
+        List<TaskTrackerAction> actions = new ArrayList();
+      
+        // Check for new tasks to be executed on the tasktracker
+        if (acceptNewTasks) {
+        Task task = getNewTaskForTaskTracker(trackerName);
+            if (task != null) {
+                LOG.debug(trackerName + "" -> LaunchTask: "" + task.getTaskId());
+                actions.add(new LaunchTaskAction(task));
+            }
+        }
+      
+        // Check for tasks to be killed
+        List<TaskTrackerAction> killTasksList = getTasksToKill(trackerName);
+        if (killTasksList != null) {
+            actions.addAll(killTasksList);
+        }
+     
+        response.setActions(
+                actions.toArray(new TaskTrackerAction[actions.size()]));
+        
+        // Update the trackerToHeartbeatResponseIDMap
+        if (newResponseId != responseId) {
+            trackerToHeartbeatResponseIDMap.put(trackerName, 
+                    new Short(newResponseId));
+        }
+
+        return response;
+    }
+    
+    /**
      * Update the last recorded status for the given task tracker.
      * It assumes that the taskTrackers are locked on entry.
      * @author Owen O'Malley
@@ -752,16 +823,21 @@
     /**
      * Process incoming heartbeat messages from the task trackers.
      */
-    public synchronized int emitHeartbeat(TaskTrackerStatus trackerStatus, boolean initialContact) {
+    private synchronized boolean processHeartbeat(
+            TaskTrackerStatus trackerStatus, 
+            boolean initialContact, boolean updateStatusTimestamp) {
         String trackerName = trackerStatus.getTrackerName();
-        trackerStatus.setLastSeen(System.currentTimeMillis());
+        if (initialContact || updateStatusTimestamp) {
+          trackerStatus.setLastSeen(System.currentTimeMillis());
+        }
 
         synchronized (taskTrackers) {
             synchronized (trackerExpiryQueue) {
                 boolean seenBefore = updateTaskTrackerStatus(trackerName,
                                                              trackerStatus);
                 if (initialContact) {
-                    // If it's first contact, then clear out any state hanging around
+                    // If it's first contact, then clear out 
+                    // any state hanging around
                     if (seenBefore) {
                         lostTaskTracker(trackerName, trackerStatus.getHost());
                     }
@@ -770,7 +846,7 @@
                     if (!seenBefore) {
                         LOG.warn(""Status from unknown Tracker : "" + trackerName);
                         taskTrackers.remove(trackerName); 
-                        return InterTrackerProtocol.UNKNOWN_TASKTRACKER;
+                        return false;
                     }
                 }
 
@@ -782,18 +858,17 @@
 
         updateTaskStatuses(trackerStatus);
         //LOG.info(""Got heartbeat from ""+trackerName);
-        return InterTrackerProtocol.TRACKERS_OK;
+        return true;
     }
 
     /**
-     * A tracker wants to know if there's a Task to run.  Returns
-     * a task we'd like the TaskTracker to execute right now.
+     * Returns a task we'd like the TaskTracker to execute right now.
      *
      * Eventually this function should compute load on the various TaskTrackers,
      * and incorporate knowledge of DFS file placement.  But for right now, it
      * just grabs a single item out of the pending task list and hands it back.
      */
-    public synchronized Task pollForNewTask(String taskTracker) {
+    private synchronized Task getNewTaskForTaskTracker(String taskTracker) {
         //
         // Compute average map and reduce task numbers across pool
         //
@@ -936,23 +1011,36 @@
      * A tracker wants to know if any of its Tasks have been
      * closed (because the job completed, whether successfully or not)
      */
-    public synchronized String[] pollForTaskWithClosedJob(String taskTracker) {
-        TreeSet taskIds = (TreeSet) trackerToTaskMap.get(taskTracker);
+    private synchronized List getTasksToKill(String taskTracker) {
+        Set<String> taskIds = (TreeSet) trackerToTaskMap.get(taskTracker);
         if (taskIds != null) {
-            ArrayList list = new ArrayList();
-            for (Iterator it = taskIds.iterator(); it.hasNext(); ) {
-                String taskId = (String) it.next();
-                TaskInProgress tip = (TaskInProgress) taskidToTIPMap.get(taskId);
-                if (tip.shouldCloseForClosedJob(taskId)) {
+            List<TaskTrackerAction> killList = new ArrayList();
+            Set<String> killJobIds = new TreeSet(); 
+            for (String killTaskId : taskIds ) {
+                TaskInProgress tip = (TaskInProgress) taskidToTIPMap.get(killTaskId);
+                if (tip.shouldCloseForClosedJob(killTaskId)) {
                     // 
                     // This is how the JobTracker ends a task at the TaskTracker.
                     // It may be successfully completed, or may be killed in
                     // mid-execution.
                     //
-                   list.add(taskId);
+                    if (tip.getJob().getStatus().getRunState() == JobStatus.RUNNING) {
+                        killList.add(new KillTaskAction(killTaskId));
+                        LOG.debug(taskTracker + "" -> KillTaskAction: "" + killTaskId);
+                    } else {
+                      //killTasksList.add(new KillJobAction(taskId));
+                        String killJobId = tip.getJob().getStatus().getJobId(); 
+                        killJobIds.add(killJobId);
+                    }
                 }
             }
-            return (String[]) list.toArray(new String[list.size()]);
+            
+            for (String killJobId : killJobIds) {
+                killList.add(new KillJobAction(killJobId));
+                LOG.debug(taskTracker + "" -> KillJobAction: "" + killJobId);
+            }
+
+            return killList;
         }
         return null;
     }
"
hadoop,13f9885213bfaf9869f88b7a5a0686fb8a7f96cf,"HADOOP-639.  Restructure InterTrackerProtocol to make task accounting more reliable.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@483651 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-07 20:38:17,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskInProgress.java b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
index b1a5bca..2cc276a 100644
--- a/src/java/org/apache/hadoop/mapred/TaskInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
@@ -228,7 +228,12 @@
             (job.getStatus().getRunState() != JobStatus.RUNNING)) {
             tasksReportedClosed.add(taskid);
             return true;
-        } else {
+        } else if( !isMapTask() && isComplete() && 
+                ! tasksReportedClosed.contains(taskid) ){
+            tasksReportedClosed.add(taskid);
+            return true; 
+        }
+        else {
             return false;
         }
     }
"
hadoop,13f9885213bfaf9869f88b7a5a0686fb8a7f96cf,"HADOOP-639.  Restructure InterTrackerProtocol to make task accounting more reliable.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@483651 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-07 20:38:17,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index f6fff98..e0547f0 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -68,6 +68,9 @@
 
     Server taskReportServer = null;
     InterTrackerProtocol jobClient;
+    
+    // last heartbeat response recieved
+    short heartbeatResponseId = -1;
 
     StatusHttpServer server = null;
     
@@ -187,7 +190,7 @@
         }
       }
     }
-    
+
     static String getCacheSubdir() {
       return TaskTracker.SUBDIR + Path.SEPARATOR + TaskTracker.CACHEDIR;
     }
@@ -458,15 +461,23 @@
               }
             }
 
-            if (!transmitHeartBeat()) {
+            // Send the heartbeat and process the jobtracker's directives
+            HeartbeatResponse heartbeatResponse = transmitHeartBeat();
+            TaskTrackerAction[] actions = heartbeatResponse.getActions();
+            LOG.debug(""Got heartbeatResponse from JobTracker with responseId: "" + 
+                    heartbeatResponse.getResponseId() + "" and "" + 
+                    ((actions != null) ? actions.length : 0) + "" actions"");
+            
+            if (reinitTaskTracker(actions)) {
               return State.STALE;
             }
+            
             lastHeartbeat = now;
             justStarted = false;
 
-            checkForNewTasks();
+            checkAndStartNewTasks(actions);
             markUnresponsiveTasks();
-            closeCompletedTasks();
+            closeCompletedTasks(actions);
             killOverflowingTasks();
             
             //we've cleaned up, resume normal operation
@@ -498,56 +509,94 @@
      * @return false if the tracker was unknown
      * @throws IOException
      */
-    private boolean transmitHeartBeat() throws IOException {
+    private HeartbeatResponse transmitHeartBeat() throws IOException {
       //
       // Build the heartbeat information for the JobTracker
       //
-      List<TaskStatus> taskReports = new ArrayList(runningTasks.size());
+      List<TaskStatus> taskReports = 
+        new ArrayList<TaskStatus>(runningTasks.size());
       synchronized (this) {
-          for (TaskInProgress tip: runningTasks.values()) {
-              taskReports.add(tip.createStatus());
-          }
+        for (TaskInProgress tip: runningTasks.values()) {
+          taskReports.add(tip.createStatus());
+        }
       }
       TaskTrackerStatus status = 
         new TaskTrackerStatus(taskTrackerName, localHostname, 
-                              httpPort, taskReports, 
-                              failures); 
-
+                httpPort, taskReports, 
+                failures); 
+      
+      //
+      // Check if we should ask for a new Task
+      //
+      boolean askForNewTask = false; 
+      if ((mapTotal < maxCurrentTasks || reduceTotal < maxCurrentTasks) &&
+              acceptNewTasks) {
+        checkLocalDirs(fConf.getLocalDirs());
+        
+        if (enoughFreeSpace(minSpaceStart)) {
+          askForNewTask = true;
+        }
+      }
+      
       //
       // Xmit the heartbeat
       //
+      HeartbeatResponse heartbeatResponse = jobClient.heartbeat(status, 
+              justStarted, askForNewTask, 
+              heartbeatResponseId);
+      heartbeatResponseId = heartbeatResponse.getResponseId();
       
-      int resultCode = jobClient.emitHeartbeat(status, justStarted);
       synchronized (this) {
-        for (TaskStatus taskStatus: taskReports) {
-            if (taskStatus.getRunState() != TaskStatus.State.RUNNING) {
-                if (taskStatus.getIsMap()) {
-                    mapTotal--;
-                } else {
-                    reduceTotal--;
-                }
-                myMetrics.completeTask();
-                runningTasks.remove(taskStatus.getTaskId());
+        for (TaskStatus taskStatus : taskReports) {
+          if (taskStatus.getRunState() != TaskStatus.State.RUNNING) {
+            if (taskStatus.getIsMap()) {
+              mapTotal--;
+            } else {
+              reduceTotal--;
             }
+            myMetrics.completeTask();
+            runningTasks.remove(taskStatus.getTaskId());
+          }
         }
       }
-      return resultCode != InterTrackerProtocol.UNKNOWN_TASKTRACKER;
+      return heartbeatResponse;
     }
 
     /**
+     * Check if the jobtracker directed a 'reset' of the tasktracker.
+     * 
+     * @param actions the directives of the jobtracker for the tasktracker.
+     * @return <code>true</code> if tasktracker is to be reset, 
+     *         <code>false</code> otherwise.
+     */
+    private boolean reinitTaskTracker(TaskTrackerAction[] actions) {
+      if (actions != null) {
+        for (TaskTrackerAction action : actions) {
+          if (action.getActionId() == 
+            TaskTrackerAction.ActionType.REINIT_TRACKER) {
+            LOG.info(""Recieved RenitTrackerAction from JobTracker"");
+            return true;
+          }
+        }
+      }
+      return false;
+    }
+    
+    /**
      * Check to see if there are any new tasks that we should run.
      * @throws IOException
      */
-    private void checkForNewTasks() throws IOException {
-      //
-      // Check if we should ask for a new Task
-      //
-      if ((mapTotal < maxCurrentTasks || reduceTotal < maxCurrentTasks) &&
-          acceptNewTasks) {
-        checkLocalDirs(fConf.getLocalDirs());
-        
-        if (enoughFreeSpace(minSpaceStart)) {
-          Task t = jobClient.pollForNewTask(taskTrackerName);
+    private void checkAndStartNewTasks(TaskTrackerAction[] actions) 
+    throws IOException {
+      if (actions == null) {
+        return;
+      }
+      
+      for (TaskTrackerAction action : actions) {
+        if (action.getActionId() == 
+          TaskTrackerAction.ActionType.LAUNCH_TASK) {
+          Task t = ((LaunchTaskAction)(action)).getTask();
+          LOG.info(""LaunchTaskAction: "" + t.getTaskId());
           if (t != null) {
             startNewTask(t);
           }
@@ -580,24 +629,73 @@
      * Ask the JobTracker if there are any tasks that we should clean up,
      * either because we don't need them any more or because the job is done.
      */
-    private void closeCompletedTasks() throws IOException {
-      String[] toCloseIds = jobClient.pollForTaskWithClosedJob(taskTrackerName);
-      if (toCloseIds != null) {
-        synchronized (this) {
-          for (int i = 0; i < toCloseIds.length; i++) {
-            TaskInProgress tip = tasks.get(toCloseIds[i]);
-            if (tip != null) {
-              // remove the task from running jobs, removing the job if 
-              // it is the last task
-              removeTaskFromJob(tip.getTask().getJobId(), tip);
-              tasksToCleanup.put(tip);
+    private void closeCompletedTasks(TaskTrackerAction[] actions) 
+    throws IOException {
+      if (actions == null) {
+        return;
+      }
+      
+      for (TaskTrackerAction action : actions) {
+        TaskTrackerAction.ActionType actionType = action.getActionId();
+        
+        if (actionType == TaskTrackerAction.ActionType.KILL_JOB) {
+          String jobId = ((KillJobAction)action).getJobId();
+          LOG.info(""Received 'KillJobAction' for job: "" + jobId);
+          synchronized (runningJobs) {
+            RunningJob rjob = runningJobs.get(jobId);
+            if (rjob == null) {
+              LOG.warn(""Unknown job "" + jobId + "" being deleted."");
             } else {
-              LOG.info(""Attempt to cleanup unknown tip "" + toCloseIds[i]);
+              synchronized (rjob) {
+                int noJobTasks = rjob.tasks.size(); 
+                int taskCtr = 0;
+                
+                // Add this tips of this job to queue of tasks to be purged 
+                for (TaskInProgress tip : rjob.tasks) {
+                  // Purge the job files for the last element in rjob.tasks
+                  if (++taskCtr == noJobTasks) {
+                    tip.setPurgeJobFiles(true);
+                  }
+
+                  tasksToCleanup.put(tip);
+                }
+                
+                // Remove this job 
+                rjob.tasks.clear();
+                runningJobs.remove(jobId);
+              }
             }
           }
+        } else if(actionType == TaskTrackerAction.ActionType.KILL_TASK) {
+          String taskId = ((KillTaskAction)action).getTaskId();
+          LOG.info(""Received KillTaskAction for task: "" + taskId);
+          purgeTask(tasks.get(taskId), false);
         }
       }
     }
+    
+    /**
+     * Remove the tip and update all relevant state.
+     * 
+     * @param tip {@link TaskInProgress} to be removed.
+     * @param purgeJobFiles <code>true</code> if the job files are to be
+     *                      purged, <code>false</code> otherwise.
+     */
+    private void purgeTask(TaskInProgress tip, boolean purgeJobFiles) {
+      if (tip != null) {
+        LOG.info(""About to purge task: "" + tip.getTask().getTaskId());
+        
+        // Cleanup the job files? 
+        tip.setPurgeJobFiles(purgeJobFiles);
+        
+        // Remove the task from running jobs, 
+        // removing the job if it's the last task
+        removeTaskFromJob(tip.getTask().getJobId(), tip);
+        
+        // Add this tip to queue of tasks to be purged 
+        tasksToCleanup.put(tip);
+      }
+    }
 
     /** Check if we're dangerously low on disk space
      * If so, kill jobs to free up space and make sure
@@ -829,6 +927,9 @@
         private boolean alwaysKeepTaskFiles;
         private TaskStatus taskStatus ; 
         private boolean keepJobFiles;
+        
+        /** Cleanup the job files when the job is complete (done/failed) */
+        private boolean purgeJobFiles = false;
 
         /**
          */
@@ -893,6 +994,10 @@
             keepFailedTaskFiles = localJobConf.getKeepFailedTaskFiles();
         }
         
+        public void setPurgeJobFiles(boolean purgeJobFiles) {
+          this.purgeJobFiles = purgeJobFiles;
+        }
+        
         /**
          */
         public synchronized TaskStatus createStatus() {
@@ -1046,10 +1151,12 @@
               LOG.warn(""Error in deleting reduce temporary output"",e); 
             }
             
-            // delete the job diretory for this task 
-            // since the job is done/failed
-            this.defaultJobConf.deleteLocalFiles(SUBDIR + Path.SEPARATOR + 
-                    JOBCACHE + Path.SEPARATOR +  task.getJobId());
+            // Delete the job directory for this  
+            // task if the job is done/failed
+            if (purgeJobFiles) {
+              this.defaultJobConf.deleteLocalFiles(SUBDIR + Path.SEPARATOR + 
+                      JOBCACHE + Path.SEPARATOR +  task.getJobId());
+            }
         }
 
         /**
"
hadoop,1a29433de1f0b752031963ea9144a05b374ee5a6,"HADOOP-738.  Change 'dfs -get' command to not create CRC files by default.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@483637 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-07 20:04:20,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSShell.java b/src/java/org/apache/hadoop/dfs/DFSShell.java
index 28f0a83..0857b36 100644
--- a/src/java/org/apache/hadoop/dfs/DFSShell.java
+++ b/src/java/org/apache/hadoop/dfs/DFSShell.java
@@ -62,12 +62,23 @@
      * and copy them to the local name. srcf is kept.
      * When copying mutiple files, the destination must be a directory. 
      * Otherwise, IOException is thrown.
-     * @param srcf: a file pattern specifying source files
-     * @param dstf: a destination local file/directory 
+     * @param argv: arguments
+     * @param pos: Ignore everything before argv[pos]  
      * @exception: IOException  
      * @see org.apache.hadoop.fs.FileSystem.globPaths 
      */
-    void copyToLocal(String srcf, String dstf) throws IOException {
+    void copyToLocal(String[]argv, int pos) throws IOException {
+      if(argv.length-pos<2 || (argv.length-pos==2 && argv[pos].equalsIgnoreCase(""-crc""))) {
+        System.err.println(""Usage: -get [-crc] <src> <dst>"");
+        System.exit(-1);
+      }
+      boolean copyCrc = false;
+      if (""-crc"".equalsIgnoreCase(argv[pos])) {
+        pos++;
+        copyCrc = true;
+      }
+      String srcf = argv[pos++];
+      String dstf = argv[pos++];
       Path [] srcs = fs.globPaths( new Path(srcf) );
       if( srcs.length > 1 && !new File( dstf ).isDirectory()) {
         throw new IOException( ""When copy multiple files, "" 
@@ -75,7 +86,7 @@
       }
       Path dst = new Path( dstf );
       for( int i=0; i<srcs.length; i++ ) {
-        fs.copyToLocalFile( srcs[i], dst );
+        fs.copyToLocalFile( srcs[i], dst, copyCrc );
       }
     }
     
@@ -431,7 +442,7 @@
             + ""destination should be a directory."" );
       }
       for( int i=0; i<srcs.length; i++ ) {
-        FileUtil.copy(fs, srcs[i], fs, dst, false, conf);
+        FileUtil.copy(fs, srcs[i], fs, dst, false, true, conf);
       }
     }
 
@@ -644,7 +655,7 @@
           } else if (""-get"".equals(cmd) || ""-copyToLocal"".equals(cmd) ||
                    ""-moveToLocal"".equals(cmd)) {
             System.err.println(""Usage: java DFSShell"" + 
-                "" ["" + cmd + "" <src> <localdst>]"");
+                "" ["" + cmd + "" [-crc] <src> <localdst>]"");
           } else if (""-cat"".equals(cmd)) {
             System.out.println(""Usage: java DFSShell"" + 
                 "" ["" + cmd + "" <src>]"");
@@ -669,11 +680,11 @@
             System.err.println(""           [-put <localsrc> <dst>]"");
             System.err.println(""           [-copyFromLocal <localsrc> <dst>]"");
             System.err.println(""           [-moveFromLocal <localsrc> <dst>]"");
-            System.err.println(""           [-get <src> <localdst>]"");
+            System.err.println(""           [-get [-crc] <src> <localdst>]"");
             System.err.println(""           [-getmerge <src> <localdst> [addnl]]"");
             System.err.println(""           [-cat <src>]"");
-            System.err.println(""           [-copyToLocal <src> <localdst>]"");
-            System.err.println(""           [-moveToLocal <src> <localdst>]"");
+            System.err.println(""           [-copyToLocal [-crc] <src> <localdst>]"");
+            System.err.println(""           [-moveToLocal [-crc] <src> <localdst>]"");
             System.err.println(""           [-mkdir <path>]"");
             System.err.println(""           [-setrep [-R] <rep> <path/file>]"");
           }
@@ -696,13 +707,18 @@
         //
         // verify that we have enough command line parameters
         //
-        if (""-put"".equals(cmd) || ""-get"".equals(cmd) || 
-            ""-copyFromLocal"".equals(cmd) || ""-moveFromLocal"".equals(cmd) || 
-            ""-copyToLocal"".equals(cmd) || ""-moveToLocal"".equals(cmd)) {
+        if (""-put"".equals(cmd) || 
+            ""-copyFromLocal"".equals(cmd) || ""-moveFromLocal"".equals(cmd)) {
                 if (argv.length != 3) {
                   printUsage(cmd);
                   return exitCode;
                 }
+        } else if (""-get"".equals(cmd) || 
+            ""-copyToLocal"".equals(cmd) || ""-moveToLocal"".equals(cmd)) {
+                if (argv.length < 3) {
+                  printUsage(cmd);
+                  return exitCode;
+                }
         } else if (""-mv"".equals(cmd) || ""-cp"".equals(cmd)) {
                 if (argv.length < 3) {
                   printUsage(cmd);
@@ -735,7 +751,7 @@
             } else if (""-moveFromLocal"".equals(cmd)) {
                 moveFromLocal(new Path(argv[i++]), argv[i++]);
             } else if (""-get"".equals(cmd) || ""-copyToLocal"".equals(cmd)) {
-                copyToLocal(argv[i++], argv[i++]);
+                copyToLocal(argv, i);
             } else if (""-getmerge"".equals(cmd)) {
                 if(argv.length>i+2)
                     copyMergeToLocal(argv[i++], new Path(argv[i++]), Boolean.parseBoolean(argv[i++]));
"
hadoop,1a29433de1f0b752031963ea9144a05b374ee5a6,"HADOOP-738.  Change 'dfs -get' command to not create CRC files by default.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@483637 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-07 20:04:20,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java b/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
index 3acf42b..787cd18 100644
--- a/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
+++ b/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
@@ -181,15 +181,15 @@
     }
 
     public void moveFromLocalFile(Path src, Path dst) throws IOException {
-      FileUtil.copy(localFs, src, this, dst, true, getConf());
+      FileUtil.copy(localFs, src, this, dst, true, true, getConf());
     }
 
     public void copyFromLocalFile(Path src, Path dst) throws IOException {
-      FileUtil.copy(localFs, src, this, dst, false, getConf());
+      FileUtil.copy(localFs, src, this, dst, false, true, getConf());
     }
 
-    public void copyToLocalFile(Path src, Path dst) throws IOException {
-      FileUtil.copy(this, src, localFs, dst, false, getConf());
+    public void copyToLocalFile(Path src, Path dst, boolean copyCrc) throws IOException {
+      FileUtil.copy(this, src, localFs, dst, false, copyCrc, getConf());
     }
 
     public Path startLocalOutput(Path fsOutputFile, Path tmpLocalFile)
"
hadoop,1a29433de1f0b752031963ea9144a05b374ee5a6,"HADOOP-738.  Change 'dfs -get' command to not create CRC files by default.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@483637 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-07 20:04:20,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FileSystem.java b/src/java/org/apache/hadoop/fs/FileSystem.java
index c5a17bf..f636af5 100644
--- a/src/java/org/apache/hadoop/fs/FileSystem.java
+++ b/src/java/org/apache/hadoop/fs/FileSystem.java
@@ -731,8 +731,19 @@
     /**
      * The src file is under FS, and the dst is on the local disk.
      * Copy it from FS control to the local dst name.
+     * If src and dst are directories, copy crc files as well.
      */
-    public abstract void copyToLocalFile(Path src, Path dst) throws IOException;
+    public void copyToLocalFile(Path src, Path dst) throws IOException {
+      copyToLocalFile(src, dst, true);
+    }
+    
+    /**
+     * The src file is under FS, and the dst is on the local disk.
+     * Copy it from FS control to the local dst name.
+     * If src and dst are directories, the copyCrc parameter
+     * determines whether to copy CRC files.
+     */
+    public abstract void copyToLocalFile(Path src, Path dst, boolean copyCrc) throws IOException;
 
     /**
      * Returns a local File that the user can write output to.  The caller
"
hadoop,1a29433de1f0b752031963ea9144a05b374ee5a6,"HADOOP-738.  Change 'dfs -get' command to not create CRC files by default.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@483637 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-07 20:04:20,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FileUtil.java b/src/java/org/apache/hadoop/fs/FileUtil.java
index 09e23d8..c659d51 100644
--- a/src/java/org/apache/hadoop/fs/FileUtil.java
+++ b/src/java/org/apache/hadoop/fs/FileUtil.java
@@ -52,11 +52,20 @@
     return dir.delete();
   }
 
+  /** Copy files between FileSystems. */
+  public static boolean copy(FileSystem srcFS, Path src, 
+                             FileSystem dstFS, Path dst, 
+                             boolean deleteSource,
+                             Configuration conf ) throws IOException {
+    return copy(srcFS, src, dstFS, dst, deleteSource, true, conf);
+  
+  }
 
   /** Copy files between FileSystems. */
   public static boolean copy(FileSystem srcFS, Path src, 
                              FileSystem dstFS, Path dst, 
                              boolean deleteSource,
+                             boolean copyCrc,
                              Configuration conf ) throws IOException {
     dst = checkDest(src.getName(), dstFS, dst);
 
@@ -67,12 +76,16 @@
       Path contents[] = srcFS.listPaths(src);
       for (int i = 0; i < contents.length; i++) {
         copy(srcFS, contents[i], dstFS, new Path(dst, contents[i].getName()),
-             deleteSource, conf);
+             deleteSource, copyCrc, conf);
       }
     } else if (srcFS.isFile(src)) {
       InputStream in = srcFS.open(src);
       try {
-        copyContent(in, dstFS.create(dst), conf);
+        OutputStream out = (copyCrc) ?
+          dstFS.create(dst) :
+          dstFS.createRaw(dst, true, dstFS.getDefaultReplication(),
+            dstFS.getDefaultBlockSize());
+        copyContent(in, out, conf);
       } finally {
         in.close();
       } 
"
hadoop,1a29433de1f0b752031963ea9144a05b374ee5a6,"HADOOP-738.  Change 'dfs -get' command to not create CRC files by default.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@483637 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-07 20:04:20,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/LocalFileSystem.java b/src/java/org/apache/hadoop/fs/LocalFileSystem.java
index 0cbff56..8fd1db0 100644
--- a/src/java/org/apache/hadoop/fs/LocalFileSystem.java
+++ b/src/java/org/apache/hadoop/fs/LocalFileSystem.java
@@ -338,8 +338,8 @@
     }
 
     // We can't delete the src file in this case.  Too bad.
-    public void copyToLocalFile(Path src, Path dst) throws IOException {
-      FileUtil.copy(this, src, this, dst, false, getConf());
+    public void copyToLocalFile(Path src, Path dst, boolean copyCrc) throws IOException {
+      FileUtil.copy(this, src, this, dst, false, copyCrc, getConf());
     }
 
     // We can write output directly to the final location
"
hadoop,1a29433de1f0b752031963ea9144a05b374ee5a6,"HADOOP-738.  Change 'dfs -get' command to not create CRC files by default.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@483637 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-07 20:04:20,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/PhasedFileSystem.java b/src/java/org/apache/hadoop/mapred/PhasedFileSystem.java
index 7d765ce..040cc79 100644
--- a/src/java/org/apache/hadoop/mapred/PhasedFileSystem.java
+++ b/src/java/org/apache/hadoop/mapred/PhasedFileSystem.java
@@ -361,7 +361,7 @@
 
   @Override
   public void copyToLocalFile(
-      Path src, Path dst)
+      Path src, Path dst, boolean copyCrc)
       throws IOException {
     throw new UnsupportedOperationException(""Operation not supported"");  
   }
"
hadoop,944922cedb851abd3a0098d64cc48de5f90c18c0,"HADOOP-774.  Limit the number of invalid blocks returned with heartbeats by the namenode to datanodes.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@483627 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-07 19:32:35,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSConstants.java b/src/java/org/apache/hadoop/dfs/FSConstants.java
index 772e6a5..edf07754 100644
--- a/src/java/org/apache/hadoop/dfs/FSConstants.java
+++ b/src/java/org/apache/hadoop/dfs/FSConstants.java
@@ -99,6 +99,9 @@
     public static final int STILL_WAITING = 1;
     public static final int COMPLETE_SUCCESS = 2;
 
+    // Chunk the block Invalidate message
+    public static final int BLOCK_INVALIDATE_CHUNK = 100;
+
     //
     // Timeouts, constants
     //
"
hadoop,944922cedb851abd3a0098d64cc48de5f90c18c0,"HADOOP-774.  Limit the number of invalid blocks returned with heartbeats by the namenode to datanodes.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@483627 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-07 19:32:35,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index 2a8d6e4..21f418b 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -1809,23 +1809,54 @@
         // only if safe mode is off.
         if( isInSafeMode() )
           return null;
-        
+       
         Collection<Block> invalidateSet = recentInvalidateSets.remove( 
                                                       nodeID.getStorageID() );
  
-        if (invalidateSet == null ) 
+        if (invalidateSet == null) {
             return null;
+        }
+
+        Iterator<Block> it = null;
+        int sendNum = invalidateSet.size();
+        int origSize = sendNum;
+        ArrayList sendBlock = new ArrayList(sendNum);
+
+        //
+        // calculate the number of blocks that we send in one message
+        //
+        if (sendNum > FSConstants.BLOCK_INVALIDATE_CHUNK) {
+            sendNum =  FSConstants.BLOCK_INVALIDATE_CHUNK;
+        }
+        //
+        // Copy the first chunk into sendBlock
+        //
+        for (it = invalidateSet.iterator(); sendNum > 0; sendNum--) {
+            assert(it.hasNext());
+            sendBlock.add(it.next());
+            it.remove();
+        }
+
+        //
+        // If we could not send everything in this message, reinsert this item
+        // into the collection.
+        //
+        if (it.hasNext()) {
+            assert(origSize > FSConstants.BLOCK_INVALIDATE_CHUNK);
+            recentInvalidateSets.put(nodeID.getStorageID(), invalidateSet);
+        }
         
-        if(NameNode.stateChangeLog.isInfoEnabled()) {
+        if (NameNode.stateChangeLog.isInfoEnabled()) {
             StringBuffer blockList = new StringBuffer();
-            for( Iterator<Block> it = invalidateSet.iterator(); it.hasNext(); ) {
+            for (int i = 0; i < sendBlock.size(); i++) {
                 blockList.append(' ');
-                blockList.append(it.next().getBlockName());
+                Block block = (Block) sendBlock.get(i);
+                blockList.append(block.getBlockName());
             }
             NameNode.stateChangeLog.debug(""BLOCK* NameSystem.blockToInvalidate: ""
                    +""ask ""+nodeID.getName()+"" to delete "" + blockList );
         }
-        return (Block[]) invalidateSet.toArray(new Block[invalidateSet.size()]);
+        return (Block[]) sendBlock.toArray(new Block[sendBlock.size()]);
     }
 
     /**
"
hadoop,6a92be821094f63e3e46139229ffc64c3e283290,"HADOOP-629.  Fix several RPC services to better check the protocol name and version.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@483609 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-07 19:07:32,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/NameNode.java b/src/java/org/apache/hadoop/dfs/NameNode.java
index eec41c6..3a24640 100644
--- a/src/java/org/apache/hadoop/dfs/NameNode.java
+++ b/src/java/org/apache/hadoop/dfs/NameNode.java
@@ -63,13 +63,17 @@
  * @author Mike Cafarella
  **********************************************************/
 public class NameNode implements ClientProtocol, DatanodeProtocol, FSConstants {
-    public long getProtocolVersion(String protocol, long clientVersion) { 
+    public long getProtocolVersion(String protocol, 
+                                   long clientVersion) throws IOException { 
       if (protocol.equals(ClientProtocol.class.getName())) {
         return ClientProtocol.versionID; 
-      } else {
+      } else if (protocol.equals(DatanodeProtocol.class.getName())){
         return DatanodeProtocol.versionID;
+      } else {
+        throw new IOException(""Unknown protocol to name node: "" + protocol);
       }
     }
+    
     public static final Log LOG = LogFactory.getLog(""org.apache.hadoop.dfs.NameNode"");
     public static final Log stateChangeLog = LogFactory.getLog( ""org.apache.hadoop.dfs.StateChange"");
 
"
hadoop,6a92be821094f63e3e46139229ffc64c3e283290,"HADOOP-629.  Fix several RPC services to better check the protocol name and version.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@483609 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-07 19:07:32,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index 109eccd..9774855 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -93,11 +93,14 @@
       }
     }
     
-    public long getProtocolVersion(String protocol, long clientVersion) {
+    public long getProtocolVersion(String protocol, 
+                                   long clientVersion) throws IOException {
       if (protocol.equals(InterTrackerProtocol.class.getName())) {
         return InterTrackerProtocol.versionID;
-      } else {
+      } else if (protocol.equals(JobSubmissionProtocol.class.getName())){
         return JobSubmissionProtocol.versionID;
+      } else {
+        throw new IOException(""Unknown protocol to job tracker: "" + protocol);
       }
     }
     /**
"
hadoop,6a92be821094f63e3e46139229ffc64c3e283290,"HADOOP-629.  Fix several RPC services to better check the protocol name and version.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@483609 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-07 19:07:32,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 74081e7..f6fff98 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -196,9 +196,16 @@
       return TaskTracker.SUBDIR + Path.SEPARATOR + TaskTracker.JOBCACHE;
     }
     
-    public long getProtocolVersion(String protocol, long clientVersion) {
-      return TaskUmbilicalProtocol.versionID;
+    public long getProtocolVersion(String protocol, 
+                                   long clientVersion) throws IOException {
+      if (protocol.equals(TaskUmbilicalProtocol.class.getName())) {
+        return TaskUmbilicalProtocol.versionID;
+      } else {
+        throw new IOException(""Unknown protocol for task tracker: "" +
+                              protocol);
+      }
     }
+    
     /**
      * Do the real constructor work here.  It's in a separate method
      * so we can call it again and ""recycle"" the object after calling
"
hadoop,d15c73ee2a21c517a1411c0c1710a37a77525b09,"HADOOP-752.  Rationalize some synchronization in DFS namenode.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@483604 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-07 18:45:43,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index 7360594..2a8d6e4 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -329,9 +329,9 @@
      *         false if file does not exist or is a directory
      * @author shv
      */
-    public boolean setReplication(String src, 
-                                  short replication
-                                ) throws IOException {
+    public synchronized boolean setReplication(String src, 
+                                               short replication
+                                              ) throws IOException {
       if( isInSafeMode() )
         throw new SafeModeException( ""Cannot set replication for "" + src, safeMode );
       verifyReplication(src, replication, null );
@@ -1745,7 +1745,7 @@
 
     /**
      */
-    public DatanodeInfo[] datanodeReport() {
+    public synchronized DatanodeInfo[] datanodeReport() {
       DatanodeInfo results[] = null;
         synchronized (heartbeats) {
           synchronized (datanodeMap) {
@@ -1760,8 +1760,8 @@
     
     /**
      */
-    public void DFSNodesStatus( ArrayList<DatanodeDescriptor> live, 
-                                ArrayList<DatanodeDescriptor> dead ) {
+    public synchronized void DFSNodesStatus( ArrayList<DatanodeDescriptor> live, 
+                                             ArrayList<DatanodeDescriptor> dead ) {
       synchronized (heartbeats) {
         synchronized (datanodeMap) {
           for(Iterator<DatanodeDescriptor> it = datanodeMap.values().iterator(); it.hasNext(); ) {
"
hadoop,6c6733df2af530d56ad5adcd8c318529becc3939,"HADOOP-777.  Use fully-qualified hostnames for tasktrackers and datanodes.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@483594 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-07 18:28:26,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/net/DNS.java b/src/java/org/apache/hadoop/net/DNS.java
index 171d18c..a274465 100644
--- a/src/java/org/apache/hadoop/net/DNS.java
+++ b/src/java/org/apache/hadoop/net/DNS.java
@@ -128,7 +128,7 @@
       }
 
     if (hosts.size() == 0)
-      return new String[] { InetAddress.getLocalHost().getHostName() };
+      return new String[] { InetAddress.getLocalHost().getCanonicalHostName() };
     else
       return (String[]) hosts.toArray(new String[] {});
   }
@@ -166,7 +166,7 @@
   public static String getDefaultHost(String strInterface, String nameserver)
     throws UnknownHostException {
     if (strInterface.equals(""default"")) 
-      return InetAddress.getLocalHost().getHostName();
+      return InetAddress.getLocalHost().getCanonicalHostName();
 
     if (nameserver.equals(""default""))
       return getDefaultHost(strInterface);
"
hadoop,af0bb7848e9a150d775ea6e12ec2bd68c0191bca,"HADOOP-779.  Fix contrib/streaming to work correctly with gzipped input.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@483293 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-06 23:44:32,Doug Cutting,"diff --git a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamLineRecordReader.java b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamLineRecordReader.java
index ae9cab8..c3b4527 100644
--- a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamLineRecordReader.java
+++ b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamLineRecordReader.java
@@ -45,7 +45,7 @@
     super(in, split, reporter, job, fs);
     gzipped_ = StreamInputFormat.isGzippedInput(job);
     if (gzipped_) {
-      din_ = new DataInputStream(new GZIPInputStream(in_));
+      din_ = new BufferedInputStream( (new GZIPInputStream(in_) ) );
     } else {
       din_ = in_;
     }
@@ -88,40 +88,24 @@
     Text tValue = (Text) value;
     byte[] line;
 
-    while (true) {
-      if (gzipped_) {
-        // figure EOS from readLine
-      } else {
-        long pos = in_.getPos();
-        if (pos >= end_) return false;
-      }
-
-      line = UTF8ByteArrayUtils.readLine((InputStream) in_);
-      if (line == null) return false;
-      try {
-        Text.validateUTF8(line);
-      } catch (MalformedInputException m) {
-        System.err.println(""line="" + line + ""|"" + new Text(line));
-        System.out.flush();
-      }
-      try {
-        int tab = UTF8ByteArrayUtils.findTab(line);
-        if (tab == -1) {
-          tKey.set(line);
-          tValue.set("""");
-        } else {
-          UTF8ByteArrayUtils.splitKeyVal(line, tKey, tValue, tab);
-        }
-        break;
-      } catch (MalformedInputException e) {
-        LOG.warn(StringUtils.stringifyException(e));
-      }
+    if ( !gzipped_  ) {
+      long pos = in_.getPos();
+      if (pos >= end_) return false;
+    }
+    
+    line = UTF8ByteArrayUtils.readLine((InputStream) din_);
+    if (line == null) return false;
+    int tab = UTF8ByteArrayUtils.findTab(line);
+    if (tab == -1) {
+      tKey.set(line);
+      tValue.set("""");
+    } else {
+      UTF8ByteArrayUtils.splitKeyVal(line, tKey, tValue, tab);
     }
     numRecStats(line, 0, line.length);
     return true;
   }
 
   boolean gzipped_;
-  GZIPInputStream zin_;
-  DataInputStream din_; // GZIP or plain  
+  InputStream din_; // GZIP or plain  
 }
"
hadoop,8184adc5761f4f01e6b86fb85ea55706493649db,"HADOOP-780 - Use ReflectionUtils to instantiate key and value objects.


git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@482999 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-06 09:56:23,Andrzej Bialecki,"diff --git a/src/java/org/apache/hadoop/io/MapFile.java b/src/java/org/apache/hadoop/io/MapFile.java
index 543f63b..d65f389 100644
--- a/src/java/org/apache/hadoop/io/MapFile.java
+++ b/src/java/org/apache/hadoop/io/MapFile.java
@@ -22,6 +22,7 @@
 import org.apache.hadoop.fs.*;
 import org.apache.hadoop.conf.*;
 import org.apache.hadoop.util.Progressable;
+import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.hadoop.io.SequenceFile.CompressionType;
 
 /** A file-based map from keys to values.
@@ -470,8 +471,8 @@
               "", got "" + dataReader.getValueClass().getName());
     }
     long cnt = 0L;
-    Writable key = (Writable)keyClass.getConstructor(new Class[0]).newInstance(new Object[0]);
-    Writable value = (Writable)valueClass.getConstructor(new Class[0]).newInstance(new Object[0]);
+    Writable key = (Writable)ReflectionUtils.newInstance(keyClass, conf);
+    Writable value = (Writable)ReflectionUtils.newInstance(valueClass, conf);
     SequenceFile.Writer indexWriter = null;
     if (!dryrun) indexWriter = SequenceFile.createWriter(fs, conf, index, keyClass, LongWritable.class);
     try {
@@ -510,11 +511,11 @@
     FileSystem fs = new LocalFileSystem(conf);
     MapFile.Reader reader = new MapFile.Reader(fs, in, conf);
     MapFile.Writer writer =
-      new MapFile.Writer(fs, out, reader.getKeyClass(), reader.getValueClass());
+      new MapFile.Writer(conf, fs, out, reader.getKeyClass(), reader.getValueClass());
 
     WritableComparable key =
-      (WritableComparable)reader.getKeyClass().newInstance();
-    Writable value = (Writable)reader.getValueClass().newInstance();
+      (WritableComparable)ReflectionUtils.newInstance(reader.getKeyClass(), conf);
+    Writable value = (Writable)ReflectionUtils.newInstance(reader.getValueClass(), conf);
 
     while (reader.next(key, value))               // copy all entries
       writer.append(key, value);
"
hadoop,8184adc5761f4f01e6b86fb85ea55706493649db,"HADOOP-780 - Use ReflectionUtils to instantiate key and value objects.


git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@482999 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-06 09:56:23,Andrzej Bialecki,"diff --git a/src/java/org/apache/hadoop/ipc/Client.java b/src/java/org/apache/hadoop/ipc/Client.java
index edff535..bdcf1bf 100644
--- a/src/java/org/apache/hadoop/ipc/Client.java
+++ b/src/java/org/apache/hadoop/ipc/Client.java
@@ -44,6 +44,7 @@
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableUtils;
 import org.apache.hadoop.io.DataOutputBuffer;
+import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.hadoop.util.StringUtils;
 
 /** A client for an IPC service.  IPC calls take a single {@link Writable} as a
@@ -259,12 +260,9 @@
                                   WritableUtils.readString(in));
             call.setResult(null, ex);
           } else {
-            Writable value = makeValue();
+            Writable value = (Writable)ReflectionUtils.newInstance(valueClass, conf);
             try {
               readingCall = call;
-              if(value instanceof Configurable) {
-                ((Configurable) value).setConf(conf);
-              }
               value.readFields(in);                 // read value
             } finally {
               readingCall = null;
@@ -528,16 +526,4 @@
     return connection;
   }
 
-  private Writable makeValue() {
-    Writable value;                             // construct value
-    try {
-      value = (Writable)valueClass.newInstance();
-    } catch (InstantiationException e) {
-      throw new RuntimeException(e.toString());
-    } catch (IllegalAccessException e) {
-      throw new RuntimeException(e.toString());
-    }
-    return value;
-  }
-
 }
"
hadoop,8184adc5761f4f01e6b86fb85ea55706493649db,"HADOOP-780 - Use ReflectionUtils to instantiate key and value objects.


git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@482999 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-06 09:56:23,Andrzej Bialecki,"diff --git a/src/java/org/apache/hadoop/ipc/Server.java b/src/java/org/apache/hadoop/ipc/Server.java
index 04f98ee..be85866 100644
--- a/src/java/org/apache/hadoop/ipc/Server.java
+++ b/src/java/org/apache/hadoop/ipc/Server.java
@@ -460,7 +460,7 @@
       if (LOG.isDebugEnabled())
         LOG.debug("" got #"" + id);
             
-      Writable param = makeParam();           // read param
+      Writable param = (Writable)ReflectionUtils.newInstance(paramClass, conf);           // read param
       param.readFields(dis);        
         
       Call call = new Call(id, param, this);
@@ -633,21 +633,5 @@
 
   /** Called for each call. */
   public abstract Writable call(Writable param) throws IOException;
-
   
-  private Writable makeParam() {
-    Writable param;                               // construct param
-    try {
-      param = (Writable)paramClass.newInstance();
-      if (param instanceof Configurable) {
-        ((Configurable)param).setConf(conf);
-      }
-    } catch (InstantiationException e) {
-      throw new RuntimeException(e.toString());
-    } catch (IllegalAccessException e) {
-      throw new RuntimeException(e.toString());
-    }
-    return param;
-  }
-
 }
"
hadoop,8184adc5761f4f01e6b86fb85ea55706493649db,"HADOOP-780 - Use ReflectionUtils to instantiate key and value objects.


git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@482999 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-06 09:56:23,Andrzej Bialecki,"diff --git a/src/java/org/apache/hadoop/mapred/ReduceTask.java b/src/java/org/apache/hadoop/mapred/ReduceTask.java
index ac24c3e..5a9ef1b 100644
--- a/src/java/org/apache/hadoop/mapred/ReduceTask.java
+++ b/src/java/org/apache/hadoop/mapred/ReduceTask.java
@@ -131,16 +131,19 @@
     private WritableComparator comparator;
     private Class keyClass;
     private Class valClass;
+    private Configuration conf;
     private DataOutputBuffer valOut = new DataOutputBuffer();
     private DataInputBuffer valIn = new DataInputBuffer();
     private DataInputBuffer keyIn = new DataInputBuffer();
 
     public ValuesIterator (SequenceFile.Sorter.RawKeyValueIterator in, 
                            WritableComparator comparator, Class keyClass,
-                           Class valClass, TaskUmbilicalProtocol umbilical)
+                           Class valClass, TaskUmbilicalProtocol umbilical,
+                           Configuration conf)
       throws IOException {
       this.in = in;
       this.umbilical = umbilical;
+      this.conf = conf;
       this.comparator = comparator;
       this.keyClass = keyClass;
       this.valClass = valClass;
@@ -183,8 +186,8 @@
 
       Writable lastKey = key;                     // save previous key
       try {
-        key = (WritableComparable)keyClass.newInstance();
-        value = (Writable)valClass.newInstance();
+        key = (WritableComparable)ReflectionUtils.newInstance(keyClass, this.conf);
+        value = (Writable)ReflectionUtils.newInstance(valClass, this.conf);
       } catch (Exception e) {
         throw new RuntimeException(e);
       }
@@ -298,7 +301,7 @@
       Class keyClass = job.getMapOutputKeyClass();
       Class valClass = job.getMapOutputValueClass();
       ValuesIterator values = new ValuesIterator(rIter, comparator, keyClass, 
-                                                 valClass, umbilical);
+                                                 valClass, umbilical, job);
       while (values.more()) {
         myMetrics.reduceInput();
         reducer.reduce(values.getKey(), values, collector, reporter);
"
hadoop,64adfedcc2ab7c264e0ca6b42d20009bb7b831b6,"HADOOP-728.  Fix contrib/streaming issues, including '-reducer=NONE'.  Contributed by Sanjay.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@481430 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-01 22:32:48,Doug Cutting,"diff --git a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java
index 4311a7b..9cd1b7c 100644
--- a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java
+++ b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java
@@ -36,6 +36,7 @@
 
 import org.apache.hadoop.mapred.FileSplit;
 import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.PhasedFileSystem;
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.mapred.OutputCollector;
 import org.apache.hadoop.util.StringUtils;
@@ -192,10 +193,6 @@
     }
   }
 
-  String makeUniqueFileSuffix() {
-    return ""."" + System.currentTimeMillis() + ""."" + job_.get(""mapred.task.id"");
-  }
-
   public void configure(JobConf job) {
     try {
       String argv = getPipeCommand(job);
@@ -259,20 +256,21 @@
         // See StreamJob.setOutputSpec(): if reducerNone_ aka optSideEffect then: 
         // client has renamed outputPath and saved the argv's original output path as:
         if (useSingleSideOutputURI_) {
-          sideEffectURI_ = new URI(sideOutputURI_);
+          finalOutputURI = new URI(sideOutputURI_);
           sideEffectPathFinal_ = null; // in-place, no renaming to final
         } else {
+          sideFs_ = new PhasedFileSystem(sideFs_, job);
           String sideOutputPath = job_.get(""stream.sideoutput.dir""); // was: job_.getOutputPath() 
           String fileName = getSideEffectFileName(); // see HADOOP-444 for rationale
           sideEffectPathFinal_ = new Path(sideOutputPath, fileName);
-          sideEffectURI_ = new URI(sideEffectPathFinal_ + makeUniqueFileSuffix()); // implicit dfs: 
+          finalOutputURI = new URI(sideEffectPathFinal_.toString()); // implicit dfs: 
         }
         // apply default scheme
-        if(sideEffectURI_.getScheme() == null) {
-          sideEffectURI_ = new URI(""file"", sideEffectURI_.getSchemeSpecificPart(), null);
+        if(finalOutputURI.getScheme() == null) {
+          finalOutputURI = new URI(""file"", finalOutputURI.getSchemeSpecificPart(), null);
         }
         boolean allowSocket = useSingleSideOutputURI_;
-        sideEffectOut_ = getURIOutputStream(sideEffectURI_, allowSocket);
+        sideEffectOut_ = getURIOutputStream(finalOutputURI, allowSocket);
       }
 
       // 
@@ -292,7 +290,7 @@
           f = null;
       }
       logprintln(""PipeMapRed exec "" + Arrays.asList(argvSplit));
-      logprintln(""sideEffectURI_="" + sideEffectURI_);
+      logprintln(""sideEffectURI_="" + finalOutputURI);
 
       Environment childEnv = (Environment) StreamUtil.env().clone();
       addJobConfToEnvironment(job_, childEnv);
@@ -505,6 +503,7 @@
           if (optSideEffect_) {
             sideEffectOut_.write(answer);
             sideEffectOut_.write('\n');
+            sideEffectOut_.flush();
           } else {
             splitKeyVal(answer, key, val);
             output.collect(key, val);
@@ -576,17 +575,11 @@
       waitOutputThreads();
       try {
         if (optSideEffect_) {
-          logprintln(""closing "" + sideEffectURI_);
+          logprintln(""closing "" + finalOutputURI);
           if (sideEffectOut_ != null) sideEffectOut_.close();
-          logprintln(""closed  "" + sideEffectURI_);
-          if (useSingleSideOutputURI_) {
-            // With sideEffectPath_ we wrote in-place. 
-            // Possibly a named pipe set up by user or a socket.
-          } else {
-            boolean del = sideFs_.delete(sideEffectPathFinal_);
-            logprintln(""deleted  ("" + del + "") "" + sideEffectPathFinal_);
-            sideFs_.rename(new Path(sideEffectURI_.getSchemeSpecificPart()), sideEffectPathFinal_);
-            logprintln(""renamed  "" + sideEffectPathFinal_);
+          logprintln(""closed  "" + finalOutputURI);
+          if ( ! useSingleSideOutputURI_) {
+            ((PhasedFileSystem)sideFs_).commit(); 
           }
         }
       } catch (IOException io) {
@@ -725,7 +718,7 @@
   boolean optUseKey_ = true;
 
   private boolean optSideEffect_;
-  private URI sideEffectURI_;
+  private URI finalOutputURI;
   private Path sideEffectPathFinal_;
 
   private boolean useSingleSideOutputURI_;
"
hadoop,64adfedcc2ab7c264e0ca6b42d20009bb7b831b6,"HADOOP-728.  Fix contrib/streaming issues, including '-reducer=NONE'.  Contributed by Sanjay.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@481430 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-01 22:32:48,Doug Cutting,"diff --git a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java
index 4459c55..431ef01 100644
--- a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java
+++ b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java
@@ -701,8 +701,6 @@
         } catch (URISyntaxException e) {
           throw (IOException) new IOException().initCause(e);
         }
-      } else {
-        mapsideoutURI_ = primary;
       }
       // an empty reduce output named ""part-00002"" will go here and not collide.
       channel0 = primary + "".NONE"";
"
hadoop,64adfedcc2ab7c264e0ca6b42d20009bb7b831b6,"HADOOP-728.  Fix contrib/streaming issues, including '-reducer=NONE'.  Contributed by Sanjay.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@481430 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-01 22:32:48,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/PhasedFileSystem.java b/src/java/org/apache/hadoop/mapred/PhasedFileSystem.java
index c57db79..7d765ce 100644
--- a/src/java/org/apache/hadoop/mapred/PhasedFileSystem.java
+++ b/src/java/org/apache/hadoop/mapred/PhasedFileSystem.java
@@ -25,7 +25,7 @@
  * better to commit(Path) individual files when done. Otherwise
  * commit() can be used to commit all open files at once. 
  */
-class PhasedFileSystem extends FileSystem {
+public class PhasedFileSystem extends FileSystem {
 
   private FileSystem baseFS ;
   // Map from final file name to temporary file name
@@ -93,7 +93,9 @@
         }catch(IOException ioe){
           // ignore if already closed
         }
-        baseFS.delete( fInfo.getTempPath() ); 
+        if( baseFS.exists(fInfo.getTempPath())){
+          baseFS.delete( fInfo.getTempPath() );
+        }
         finalNameToFileInfo.remove(finalFile); 
       }
     }
"
hadoop,64adfedcc2ab7c264e0ca6b42d20009bb7b831b6,"HADOOP-728.  Fix contrib/streaming issues, including '-reducer=NONE'.  Contributed by Sanjay.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@481430 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-01 22:32:48,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 4bed4da..74081e7 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -1030,9 +1030,11 @@
             // Delete temp directory in case any task used PhasedFileSystem.
             try{
               String systemDir = task.getConf().get(""mapred.system.dir"");
-              String taskTempDir = systemDir + ""/"" + 
-                  task.getJobId() + ""/"" + task.getTipId();
-              fs.delete(new Path(taskTempDir)) ;
+              Path taskTempDir = new Path(systemDir + ""/"" + 
+                  task.getJobId() + ""/"" + task.getTipId());
+              if( fs.exists(taskTempDir)){
+                fs.delete(taskTempDir) ;
+              }
             }catch(IOException e){
               LOG.warn(""Error in deleting reduce temporary output"",e); 
             }
"
hadoop,e19ea19f88e52acc13a3aa8351e5b9e557a51d94,"HADOOP-750.  Fix a potential race condition during the mapreduce shuffle.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@481429 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-01 22:26:30,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java b/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
index 5e93b53..e4906213 100644
--- a/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
+++ b/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
@@ -121,24 +121,14 @@
   }
 
   private class PingTimer implements Progressable {
-    private long pingTime;
-    
-    public synchronized void reset() {
-      pingTime = 0;
-    }
-    
-    public synchronized long getLastPing() {
-      return pingTime;
-    }
+    Task task = getTask();
+    TaskTracker tracker = getTracker();
     
     public void progress() {
-      synchronized (this) {
-        pingTime = System.currentTimeMillis();
-        getTask().reportProgress(getTracker());
-      }
+      task.reportProgress(tracker);
     }
   }
-  
+
   private static int nextMapOutputCopierId = 0;
 
   /** Copies map outputs as they become available */
@@ -149,14 +139,8 @@
     private int id = nextMapOutputCopierId++;
     
     public MapOutputCopier() {
-    }
-    
-    /**
-     * Get the last time that this copier made progress.
-     * @return the System.currentTimeMillis when this copier last made progress
-     */
-    public long getLastProgressTime() {
-      return pingTimer.getLastPing();
+      setName(""MapOutputCopier "" + reduceTask.getTaskId() + ""."" + id);
+      LOG.debug(getName() + "" created"");
     }
     
     /**
@@ -185,6 +169,7 @@
     
     private synchronized void finish(long size) {
       if (currentLocation != null) {
+        LOG.debug(getName() + "" finishing "" + currentLocation + "" = "" + size);
         synchronized (copyResults) {
           copyResults.add(new CopyResult(currentLocation, size));
           copyResults.notify();
@@ -211,15 +196,14 @@
 
           try {
             start(loc);
-            pingTimer.progress();
             size = copyOutput(loc, pingTimer);
-            pingTimer.reset();
           } catch (IOException e) {
             LOG.warn(reduceTask.getTaskId() + "" copy failed: "" +
                         loc.getMapTaskId() + "" from "" + loc.getHost());
             LOG.warn(StringUtils.stringifyException(e));
+          } finally {
+            finish(size);
           }
-          finish(size);
         } catch (InterruptedException e) { 
           return; // ALL DONE
         } catch (Throwable th) {
@@ -268,49 +252,6 @@
     }
 
   }
-  
-  private class MapCopyLeaseChecker extends Thread {
-    private static final long STALLED_COPY_CHECK = 60 * 1000;
-    private long lastStalledCheck = 0;
-    
-    public void run() {
-      while (true) {
-        try {
-          long currentTime = System.currentTimeMillis();
-          if (currentTime - lastStalledCheck > STALLED_COPY_CHECK) {
-            lastStalledCheck = currentTime;
-            synchronized (copiers) {
-              for(int i=0; i < copiers.length; ++i) {
-                if (copiers[i] == null) {
-                  break;
-                }
-                long lastProgress = copiers[i].getLastProgressTime();
-                if (lastProgress != 0 && 
-                    currentTime - lastProgress > STALLED_COPY_TIMEOUT)  {
-                  LOG.warn(""Map output copy stalled on "" +
-                           copiers[i].getLocation());
-                  // mark the current file as failed
-                  copiers[i].fail();
-                  // tell the thread to stop
-                  copiers[i].interrupt();
-                  // create a replacement thread
-                  copiers[i] = new MapOutputCopier();
-                  copiers[i].start();
-                }
-              }
-            }
-          } else {
-            Thread.sleep(lastStalledCheck + STALLED_COPY_CHECK - currentTime);
-          }
-        } catch (InterruptedException ie) {
-          return;
-        } catch (Throwable th) {
-          LOG.error(""MapCopyLeaseChecker error: "" + 
-                    StringUtils.stringifyException(th));
-        }
-      }      
-    }
-  }
 
   public ReduceTaskRunner(Task task, TaskTracker tracker, 
                           JobConf conf) throws IOException {
@@ -352,7 +293,6 @@
     DecimalFormat  mbpsFormat = new DecimalFormat(""0.00"");
     Random         backoff = new Random();
     final Progress copyPhase = getTask().getProgress().phase();
-    MapCopyLeaseChecker leaseChecker = null;
     
     for (int i = 0; i < numOutputs; i++) {
       neededOutputs.add(new Integer(i));
@@ -367,8 +307,6 @@
       copiers[i] = new MapOutputCopier();
       copiers[i].start();
     }
-    leaseChecker = new MapCopyLeaseChecker();
-    leaseChecker.start();
     
     // start the clock for bandwidth measurement
     long startTime = System.currentTimeMillis();
@@ -450,6 +388,7 @@
       } catch (InterruptedException e) { } // IGNORE
 
       while (!killed && numInFlight > 0) {
+        LOG.debug(reduceTask.getTaskId() + "" numInFlight = "" + numInFlight);
         CopyResult cr = getCopyResult();
         
         if (cr != null) {
@@ -506,7 +445,6 @@
     }
 
     // all done, inform the copiers to exit
-    leaseChecker.interrupt();
     synchronized (copiers) {
       synchronized (scheduledCopies) {
         for (int i=0; i < copiers.length; i++) {
"
hadoop,e19ea19f88e52acc13a3aa8351e5b9e557a51d94,"HADOOP-750.  Fix a potential race condition during the mapreduce shuffle.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@481429 13f79535-47bb-0310-9956-ffa450edef68
",2006-12-01 22:26:30,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskRunner.java b/src/java/org/apache/hadoop/mapred/TaskRunner.java
index d6c9684..9c8921f 100644
--- a/src/java/org/apache/hadoop/mapred/TaskRunner.java
+++ b/src/java/org/apache/hadoop/mapred/TaskRunner.java
@@ -33,7 +33,7 @@
   public static final Log LOG =
     LogFactory.getLog(""org.apache.hadoop.mapred.TaskRunner"");
 
-  boolean killed = false;
+  volatile boolean killed = false;
   private Process process;
   private Task t;
   private TaskTracker tracker;
"
hadoop,f0067095b5e885dbce2b5c764e051add624d110f,"HADOOP-430.  Stop datanode's HTTP server when registration with namenode fails.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@480730 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-29 22:08:17,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index dbfe4cd..dce578a 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -166,7 +166,15 @@
         this.infoServer.start();
         this.dnRegistration.infoPort = this.infoServer.getPort();
         // register datanode
-        register();
+        try {
+          register();
+        } catch (IOException ie) {
+          try {
+            infoServer.stop();
+          } catch (Exception e) {
+          }
+          throw ie;
+        }
         datanodeObject = this;
     }
     
"
hadoop,70d274173ab60f69e37c285a1094fc6e5ebb1bc9,"HADOOP-698.  Fix HDFS client to not retry the same datanode on read failures.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@480291 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-28 23:50:39,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSClient.java b/src/java/org/apache/hadoop/dfs/DFSClient.java
index 8ba9217..64e85af 100644
--- a/src/java/org/apache/hadoop/dfs/DFSClient.java
+++ b/src/java/org/apache/hadoop/dfs/DFSClient.java
@@ -536,7 +536,7 @@
          * Open a DataInputStream to a DataNode so that it can be read from.
          * We get block ID and the IDs of the destinations at startup, from the namenode.
          */
-        private synchronized void blockSeekTo(long target) throws IOException {
+        private synchronized DatanodeInfo blockSeekTo(long target, TreeSet deadNodes) throws IOException {
             if (target >= filelen) {
                 throw new IOException(""Attempted to read past end of file"");
             }
@@ -572,10 +572,10 @@
             // Connect to best DataNode for desired Block, with potential offset
             //
             int failures = 0;
-            TreeSet deadNodes = new TreeSet();
+            DatanodeInfo chosenNode = null;
             while (s == null) {
                 DNAddrPair retval = chooseDataNode(targetBlock, deadNodes);
-                DatanodeInfo chosenNode = retval.info;
+                chosenNode = retval.info;
                 InetSocketAddress targetAddr = retval.addr;
             
                 try {
@@ -608,6 +608,7 @@
                     this.pos = target;
                     this.blockEnd = targetBlockEnd;
                     this.blockStream = in;
+                    return chosenNode;
                 } catch (IOException ex) {
                     // Put chosen node into dead list, continue
                     LOG.debug(""Failed to connect to "" + targetAddr + "":"" 
@@ -622,6 +623,7 @@
                     s = null;
                 }
             }
+            return chosenNode;
         }
 
         /**
@@ -653,7 +655,7 @@
             int result = -1;
             if (pos < filelen) {
                 if (pos > blockEnd) {
-                    blockSeekTo(pos);
+                    blockSeekTo(pos, new TreeSet());
                 }
                 result = blockStream.read();
                 if (result >= 0) {
@@ -673,10 +675,15 @@
             }
             if (pos < filelen) {
               int retries = 2;
+              DatanodeInfo chosenNode = null;
+              TreeSet deadNodes = null;
               while (retries > 0) {
                 try {
                   if (pos > blockEnd) {
-                      blockSeekTo(pos);
+                      if (deadNodes == null) {
+                        deadNodes = new TreeSet();
+                      }
+                      chosenNode = blockSeekTo(pos, deadNodes);
                   }
                   int realLen = Math.min(len, (int) (blockEnd - pos + 1));
                   int result = blockStream.read(buf, off, realLen);
@@ -687,6 +694,8 @@
                 } catch (IOException e) {
                   LOG.warn(""DFS Read: "" + StringUtils.stringifyException(e));
                   blockEnd = -1;
+                  if (deadNodes == null) { deadNodes = new TreeSet(); }
+                  if (chosenNode != null) { deadNodes.add(chosenNode); }
                   if (--retries == 0) {
                     throw e;
                   }
"
hadoop,70d274173ab60f69e37c285a1094fc6e5ebb1bc9,"HADOOP-698.  Fix HDFS client to not retry the same datanode on read failures.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@480291 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-28 23:50:39,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java b/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java
index 52d2ed7..1424b11 100644
--- a/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java
+++ b/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java
@@ -35,13 +35,12 @@
   private Thread dataNodeThreads[];
   private NameNodeRunner nameNode;
   private DataNodeRunner dataNodes[];
-  private int maxRetries = 10;
   private int MAX_RETRIES  = 10;
   private int MAX_RETRIES_PER_PORT = 10;
 
   private int nameNodePort = 0;
   private int nameNodeInfoPort = 0;
-
+  
   /**
    * An inner class that runs a name node.
    */
@@ -107,10 +106,12 @@
         String[] dirs = conf.getStrings(""dfs.data.dir"");
         for (int idx = 0; idx < dirs.length; idx++) {
           File dataDir = new File(dirs[idx]);
-          if (!dataDir.mkdirs()) {      
-            if (!dataDir.isDirectory()) {
-              throw new RuntimeException(""Mkdirs failed to create directory "" +
-                                         dataDir.toString());
+          synchronized (DataNodeRunner.class) {
+            if (!dataDir.mkdirs()) {
+              if (!dataDir.isDirectory()) {
+                throw new RuntimeException(""Mkdirs failed to create directory "" +
+                    dataDir.toString());
+              }
             }
           }
         }
@@ -143,7 +144,7 @@
   public MiniDFSCluster(int namenodePort, 
                         Configuration conf,
                         boolean dataNodeFirst) throws IOException {
-    this(namenodePort, conf, 1, dataNodeFirst);
+    this(namenodePort, conf, 1, dataNodeFirst, true);
   }
   
   /**
@@ -158,18 +159,36 @@
                         Configuration conf,
                         int nDatanodes,
                         boolean dataNodeFirst) throws IOException {
+    this(namenodePort, conf, nDatanodes, dataNodeFirst, true);
+  }
+  
+  /**
+   * Create the config and start up the servers.  If either the rpc or info port is already 
+   * in use, we will try new ports.
+   * @param namenodePort suggestion for which rpc port to use.  caller should use 
+   *                     getNameNodePort() to get the actual port used.
+   * @param nDatanodes Number of datanodes   
+   * @param dataNodeFirst should the datanode be brought up before the namenode?
+   * @param formatNamenode should the namenode be formatted before starting up ?
+   */
+  public MiniDFSCluster(int namenodePort, 
+                        Configuration conf,
+                        int nDatanodes,
+                        boolean dataNodeFirst,
+                        boolean formatNamenode) throws IOException {
 
     this.conf = conf;
 
     this.nDatanodes = nDatanodes;
     this.nameNodePort = namenodePort;
-    this.nameNodeInfoPort = 50080;   // We just want this port to be different from the default. 
+    this.nameNodeInfoPort = 50080;   // We just want this port to be different from the default.
     File base_dir = new File(System.getProperty(""test.build.data""),
                              ""dfs/"");
     File data_dir = new File(base_dir, ""data"");
     conf.set(""dfs.name.dir"", new File(base_dir, ""name1"").getPath()+"",""+
         new File(base_dir, ""name2"").getPath());
     conf.setInt(""dfs.replication"", Math.min(3, nDatanodes));
+    conf.setInt(""dfs.safemode.extension"", 0);
     // this timeout seems to control the minimum time for the test, so
     // decrease it considerably.
     conf.setInt(""ipc.client.timeout"", 1000);
@@ -183,7 +202,7 @@
                ""localhost:""+ Integer.toString(nameNodePort));
       conf.set(""dfs.info.port"", nameNodeInfoPort);
       
-      NameNode.format(conf);
+      if (formatNamenode) { NameNode.format(conf); }
       nameNode = new NameNodeRunner();
       nameNodeThread = new Thread(nameNode);
       dataNodes = new DataNodeRunner[nDatanodes];
"
hadoop,8903198e28cc72e945f6d4c638674be7aa38efcb,"HADOOP-747.  Fix record serialization to work correctly when records are included in Maps.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@480282 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-28 23:35:16,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/JBuffer.java b/src/java/org/apache/hadoop/record/compiler/JBuffer.java
index 6407b69..e25ce1b 100644
--- a/src/java/org/apache/hadoop/record/compiler/JBuffer.java
+++ b/src/java/org/apache/hadoop/record/compiler/JBuffer.java
@@ -48,11 +48,11 @@
         if (decl) {
             ret = ""    java.io.ByteArrayOutputStream ""+fname+"";\n"";
         }
-        return ret + ""        java.io.ByteArrayOutputStream ""+fname+""=a_.readBuffer(\""""+tag+""\"");\n"";
+        return ret + ""        ""+fname+""=a_.readBuffer(\""""+tag+""\"");\n"";
     }
     
     public String genJavaWriteWrapper(String fname, String tag) {
-        return ""        a_.writeBuffer(""+fname+""\""""+tag+""\"");\n"";
+        return ""        a_.writeBuffer(""+fname+"",\""""+tag+""\"");\n"";
     }
     
     public String genJavaCompareTo(String fname) {
"
hadoop,a0a06dd628f0f9b74919c38d6cd68811b63ed8ac,"HADOOP-741.  Fix some issues with speculative execution.  Contributed by Sanjay.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@478358 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-22 22:49:29,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobInProgress.java b/src/java/org/apache/hadoop/mapred/JobInProgress.java
index dfc89ce..16a7800 100644
--- a/src/java/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/JobInProgress.java
@@ -328,8 +328,7 @@
         return null;
       }
       ArrayList mapCache = (ArrayList)hostToMaps.get(tts.getHost());
-      double avgProgress = status.mapProgress() / maps.length;
-      int target = findNewTask(tts, clusterSize, avgProgress, 
+      int target = findNewTask(tts, clusterSize, status.mapProgress(), 
                                   maps, mapCache);
       if (target == -1) {
         return null;
@@ -357,8 +356,7 @@
             return null;
         }
 
-        double avgProgress = status.reduceProgress() ;
-        int target = findNewTask(tts, clusterSize, avgProgress, 
+        int target = findNewTask(tts, clusterSize, status.reduceProgress() , 
                                     reduces, null);
         if (target == -1) {
           return null;
@@ -441,7 +439,6 @@
                          task.hasSpeculativeTask(avgProgress) && 
                          ! task.hasRunOnMachine(taskTracker)) {
                 specTarget = i;
-                break ;
               }
             }
           }
@@ -696,8 +693,8 @@
         
         // Delete temp dfs dirs created if any, like in case of 
         // speculative exn of reduces.  
-     //   String tempDir = conf.get(""mapred.system.dir"") + ""/job_"" + uniqueString; 
-     //   fs.delete(new Path(tempDir)); 
+        String tempDir = conf.get(""mapred.system.dir"") + ""/job_"" + uniqueString; 
+        fs.delete(new Path(tempDir)); 
 
       } catch (IOException e) {
         LOG.warn(""Error cleaning up ""+profile.getJobId()+"": ""+e);
"
hadoop,a0a06dd628f0f9b74919c38d6cd68811b63ed8ac,"HADOOP-741.  Fix some issues with speculative execution.  Contributed by Sanjay.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@478358 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-22 22:49:29,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskInProgress.java b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
index a52e03f..b1a5bca 100644
--- a/src/java/org/apache/hadoop/mapred/TaskInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
@@ -42,7 +42,6 @@
     static final int MAX_TASK_FAILURES = 4;    
     static final double SPECULATIVE_GAP = 0.2;
     static final long SPECULATIVE_LAG = 60 * 1000;
-    static final int MAX_CONCURRENT_TASKS = 2; 
     private static NumberFormat idFormat = NumberFormat.getInstance();
     static {
       idFormat.setMinimumIntegerDigits(6);
@@ -445,21 +444,14 @@
         // REMIND - mjc - these constants should be examined
         // in more depth eventually...
         //
-        if (isMapTask() &&
-            activeTasks.size() <= MAX_TASK_EXECS &&
+      
+      if( activeTasks.size() <= MAX_TASK_EXECS &&
             runSpeculative &&
             (averageProgress - progress >= SPECULATIVE_GAP) &&
-            (System.currentTimeMillis() - startTime >= SPECULATIVE_LAG)) {
-            return true;
-        }else{
-          //Note: validate criteria for speculative reduce execution
-          if( runSpeculative && (activeTasks.size() < MAX_CONCURRENT_TASKS ) && 
-              (averageProgress - progress >= SPECULATIVE_GAP) &&
-              completes <= 0 &&
-              (System.currentTimeMillis() - startTime >= SPECULATIVE_LAG)) {
-            return true ; 
-          }
-        }
+            (System.currentTimeMillis() - startTime >= SPECULATIVE_LAG) 
+            && completes == 0) {
+          return true;
+      }
         return false;
     }
     
"
hadoop,a0a06dd628f0f9b74919c38d6cd68811b63ed8ac,"HADOOP-741.  Fix some issues with speculative execution.  Contributed by Sanjay.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@478358 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-22 22:49:29,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 81843ea..4bed4da 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -1026,6 +1026,17 @@
             }
             if (keepJobFiles)
               return;
+            
+            // Delete temp directory in case any task used PhasedFileSystem.
+            try{
+              String systemDir = task.getConf().get(""mapred.system.dir"");
+              String taskTempDir = systemDir + ""/"" + 
+                  task.getJobId() + ""/"" + task.getTipId();
+              fs.delete(new Path(taskTempDir)) ;
+            }catch(IOException e){
+              LOG.warn(""Error in deleting reduce temporary output"",e); 
+            }
+            
             // delete the job diretory for this task 
             // since the job is done/failed
             this.defaultJobConf.deleteLocalFiles(SUBDIR + Path.SEPARATOR + 
@@ -1053,26 +1064,6 @@
                 runstate = TaskStatus.State.KILLED;
               }
             }
-            
-            // the temporary file names in speculative exn are generated in 
-            // the launched JVM, and we dont talk to it when killing so cleanup
-            // should happen here. find the task id and delete the temp directory 
-            // for the task. only for killed speculative reduce instances
-            
-            // Note: it would be better to couple this with delete localfiles
-            // which is in conf currently, it doenst belong there. 
-
-            if( !task.isMapTask() && 
-                this.defaultJobConf.getSpeculativeExecution() ){
-              try{
-                String systemDir = task.getConf().get(""mapred.system.dir"");
-                String taskTempDir = systemDir + ""/"" + 
-                    task.getJobId() + ""/"" + task.getTipId();
-                fs.delete(new Path(taskTempDir)) ;
-              }catch(IOException e){
-                LOG.warn(""Error in deleting reduce temporary output"",e); 
-              }
-            }
         }
 
         /**
"
hadoop,ebc3b2f1159863426fc6d54bf43e801c9a7efedc,"HADOOP-736.  Roll back to Jetty 5.1.4, reverting changes in revisions 472349 and 472202, from HADOOP-565.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@478346 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-22 22:26:51,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/StatusHttpServer.java b/src/java/org/apache/hadoop/mapred/StatusHttpServer.java
index c9f1b92..37a4df4 100644
--- a/src/java/org/apache/hadoop/mapred/StatusHttpServer.java
+++ b/src/java/org/apache/hadoop/mapred/StatusHttpServer.java
@@ -18,7 +18,6 @@
 package org.apache.hadoop.mapred;
 
 import java.io.*;
-import java.net.BindException;
 import java.net.URL;
 import java.net.URLDecoder;
 
@@ -27,16 +26,13 @@
 import javax.servlet.http.HttpServletRequest;
 import javax.servlet.http.HttpServletResponse;
 
-import org.mortbay.jetty.handler.ContextHandlerCollection;
-import org.mortbay.jetty.servlet.Context;
-import org.mortbay.jetty.servlet.ServletHolder;
-import org.mortbay.jetty.webapp.WebAppContext;
-import org.mortbay.thread.BoundedThreadPool;
-import org.mortbay.util.MultiException;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.util.*;
-
+import org.mortbay.http.HttpContext;
+import org.mortbay.http.handler.ResourceHandler;
+import org.mortbay.http.SocketListener;
+import org.mortbay.jetty.servlet.WebApplicationContext;
 
 /**
  * Create a Jetty embedded server to answer http requests. The primary goal
@@ -51,8 +47,9 @@
   private static final boolean isWindows = 
     System.getProperty(""os.name"").startsWith(""Windows"");
   private org.mortbay.jetty.Server webServer;
-  private WebAppContext webAppContext ;  
+  private SocketListener listener;
   private boolean findPort;
+  private WebApplicationContext webAppContext;
   private static final Log LOG =
     LogFactory.getLog(StatusHttpServer.class.getName());
   
@@ -66,37 +63,34 @@
    */
   public StatusHttpServer(String name, String bindAddress, int port, 
                           boolean findPort) throws IOException {
-    webServer = new org.mortbay.jetty.Server(port);
+    webServer = new org.mortbay.jetty.Server();
     this.findPort = findPort;
-
-    ContextHandlerCollection contexts = new ContextHandlerCollection();
+    listener = new SocketListener();
+    listener.setPort(port);
+    listener.setHost(bindAddress);
+    webServer.addListener(listener);
 
     // set up the context for ""/logs/""
-    Context logContext = new Context(contexts,""/logs"");
-    
+    HttpContext logContext = new HttpContext();
+    logContext.setContextPath(""/logs/*"");
     String logDir = System.getProperty(""hadoop.log.dir"");
     logContext.setResourceBase(logDir);
-    logContext.addServlet(""org.mortbay.jetty.servlet.DefaultServlet"",""/"");
-    
+    logContext.addHandler(new ResourceHandler());
+    webServer.addContext(logContext);
+
     // set up the context for ""/static/*""
     String appDir = getWebAppsPath();
-    Context staticContext = new Context(contexts,""/static"");
+    HttpContext staticContext = new HttpContext();
+    staticContext.setContextPath(""/static/*"");
     staticContext.setResourceBase(appDir + File.separator + ""static"");
-    staticContext.addServlet(""org.mortbay.jetty.servlet.DefaultServlet"",""/"");
+    staticContext.addHandler(new ResourceHandler());
+    webServer.addContext(staticContext);
 
     // set up the context for ""/"" jsp files
-    webAppContext = new WebAppContext() ;
-    webAppContext.setContextPath(""/"");
-    webAppContext.setWar(appDir + File.separator + name); 
-    contexts.addHandler(webAppContext); 
-
-    Context stackContext = new Context(contexts,""/stacks"");
-    stackContext.addServlet(StackServlet.class, ""/""); 
-    // used as default but still set, in case it changes in future versions
-    webServer.setThreadPool(new BoundedThreadPool()); 
-    webServer.setHandler(contexts);
+    webAppContext = 
+      webServer.addWebApplication(""/"", appDir + File.separator + name);
+    addServlet(""stacks"", ""/stacks"", StackServlet.class);
   }
-  
 
   /**
    * Set a value in the webapp context. These values are available to the jsp
@@ -117,18 +111,20 @@
   public <T extends HttpServlet> 
   void addServlet(String name, String pathSpec, 
                   Class<T> servletClass) {
-    WebAppContext context = webAppContext;
+    WebApplicationContext context = webAppContext;
     try {
       if (name == null) {
-        context.addServlet(servletClass, pathSpec);
+        context.addServlet(pathSpec, servletClass.getName());
       } else {
-        ServletHolder holder = new ServletHolder(servletClass); 
-        holder.setName(name); 
-        context.addServlet(holder, pathSpec);
+        context.addServlet(name, pathSpec, servletClass.getName());
       } 
-    } catch (Throwable ex) {
+    } catch (ClassNotFoundException ex) {
       throw makeRuntimeException(""Problem instantiating class"", ex);
-    } 
+    } catch (InstantiationException ex) {
+      throw makeRuntimeException(""Problem instantiating class"", ex);
+    } catch (IllegalAccessException ex) {
+      throw makeRuntimeException(""Problem instantiating class"", ex);
+    }
   }
   
   private static RuntimeException makeRuntimeException(String msg, 
@@ -171,13 +167,12 @@
    * @return the port
    */
   public int getPort() {
-    return webServer.getConnectors()[0].getPort(); 
+    return listener.getPort();
   }
 
   public void setThreads(int min, int max) {
-    BoundedThreadPool pool = (BoundedThreadPool) webServer.getThreadPool() ;
-    pool.setMinThreads(min); 
-    pool.setMaxThreads(max); 
+    listener.setMinThreads(min);
+    listener.setMaxThreads(max);
   }
   /**
    * Start the server. Does not wait for the server to start.
@@ -188,34 +183,27 @@
         try {
           webServer.start();
           break;
-        } catch(BindException be){
-          if( findPort ){
-            webServer.getConnectors()[0].setPort(getPort() + 1);
-          }else{
-            throw be ; 
-          }
-        }catch (MultiException ex) {
+        } catch (org.mortbay.util.MultiException ex) {
           // look for the multi exception containing a bind exception,
           // in that case try the next port number.
           boolean needNewPort = false;
           for(int i=0; i < ex.size(); ++i) {
-            Throwable sub = ex.getThrowable(i);
-            
+            Exception sub = ex.getException(i);
             if (sub instanceof java.net.BindException) {
               needNewPort = true;
+              break;
             }
           }
           if (!findPort || !needNewPort) {
             throw ex;
           } else {
-            // Not using multiple connectors
-           webServer.getConnectors()[0].setPort(getPort() + 1);
+            listener.setPort(listener.getPort() + 1);
           }
         }
       }
-    }catch (IOException ie) {
+    } catch (IOException ie) {
       throw ie;
-    }catch (Exception e) {
+    } catch (Exception e) {
       IOException ie = new IOException(""Problem starting http server"");
       ie.initCause(e);
       throw ie;
@@ -226,13 +214,7 @@
    * stop the server
    */
   public void stop() throws InterruptedException {
-    try{
-      webServer.stop();
-    }catch(InterruptedException ex){
-      throw ex ; 
-    }catch(Exception e){
-      e.printStackTrace(); 
-    }
+    webServer.stop();
   }
   
   /**
"
hadoop,5e0e0b34087a54e66afa1b4358ac5265e3a4c126,"HADOOP-699.  Fix DFS web interface port number problem.  Contributed by Raghu.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@477850 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-21 19:53:40,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index ede7fa4..dbfe4cd 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -158,8 +158,6 @@
         this(InetAddress.getLocalHost().getHostName(), 
              dataDirs,
              createSocketAddr(conf.get(""fs.default.name"", ""local"")), conf);
-        // register datanode
-        register();
         int infoServerPort = conf.getInt(""dfs.datanode.info.port"", 50075);
         String infoServerBindAddress = conf.get(""dfs.datanode.info.bindAddress"", ""0.0.0.0"");
         this.infoServer = new StatusHttpServer(""datanode"", infoServerBindAddress, infoServerPort, true);
@@ -167,6 +165,8 @@
         this.infoServer.addServlet(null, ""/streamFile/*"", StreamFile.class);
         this.infoServer.start();
         this.dnRegistration.infoPort = this.infoServer.getPort();
+        // register datanode
+        register();
         datanodeObject = this;
     }
     
"
hadoop,5e0e0b34087a54e66afa1b4358ac5265e3a4c126,"HADOOP-699.  Fix DFS web interface port number problem.  Contributed by Raghu.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@477850 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-21 19:53:40,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DatanodeDescriptor.java b/src/java/org/apache/hadoop/dfs/DatanodeDescriptor.java
index c9ed741..a372439 100644
--- a/src/java/org/apache/hadoop/dfs/DatanodeDescriptor.java
+++ b/src/java/org/apache/hadoop/dfs/DatanodeDescriptor.java
@@ -74,6 +74,10 @@
     this.xceiverCount = 0;
     this.blocks.clear();
   }
+
+  int numBlocks() {
+    return blocks.size();
+  }
   
   /**
    */
"
hadoop,5e0e0b34087a54e66afa1b4358ac5265e3a4c126,"HADOOP-699.  Fix DFS web interface port number problem.  Contributed by Raghu.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@477850 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-21 19:53:40,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DatanodeID.java b/src/java/org/apache/hadoop/dfs/DatanodeID.java
index 9dee0ee..08e8901 100644
--- a/src/java/org/apache/hadoop/dfs/DatanodeID.java
+++ b/src/java/org/apache/hadoop/dfs/DatanodeID.java
@@ -94,6 +94,16 @@
     return name;
   }
   
+  /**
+   * Update fields when a new registration request comes in.
+   * Note that this does not update storageID.
+   */
+  void updateRegInfo( DatanodeID nodeReg ) {
+      name = nodeReg.getName();
+      infoPort = nodeReg.getInfoPort();
+      // update any more fields added in future.
+  }
+    
   /** Comparable.
    * Basis of compare is the String name (host:portNumber) only.
    * @param o
"
hadoop,5e0e0b34087a54e66afa1b4358ac5265e3a4c126,"HADOOP-699.  Fix DFS web interface port number problem.  Contributed by Raghu.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@477850 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-21 19:53:40,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index 9de6a48..7360594 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -1198,17 +1198,7 @@
       DatanodeDescriptor nodeS = datanodeMap.get(nodeReg.getStorageID());
       DatanodeDescriptor nodeN = getDatanodeByName( nodeReg.getName() );
       
-      if( nodeN != null && nodeS != null && nodeN == nodeS ) {
-        // The same datanode has been just restarted to serve the same data 
-        // storage. We do not need to remove old data blocks, the delta will  
-        // be calculated on the next block report from the datanode
-        NameNode.stateChangeLog.info(
-            ""BLOCK* NameSystem.registerDatanode: ""
-            + ""node restarted."" );
-        return;
-      }
-      
-      if( nodeN != null ) {
+      if( nodeN != null && nodeN != nodeS ) {
         // nodeN previously served a different data storage, 
         // which is not served by anybody anymore.
         removeDatanode( nodeN );
@@ -1218,18 +1208,25 @@
         getEditLog().logRemoveDatanode( nodeN );
         nodeN = null;
       }
-      
-      // nodeN is not found
-      if( nodeS != null ) {
-        // nodeS is found
-        // The registering datanode is a replacement node for the existing 
-        // data storage, which from now on will be served by a new node.
-        NameNode.stateChangeLog.debug(
+
+      if ( nodeS != null ) {
+        if( nodeN == nodeS ) {
+          // The same datanode has been just restarted to serve the same data 
+          // storage. We do not need to remove old data blocks, the delta will
+          // be calculated on the next block report from the datanode
+          NameNode.stateChangeLog.debug(""BLOCK* NameSystem.registerDatanode: ""
+                                        + ""node restarted."" );
+        } else {
+          // nodeS is found
+          // The registering datanode is a replacement node for the existing 
+          // data storage, which from now on will be served by a new node.
+          NameNode.stateChangeLog.debug(
             ""BLOCK* NameSystem.registerDatanode: ""
             + ""node "" + nodeS.name
             + "" is replaced by "" + nodeReg.getName() + ""."" );
+        }
         getEditLog().logRemoveDatanode( nodeS );
-        nodeS.name = nodeReg.getName();
+        nodeS.updateRegInfo( nodeReg );
         getEditLog().logAddDatanode( nodeS );
         return;
       }
@@ -1763,8 +1760,8 @@
     
     /**
      */
-    public void DFSNodesStatus( Vector<DatanodeDescriptor> live, 
-                                Vector<DatanodeDescriptor> dead) {
+    public void DFSNodesStatus( ArrayList<DatanodeDescriptor> live, 
+                                ArrayList<DatanodeDescriptor> dead ) {
       synchronized (heartbeats) {
         synchronized (datanodeMap) {
           for(Iterator<DatanodeDescriptor> it = datanodeMap.values().iterator(); it.hasNext(); ) {
"
hadoop,5e0e0b34087a54e66afa1b4358ac5265e3a4c126,"HADOOP-699.  Fix DFS web interface port number problem.  Contributed by Raghu.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@477850 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-21 19:53:40,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/JspHelper.java b/src/java/org/apache/hadoop/dfs/JspHelper.java
index 2e6b350..b57acbe 100644
--- a/src/java/org/apache/hadoop/dfs/JspHelper.java
+++ b/src/java/org/apache/hadoop/dfs/JspHelper.java
@@ -140,16 +140,10 @@
       in.close();
       out.print(new String(buf));
     }
-    public void DFSNodesStatus(Vector live, Vector dead) {
-      if (fsn == null) return;
-      TreeMap nodesSortedByName = new TreeMap();
-      fsn.DFSNodesStatus(live, dead);
-      for (int num = 0; num < live.size(); num++) {
-        DatanodeInfo d = (DatanodeInfo)live.elementAt(num);
-        nodesSortedByName.put(d.getName(), d);
-      }
-      live.clear();
-      live.addAll(nodesSortedByName.values());
+    public void DFSNodesStatus( ArrayList<DatanodeDescriptor> live,
+                                ArrayList<DatanodeDescriptor> dead ) {
+        if ( fsn != null )
+            fsn.DFSNodesStatus(live, dead);
     }
     public void addTableHeader(JspWriter out) throws IOException {
       out.print(""<table border=\""1\""""+
@@ -183,6 +177,72 @@
     public String getSafeModeText() {
       if( ! fsn.isInSafeMode() )
         return """";
-      return ""Safe mode is ON. <em>"" + fsn.getSafeModeTip() + ""</em>"";
+      return ""Safe mode is ON. <em>"" + fsn.getSafeModeTip() + ""</em><br>"";
+    }
+    
+    public void sortNodeList(ArrayList<DatanodeDescriptor> nodes,
+                             String field, String order) {
+        
+        class NodeComapare implements Comparator<DatanodeDescriptor> {
+            static final int 
+                FIELD_NAME              = 1,
+                FIELD_LAST_CONTACT      = 2,
+                FIELD_BLOCKS            = 3,
+                FIELD_SIZE              = 4,
+                FIELD_DISK_USED         = 5,
+                SORT_ORDER_ASC          = 1,
+                SORT_ORDER_DSC          = 2;
+
+            int sortField = FIELD_NAME;
+            int sortOrder = SORT_ORDER_ASC;
+            
+            public NodeComapare(String field, String order) {
+                if ( field.equals( ""lastcontact"" ) ) {
+                    sortField = FIELD_LAST_CONTACT;
+                } else if ( field.equals( ""size"" ) ) {
+                    sortField = FIELD_SIZE;
+                } else if ( field.equals( ""blocks"" ) ) {
+                    sortField = FIELD_BLOCKS;
+                } else if ( field.equals( ""pcused"" ) ) {
+                    sortField = FIELD_DISK_USED;
+                } else {
+                    sortField = FIELD_NAME;
+                }
+                
+                if ( order.equals(""DSC"") ) {
+                    sortOrder = SORT_ORDER_DSC;
+                } else {
+                    sortOrder = SORT_ORDER_ASC;
+                }
+            }
+
+            public int compare( DatanodeDescriptor d1,
+                                DatanodeDescriptor d2 ) {
+                int ret = 0;
+                switch ( sortField ) {
+                case FIELD_LAST_CONTACT:
+                    ret = (int) (d2.getLastUpdate() - d1.getLastUpdate());
+                    break;
+                case FIELD_BLOCKS:
+                    ret = d1.numBlocks() - d2.numBlocks();
+                    break;
+                case FIELD_SIZE:
+                    long  dlong = d1.getCapacity() - d2.getCapacity();
+                    ret = (dlong < 0) ? -1 : ( (dlong > 0) ? 1 : 0 );
+                    break;
+                case FIELD_DISK_USED:
+                    double ddbl =((d2.getRemaining()*1.0/d2.getCapacity())-
+                                  (d1.getRemaining()*1.0/d1.getCapacity()));
+                    ret = (ddbl < 0) ? -1 : ( (ddbl > 0) ? 1 : 0 );
+                    break;
+                case FIELD_NAME: 
+                    ret = d1.getName().compareTo(d2.getName());
+                    break;
+                }
+                return ( sortOrder == SORT_ORDER_DSC ) ? -ret : ret;
+            }
+        }
+        
+        Collections.sort( nodes, new NodeComapare( field, order ) );
     }
 }
"
hadoop,343bec67df1b4647d9ae586ba652129d09b745ce,"HADOOP-709.  Fix contrib/streaming to work with commands that contain control characters.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@477430 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-21 00:17:10,Doug Cutting,"diff --git a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeCombiner.java b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeCombiner.java
index b5b4a42..d948ea0 100644
--- a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeCombiner.java
+++ b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeCombiner.java
@@ -19,7 +19,9 @@
 package org.apache.hadoop.streaming;
 
 import java.io.IOException;
+import java.io.UnsupportedEncodingException;
 import java.util.Iterator;
+import java.net.URLDecoder;
 
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.Reporter;
@@ -45,7 +47,17 @@
 public class PipeCombiner extends PipeReducer {
 
   String getPipeCommand(JobConf job) {
-    return job.get(""stream.combine.streamprocessor"");
+    String str = job.get(""stream.combine.streamprocessor"");
+    if (str == null) {
+      System.err.println(""X1003"");
+      return str;
+    }
+    try {
+      return URLDecoder.decode(str, ""UTF-8"");
+    } catch (UnsupportedEncodingException e) {
+        System.err.println(""stream.combine.streamprocessor in jobconf not found"");
+        return null;
+    }
   }
 
 }
"
hadoop,343bec67df1b4647d9ae586ba652129d09b745ce,"HADOOP-709.  Fix contrib/streaming to work with commands that contain control characters.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@477430 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-21 00:17:10,Doug Cutting,"diff --git a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapper.java b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapper.java
index 2e8572b..e79f7e6 100644
--- a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapper.java
+++ b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapper.java
@@ -19,6 +19,7 @@
 package org.apache.hadoop.streaming;
 
 import java.io.*;
+import java.net.URLDecoder;
 
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.Mapper;
@@ -35,7 +36,17 @@
 public class PipeMapper extends PipeMapRed implements Mapper {
 
   String getPipeCommand(JobConf job) {
-    return job.get(""stream.map.streamprocessor"");
+    String str = job.get(""stream.map.streamprocessor"");
+    if (str == null) {
+      return str;
+    }
+    try {
+      return URLDecoder.decode(str, ""UTF-8"");
+    }
+    catch (UnsupportedEncodingException e) {
+      System.err.println(""stream.map.streamprocessor in jobconf not found"");
+      return null;
+    }
   }
 
   String getKeyColPropName() {
"
hadoop,343bec67df1b4647d9ae586ba652129d09b745ce,"HADOOP-709.  Fix contrib/streaming to work with commands that contain control characters.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@477430 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-21 00:17:10,Doug Cutting,"diff --git a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeReducer.java b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeReducer.java
index b352bee..bebde8c 100644
--- a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeReducer.java
+++ b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeReducer.java
@@ -19,7 +19,9 @@
 package org.apache.hadoop.streaming;
 
 import java.io.IOException;
+import java.io.UnsupportedEncodingException;
 import java.util.Iterator;
+import java.net.URLDecoder;
 
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.Reducer;
@@ -36,7 +38,16 @@
 public class PipeReducer extends PipeMapRed implements Reducer {
 
   String getPipeCommand(JobConf job) {
-    return job.get(""stream.reduce.streamprocessor"");
+    String str = job.get(""stream.reduce.streamprocessor"");
+    if (str == null) {
+      return str;
+    }
+    try {
+      return URLDecoder.decode(str, ""UTF-8"");
+    } catch (UnsupportedEncodingException e) {
+        System.err.println(""stream.reduce.streamprocessor in jobconf not found"");
+        return null;
+    }
   }
 
   boolean getDoPipe() {
"
hadoop,343bec67df1b4647d9ae586ba652129d09b745ce,"HADOOP-709.  Fix contrib/streaming to work with commands that contain control characters.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@477430 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-21 00:17:10,Doug Cutting,"diff --git a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java
index edc54d9..4459c55 100644
--- a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java
+++ b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java
@@ -22,6 +22,7 @@
 import java.io.IOException;
 import java.net.URI;
 import java.net.URISyntaxException;
+import java.net.URLEncoder;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Iterator;
@@ -549,7 +550,7 @@
       jobConf_.setMapperClass(c);
     } else {
       jobConf_.setMapperClass(PipeMapper.class);
-      jobConf_.set(""stream.map.streamprocessor"", mapCmd_);
+      jobConf_.set(""stream.map.streamprocessor"", URLEncoder.encode(mapCmd_, ""UTF-8""));
     }
 
     if (comCmd_ != null) {
@@ -558,7 +559,7 @@
         jobConf_.setCombinerClass(c);
       } else {
         jobConf_.setCombinerClass(PipeCombiner.class);
-        jobConf_.set(""stream.combine.streamprocessor"", comCmd_);
+        jobConf_.set(""stream.combine.streamprocessor"", URLEncoder.encode(comCmd_, ""UTF-8""));
       }
     }
 
@@ -570,7 +571,7 @@
         jobConf_.setReducerClass(c);
       } else {
         jobConf_.setReducerClass(PipeReducer.class);
-        jobConf_.set(""stream.reduce.streamprocessor"", redCmd_);
+        jobConf_.set(""stream.reduce.streamprocessor"", URLEncoder.encode(redCmd_, ""UTF-8""));
       }
     }
 
"
hadoop,343bec67df1b4647d9ae586ba652129d09b745ce,"HADOOP-709.  Fix contrib/streaming to work with commands that contain control characters.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@477430 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-21 00:17:10,Doug Cutting,"diff --git a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamUtil.java b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamUtil.java
index 27df62c..7f20296 100644
--- a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamUtil.java
+++ b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamUtil.java
@@ -455,6 +455,15 @@
 
   static boolean getUseMapSideEffect(JobConf job) {
     String reduce = job.get(""stream.reduce.streamprocessor"");
+    if (reduce == null) {
+      return false;
+    }
+    try {
+      reduce = URLDecoder.decode(reduce, ""UTF-8"");
+    } catch (UnsupportedEncodingException e) {
+      System.err.println(""stream.reduce.streamprocessor in jobconf not found"");
+      return false;
+    }
     return StreamJob.REDUCE_NONE.equals(reduce);
   }
 
"
hadoop,10376d48cc0bca87898d03e92f0ea48450b037bb,"HADOOP-733.  Make exit codes in DFSShell consistent and add a unit test.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@477423 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-21 00:09:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSShell.java b/src/java/org/apache/hadoop/dfs/DFSShell.java
index 281bfc1..47c6035 100644
--- a/src/java/org/apache/hadoop/dfs/DFSShell.java
+++ b/src/java/org/apache/hadoop/dfs/DFSShell.java
@@ -216,7 +216,7 @@
     	
       Path items[] = fs.listPaths(src);
       if (items == null) {
-      	System.out.println(""Could not get listing for "" + src);
+      	throw new IOException(""Could not get listing for "" + src);
       } else {
 
       	for (int i = 0; i < items.length; i++) {
@@ -310,7 +310,7 @@
     public void mkdir(String src) throws IOException {
         Path f = new Path(src);
         if (!fs.mkdirs(f)) {
-          System.out.println(""Mkdirs failed to create "" + src);
+          throw new IOException(""Mkdirs failed to create "" + src);
         }
     }
     
@@ -499,15 +499,14 @@
     /* delete an DFS file */
     private void delete(Path src, boolean recursive ) throws IOException {
       if (fs.isDirectory(src) && !recursive) {
-        System.out.println(""Cannot remove directory \"""" + src +
+        throw new IOException(""Cannot remove directory \"""" + src +
                            ""\"", use -rmr instead"");
-        return;
       }
 
       if (fs.delete(src)) {
         System.out.println(""Deleted "" + src);
       } else {
-        System.out.println(""Delete failed "" + src);
+        throw new IOException(""Delete failed "" + src);
       }
     }
 
@@ -730,20 +729,20 @@
                 else
                     copyMergeToLocal(argv[i++], new Path(argv[i++]));
             } else if (""-cat"".equals(cmd)) {
-                doall(cmd, argv, conf, i);
+                exitCode = doall(cmd, argv, conf, i);
             } else if (""-moveToLocal"".equals(cmd)) {
                 moveToLocal(argv[i++], new Path(argv[i++]));
             } else if (""-setrep"".equals(cmd)) {
             	setReplication(argv, i);           
             } else if (""-ls"".equals(cmd)) {
                 if (i < argv.length) {
-                    doall(cmd, argv, conf, i);
+                    exitCode = doall(cmd, argv, conf, i);
                 } else {
                     ls("""", false);
                 } 
             } else if (""-lsr"".equals(cmd)) {
                 if (i < argv.length) {
-                    doall(cmd, argv, conf, i);
+                    exitCode = doall(cmd, argv, conf, i);
                 } else {
                     ls("""", true);
                 } 
@@ -752,17 +751,17 @@
             } else if (""-cp"".equals(cmd)) {
                 exitCode = copy(argv, conf);
             } else if (""-rm"".equals(cmd)) {
-                doall(cmd, argv, conf, i);
+                exitCode = doall(cmd, argv, conf, i);
             } else if (""-rmr"".equals(cmd)) {
-                doall(cmd, argv, conf, i);
+                exitCode = doall(cmd, argv, conf, i);
             } else if (""-du"".equals(cmd)) {
                 if (i < argv.length) {
-                    doall(cmd, argv, conf, i);
+                    exitCode = doall(cmd, argv, conf, i);
                 } else {
                     du("""");
                 }
             } else if (""-mkdir"".equals(cmd)) {
-                doall(cmd, argv, conf, i);
+                exitCode = doall(cmd, argv, conf, i);
             } else {
                 exitCode = -1;
                 System.err.println(cmd.substring(1) + "": Unknown command"");
"
hadoop,983f2a51654f814e9fdbd60564b82b3c1a6e3696,"HADOOP-447.  Fix getBlockSize(Path) to work with relative paths.  Contributed by Raghu.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@477411 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-20 23:47:25,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java b/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
index 4eb585e..3acf42b 100644
--- a/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
+++ b/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
@@ -62,7 +62,7 @@
     }
     
     public long getBlockSize(Path f) throws IOException {
-      return dfs.getBlockSize(f);
+      return dfs.getBlockSize(makeAbsolute(f));
     }
     
     public short getDefaultReplication() {
"
hadoop,734f2ac9e40768b50f505cf8cc94f80cfdd020b4,"HADOOP-723.  Fix a race condition during the shuffle.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@477407 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-20 23:35:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/MapOutputLocation.java b/src/java/org/apache/hadoop/mapred/MapOutputLocation.java
index e5f0b78..909d353 100644
--- a/src/java/org/apache/hadoop/mapred/MapOutputLocation.java
+++ b/src/java/org/apache/hadoop/mapred/MapOutputLocation.java
@@ -100,22 +100,32 @@
    * @param localFilename the filename to write the data into
    * @param reduce the reduce id to get for
    * @param pingee a status object that wants to know when we make progress
+   * @param timeout number of ms for connection and read timeout
    * @throws IOException when something goes wrong
    */
   public long getFile(FileSystem fileSys, 
                       Path localFilename, 
                       int reduce,
-                      Progressable pingee) throws IOException {
+                      Progressable pingee,
+                      int timeout) throws IOException, InterruptedException {
     boolean good = false;
     long totalBytes = 0;
+    Thread currentThread = Thread.currentThread();
     URL path = new URL(toString() + ""&reduce="" + reduce);
     try {
       URLConnection connection = path.openConnection();
+      if (timeout > 0) {
+        connection.setConnectTimeout(timeout);
+        connection.setReadTimeout(timeout);
+      }
       InputStream input = connection.getInputStream();
       try {
         OutputStream output = fileSys.create(localFilename);
         try {
           byte[] buffer = new byte[64 * 1024];
+          if (currentThread.isInterrupted()) {
+            throw new InterruptedException();
+          }
           int len = input.read(buffer);
           while (len > 0) {
             totalBytes += len;
@@ -123,6 +133,9 @@
             if (pingee != null) {
               pingee.progress();
             }
+            if (currentThread.isInterrupted()) {
+              throw new InterruptedException();
+            }
             len = input.read(buffer);
           }
         } finally {
"
hadoop,734f2ac9e40768b50f505cf8cc94f80cfdd020b4,"HADOOP-723.  Fix a race condition during the shuffle.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@477407 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-20 23:35:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java b/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
index dc4bdb6..5e93b53 100644
--- a/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
+++ b/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
@@ -28,6 +28,8 @@
 
 /** Runs a reduce task. */
 class ReduceTaskRunner extends TaskRunner {
+  /** Number of ms before timing out a copy */
+  private static final int STALLED_COPY_TIMEOUT = 3 * 60 * 1000;
   
   /** 
    * for cleaning up old map outputs
@@ -137,11 +139,14 @@
     }
   }
   
+  private static int nextMapOutputCopierId = 0;
+
   /** Copies map outputs as they become available */
   private class MapOutputCopier extends Thread {
 
     private PingTimer pingTimer = new PingTimer();
     private MapOutputLocation currentLocation = null;
+    private int id = nextMapOutputCopierId++;
     
     public MapOutputCopier() {
     }
@@ -192,8 +197,8 @@
      * The thread exits when it is interrupted by the {@link ReduceTaskRunner}
      */
     public void run() {
-      try {
-        while (true) {        
+      while (true) {        
+        try {
           MapOutputLocation loc = null;
           long size = -1;
           
@@ -215,8 +220,13 @@
             LOG.warn(StringUtils.stringifyException(e));
           }
           finish(size);
+        } catch (InterruptedException e) { 
+          return; // ALL DONE
+        } catch (Throwable th) {
+          LOG.error(""Map output copy failure: "" + 
+                    StringUtils.stringifyException(th));
         }
-      } catch (InterruptedException e) { }  // ALL DONE!
+      }
     }
 
     /** Copies a a map output from a remote host, using raw RPC. 
@@ -224,44 +234,48 @@
      * @param pingee a status object to ping as we make progress
      * @return the size of the copied file
      * @throws IOException if there is an error copying the file
+     * @throws InterruptedException if the copier should give up
      */
     private long copyOutput(MapOutputLocation loc, 
-                            Progressable pingee)
-    throws IOException {
+                            Progressable pingee
+                            ) throws IOException, InterruptedException {
 
       String reduceId = reduceTask.getTaskId();
       LOG.info(reduceId + "" Copying "" + loc.getMapTaskId() +
                "" output from "" + loc.getHost() + ""."");
-
-      try {
-        // this copies the map output file
-        Path filename = conf.getLocalPath(reduceId + ""/map_"" +
-                                          loc.getMapId() + "".out"");
-        long bytes = loc.getFile(localFileSys, filename,
-                                 reduceTask.getPartition(), pingee);
-
-        LOG.info(reduceTask.getTaskId() + "" done copying "" + loc.getMapTaskId() +
-                 "" output from "" + loc.getHost() + ""."");
-
-        return bytes;
+      // the place where the file should end up
+      Path finalFilename = conf.getLocalPath(reduceId + ""/map_"" +
+                                             loc.getMapId() + "".out"");
+      // a working filename that will be unique to this attempt
+      Path tmpFilename = new Path(finalFilename + ""-"" + id);
+      // this copies the map output file
+      long bytes = loc.getFile(localFileSys, tmpFilename,
+                               reduceTask.getPartition(), pingee,
+                               STALLED_COPY_TIMEOUT);
+      // lock the ReduceTaskRunner while we do the rename
+      synchronized (ReduceTaskRunner.this) {
+        // if we can't rename the file, something is broken
+        if (!(new File(tmpFilename.toString()).
+                 renameTo(new File(finalFilename.toString())))) {
+          localFileSys.delete(tmpFilename);
+          throw new IOException(""failure to rename map output "" + tmpFilename);
+        }
       }
-      catch (IOException e) {
-        LOG.warn(reduceTask.getTaskId() + "" failed to copy "" + loc.getMapTaskId() +
-                    "" output from "" + loc.getHost() + ""."");
-        throw e;
-      }
+      LOG.info(reduceTask.getTaskId() + "" done copying "" + loc.getMapTaskId() +
+               "" output from "" + loc.getHost() + ""."");
+      
+      return bytes;
     }
 
   }
   
   private class MapCopyLeaseChecker extends Thread {
-    private static final long STALLED_COPY_TIMEOUT = 3 * 60 * 1000;
     private static final long STALLED_COPY_CHECK = 60 * 1000;
     private long lastStalledCheck = 0;
     
     public void run() {
-      try {
-        while (true) {
+      while (true) {
+        try {
           long currentTime = System.currentTimeMillis();
           if (currentTime - lastStalledCheck > STALLED_COPY_CHECK) {
             lastStalledCheck = currentTime;
@@ -288,9 +302,13 @@
           } else {
             Thread.sleep(lastStalledCheck + STALLED_COPY_CHECK - currentTime);
           }
+        } catch (InterruptedException ie) {
+          return;
+        } catch (Throwable th) {
+          LOG.error(""MapCopyLeaseChecker error: "" + 
+                    StringUtils.stringifyException(th));
         }
-      } catch (InterruptedException ie) {}
-      
+      }      
     }
   }
 
"
hadoop,e5f08bc551867c5c20874839fabb875a7a29d72f,"HADOOP-652.  In DFS, when a file is deleted, the block count is now decremented.  Contributed by Vladimir Krokhmalyov.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@477392 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-20 23:09:58,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSDataset.java b/src/java/org/apache/hadoop/dfs/FSDataset.java
index 7b448ec..4a7fb89 100644
--- a/src/java/org/apache/hadoop/dfs/FSDataset.java
+++ b/src/java/org/apache/hadoop/dfs/FSDataset.java
@@ -170,6 +170,16 @@
             }
         }
         
+        void clearPath(File f) {
+          if (dir.compareTo(f) == 0) numBlocks--;
+          else {
+            if ((siblings != null) && (myIdx != (siblings.length - 1)))
+              siblings[myIdx + 1].clearPath(f);
+            else if (children != null)
+              children[0].clearPath(f);
+          }
+        }
+        
         public String toString() {
           return ""FSDir{"" +
               ""dir="" + dir +
@@ -261,6 +271,10 @@
         dataDir.getBlockMap(blockMap);
       }
       
+      void clearPath(File f) {
+        dataDir.clearPath(f);
+      }
+      
       public String toString() {
         return dir.getAbsolutePath();
       }
@@ -498,15 +512,18 @@
      */
     public void invalidate(Block invalidBlks[]) throws IOException {
       for (int i = 0; i < invalidBlks.length; i++) {
-        synchronized ( this ) {
-          File f = getFile(invalidBlks[i]);
-          if (!f.delete()) {
-            throw new IOException(""Unexpected error trying to delete block ""
-                                  + invalidBlks[i] + "" at file "" + f);
-          }
+        File f;
+        synchronized (this) {
+          f = getFile(invalidBlks[i]);
+          FSVolume v = volumeMap.get(invalidBlks[i]);
+          v.clearPath(f.getParentFile());
           blockMap.remove(invalidBlks[i]);
           volumeMap.remove(invalidBlks[i]);
-        } 
+        }
+        if (!f.delete()) {
+            throw new IOException(""Unexpected error trying to delete block ""
+                                  + invalidBlks[i] + "" at file "" + f);
+        }
         DataNode.LOG.info(""Deleting block "" + invalidBlks[i]);
       }
     }
"
hadoop,14f6cab132d62f8acf0f4dbfd5f7eaa7a88474c3,"HADOOP-695.  Fix a NullPointerException in contrib/streaming.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@475393 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-15 20:09:38,Doug Cutting,"diff --git a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamLineRecordReader.java b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamLineRecordReader.java
index bc8c0cb..ae9cab8 100644
--- a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamLineRecordReader.java
+++ b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamLineRecordReader.java
@@ -97,13 +97,13 @@
       }
 
       line = UTF8ByteArrayUtils.readLine((InputStream) in_);
+      if (line == null) return false;
       try {
         Text.validateUTF8(line);
       } catch (MalformedInputException m) {
         System.err.println(""line="" + line + ""|"" + new Text(line));
         System.out.flush();
       }
-      if (line == null) return false;
       try {
         int tab = UTF8ByteArrayUtils.findTab(line);
         if (tab == -1) {
"
hadoop,a13a8334c274e194e402e7a25b023990ad1d9f13,"HADOOP-610.  Fix a problem when the DFS block size is configured to be smaller than the buffer size.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@475371 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-15 19:19:20,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSClient.java b/src/java/org/apache/hadoop/dfs/DFSClient.java
index fc63167..8ba9217 100644
--- a/src/java/org/apache/hadoop/dfs/DFSClient.java
+++ b/src/java/org/apache/hadoop/dfs/DFSClient.java
@@ -1080,7 +1080,8 @@
                 throw new IOException(""Stream closed"");
             }
             while (len > 0) {
-              int remaining = BUFFER_SIZE - pos;
+              int remaining = Math.min(BUFFER_SIZE - pos,
+                  (int)((blockSize - bytesWrittenToBlock) - pos));
               int toWrite = Math.min(remaining, len);
               System.arraycopy(b, off, outBuf, pos, toWrite);
               pos += toWrite;
"
hadoop,449bd3db2f2b0ccb3b84d6ce42a5d020916ffe9d,"HADOOP-716.  Minor javadoc fix.  Contributed by Devaraj.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@474991 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-14 21:35:30,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/SequenceFile.java b/src/java/org/apache/hadoop/io/SequenceFile.java
index d41a685..9ad4ba6 100644
--- a/src/java/org/apache/hadoop/io/SequenceFile.java
+++ b/src/java/org/apache/hadoop/io/SequenceFile.java
@@ -1873,7 +1873,7 @@
     /**
      * Clones the attributes (like compression of the input file and creates a 
      * corresponding Writer
-     * @param FileSystem
+     * @param fileSys the FileSystem object
      * @param inputFile the path of the input file whose attributes should be 
      * cloned 
      * @param outputFile the path of the output file 
"
hadoop,6c984cc3618d7fe0b231ed7d2c914397da3a427a,"HADOOP-712.  Fix record io's xml serialization to correctly handle control characters.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@474946 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-14 20:19:10,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/Utils.java b/src/java/org/apache/hadoop/record/Utils.java
index ce52a7a..f6d5246 100644
--- a/src/java/org/apache/hadoop/record/Utils.java
+++ b/src/java/org/apache/hadoop/record/Utils.java
@@ -62,7 +62,7 @@
         return true;
     }
     
-    public static final byte[] hexchars = { '0', '1', '2', '3', '4', '5',
+    public static final char[] hexchars = { '0', '1', '2', '3', '4', '5',
                                             '6', '7', '8', '9', 'A', 'B',
                                             'C', 'D', 'E', 'F' };
     /**
"
hadoop,7ab58423d276d5f22c07577cadaec5ecdc2ece05,"HADOOP-705.  Fix a bug in the JobTracker when failed jobs were not completely cleaned up.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@474907 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-14 18:25:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 60ea930..7a5b1b0 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -276,13 +276,25 @@
                                   jobId + Path.SEPARATOR + ""job.jar"");
   
           String jobFile = t.getJobFile();
+          FileSystem localFs = FileSystem.getNamed(""local"", fConf);
+          // this will happen on a partial execution of localizeJob.
+          // Sometimes the job.xml gets copied but copying job.jar
+          // might throw out an exception
+          // we should clean up and then try again
+          Path jobDir = localJobFile.getParent();
+          if (localFs.exists(jobDir)){
+            localFs.delete(jobDir);
+            boolean b = localFs.mkdirs(jobDir);
+            if (!b)
+              throw new IOException(""Not able to create job directory ""
+                  + jobDir.toString());
+          }
           fs.copyToLocalFile(new Path(jobFile), localJobFile);
           JobConf localJobConf = new JobConf(localJobFile);
           String jarFile = localJobConf.getJar();
           if (jarFile != null) {
             fs.copyToLocalFile(new Path(jarFile), localJarFile);
             localJobConf.setJar(localJarFile.toString());
-            FileSystem localFs = FileSystem.getNamed(""local"", fConf);
             OutputStream out = localFs.create(localJobFile);
             try {
               localJobConf.write(out);
"
hadoop,e7dcde5bbd3b11f4302cf97e33cdd2abce252475,"HADOOP-646.  Fix DFS namenode to handle edits files larger than 2GB.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@473522 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-10 21:53:40,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSEditLog.java b/src/java/org/apache/hadoop/dfs/FSEditLog.java
index 4c9c61b..063a7d7 100644
--- a/src/java/org/apache/hadoop/dfs/FSEditLog.java
+++ b/src/java/org/apache/hadoop/dfs/FSEditLog.java
@@ -20,6 +20,7 @@
 import java.io.BufferedInputStream;
 import java.io.DataInputStream;
 import java.io.DataOutputStream;
+import java.io.EOFException;
 import java.io.File;
 import java.io.FileDescriptor;
 import java.io.FileInputStream;
@@ -157,8 +158,15 @@
               new FileInputStream(edits)));
       // Read log file version. Could be missing. 
       in.mark( 4 );
-      if( in.available() > 0 ) {
+      // If edits log is greater than 2G, available method will return negative
+      // numbers, so we avoid having to call available
+      boolean available = true;
+      try {
         logVersion = in.readByte();
+      } catch (EOFException e) {
+        available = false;
+      }
+      if (available) {
         in.reset();
         if( logVersion >= 0 )
           logVersion = 0;
@@ -174,8 +182,13 @@
       
       short replication = (short)conf.getInt(""dfs.replication"", 3);
       try {
-        while (in.available() > 0) {
-          byte opcode = in.readByte();
+        while (true) {
+          byte opcode = -1;
+          try {
+            opcode = in.readByte();
+          } catch (EOFException e) {
+            break; // no more transactions
+          }
           numEdits++;
           switch (opcode) {
           case OP_ADD: {
"
hadoop,c2b57e65194b2f21c3d6b821e75eb308f53aee25,"HADOOP-637.  Fix a memory leak in IPC server.  Contributed by Raghu.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@473062 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-09 20:25:30,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/Server.java b/src/java/org/apache/hadoop/ipc/Server.java
index 81cf6da..2b75d83 100644
--- a/src/java/org/apache/hadoop/ipc/Server.java
+++ b/src/java/org/apache/hadoop/ipc/Server.java
@@ -366,11 +366,11 @@
       this.channel = channel;
       this.lastContact = lastContact;
       this.data = null;
-      this.dataLengthBuffer = null;
+      this.dataLengthBuffer = ByteBuffer.allocate(4);
       this.socket = channel.socket();
       this.out = new DataOutputStream
         (new BufferedOutputStream(
-         this.channelOut = new SocketChannelOutputStream(channel, 4096)));
+         this.channelOut = new SocketChannelOutputStream( channel )));
       InetAddress addr = socket.getInetAddress();
       if (addr == null) {
         this.hostAddress = ""*Unknown*"";
@@ -410,31 +410,27 @@
 
     public int readAndProcess() throws IOException, InterruptedException {
       int count = -1;
-      if (dataLengthBuffer == null)
-        dataLengthBuffer = ByteBuffer.allocateDirect(4);
       if (dataLengthBuffer.remaining() > 0) {
-        count = channel.read(dataLengthBuffer);
-        if (count < 0) return count;
-        if (dataLengthBuffer.remaining() == 0) {
-          dataLengthBuffer.flip(); 
-          dataLength = dataLengthBuffer.getInt();
-          data = ByteBuffer.allocateDirect(dataLength);
-        }
-        //return count;
+        count = channel.read(dataLengthBuffer);       
+        if ( count < 0 || dataLengthBuffer.remaining() > 0 ) 
+          return count;        
+        dataLengthBuffer.flip(); 
+        dataLength = dataLengthBuffer.getInt();
+        data = ByteBuffer.allocate(dataLength);
       }
       count = channel.read(data);
       if (data.remaining() == 0) {
         data.flip();
         processData();
-        data = dataLengthBuffer = null; 
+        dataLengthBuffer.flip();
+        data = null; 
       }
       return count;
     }
 
     private void processData() throws  IOException, InterruptedException {
-      byte[] bytes = new byte[dataLength];
-      data.get(bytes);
-      DataInputStream dis = new DataInputStream(new ByteArrayInputStream(bytes));
+      DataInputStream dis =
+          new DataInputStream(new ByteArrayInputStream( data.array() ));
       int id = dis.readInt();                    // try to read an id
         
       if (LOG.isDebugEnabled())
"
hadoop,c2b57e65194b2f21c3d6b821e75eb308f53aee25,"HADOOP-637.  Fix a memory leak in IPC server.  Contributed by Raghu.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@473062 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-09 20:25:30,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/SocketChannelOutputStream.java b/src/java/org/apache/hadoop/ipc/SocketChannelOutputStream.java
index e52baa5..a63d77a 100644
--- a/src/java/org/apache/hadoop/ipc/SocketChannelOutputStream.java
+++ b/src/java/org/apache/hadoop/ipc/SocketChannelOutputStream.java
@@ -42,10 +42,10 @@
     /** Constructor.
      * 
      */
-    public SocketChannelOutputStream(SocketChannel channel, int bufferSize)
+    public SocketChannelOutputStream(SocketChannel channel)
     {
         this.channel = channel;
-        buffer = ByteBuffer.allocateDirect(bufferSize);
+        buffer = ByteBuffer.allocate(8); // only for small writes
     }
 
     /* ------------------------------------------------------------------------------- */
@@ -85,16 +85,8 @@
      */
     public void write(byte[] buf, int offset, int length) throws IOException
     {
-        if (length > buffer.capacity())
-            flush = ByteBuffer.wrap(buf,offset,length);
-        else
-         {
-             buffer.clear();
-             buffer.put(buf,offset,length);
-             buffer.flip();
-             flush = buffer;
-         }
-         flushBuffer();
+        flush = ByteBuffer.wrap(buf,offset,length);
+        flushBuffer();
     }
 
     /* ------------------------------------------------------------------------------- */
@@ -103,16 +95,8 @@
      */
     public void write(byte[] buf) throws IOException
     {
-        if (buf.length > buffer.capacity())
-            flush = ByteBuffer.wrap(buf);
-        else
-         {
-             buffer.clear();
-             buffer.put(buf);
-             buffer.flip();
-             flush = buffer;
-         }
-         flushBuffer();
+        flush = ByteBuffer.wrap(buf);
+        flushBuffer();
     }
 
 
@@ -144,6 +128,7 @@
                 }
             }
         }
+        flush = null;
     }
 
     /* ------------------------------------------------------------------------------- */
"
hadoop,c733c780a12ab7698b33783e5a42245d7e8918a9,"HADOOP-694.  Fix a NullPointerException in jobtracker.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@472709 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-09 00:03:05,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index 9739c49..109eccd 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -140,11 +140,16 @@
                       String trackerName = getAssignedTracker(taskId);
                       TaskTrackerStatus trackerStatus = 
                         getTaskTracker(trackerName);
-                      job.failedTask(tip, taskId, ""Error launching task"", 
-                                     tip.isMapTask()? TaskStatus.Phase.MAP:
-                                       TaskStatus.Phase.STARTING,
-                                     trackerStatus.getHost(), trackerName,
-                                     myMetrics);
+                      // This might happen when the tasktracker has already
+                      // expired and this thread tries to call failedtask
+                      // again. expire tasktracker should have called failed
+                      // task!
+                      if (trackerStatus != null)
+                        job.failedTask(tip, taskId, ""Error launching task"", 
+                                       tip.isMapTask()? TaskStatus.Phase.MAP:
+                                         TaskStatus.Phase.STARTING,
+                                       trackerStatus.getHost(), trackerName,
+                                       myMetrics);
                     }
                     itr.remove();
                   } else {
"
hadoop,237a92b18fa6bcf5977e4c987d4e2dc4e9988b77,"HADOOP-604.  Fix some synchronization issues and a NullPointerException in DFS datanode.  Contributed by Raghu.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@472681 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-08 22:59:50,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSDataset.java b/src/java/org/apache/hadoop/dfs/FSDataset.java
index 0e84046..7b448ec 100644
--- a/src/java/org/apache/hadoop/dfs/FSDataset.java
+++ b/src/java/org/apache/hadoop/dfs/FSDataset.java
@@ -34,7 +34,7 @@
  ***************************************************/
 class FSDataset implements FSConstants {
 
-	
+
   /**
      * A node type that can be built into a tree reflecting the
      * hierarchy of blocks on the local disk.
@@ -274,7 +274,7 @@
         this.volumes = volumes;
       }
       
-      FSVolume getNextVolume(long blockSize) throws IOException {
+      synchronized FSVolume getNextVolume(long blockSize) throws IOException {
         int startVolume = curVolume;
         while (true) {
           FSVolume volume = volumes[curVolume];
@@ -302,25 +302,25 @@
         return remaining;
       }
       
-      void getBlockInfo(TreeSet<Block> blockSet) {
+      synchronized void getBlockInfo(TreeSet<Block> blockSet) {
         for (int idx = 0; idx < volumes.length; idx++) {
           volumes[idx].getBlockInfo(blockSet);
         }
       }
       
-      void getVolumeMap(HashMap<Block, FSVolume> volumeMap) {
+      synchronized void getVolumeMap(HashMap<Block, FSVolume> volumeMap) {
         for (int idx = 0; idx < volumes.length; idx++) {
           volumes[idx].getVolumeMap(volumeMap);
         }
       }
       
-      void getBlockMap(HashMap<Block, File> blockMap) {
+      synchronized void getBlockMap(HashMap<Block, File> blockMap) {
         for (int idx = 0; idx < volumes.length; idx++) {
           volumes[idx].getBlockMap(blockMap);
         }
       }
       
-      void checkDirs() throws DiskErrorException {
+      synchronized void checkDirs() throws DiskErrorException {
         for (int idx = 0; idx < volumes.length; idx++) {
           volumes[idx].checkDirs();
         }
@@ -391,10 +391,11 @@
     /**
      * Get a stream of data from the indicated block.
      */
-    public InputStream getBlockData(Block b) throws IOException {
+    public synchronized InputStream getBlockData(Block b) throws IOException {
         if (! isValidBlock(b)) {
             throw new IOException(""Block "" + b + "" is not valid."");
         }
+        // File should be opened with the lock.
         return new FileInputStream(getFile(b));
     }
 
@@ -414,7 +415,7 @@
         // Serialize access to /tmp, and check if file already there.
         //
         File f = null;
-        synchronized (ongoingCreates) {
+        synchronized ( this ) {
             //
             // Is it already in the create process?
             //
@@ -422,11 +423,12 @@
                 throw new IOException(""Block "" + b +
                     "" has already been started (though not completed), and thus cannot be created."");
             }
-
-            FSVolume v = volumes.getNextVolume(blockSize);
-            
-            // create temporary file to hold block in the designated volume
-            f = v.createTmpFile(b);
+            FSVolume v = null;
+            synchronized ( volumes ) {
+              v = volumes.getNextVolume(blockSize);
+              // create temporary file to hold block in the designated volume
+              f = v.createTmpFile(b);
+            }
             ongoingCreates.put(b, f);
             volumeMap.put(b, v);
         }
@@ -450,8 +452,7 @@
     /**
      * Complete the block write!
      */
-    public void finalizeBlock(Block b) throws IOException {
-      synchronized (ongoingCreates) {
+    public synchronized void finalizeBlock(Block b) throws IOException {
         File f = ongoingCreates.get(b);
         if (f == null || ! f.exists()) {
           throw new IOException(""No temporary file "" + f + "" for block "" + b);
@@ -460,10 +461,12 @@
         b.setNumBytes(finalLen);
         FSVolume v = volumeMap.get(b);
         
-        File dest = v.addBlock(b, f);
+        File dest = null;
+        synchronized ( volumes ) {
+          dest = v.addBlock(b, f);
+        }
         blockMap.put(b, dest);
         ongoingCreates.remove(b);
-      }
     }
 
     /**
@@ -495,12 +498,15 @@
      */
     public void invalidate(Block invalidBlks[]) throws IOException {
       for (int i = 0; i < invalidBlks.length; i++) {
-        File f = getFile(invalidBlks[i]);
-        if (!f.delete()) {
-          throw new IOException(""Unexpected error trying to delete block ""
-              + invalidBlks[i] + "" at file "" + f);
-        }
-        blockMap.remove(invalidBlks[i]);
+        synchronized ( this ) {
+          File f = getFile(invalidBlks[i]);
+          if (!f.delete()) {
+            throw new IOException(""Unexpected error trying to delete block ""
+                                  + invalidBlks[i] + "" at file "" + f);
+          }
+          blockMap.remove(invalidBlks[i]);
+          volumeMap.remove(invalidBlks[i]);
+        } 
         DataNode.LOG.info(""Deleting block "" + invalidBlks[i]);
       }
     }
@@ -508,7 +514,7 @@
     /**
      * Turn the block identifier into a filename.
      */
-    File getFile(Block b) {
+    synchronized File getFile(Block b) {
       return blockMap.get(b);
     }
 
"
hadoop,fd4ed70abfa46bbf1057da5819933c97a5c24b6d,"HADOOP-645.  Fix a bug in contrib/streaming when -reducer is NONE.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@472283 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-07 21:30:46,Doug Cutting,"diff --git a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java
index 57ccbdb..4311a7b 100644
--- a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java
+++ b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java
@@ -291,7 +291,6 @@
           }
           f = null;
       }
-      System.out.println(""XXX2 argvSplit[0] = "" + argvSplit[0]);
       logprintln(""PipeMapRed exec "" + Arrays.asList(argvSplit));
       logprintln(""sideEffectURI_="" + sideEffectURI_);
 
@@ -580,9 +579,6 @@
           logprintln(""closing "" + sideEffectURI_);
           if (sideEffectOut_ != null) sideEffectOut_.close();
           logprintln(""closed  "" + sideEffectURI_);
-          if (sideEffectURI_.getScheme().equals(""file"")) {
-            logprintln(""size  "" + new File(sideEffectURI_).length());
-          }
           if (useSingleSideOutputURI_) {
             // With sideEffectPath_ we wrote in-place. 
             // Possibly a named pipe set up by user or a socket.
"
hadoop,38afdcbba8c551b2686eece909dd1cb69ea22007,"HADOOP-682.  Fix DFS format command to work correctly when configured with a non-existent directory.  Contributed by Sanjay.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@472212 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-07 19:21:26,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/NameNode.java b/src/java/org/apache/hadoop/dfs/NameNode.java
index b021ac6..eec41c6 100644
--- a/src/java/org/apache/hadoop/dfs/NameNode.java
+++ b/src/java/org/apache/hadoop/dfs/NameNode.java
@@ -574,6 +574,9 @@
                 System.err.println(""Formatted ""+dirs[idx]);
               }
               System.in.read(); // discard the enter-key
+            }else{
+              format(dirs[idx]);
+              System.err.println(""Formatted ""+dirs[idx]);
             }
           }
           System.exit(aborted ? 1 : 0);
"
hadoop,30d2a9a7d9d4cb5b5fbf0d8a14d6cef65e5721c3,"HADOOP-665.  Extend many DFSShell commands to accept multiple arguments.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@471023 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-03 22:33:53,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSShell.java b/src/java/org/apache/hadoop/dfs/DFSShell.java
index 45fe907..90847f2 100644
--- a/src/java/org/apache/hadoop/dfs/DFSShell.java
+++ b/src/java/org/apache/hadoop/dfs/DFSShell.java
@@ -608,6 +608,64 @@
     }
 
     /**
+     * Apply operation specified by 'cmd' on all parameters
+     * starting from argv[startindex].
+     */
+    private int doall(String cmd, String argv[], Configuration conf, 
+                      int startindex) {
+      int exitCode = 0;
+      int i = startindex;
+      //
+      // for each source file, issue the command
+      //
+      for (; i < argv.length; i++) {
+        try {
+          //
+          // issue the command to the remote dfs server
+          //
+          if (""-cat"".equals(cmd)) {
+              cat(argv[i]);
+          } else if (""-mkdir"".equals(cmd)) {
+              mkdir(argv[i]);
+          } else if (""-rm"".equals(cmd)) {
+              delete(argv[i], false);
+          } else if (""-rmr"".equals(cmd)) {
+              delete(argv[i], true);
+          } else if (""-du"".equals(cmd)) {
+              du(argv[i]);
+          } else if (""-ls"".equals(cmd)) {
+              ls(argv[i], false);
+          } else if (""-lsr"".equals(cmd)) {
+              ls(argv[i], true);
+          }
+        } catch (RemoteException e) {
+          //
+          // This is a error returned by hadoop server. Print
+          // out the first line of the error mesage.
+          //
+          exitCode = -1;
+          try {
+            String[] content;
+            content = e.getLocalizedMessage().split(""\n"");
+            System.err.println(cmd.substring(1) + "": "" +
+                               content[0]);
+          } catch (Exception ex) {
+            System.err.println(cmd.substring(1) + "": "" +
+                               ex.getLocalizedMessage());
+          }
+        } catch (IOException e) {
+          //
+          // IO exception encountered locally.
+          //
+          exitCode = -1;
+          System.err.println(cmd.substring(1) + "": "" +
+                             e.getLocalizedMessage());
+        }
+      }
+      return exitCode;
+    }
+
+    /**
      * Displays format of commands.
      * 
      */
@@ -703,9 +761,7 @@
                   printUsage(cmd);
                   return exitCode;
                 }
-        } else if (""-rm"".equals(cmd) || ""-rmr"".equals(cmd) ||
-                 ""-cat"".equals(cmd) || ""-mkdir"".equals(cmd) ||
-                 ""-safemode"".equals(cmd)) {
+        } else if (""-safemode"".equals(cmd)) {
                 if (argv.length != 2) {
                   printUsage(cmd);
                   return exitCode;
@@ -720,6 +776,12 @@
                   printUsage(cmd);
                   return exitCode;
                 }
+        } else if (""-rm"".equals(cmd) || ""-rmr"".equals(cmd) ||
+                   ""-cat"".equals(cmd) || ""-mkdir"".equals(cmd)) {
+                if (argv.length < 2) {
+                  printUsage(cmd);
+                  return exitCode;
+                }
         }
 
         // initialize DFSShell
@@ -748,30 +810,39 @@
                 else
                     copyMergeToLocal(argv[i++], new Path(argv[i++]));
             } else if (""-cat"".equals(cmd)) {
-                cat(argv[i++]);
+                doall(cmd, argv, conf, i);
             } else if (""-moveToLocal"".equals(cmd)) {
                 moveToLocal(argv[i++], new Path(argv[i++]));
             } else if (""-setrep"".equals(cmd)) {
             	setReplication(argv, i);           
             } else if (""-ls"".equals(cmd)) {
-                String arg = i < argv.length ? argv[i++] : """";
-                ls(arg, false);
+                if (i < argv.length) {
+                    doall(cmd, argv, conf, i);
+                } else {
+                    ls("""", false);
+                } 
             } else if (""-lsr"".equals(cmd)) {
-                String arg = i < argv.length ? argv[i++] : """";
-                ls(arg, true);
+                if (i < argv.length) {
+                    doall(cmd, argv, conf, i);
+                } else {
+                    ls("""", true);
+                } 
             } else if (""-mv"".equals(cmd)) {
                 exitCode = rename(argv, conf);
             } else if (""-cp"".equals(cmd)) {
                 exitCode = copy(argv, conf);
             } else if (""-rm"".equals(cmd)) {
-                delete(argv[i++], false);
+                doall(cmd, argv, conf, i);
             } else if (""-rmr"".equals(cmd)) {
-                delete(argv[i++], true);
+                doall(cmd, argv, conf, i);
             } else if (""-du"".equals(cmd)) {
-                String arg = i < argv.length ? argv[i++] : """";
-                du(arg);
+                if (i < argv.length) {
+                    doall(cmd, argv, conf, i);
+                } else {
+                    du("""");
+                }
             } else if (""-mkdir"".equals(cmd)) {
-                mkdir(argv[i++]);
+                doall(cmd, argv, conf, i);
             } else if (""-report"".equals(cmd)) {
                 report();
             } else if (""-safemode"".equals(cmd)) {
"
hadoop,018ed021d8a8c349964a1e314ea3d521f170fc9b,"HADOOP-671.  Fix file cache to check for pre-existence before creating symlinks.   Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470226 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-02 04:28:46,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/filecache/DistributedCache.java b/src/java/org/apache/hadoop/filecache/DistributedCache.java
index 4366ffa..3966b68 100644
--- a/src/java/org/apache/hadoop/filecache/DistributedCache.java
+++ b/src/java/org/apache/hadoop/filecache/DistributedCache.java
@@ -164,19 +164,23 @@
     boolean doSymlink = getSymlink(conf);
     FileSystem dfs = getFileSystem(cache, conf);
     b = ifExistsAndFresh(cacheStatus, cache, dfs, md5, conf);
+    String link = currentWorkDir.toString() + Path.SEPARATOR + cache.getFragment();
+    File flink = new File(link);
     if (b) {
       if (isArchive) {
-        if (doSymlink)
-        FileUtil.symLink(cacheStatus.localLoadPath.toString(), 
-            currentWorkDir.toString() + Path.SEPARATOR + cache.getFragment());
-        
+        if (doSymlink){
+          if (!flink.exists())
+            FileUtil.symLink(cacheStatus.localLoadPath.toString(), 
+                link);
+        }
         return cacheStatus.localLoadPath;
       }
       else {
-        if (doSymlink)
-          FileUtil.symLink(cacheFilePath(cacheStatus.localLoadPath).toString(), 
-              currentWorkDir.toString() + Path.SEPARATOR + cache.getFragment());
-       
+        if (doSymlink){
+          if (!flink.exists())
+            FileUtil.symLink(cacheFilePath(cacheStatus.localLoadPath).toString(), 
+              link);
+        }
         return cacheFilePath(cacheStatus.localLoadPath);
       }
     } else {
@@ -219,16 +223,21 @@
       cacheStatus.currentStatus = true;
       cacheStatus.md5 = checkSum;
     }
+    
     if (isArchive){
-      if (doSymlink)
-        FileUtil.symLink(cacheStatus.localLoadPath.toString(), 
-            currentWorkDir.toString() + Path.SEPARATOR + cache.getFragment());
+      if (doSymlink){
+        if (!flink.exists())
+          FileUtil.symLink(cacheStatus.localLoadPath.toString(), 
+            link);
+      }
       return cacheStatus.localLoadPath;
     }
     else {
-      if (doSymlink)
-        FileUtil.symLink(cacheFilePath(cacheStatus.localLoadPath).toString(), 
-            currentWorkDir.toString() + Path.SEPARATOR + cache.getFragment());
+      if (doSymlink){
+        if (!flink.exists())
+          FileUtil.symLink(cacheFilePath(cacheStatus.localLoadPath).toString(), 
+            link);
+      }
       return cacheFilePath(cacheStatus.localLoadPath);
     }
   }
"
hadoop,cbc4151701fe2e7c7d99c34717435224acc18854,"HADOOP-373.  Consistently check the value of FileSystem.mkdirs().  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470195 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-02 02:37:29,Doug Cutting,"diff --git a/src/examples/org/apache/hadoop/examples/NNBench.java b/src/examples/org/apache/hadoop/examples/NNBench.java
index 501f088..1e63058 100644
--- a/src/examples/org/apache/hadoop/examples/NNBench.java
+++ b/src/examples/org/apache/hadoop/examples/NNBench.java
@@ -81,7 +81,9 @@
                     Reporter reporter) throws IOException {
       int nFiles = ((IntWritable) value).get();
       Path taskDir = new Path(topDir, taskId);
-      fileSys.mkdirs(taskDir);
+      if (!fileSys.mkdirs(taskDir)) {
+        throw new IOException(""Mkdirs failed to create "" + taskDir.toString());
+      }
       byte[] buffer = new byte[32768];
       for (int index = 0; index < nFiles; index++) {
         FSDataOutputStream out = fileSys.create(
@@ -186,7 +188,11 @@
       return;
     }
     fileSys.delete(tmpDir);
-    fileSys.mkdirs(inDir);
+    if (!fileSys.mkdirs(inDir)) {
+      System.out.println(""Error: Mkdirs failed to create "" + 
+                         inDir.toString());
+      return;
+    }
 
     for(int i=0; i < numMaps; ++i) {
       Path file = new Path(inDir, ""part""+i);
"
hadoop,cbc4151701fe2e7c7d99c34717435224acc18854,"HADOOP-373.  Consistently check the value of FileSystem.mkdirs().  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470195 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-02 02:37:29,Doug Cutting,"diff --git a/src/examples/org/apache/hadoop/examples/PiBenchmark.java b/src/examples/org/apache/hadoop/examples/PiBenchmark.java
index 6aaef1d..cee4d75 100644
--- a/src/examples/org/apache/hadoop/examples/PiBenchmark.java
+++ b/src/examples/org/apache/hadoop/examples/PiBenchmark.java
@@ -168,7 +168,9 @@
     Path outDir = new Path(tmpDir, ""out"");
     FileSystem fileSys = FileSystem.get(jobConf);
     fileSys.delete(tmpDir);
-    fileSys.mkdirs(inDir);
+    if (!fileSys.mkdirs(inDir)) {
+      throw new IOException(""Mkdirs failed to create "" + inDir.toString());
+    }
     
     jobConf.setInputPath(inDir);
     jobConf.setOutputPath(outDir);
"
hadoop,cbc4151701fe2e7c7d99c34717435224acc18854,"HADOOP-373.  Consistently check the value of FileSystem.mkdirs().  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470195 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-02 02:37:29,Doug Cutting,"diff --git a/src/examples/org/apache/hadoop/examples/RandomWriter.java b/src/examples/org/apache/hadoop/examples/RandomWriter.java
index bf2b5c8..c21bbda 100644
--- a/src/examples/org/apache/hadoop/examples/RandomWriter.java
+++ b/src/examples/org/apache/hadoop/examples/RandomWriter.java
@@ -189,7 +189,11 @@
       return;
     }
     fileSys.delete(tmpDir);
-    fileSys.mkdirs(inDir);
+    if (!fileSys.mkdirs(inDir)) {
+      System.out.println(""Error: Mkdirs failed to create "" + 
+                         inDir.toString());
+      return;
+    }
     NumberFormat numberFormat = NumberFormat.getInstance();
     numberFormat.setMinimumIntegerDigits(6);
     numberFormat.setGroupingUsed(false);
"
hadoop,cbc4151701fe2e7c7d99c34717435224acc18854,"HADOOP-373.  Consistently check the value of FileSystem.mkdirs().  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470195 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-02 02:37:29,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSShell.java b/src/java/org/apache/hadoop/dfs/DFSShell.java
index 63bd3ca..45fe907 100644
--- a/src/java/org/apache/hadoop/dfs/DFSShell.java
+++ b/src/java/org/apache/hadoop/dfs/DFSShell.java
@@ -309,7 +309,9 @@
      */
     public void mkdir(String src) throws IOException {
         Path f = new Path(src);
-        fs.mkdirs(f);
+        if (!fs.mkdirs(f)) {
+          System.out.println(""Mkdirs failed to create "" + src);
+        }
     }
     
     /**
"
hadoop,cbc4151701fe2e7c7d99c34717435224acc18854,"HADOOP-373.  Consistently check the value of FileSystem.mkdirs().  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470195 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-02 02:37:29,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSDataset.java b/src/java/org/apache/hadoop/dfs/FSDataset.java
index df76aa9..0e84046 100644
--- a/src/java/org/apache/hadoop/dfs/FSDataset.java
+++ b/src/java/org/apache/hadoop/dfs/FSDataset.java
@@ -48,13 +48,17 @@
 
         /**
          */
-        public FSDir(File dir, int myIdx, FSDir[] siblings) {
+        public FSDir(File dir, int myIdx, FSDir[] siblings) 
+            throws IOException {
             this.dir = dir;
             this.myIdx = myIdx;
             this.siblings = siblings;
             this.children = null;
             if (! dir.exists()) {
-              dir.mkdirs();
+              if (! dir.mkdirs()) {
+                throw new IOException(""Mkdirs failed to create "" + 
+                                      dir.toString());
+              }
             } else {
               File[] files = dir.listFiles();
               int numChildren = 0;
@@ -80,7 +84,7 @@
 
         /**
          */
-        public File addBlock(Block b, File src) {
+        public File addBlock(Block b, File src) throws IOException {
             if (numBlocks < maxBlocksPerDir) {
               File dest = new File(dir, b.getBlockName());
               src.renameTo(dest);
@@ -194,7 +198,11 @@
         if (tmpDir.exists()) {
           FileUtil.fullyDelete(tmpDir);
         }
-        tmpDir.mkdirs();
+        if (!tmpDir.mkdirs()) {
+          if (!tmpDir.isDirectory()) {
+            throw new IOException(""Mkdirs failed to create "" + tmpDir.toString());
+          }
+        }
         this.usage = new DF(dir, conf);
       }
       
@@ -232,7 +240,7 @@
         return f;
       }
       
-      File addBlock(Block b, File f) {
+      File addBlock(Block b, File f) throws IOException {
         return dataDir.addBlock(b, f);
       }
       
"
hadoop,cbc4151701fe2e7c7d99c34717435224acc18854,"HADOOP-373.  Consistently check the value of FileSystem.mkdirs().  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470195 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-02 02:37:29,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSDirectory.java b/src/java/org/apache/hadoop/dfs/FSDirectory.java
index 2635a9a..36f09d4 100644
--- a/src/java/org/apache/hadoop/dfs/FSDirectory.java
+++ b/src/java/org/apache/hadoop/dfs/FSDirectory.java
@@ -366,7 +366,9 @@
 
         // Always do an implicit mkdirs for parent directory tree
         String pathString = path.toString();
-        mkdirs(new Path(pathString).getParent().toString());
+        if( ! mkdirs(new Path(pathString).getParent().toString()) ) {
+           return false;
+        }
         INode newNode = new INode( new File(pathString).getName(), blocks, replication);
         if( ! unprotectedAddFile(path, newNode) ) {
            NameNode.stateChangeLog.info(""DIR* FSDirectory.addFile: ""
"
hadoop,cbc4151701fe2e7c7d99c34717435224acc18854,"HADOOP-373.  Consistently check the value of FileSystem.mkdirs().  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470195 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-02 02:37:29,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/filecache/DistributedCache.java b/src/java/org/apache/hadoop/filecache/DistributedCache.java
index f3a3206..4366ffa 100644
--- a/src/java/org/apache/hadoop/filecache/DistributedCache.java
+++ b/src/java/org/apache/hadoop/filecache/DistributedCache.java
@@ -192,7 +192,10 @@
       localFs.delete(cacheStatus.localLoadPath);
       Path parchive = new Path(cacheStatus.localLoadPath,
                                new Path(cacheStatus.localLoadPath.getName()));
-      localFs.mkdirs(cacheStatus.localLoadPath);
+      if (!localFs.mkdirs(cacheStatus.localLoadPath)) {
+          throw new IOException(""Mkdirs failed to create directory "" + 
+                                cacheStatus.localLoadPath.toString());
+      }
       String cacheId = cache.getPath();
       dfs.copyToLocalFile(new Path(cacheId), parchive);
       dfs.copyToLocalFile(new Path(cacheId + ""_md5""), new Path(parchive
"
hadoop,cbc4151701fe2e7c7d99c34717435224acc18854,"HADOOP-373.  Consistently check the value of FileSystem.mkdirs().  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470195 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-02 02:37:29,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FileUtil.java b/src/java/org/apache/hadoop/fs/FileUtil.java
index 379c2c1..355033f 100644
--- a/src/java/org/apache/hadoop/fs/FileUtil.java
+++ b/src/java/org/apache/hadoop/fs/FileUtil.java
@@ -67,7 +67,9 @@
     dst = checkDest(src.getName(), dstFS, dst);
 
     if (srcFS.isDirectory(src)) {
-      dstFS.mkdirs(dst);
+      if (!dstFS.mkdirs(dst)) {
+        return false;
+      }
       Path contents[] = srcFS.listPaths(src);
       for (int i = 0; i < contents.length; i++) {
         copy(srcFS, contents[i], dstFS, new Path(dst, contents[i].getName()),
@@ -137,7 +139,9 @@
     dst = checkDest(src.getName(), dstFS, dst);
 
     if (src.isDirectory()) {
-      dstFS.mkdirs(dst);
+      if (!dstFS.mkdirs(dst)) {
+        return false;
+      }
       File contents[] = src.listFiles();
       for (int i = 0; i < contents.length; i++) {
         copy(contents[i], dstFS, new Path(dst, contents[i].getName()),
@@ -166,7 +170,9 @@
     dst = checkDest(src.getName(), dst);
 
     if (srcFS.isDirectory(src)) {
-      dst.mkdirs();
+      if (!dst.mkdirs()) {
+        return false;
+      }
       Path contents[] = srcFS.listPaths(src);
       for (int i = 0; i < contents.length; i++) {
         copy(srcFS, contents[i], new File(dst, contents[i].getName()),
@@ -281,7 +287,12 @@
           InputStream in = zipFile.getInputStream(entry);
           try {
             File file = new File(unzipDir, entry.getName());
-            file.getParentFile().mkdirs();
+            if (!file.getParentFile().mkdirs()) {           
+              if (!file.getParentFile().isDirectory()) {
+                throw new IOException(""Mkdirs failed to create "" + 
+                                      file.getParentFile().toString());
+              }
+            }
             OutputStream out = new FileOutputStream(file);
             try {
               byte[] buffer = new byte[8192];
"
hadoop,cbc4151701fe2e7c7d99c34717435224acc18854,"HADOOP-373.  Consistently check the value of FileSystem.mkdirs().  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470195 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-02 02:37:29,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/LocalFileSystem.java b/src/java/org/apache/hadoop/fs/LocalFileSystem.java
index 4582c04..0cbff56 100644
--- a/src/java/org/apache/hadoop/fs/LocalFileSystem.java
+++ b/src/java/org/apache/hadoop/fs/LocalFileSystem.java
@@ -179,9 +179,11 @@
             throw new IOException(""File already exists:""+f);
         }
         Path parent = f.getParent();
-        if (parent != null)
-          mkdirs(parent);
-
+        if (parent != null) {
+          if (!mkdirs(parent)) {
+            throw new IOException(""Mkdirs failed to create "" + parent.toString());
+          }
+        }
         return new LocalFSFileOutputStream(f);
     }
 
@@ -379,7 +381,11 @@
 
         // move the file there
         File badDir = new File(dir, ""bad_files"");
-        badDir.mkdirs();
+        if (!badDir.mkdirs()) {
+          if (!badDir.isDirectory()) {
+            throw new IOException(""Mkdirs failed to create "" + badDir.toString());
+          }
+        }
         String suffix = ""."" + new Random().nextInt();
         File badFile = new File(badDir,f.getName()+suffix);
         LOG.warn(""Moving bad file "" + f + "" to "" + badFile);
"
hadoop,cbc4151701fe2e7c7d99c34717435224acc18854,"HADOOP-373.  Consistently check the value of FileSystem.mkdirs().  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470195 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-02 02:37:29,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/MapFile.java b/src/java/org/apache/hadoop/io/MapFile.java
index 25da8cc..59d30f5 100644
--- a/src/java/org/apache/hadoop/io/MapFile.java
+++ b/src/java/org/apache/hadoop/io/MapFile.java
@@ -137,8 +137,9 @@
       this.lastKey = comparator.newKey();
 
       Path dir = new Path(dirName);
-      fs.mkdirs(dir);
-
+      if (!fs.mkdirs(dir)) {
+          throw new IOException(""Mkdirs failed to create directory "" + dir.toString());
+      }
       Path dataFile = new Path(dir, DATA_FILE_NAME);
       Path indexFile = new Path(dir, INDEX_FILE_NAME);
 
"
hadoop,cbc4151701fe2e7c7d99c34717435224acc18854,"HADOOP-373.  Consistently check the value of FileSystem.mkdirs().  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470195 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-02 02:37:29,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobHistory.java b/src/java/org/apache/hadoop/mapred/JobHistory.java
index 44617d9..67aabb0 100644
--- a/src/java/org/apache/hadoop/mapred/JobHistory.java
+++ b/src/java/org/apache/hadoop/mapred/JobHistory.java
@@ -88,7 +88,9 @@
       try{
         File logDir = new File(LOG_DIR); 
         if( ! logDir.exists() ){
-          logDir.mkdirs(); 
+          if( ! logDir.mkdirs() ){
+            throw new IOException(""Mkdirs failed to create "" + logDir.toString());
+          }
         }
         masterIndex = 
           new PrintWriter(
"
hadoop,cbc4151701fe2e7c7d99c34717435224acc18854,"HADOOP-373.  Consistently check the value of FileSystem.mkdirs().  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470195 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-02 02:37:29,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index c392eed..9739c49 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -495,7 +495,9 @@
         this.systemDir = jobConf.getSystemDir();
         this.fs = FileSystem.get(conf);
         fs.delete(systemDir);
-        fs.mkdirs(systemDir);
+        if (!fs.mkdirs(systemDir)) {
+          throw new IOException(""Mkdirs failed to create "" + systemDir.toString());
+        }
 
         // Same with 'localDir' except it's always on the local disk.
         jobConf.deleteLocalFiles(SUBDIR);
"
hadoop,cbc4151701fe2e7c7d99c34717435224acc18854,"HADOOP-373.  Consistently check the value of FileSystem.mkdirs().  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470195 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-02 02:37:29,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/LocalJobRunner.java b/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
index f5dea69..adc48d8 100644
--- a/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
+++ b/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
@@ -115,7 +115,10 @@
           String mapId = (String)mapIds.get(i);
           Path mapOut = this.mapoutputFile.getOutputFile(mapId, 0);
           Path reduceIn = this.mapoutputFile.getInputFile(i, reduceId);
-          localFs.mkdirs(reduceIn.getParent());
+          if (!localFs.mkdirs(reduceIn.getParent())) {
+            throw new IOException(""Mkdirs failed to create "" + 
+                                  reduceIn.getParent().toString());
+          }
           if (!localFs.rename(mapOut, reduceIn))
             throw new IOException(""Couldn't rename "" + mapOut);
           this.mapoutputFile.removeAll(mapId);
"
hadoop,cbc4151701fe2e7c7d99c34717435224acc18854,"HADOOP-373.  Consistently check the value of FileSystem.mkdirs().  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470195 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-02 02:37:29,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskRunner.java b/src/java/org/apache/hadoop/mapred/TaskRunner.java
index 5ad7756..4cf10bd 100644
--- a/src/java/org/apache/hadoop/mapred/TaskRunner.java
+++ b/src/java/org/apache/hadoop/mapred/TaskRunner.java
@@ -124,7 +124,11 @@
       // start with same classpath as parent process
       classPath.append(System.getProperty(""java.class.path""));
       classPath.append(sep);
-      workDir.mkdirs();
+      if (!workDir.mkdirs()) {
+        if (!workDir.isDirectory()) {
+          LOG.fatal(""Mkdirs failed to create "" + workDir.toString());
+        }
+      }
 	  
       String jar = conf.getJar();
       if (jar != null) {       
"
hadoop,cbc4151701fe2e7c7d99c34717435224acc18854,"HADOOP-373.  Consistently check the value of FileSystem.mkdirs().  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470195 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-02 02:37:29,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 459915f..60ea930 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -294,7 +294,11 @@
             File workDir = new File(
                                     new File(localJobFile.toString()).getParent(),
                                     ""work"");
-            workDir.mkdirs();
+            if (!workDir.mkdirs()) {
+              if (!workDir.isDirectory()) {
+                throw new IOException(""Mkdirs failed to create "" + workDir.toString());
+              }
+            }
             RunJar.unJar(new File(localJarFile.toString()), workDir);
           }
           rjob.localized = true;
@@ -831,7 +835,9 @@
               new Path(this.defaultJobConf.getLocalPath(TaskTracker.getJobCacheSubdir()), 
                 (task.getJobId() + Path.SEPARATOR + task.getTaskId()));
            FileSystem localFs = FileSystem.getNamed(""local"", fConf);
-           localFs.mkdirs(localTaskDir);
+           if (!localFs.mkdirs(localTaskDir)) {
+             throw new IOException(""Mkdirs failed to create "" + localTaskDir.toString());
+           }
            Path localTaskFile = new Path(localTaskDir, ""job.xml"");
            task.setJobFile(localTaskFile.toString());
            localJobConf.set(""mapred.local.dir"",
"
hadoop,cbc4151701fe2e7c7d99c34717435224acc18854,"HADOOP-373.  Consistently check the value of FileSystem.mkdirs().  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470195 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-02 02:37:29,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/util/CopyFiles.java b/src/java/org/apache/hadoop/util/CopyFiles.java
index f6fef6c..7e561ea 100644
--- a/src/java/org/apache/hadoop/util/CopyFiles.java
+++ b/src/java/org/apache/hadoop/util/CopyFiles.java
@@ -184,7 +184,12 @@
       // create directories to hold destination file and create destFile
       Path destFile = new Path(destPath, src);
       Path destParent = destFile.getParent();
-      if (destParent != null) { destFileSys.mkdirs(destParent); }
+      if (destParent != null) { 
+        if (!destFileSys.mkdirs(destParent)) {
+          throw new IOException(""Mkdirs failed to create "" + 
+                                destParent.toString()); 
+        }
+      }
       FSDataOutputStream out = destFileSys.create(destFile);
       
       // copy file
@@ -285,7 +290,10 @@
       Path inDir = new Path(jobDirectory, ""in"");
       Path fakeOutDir = new Path(jobDirectory, ""out"");
       FileSystem fileSys = FileSystem.get(jobConf);
-      fileSys.mkdirs(inDir);
+      if (!fileSys.mkdirs(inDir)) {
+        throw new IOException(""Mkdirs failed to create "" +
+                              inDir.toString());
+      }
       jobConf.set(""distcp.job.dir"", jobDirectory.toString());
       
       jobConf.setInputPath(inDir);
@@ -480,7 +488,9 @@
       Path jobDirectory = new Path(jobConf.getSystemDir(), ""distcp_"" + 
           Integer.toString(Math.abs(r.nextInt()), 36));
       Path jobInputDir = new Path(jobDirectory, ""in"");
-      fileSystem.mkdirs(jobInputDir);
+      if (!fileSystem.mkdirs(jobInputDir)) {
+        throw new IOException(""Mkdirs failed to create "" + jobInputDir.toString());
+      }
       jobConf.setInputPath(jobInputDir);
       
       jobConf.set(""distcp.job.dir"", jobDirectory.toString());
"
hadoop,cbc4151701fe2e7c7d99c34717435224acc18854,"HADOOP-373.  Consistently check the value of FileSystem.mkdirs().  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470195 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-02 02:37:29,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/util/RunJar.java b/src/java/org/apache/hadoop/util/RunJar.java
index 78d5022..10a3467 100644
--- a/src/java/org/apache/hadoop/util/RunJar.java
+++ b/src/java/org/apache/hadoop/util/RunJar.java
@@ -41,7 +41,12 @@
           InputStream in = jar.getInputStream(entry);
           try {
             File file = new File(toDir, entry.getName());
-            file.getParentFile().mkdirs();
+            if (!file.getParentFile().mkdirs()) {
+              if (!file.getParentFile().isDirectory()) {
+                throw new IOException(""Mkdirs failed to create "" + 
+                                      file.getParentFile().toString());
+              }
+            }
             OutputStream out = new FileOutputStream(file);
             try {
               byte[] buffer = new byte[8192];
@@ -102,7 +107,12 @@
 
     final File workDir = File.createTempFile(""hadoop-unjar"","""");
     workDir.delete();
-    workDir.mkdirs();
+    if (!workDir.mkdirs()) {
+      if (!workDir.isDirectory()) {
+        System.err.println(""Mkdirs failed to create "" + workDir.toString());
+        System.exit(-1);
+      }
+    }
 
     Runtime.getRuntime().addShutdownHook(new Thread() {
         public void run() {
"
hadoop,cbc4151701fe2e7c7d99c34717435224acc18854,"HADOOP-373.  Consistently check the value of FileSystem.mkdirs().  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470195 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-02 02:37:29,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java b/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java
index 7c34dff..2f97664 100644
--- a/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java
+++ b/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java
@@ -97,7 +97,12 @@
         String[] dirs = conf.getStrings(""dfs.data.dir"");
         for (int idx = 0; idx < dirs.length; idx++) {
           File dataDir = new File(dirs[idx]);
-          dataDir.mkdirs();
+          if (!dataDir.mkdirs()) {      
+            if (!dataDir.isDirectory()) {
+              throw new RuntimeException(""Mkdirs failed to create directory "" +
+                                         dataDir.toString());
+            }
+          }
         }
         node = new DataNode(conf, dirs);
         node.run();
"
hadoop,cbc4151701fe2e7c7d99c34717435224acc18854,"HADOOP-373.  Consistently check the value of FileSystem.mkdirs().  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470195 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-02 02:37:29,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/mapred/MRCaching.java b/src/test/org/apache/hadoop/mapred/MRCaching.java
index 71c079d..a7729e3 100644
--- a/src/test/org/apache/hadoop/mapred/MRCaching.java
+++ b/src/test/org/apache/hadoop/mapred/MRCaching.java
@@ -65,7 +65,9 @@
         // read the cached files (unzipped, unjarred and text)
         // and put it into a single file /tmp/test.txt
         Path file = new Path(""/tmp"");
-        fs.mkdirs(file);
+        if (!fs.mkdirs(file)) {
+          throw new IOException(""Mkdirs failed to create "" + file.toString());
+        }
         Path fileOut = new Path(file, ""test.txt"");
         fs.delete(fileOut);
         DataOutputStream out = fs.create(fileOut);
@@ -130,8 +132,9 @@
     final Path outDir = new Path(outdir);
     FileSystem fs = FileSystem.getNamed(fileSys, conf);
     fs.delete(outDir);
-    fs.mkdirs(inDir);
-
+    if (!fs.mkdirs(inDir)) {
+      throw new IOException(""Mkdirs failed to create "" + inDir.toString());
+    }
     {
       DataOutputStream file = fs.create(new Path(inDir, ""part-0""));
       file.writeBytes(input);
@@ -160,7 +163,9 @@
     Path zipPath = new Path(localPath, new Path(""test.zip""));
     Path cacheTest = new Path(""/tmp/cachedir"");
     fs.delete(cacheTest);
-    fs.mkdirs(cacheTest);
+    if (!fs.mkdirs(cacheTest)) {
+      throw new IOException(""Mkdirs failed to create "" + cacheTest.toString());
+    }
     fs.copyFromLocalFile(txtPath, cacheTest);
     fs.copyFromLocalFile(jarPath, cacheTest);
     fs.copyFromLocalFile(zipPath, cacheTest);
"
hadoop,cbc4151701fe2e7c7d99c34717435224acc18854,"HADOOP-373.  Consistently check the value of FileSystem.mkdirs().  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470195 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-02 02:37:29,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/mapred/MiniMRCluster.java b/src/test/org/apache/hadoop/mapred/MiniMRCluster.java
index a708566..5f772d2 100644
--- a/src/test/org/apache/hadoop/mapred/MiniMRCluster.java
+++ b/src/test/org/apache/hadoop/mapred/MiniMRCluster.java
@@ -116,12 +116,21 @@
                 File localDir = new File(jc.get(""mapred.local.dir""));
                 String mapredDir = """";
                 File ttDir = new File(localDir, Integer.toString(taskTrackerPort) + ""_"" + 0);
-                ttDir.mkdirs();
+                if (!ttDir.mkdirs()) {
+                  if (!ttDir.isDirectory()) {
+                    throw new IOException(""Mkdirs failed to create "" + ttDir.toString());
+                  }
+                }
                 this.localDir[0] = ttDir.getAbsolutePath();
                 mapredDir = ttDir.getAbsolutePath();
                 for (int i = 1; i < numDir; i++){
                   ttDir = new File(localDir, Integer.toString(taskTrackerPort) + ""_"" + i);
                   ttDir.mkdirs();
+                  if (!ttDir.mkdirs()) {
+                    if (!ttDir.isDirectory()) {
+                      throw new IOException(""Mkdirs failed to create "" + ttDir.toString());
+                    }
+                  }
                   this.localDir[i] = ttDir.getAbsolutePath();
                   mapredDir = mapredDir + "","" + ttDir.getAbsolutePath();
                 }
"
hadoop,cbc4151701fe2e7c7d99c34717435224acc18854,"HADOOP-373.  Consistently check the value of FileSystem.mkdirs().  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470195 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-02 02:37:29,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/mapred/PiEstimator.java b/src/test/org/apache/hadoop/mapred/PiEstimator.java
index 0de9735..4354cf9 100644
--- a/src/test/org/apache/hadoop/mapred/PiEstimator.java
+++ b/src/test/org/apache/hadoop/mapred/PiEstimator.java
@@ -164,8 +164,9 @@
     Path outDir = new Path(tmpDir, ""out"");
     FileSystem fileSys = FileSystem.get(jobConf);
     fileSys.delete(tmpDir);
-    fileSys.mkdirs(inDir);
-    
+    if (!fileSys.mkdirs(inDir)) {
+      throw new IOException(""Mkdirs failed to create "" + inDir.toString());
+    }
     jobConf.setInputPath(inDir);
     jobConf.setOutputPath(outDir);
     
"
hadoop,c8139b1f5e869b47aee34f5372a0099fac980e5a,"HADOOP-669.  Fix a problem introduced by HADOOP-90 that could cause DFS to lose files.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470096 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-01 20:47:28,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSImage.java b/src/java/org/apache/hadoop/dfs/FSImage.java
index 23507b9..c8db70a 100644
--- a/src/java/org/apache/hadoop/dfs/FSImage.java
+++ b/src/java/org/apache/hadoop/dfs/FSImage.java
@@ -105,7 +105,7 @@
     
     // Now check all curFiles and see which is the newest
     File curFile = null;
-    long maxTimeStamp = 0;
+    long maxTimeStamp = Long.MIN_VALUE;
     for (int idx = 0; idx < imageDirs.length; idx++) {
       File file = new File(imageDirs[idx], FS_IMAGE);
       if (file.exists()) {
"
hadoop,eea102a75a608651785b2e026139b6cd66914e21,"HADOOP-633.  Keep jobtracker from dying when job initialization throws exceptions.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470034 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-01 18:53:30,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/Client.java b/src/java/org/apache/hadoop/ipc/Client.java
index d891ea8..e9073df 100644
--- a/src/java/org/apache/hadoop/ipc/Client.java
+++ b/src/java/org/apache/hadoop/ipc/Client.java
@@ -122,7 +122,7 @@
          throw new UnknownHostException(""unknown host: "" + address.getHostName());
       }
       this.address = address;
-      this.setName(""Client connection to "" + address.toString());
+      this.setName(""IPC Client connection to "" + address.toString());
       this.setDaemon(true);
     }
 
@@ -421,8 +421,9 @@
 
     Thread t = new ConnectionCuller();
     t.setDaemon(true);
-    t.setName(valueClass.getName()
-              +"" ConnectionCuller maxidletime=""+maxIdleTime+""ms"");
+    t.setName(valueClass.getName() + "" Connection Culler"");
+    LOG.info(valueClass.getName() + 
+             ""Connection culler maxidletime= "" + maxIdleTime + ""ms"");
     t.start();
   }
  
"
hadoop,eea102a75a608651785b2e026139b6cd66914e21,"HADOOP-633.  Keep jobtracker from dying when job initialization throws exceptions.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470034 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-01 18:53:30,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/Server.java b/src/java/org/apache/hadoop/ipc/Server.java
index 3951b9d..e6888c2 100644
--- a/src/java/org/apache/hadoop/ipc/Server.java
+++ b/src/java/org/apache/hadoop/ipc/Server.java
@@ -158,7 +158,7 @@
 
       // Register accepts on the server socket with the selector.
       acceptChannel.register(selector, SelectionKey.OP_ACCEPT);
-      this.setName(""Server listener on port "" + port);
+      this.setName(""IPC Server listener on "" + port);
       this.setDaemon(true);
     }
     /** cleanup connections from connectionList. Choose a random range
@@ -476,7 +476,7 @@
   private class Handler extends Thread {
     public Handler(int instanceNumber) {
       this.setDaemon(true);
-      this.setName(""Server handler ""+ instanceNumber + "" on "" + port);
+      this.setName(""IPC Server handler ""+ instanceNumber + "" on "" + port);
     }
 
     public void run() {
"
hadoop,eea102a75a608651785b2e026139b6cd66914e21,"HADOOP-633.  Keep jobtracker from dying when job initialization throws exceptions.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470034 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-01 18:53:30,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index 77c1d4e..c392eed 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -40,7 +40,6 @@
  * @author Mike Cafarella
  *******************************************************/
 public class JobTracker implements MRConstants, InterTrackerProtocol, JobSubmissionProtocol {
-    static long JOBINIT_SLEEP_INTERVAL = 2000;
     static long RETIRE_JOB_INTERVAL;
     static long RETIRE_JOB_CHECK_INTERVAL;
     static float TASK_ALLOC_EPSILON;
@@ -269,11 +268,9 @@
          */
         public void run() {
             while (shouldRun) {
-                try {
-                    Thread.sleep(RETIRE_JOB_CHECK_INTERVAL);
-                } catch (InterruptedException ie) {
-                }
-                
+              try {
+                Thread.sleep(RETIRE_JOB_CHECK_INTERVAL);
+                 
                 synchronized (jobs) {
                     synchronized (jobsByArrival) {
                         synchronized (jobInitQueue) {
@@ -293,11 +290,14 @@
                         }
                     }
                 }
+              } catch (InterruptedException t) {
+                shouldRun = false;
+              } catch (Throwable t) {
+                LOG.error(""Error in retiring job:\n"" +
+                          StringUtils.stringifyException(t));
+              }
             }
         }
-        public void stopRetirer() {
-            shouldRun = false;
-        }
     }
 
     /////////////////////////////////////////////////////////////////
@@ -308,31 +308,27 @@
         public JobInitThread() {
         }
         public void run() {
-            while (shouldRun) {
-                JobInProgress job = null;
-                synchronized (jobInitQueue) {
-                    if (jobInitQueue.size() > 0) {
-                        job = (JobInProgress) jobInitQueue.elementAt(0);
-                        jobInitQueue.remove(job);
-                    } else {
-                        try {
-                            jobInitQueue.wait(JOBINIT_SLEEP_INTERVAL);
-                        } catch (InterruptedException iex) {
-                        }
-                    }
+          JobInProgress job;
+          while (shouldRun) {
+            job = null;
+            try {
+              synchronized (jobInitQueue) {
+                while (jobInitQueue.isEmpty()) {
+                  jobInitQueue.wait();
                 }
-                try {
-                    if (job != null) {
-                        job.initTasks();
-                    }
-                } catch (Exception e) {
-                    LOG.warn(""job init failed"", e);
-                    job.kill();
-                }
+                job = jobInitQueue.remove(0);
+              }
+              job.initTasks();
+            } catch (InterruptedException t) {
+              shouldRun = false;
+            } catch (Throwable t) {
+              LOG.error(""Job initialization failed:\n"" +
+                        StringUtils.stringifyException(t));
+              if (job != null) {
+                job.kill();
+              }
             }
-        }
-        public void stopIniter() {
-            shouldRun = false;
+          }
         }
     }
 
@@ -430,7 +426,7 @@
     int totalMaps = 0;
     int totalReduces = 0;
     private TreeMap taskTrackers = new TreeMap();
-    Vector jobInitQueue = new Vector();
+    List<JobInProgress> jobInitQueue = new ArrayList();
     ExpireTrackers expireTrackers = new ExpireTrackers();
     Thread expireTrackersThread = null;
     RetireJobs retireJobs = new RetireJobs();
@@ -438,7 +434,8 @@
     JobInitThread initJobs = new JobInitThread();
     Thread initJobsThread = null;
     ExpireLaunchingTasks expireLaunchingTasks = new ExpireLaunchingTasks();
-    Thread expireLaunchingTaskThread = new Thread(expireLaunchingTasks);
+    Thread expireLaunchingTaskThread = new Thread(expireLaunchingTasks,
+                                                  ""expireLaunchingTasks"");
     
     /**
      * It might seem like a bug to maintain a TreeSet of status objects,
@@ -524,11 +521,12 @@
         this.startTime = System.currentTimeMillis();
 
         myMetrics = new JobTrackerMetrics();
-        this.expireTrackersThread = new Thread(this.expireTrackers);
+        this.expireTrackersThread = new Thread(this.expireTrackers,
+                                               ""expireTrackers"");
         this.expireTrackersThread.start();
-        this.retireJobsThread = new Thread(this.retireJobs);
+        this.retireJobsThread = new Thread(this.retireJobs, ""retireJobs"");
         this.retireJobsThread.start();
-        this.initJobsThread = new Thread(this.initJobs);
+        this.initJobsThread = new Thread(this.initJobs, ""initJobs"");
         this.initJobsThread.start();
         expireLaunchingTaskThread.start();
     }
@@ -582,9 +580,8 @@
         }
         if (this.retireJobs != null) {
             LOG.info(""Stopping retirer"");
-            this.retireJobs.stopRetirer();
+            this.retireJobsThread.interrupt();
             try {
-                this.retireJobsThread.interrupt();
                 this.retireJobsThread.join();
             } catch (InterruptedException ex) {
                 ex.printStackTrace();
@@ -592,9 +589,8 @@
         }
         if (this.initJobs != null) {
             LOG.info(""Stopping initer"");
-            this.initJobs.stopIniter();
+            this.initJobsThread.interrupt();
             try {
-                this.initJobsThread.interrupt();
                 this.initJobsThread.join();
             } catch (InterruptedException ex) {
                 ex.printStackTrace();
"
hadoop,eea102a75a608651785b2e026139b6cd66914e21,"HADOOP-633.  Keep jobtracker from dying when job initialization throws exceptions.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470034 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-01 18:53:30,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 9ded3e0..459915f 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -147,7 +147,7 @@
             }
           }
         }
-      });
+      }, ""taskCleanup"");
     {
       taskCleanupThread.setDaemon(true);
       taskCleanupThread.start();
@@ -356,7 +356,7 @@
         // in parallel, as RPC servers can take a long
         // time to shutdown.  (They need to wait a full
         // RPC timeout, which might be 10-30 seconds.)
-        new Thread() {
+        new Thread(""RPC shutdown"") {
             public void run() {
                 if (taskReportServer != null) {
                     taskReportServer.stop();
"
hadoop,778d07319277f837f98647e46c1b1642c32b558c,"HADOOP-663.  Fix a few unit test issues.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@470022 13f79535-47bb-0310-9956-ffa450edef68
",2006-11-01 18:28:36,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/filecache/DistributedCache.java b/src/java/org/apache/hadoop/filecache/DistributedCache.java
index f69a39b..f3a3206 100644
--- a/src/java/org/apache/hadoop/filecache/DistributedCache.java
+++ b/src/java/org/apache/hadoop/filecache/DistributedCache.java
@@ -125,7 +125,7 @@
         if (lcacheStatus.refcount == 0) {
           // delete this cache entry
           FileSystem.getNamed(""local"", conf).delete(lcacheStatus.localLoadPath);
-          cachedArchives.remove(cacheId);
+          it.remove();
         }
       }
     }
"
hadoop,5e8b5802425e13ffecc4dca42a85fdb4bac963e2,"HADOOP-599.  Fix web ui and command line to correctly report DFS filesystem size statistics.  Contributed by Raghu.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469690 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:42:20,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSShell.java b/src/java/org/apache/hadoop/dfs/DFSShell.java
index 60b8769..63bd3ca 100644
--- a/src/java/org/apache/hadoop/dfs/DFSShell.java
+++ b/src/java/org/apache/hadoop/dfs/DFSShell.java
@@ -520,10 +520,16 @@
             ending = "" k"";
         } else if (len < 1024 * 1024 * 1024) {
             val = (1.0 * len) / (1024 * 1024);
-            ending = "" Mb"";
-        } else {
+            ending = "" MB"";
+        } else if (len < 128L * 1024 * 1024 * 1024 ) {
             val = (1.0 * len) / (1024 * 1024 * 1024);
-            ending = "" Gb"";
+            ending = "" GB"";
+        } else if (len < 1024L * 1024 * 1024 * 1024 * 1024) {
+            val = (1.0 * len) / (1024L * 1024 * 1024 * 1024);
+            ending = "" TB"";
+        } else {
+            val = (1.0 * len) / (1024L * 1024 * 1024 * 1024 * 1024);
+            ending = "" PB"";
         }
         return limitDecimal(val, 2) + ending;
     }
"
hadoop,5e8b5802425e13ffecc4dca42a85fdb4bac963e2,"HADOOP-599.  Fix web ui and command line to correctly report DFS filesystem size statistics.  Contributed by Raghu.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469690 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:42:20,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSDataset.java b/src/java/org/apache/hadoop/dfs/FSDataset.java
index ec6afcb..df76aa9 100644
--- a/src/java/org/apache/hadoop/dfs/FSDataset.java
+++ b/src/java/org/apache/hadoop/dfs/FSDataset.java
@@ -261,7 +261,6 @@
     class FSVolumeSet {
       FSVolume[] volumes = null;
       int curVolume = 0;
-      HashMap<String,Long> mountMap = new HashMap<String,Long>();
       
       FSVolumeSet(FSVolume[] volumes) {
         this.volumes = volumes;
@@ -280,27 +279,17 @@
       }
       
       synchronized long getCapacity() throws IOException {
-        for (int idx = 0; idx < volumes.length; idx++) {
-          String mount = volumes[idx].getMount();
-          Long capacity = new Long(volumes[idx].getCapacity());
-          mountMap.put(mount, capacity);
-        }
         long capacity = 0L;
-        for (Iterator<Long> iter = mountMap.values().iterator(); iter.hasNext();) {
-          capacity += iter.next().longValue();
+        for (int idx = 0; idx < volumes.length; idx++) {
+            capacity += volumes[idx].getCapacity();
         }
         return capacity;
       }
       
       synchronized long getRemaining() throws IOException {
-        for (int idx = 0; idx < volumes.length; idx++) {
-          String mount = volumes[idx].getMount();
-          Long remaining = new Long(volumes[idx].getCapacity());
-          mountMap.put(mount, remaining);
-        }
         long remaining = 0L;
-        for (Iterator<Long> iter = mountMap.values().iterator(); iter.hasNext();) {
-          remaining += iter.next().longValue();
+        for (int idx = 0; idx < volumes.length; idx++) {
+          remaining += volumes[idx].getAvailable();
         }
         return remaining;
       }
"
hadoop,5e8b5802425e13ffecc4dca42a85fdb4bac963e2,"HADOOP-599.  Fix web ui and command line to correctly report DFS filesystem size statistics.  Contributed by Raghu.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469690 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:42:20,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/JspHelper.java b/src/java/org/apache/hadoop/dfs/JspHelper.java
index cfcd72f..8de4bcb 100644
--- a/src/java/org/apache/hadoop/dfs/JspHelper.java
+++ b/src/java/org/apache/hadoop/dfs/JspHelper.java
@@ -170,6 +170,6 @@
     public String getSafeModeText() {
       if( ! fsn.isInSafeMode() )
         return """";
-      return ""Safe mode is ON. <em>"" + fsn.getSafeModeTip() + ""<em>"";
+      return ""Safe mode is ON. <em>"" + fsn.getSafeModeTip() + ""</em>"";
     }
 }
"
hadoop,7f4e8be648a3ddd267beb8d47657861d4ec3dda0,"HADOOP-399.  Fix javadoc warnings.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:35:06,Doug Cutting,"diff --git a/src/examples/org/apache/hadoop/examples/ExampleDriver.java b/src/examples/org/apache/hadoop/examples/ExampleDriver.java
index c2d56ba..a85194c 100644
--- a/src/examples/org/apache/hadoop/examples/ExampleDriver.java
+++ b/src/examples/org/apache/hadoop/examples/ExampleDriver.java
@@ -19,15 +19,13 @@
 package org.apache.hadoop.examples;
 import org.apache.hadoop.util.ProgramDriver;
 
+/**
+ * A description of an example program based on its class and a 
+ * human-readable description.
+ * @author Owen O'Malley
+ */
 public class ExampleDriver {
   
-  /**
-   * A description of an example program based on its class and a 
-   * human-readable description.
-   * @author Owen O'Malley
-   * @date april 2006
-   */
-    
     public static void main(String argv[]){
         ProgramDriver pgd = new ProgramDriver();
         try {
"
hadoop,7f4e8be648a3ddd267beb8d47657861d4ec3dda0,"HADOOP-399.  Fix javadoc warnings.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:35:06,Doug Cutting,"diff --git a/src/examples/org/apache/hadoop/examples/PiBenchmark.java b/src/examples/org/apache/hadoop/examples/PiBenchmark.java
index ac687e9..6aaef1d 100644
--- a/src/examples/org/apache/hadoop/examples/PiBenchmark.java
+++ b/src/examples/org/apache/hadoop/examples/PiBenchmark.java
@@ -58,7 +58,7 @@
     
     /** Map method.
      * @param key
-     * @param value not-used.
+     * @param val not-used
      * @param out
      * @param reporter
      */
@@ -101,7 +101,7 @@
           conf = job;
       }
       /** Reduce method.
-       * @ param key
+       * @param key
        * @param values
        * @param output
        * @param reporter
"
hadoop,7f4e8be648a3ddd267beb8d47657861d4ec3dda0,"HADOOP-399.  Fix javadoc warnings.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:35:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/conf/Configuration.java b/src/java/org/apache/hadoop/conf/Configuration.java
index 11de37c..cf4bbec 100644
--- a/src/java/org/apache/hadoop/conf/Configuration.java
+++ b/src/java/org/apache/hadoop/conf/Configuration.java
@@ -58,7 +58,7 @@
  * This String is processed for <b>variable expansion</b>. The available variables are the 
  * <em>System properties</em> and the <em>other properties</em> defined in this Configuration.
  * <p>The only <tt>get*</tt> method that is not processed for variable expansion is
- * {@link getObject} (as it cannot assume that the returned values are String). 
+ * {@link #getObject(String)} (as it cannot assume that the returned values are String). 
  * You can use <tt>getObject</tt> to obtain the raw value of a String property without 
  * variable expansion: if <tt>(String)conf.getObject(""my.jdk"")</tt> is <tt>""JDK ${java.version}""</tt>
  * then conf.get(""my.jdk"")</tt> is <tt>""JDK 1.5.0""</tt> 
"
hadoop,7f4e8be648a3ddd267beb8d47657861d4ec3dda0,"HADOOP-399.  Fix javadoc warnings.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:35:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSShell.java b/src/java/org/apache/hadoop/dfs/DFSShell.java
index 8fcb0f8..60b8769 100644
--- a/src/java/org/apache/hadoop/dfs/DFSShell.java
+++ b/src/java/org/apache/hadoop/dfs/DFSShell.java
@@ -191,12 +191,12 @@
     /**
      * Set the replication for files that match file pattern <i>srcf</i>
      * if it's a directory and recursive is true,
-     * set replication for all the subdirs and those files too
-     * @param newRep: new replication factor
-     * @param srcf: a file pattern specifying source files
-     * @param recursive: if need to set replication factor for files in subdirs
-     * @exception: IOException  
-     * @see org.apache.hadoop.fs.FileSystem.globPaths 
+     * set replication for all the subdirs and those files too.
+     * @param newRep new replication factor
+     * @param srcf a file pattern specifying source files
+     * @param recursive if need to set replication factor for files in subdirs
+     * @throws IOException  
+     * @see org.apache.hadoop.fs.FileSystem#globPaths(Path)
      */
     public void setReplication(short newRep, String srcf, boolean recursive)
         throws IOException {
@@ -248,11 +248,11 @@
     
     
     /**
-     * Get a listing of all files in DFS that match the file pattern <i>srcf</i>
-     * @param srcf: a file pattern specifying source files
-     * @param recursive: if need to list files in subdirs
-     * @exception: IOException  
-     * @see org.apache.hadoop.fs.FileSystem.globPaths 
+     * Get a listing of all files in DFS that match the file pattern <i>srcf</i>.
+     * @param srcf a file pattern specifying source files
+     * @param recursive if need to list files in subdirs
+     * @throws IOException  
+     * @see org.apache.hadoop.fs.FileSystem#globPaths(Path)
      */
     public void ls(String srcf, boolean recursive) throws IOException {
       Path[] srcs = fs.globPaths( new Path(srcf) );
@@ -286,10 +286,10 @@
     }
 
     /**
-     * Show the size of all files in DFS that match the file pattern <i>srcf</i>
-     * @param srcf: a file pattern specifying source files
-     * @exception: IOException  
-     * @see org.apache.hadoop.fs.FileSystem.globPaths 
+     * Show the size of all files in DFS that match the file pattern <i>src</i>
+     * @param src a file pattern specifying source files
+     * @throws IOException  
+     * @see org.apache.hadoop.fs.FileSystem#globPaths(Path)
      */
     public void du(String src) throws IOException {
         Path items[] = fs.listPaths( fs.globPaths( new Path(src) ) );
@@ -317,10 +317,10 @@
      * to a destination dfs file.
      * When moving mutiple files, the destination must be a directory. 
      * Otherwise, IOException is thrown.
-     * @param srcf: a file pattern specifying source files
-     * @param dstf: a destination local file/directory 
-     * @exception: IOException  
-     * @see org.apache.hadoop.fs.FileSystem.globPaths 
+     * @param srcf a file pattern specifying source files
+     * @param dstf a destination local file/directory 
+     * @throws IOException  
+     * @see org.apache.hadoop.fs.FileSystem#globPaths(Path)
      */
     public void rename(String srcf, String dstf) throws IOException {
       Path [] srcs = fs.globPaths( new Path(srcf) );
@@ -403,10 +403,10 @@
      * to a destination dfs file.
      * When copying mutiple files, the destination must be a directory. 
      * Otherwise, IOException is thrown.
-     * @param srcf: a file pattern specifying source files
-     * @param dstf: a destination local file/directory 
-     * @exception: IOException  
-     * @see org.apache.hadoop.fs.FileSystem.globPaths 
+     * @param srcf a file pattern specifying source files
+     * @param dstf a destination local file/directory 
+     * @throws IOException  
+     * @see org.apache.hadoop.fs.FileSystem#globPaths(Path)
      */
     public void copy(String srcf, String dstf, Configuration conf) throws IOException {
       Path [] srcs = fs.globPaths( new Path(srcf) );
@@ -481,11 +481,11 @@
     }
 
     /**
-     * Delete all files in DFS that match the file pattern <i>srcf</i>
-     * @param srcf: a file pattern specifying source files
-     * @param recursive: if need to delete subdirs
-     * @exception: IOException  
-     * @see org.apache.hadoop.fs.FileSystem.globPaths 
+     * Delete all files in DFS that match the file pattern <i>srcf</i>.
+     * @param srcf a file pattern specifying source files
+     * @param recursive if need to delete subdirs
+     * @throws IOException  
+     * @see org.apache.hadoop.fs.FileSystem#globPaths(Path)
      */
     public void delete(String srcf, boolean recursive) throws IOException {
       Path [] srcs = fs.globPaths( new Path(srcf) );
"
hadoop,7f4e8be648a3ddd267beb8d47657861d4ec3dda0,"HADOOP-399.  Fix javadoc warnings.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:35:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSck.java b/src/java/org/apache/hadoop/dfs/DFSck.java
index ca6aa09..1b64f21 100644
--- a/src/java/org/apache/hadoop/dfs/DFSck.java
+++ b/src/java/org/apache/hadoop/dfs/DFSck.java
@@ -38,11 +38,11 @@
  * <li>files with blocks that are completely missing from all datanodes.<br/>
  * In this case the tool can perform one of the following actions:
  *  <ul>
- *      <li>none ({@link #FIXING_NONE})</li>
+ *      <li>none ({@link NamenodeFsck#FIXING_NONE})</li>
  *      <li>move corrupted files to /lost+found directory on DFS
- *      ({@link #FIXING_MOVE}). Remaining data blocks are saved as a
+ *      ({@link NamenodeFsck#FIXING_MOVE}). Remaining data blocks are saved as a
  *      block chains, representing longest consecutive series of valid blocks.</li>
- *      <li>delete corrupted files ({@link #FIXING_DELETE})</li>
+ *      <li>delete corrupted files ({@link NamenodeFsck#FIXING_DELETE})</li>
  *  </ul>
  *  </li>
  *  <li>detect files with under-replicated or over-replicated blocks</li>
"
hadoop,7f4e8be648a3ddd267beb8d47657861d4ec3dda0,"HADOOP-399.  Fix javadoc warnings.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:35:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DatanodeProtocol.java b/src/java/org/apache/hadoop/dfs/DatanodeProtocol.java
index 7280d4e..74c10b9 100644
--- a/src/java/org/apache/hadoop/dfs/DatanodeProtocol.java
+++ b/src/java/org/apache/hadoop/dfs/DatanodeProtocol.java
@@ -51,10 +51,10 @@
   /** 
    * Register Datanode.
    *
-   * @see DataNode#register()
-   * @see FSNamesystem#registerDatanode(DatanodeRegistration)
+   * @see org.apache.hadoop.dfs.DataNode#register()
+   * @see org.apache.hadoop.dfs.FSNamesystem#registerDatanode(DatanodeRegistration)
    * 
-   * @return updated {@link DatanodeRegistration}, which contains 
+   * @return updated {@link org.apache.hadoop.dfs.DatanodeRegistration}, which contains 
    * new storageID if the datanode did not have one and
    * registration ID for further communication.
    */
"
hadoop,7f4e8be648a3ddd267beb8d47657861d4ec3dda0,"HADOOP-399.  Fix javadoc warnings.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:35:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java b/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
index 87f1d86..4eb585e 100644
--- a/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
+++ b/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
@@ -258,10 +258,8 @@
     
     /**
      * Enter, leave or get safe mode.
-     * See {@link ClientProtocol#setSafeMode(FSConstants.SafeModeAction)} 
-     * for more details.
      *  
-     * @see ClientProtocol#setSafeMode(FSConstants.SafeModeAction)
+     * @see org.apache.hadoop.dfs.ClientProtocol#setSafeMode(FSConstants.SafeModeAction)
      */
     public boolean setSafeMode( FSConstants.SafeModeAction action ) 
     throws IOException {
"
hadoop,7f4e8be648a3ddd267beb8d47657861d4ec3dda0,"HADOOP-399.  Fix javadoc warnings.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:35:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/NamenodeFsck.java b/src/java/org/apache/hadoop/dfs/NamenodeFsck.java
index 8ea9bc8..5a5ebfc 100644
--- a/src/java/org/apache/hadoop/dfs/NamenodeFsck.java
+++ b/src/java/org/apache/hadoop/dfs/NamenodeFsck.java
@@ -88,12 +88,11 @@
   
   /**
    * Filesystem checker.
-   * @param conf current Configuration
-   * @param fixing one of pre-defined values
-   * @param showFiles show each file being checked
-   * @param showBlocks for each file checked show its block information
-   * @param showLocations for each block in each file show block locations
-   * @throws Exception
+   * @param conf configuration (namenode config)
+   * @param nn namenode that this fsck is going to use
+   * @param pmap key=value[] map that is passed to the http servlet as url parameters
+   * @param response the object into which  this servelet writes the url contents
+   * @throws IOException
    */
   public NamenodeFsck(Configuration conf,
       NameNode nn,
@@ -457,7 +456,6 @@
     
     /**
      * DFS is considered healthy if there are no missing blocks.
-     * @return
      */
     public boolean isHealthy() {
       return missingIds.size() == 0;
"
hadoop,7f4e8be648a3ddd267beb8d47657861d4ec3dda0,"HADOOP-399.  Fix javadoc warnings.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:35:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FileSystem.java b/src/java/org/apache/hadoop/fs/FileSystem.java
index e57116c..b4d6f51 100644
--- a/src/java/org/apache/hadoop/fs/FileSystem.java
+++ b/src/java/org/apache/hadoop/fs/FileSystem.java
@@ -537,7 +537,7 @@
 
     /** 
      * Filter raw files in a list directories using the default checksum filter. 
-     * @param files: a list of paths
+     * @param files a list of paths
      * @return a list of files under the source paths
      * @exception IOException
      */
@@ -547,7 +547,7 @@
     
     /** 
      * Filter raw files in a list directories using user-supplied path filter. 
-     * @param files: a list of paths
+     * @param files a list of paths
      * @return a list of files under the source paths
      * @exception IOException
      */
@@ -604,7 +604,7 @@
      *  </dd>
      * </dl>
      *
-     * @param filePattern: a regular expression specifying file pattern
+     * @param filePattern a regular expression specifying file pattern
 
      * @return an array of paths that match the file pattern
      * @throws IOException
@@ -614,8 +614,7 @@
     }
     
     /** glob all the file names that matches filePattern
-     * and is accepted by filter
-     * @param 
+     * and is accepted by filter.
      */
     public Path[] globPaths(Path filePattern, PathFilter filter) 
         throws IOException {
"
hadoop,7f4e8be648a3ddd267beb8d47657861d4ec3dda0,"HADOOP-399.  Fix javadoc warnings.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:35:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FileUtil.java b/src/java/org/apache/hadoop/fs/FileUtil.java
index f3bb75a..379c2c1 100644
--- a/src/java/org/apache/hadoop/fs/FileUtil.java
+++ b/src/java/org/apache/hadoop/fs/FileUtil.java
@@ -306,7 +306,7 @@
    * Create a soft link between a src and destination
    * only on a local disk. HDFS does not support this
    * @param target the target for symlink 
-   * @param destination the symlink
+   * @param linkname the symlink
    * @return value returned by the command
    */
   public static int symLink(String target, String linkname) throws IOException{
"
hadoop,7f4e8be648a3ddd267beb8d47657861d4ec3dda0,"HADOOP-399.  Fix javadoc warnings.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:35:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/Path.java b/src/java/org/apache/hadoop/fs/Path.java
index 166c69e..09d71af 100644
--- a/src/java/org/apache/hadoop/fs/Path.java
+++ b/src/java/org/apache/hadoop/fs/Path.java
@@ -123,7 +123,7 @@
     return new Path(isAbsolute, newElements, drive);
   }
 
-  /** Adds a suffix to a the final name in the path.*/
+  /** Adds a suffix to the final name in the path.*/
   public Path suffix(String suffix) {
     return new Path(getParent(), getName()+suffix);
   }
"
hadoop,7f4e8be648a3ddd267beb8d47657861d4ec3dda0,"HADOOP-399.  Fix javadoc warnings.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:35:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/BytesWritable.java b/src/java/org/apache/hadoop/io/BytesWritable.java
index af42abc..e3d64e7 100644
--- a/src/java/org/apache/hadoop/io/BytesWritable.java
+++ b/src/java/org/apache/hadoop/io/BytesWritable.java
@@ -60,7 +60,6 @@
   
   /**
    * Get the current size of the buffer.
-   * @return
    */
   public int getSize() {
     return size;
"
hadoop,7f4e8be648a3ddd267beb8d47657861d4ec3dda0,"HADOOP-399.  Fix javadoc warnings.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:35:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/SequenceFile.java b/src/java/org/apache/hadoop/io/SequenceFile.java
index 8668ded..8ebe119 100644
--- a/src/java/org/apache/hadoop/io/SequenceFile.java
+++ b/src/java/org/apache/hadoop/io/SequenceFile.java
@@ -56,7 +56,7 @@
   public static final int SYNC_INTERVAL = 100*SYNC_SIZE; 
 
   /** The type of compression.
-   * @see SequenceFile#Writer
+   * @see SequenceFile.Writer
    */
   public static enum CompressionType {
     /** Do not compress records. */
@@ -596,9 +596,9 @@
 
     /** Returns the current length of the output file.
      *
-     * <p>This always returns a synchronized position.  In other words, {@link
-     * immediately after calling {@link Reader#seek(long)} with a position
-     * returned by this method, Reader#next(Writable) may be called.  However
+     * <p>This always returns a synchronized position.  In other words,
+     * immediately after calling {@link SequenceFile.Reader#seek(long)} with a position
+     * returned by this method, {@link SequenceFile.Reader#next(Writable)} may be called.  However
      * the key may be earlier in the file than key last written when this
      * method was called (e.g., with block-compression, it may be the first key
      * in the block that was being written when this method was called).
@@ -1388,8 +1388,8 @@
     /** Set the current byte position in the input file.
      *
      * <p>The position passed must be a position returned by {@link
-     * Writer#getLength()} when writing this file.  To seek to an arbitrary
-     * position, use {@link Reader#sync(long)}.
+     * SequenceFile.Writer#getLength()} when writing this file.  To seek to an arbitrary
+     * position, use {@link SequenceFile.Reader#sync(long)}.
      */
     public synchronized void seek(long position) throws IOException {
       in.seek(position);
"
hadoop,7f4e8be648a3ddd267beb8d47657861d4ec3dda0,"HADOOP-399.  Fix javadoc warnings.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:35:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/Text.java b/src/java/org/apache/hadoop/io/Text.java
index 574b8a0..23a2e18 100644
--- a/src/java/org/apache/hadoop/io/Text.java
+++ b/src/java/org/apache/hadoop/io/Text.java
@@ -96,7 +96,7 @@
    * Returns the Unicode Scalar Value (32-bit integer value)
    * for the character at <code>position</code>. Note that this
    * method avoids using the converter or doing String instatiation
-   * @returns the Unicode scalar value at position or -1
+   * @return the Unicode scalar value at position or -1
    *          if the position is invalid or points to a
    *          trailing byte
    */
@@ -410,8 +410,8 @@
 
   /** 
    * Check if a byte array contains valid utf-8
-   * @param utf8: byte array
-   * @exception MalformedInputException if the byte array contains invalid utf-8
+   * @param utf8 byte array
+   * @throws MalformedInputException if the byte array contains invalid utf-8
    */
   public static void validateUTF8(byte[] utf8) throws MalformedInputException {
      validateUTF8(utf8, 0, utf8.length);     
"
hadoop,7f4e8be648a3ddd267beb8d47657861d4ec3dda0,"HADOOP-399.  Fix javadoc warnings.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:35:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/WritableUtils.java b/src/java/org/apache/hadoop/io/WritableUtils.java
index 28de160..22d313e 100644
--- a/src/java/org/apache/hadoop/io/WritableUtils.java
+++ b/src/java/org/apache/hadoop/io/WritableUtils.java
@@ -336,10 +336,8 @@
 
   /**
    * Get the encoded length if an integer is stored in a variable-length format
-   * @param i: a long
    * @return the encoded length 
    */
-  
   public static int getVIntSize(long i) {
       if (i >= -112 && i <= 127) {
           return 1;
"
hadoop,7f4e8be648a3ddd267beb8d47657861d4ec3dda0,"HADOOP-399.  Fix javadoc warnings.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:35:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/compress/CompressionInputStream.java b/src/java/org/apache/hadoop/io/compress/CompressionInputStream.java
index f3a7cc1..079539a 100644
--- a/src/java/org/apache/hadoop/io/compress/CompressionInputStream.java
+++ b/src/java/org/apache/hadoop/io/compress/CompressionInputStream.java
@@ -39,7 +39,7 @@
   /**
    * Create a compression input stream that reads
    * the decompressed bytes from the given stream.
-   * @param out
+   * @param in
    */
   protected CompressionInputStream(InputStream in) {
     this.in = in;
"
hadoop,7f4e8be648a3ddd267beb8d47657861d4ec3dda0,"HADOOP-399.  Fix javadoc warnings.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:35:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/FileSplit.java b/src/java/org/apache/hadoop/mapred/FileSplit.java
index 8fa009c..e8804a3 100644
--- a/src/java/org/apache/hadoop/mapred/FileSplit.java
+++ b/src/java/org/apache/hadoop/mapred/FileSplit.java
@@ -30,7 +30,7 @@
 
 /** A section of an input file.  Returned by {@link
  * InputFormat#getSplits(FileSystem, JobConf, int)} and passed to
- * InputFormat#getRecordReader(FileSystem,FileSplit,JobConf,Reporter). */
+ * {@link InputFormat#getRecordReader(FileSystem,FileSplit,JobConf,Reporter)}. */
 public class FileSplit implements Writable {
   private Path file;
   private long start;
"
hadoop,7f4e8be648a3ddd267beb8d47657861d4ec3dda0,"HADOOP-399.  Fix javadoc warnings.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:35:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobConf.java b/src/java/org/apache/hadoop/mapred/JobConf.java
index 8f851d7..226ead1 100644
--- a/src/java/org/apache/hadoop/mapred/JobConf.java
+++ b/src/java/org/apache/hadoop/mapred/JobConf.java
@@ -73,7 +73,6 @@
 
   /** 
    * Construct a map/reduce job configuration.
-   * @param conf a Configuration whose settings will be inherited.
    * @param exampleClass a class whose containing jar is used as the job's jar.
    */
   public JobConf(Class exampleClass) {
@@ -83,8 +82,7 @@
   /**
    * Construct a map/reduce job configuration.
    * 
-   * @param conf
-   *          a Configuration whose settings will be inherited.
+   * @param conf a Configuration whose settings will be inherited.
    */
   public JobConf(Configuration conf) {
     super(conf);
"
hadoop,7f4e8be648a3ddd267beb8d47657861d4ec3dda0,"HADOOP-399.  Fix javadoc warnings.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:35:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobHistory.java b/src/java/org/apache/hadoop/mapred/JobHistory.java
index bc28bcf..44617d9 100644
--- a/src/java/org/apache/hadoop/mapred/JobHistory.java
+++ b/src/java/org/apache/hadoop/mapred/JobHistory.java
@@ -225,7 +225,6 @@
      * Convert value from history to int and return. 
      * if no value is found it returns 0.
      * @param k key 
-     * @return
      */
     public int getInt(Keys k){
       String s = values.get(k); 
@@ -238,7 +237,6 @@
      * Convert value from history to int and return. 
      * if no value is found it returns 0.
      * @param k
-     * @return
      */
     public long getLong(Keys k){
       String s = values.get(k); 
@@ -271,7 +269,6 @@
     }
     /**
      * Returns Map containing all key-values. 
-     * @return
      */
     public Map<Keys, String> getValues(){
       return values; 
@@ -292,7 +289,6 @@
 
     /**
      * Returns all map and reduce tasks <taskid-Task>. 
-     * @return
      */
     public Map<String, Task> getAllTasks() { return allTasks; }
     
@@ -486,12 +482,12 @@
     }
     /**
      * Returns all task attempts for this task. <task attempt id - TaskAttempt>
-     * @return
      */
     public Map<String, TaskAttempt> getTaskAttempts(){
       return this.taskAttempts;
     }
   }
+
   /**
    * Base class for Map and Reduce TaskAttempts. 
    */
@@ -745,4 +741,4 @@
       isRunning = false ; 
     }
   }
-}
\ No newline at end of file
+}
"
hadoop,7f4e8be648a3ddd267beb8d47657861d4ec3dda0,"HADOOP-399.  Fix javadoc warnings.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:35:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/SequenceFileInputFilter.java b/src/java/org/apache/hadoop/mapred/SequenceFileInputFilter.java
index f603fd3..55a4d60 100644
--- a/src/java/org/apache/hadoop/mapred/SequenceFileInputFilter.java
+++ b/src/java/org/apache/hadoop/mapred/SequenceFileInputFilter.java
@@ -107,8 +107,8 @@
     public static class RegexFilter extends FilterBase {
         private Pattern p;
         /** Define the filtering regex and stores it in conf
-         * @argument conf where the regex is set
-         * @argument regex regex used as a filter
+         * @param conf where the regex is set
+         * @param regex regex used as a filter
          */
         public static void setPattern(Configuration conf, String regex )
             throws PatternSyntaxException {
"
hadoop,7f4e8be648a3ddd267beb8d47657861d4ec3dda0,"HADOOP-399.  Fix javadoc warnings.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:35:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/StatusHttpServer.java b/src/java/org/apache/hadoop/mapred/StatusHttpServer.java
index 6724d84..37a4df4 100644
--- a/src/java/org/apache/hadoop/mapred/StatusHttpServer.java
+++ b/src/java/org/apache/hadoop/mapred/StatusHttpServer.java
@@ -103,10 +103,10 @@
   }
 
   /**
-   * Add a servlet in the server
+   * Add a servlet in the server.
    * @param name The name of the servlet (can be passed as null)
    * @param pathSpec The path spec for the servlet
-   * @param classname The class name for the servlet
+   * @param servletClass The servlet class
    */
   public <T extends HttpServlet> 
   void addServlet(String name, String pathSpec, 
"
hadoop,7f4e8be648a3ddd267beb8d47657861d4ec3dda0,"HADOOP-399.  Fix javadoc warnings.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:35:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index c5dbf73..9ded3e0 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -408,7 +408,6 @@
     }
         
     /**Return the DFS filesystem
-     * @return
      */
     public FileSystem getFileSystem(){
       return fs;
"
hadoop,7f4e8be648a3ddd267beb8d47657861d4ec3dda0,"HADOOP-399.  Fix javadoc warnings.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:35:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/util/ReflectionUtils.java b/src/java/org/apache/hadoop/util/ReflectionUtils.java
index aab1a86..9bc9f09 100644
--- a/src/java/org/apache/hadoop/util/ReflectionUtils.java
+++ b/src/java/org/apache/hadoop/util/ReflectionUtils.java
@@ -77,8 +77,8 @@
     }
     
     /**
-     * Print all of the thread's information and stack traces
-     * @author Owen O'Malley
+     * Print all of the thread's information and stack traces.
+     * 
      * @param stream the stream to
      * @param title a string title for the stack trace
      */
"
hadoop,7f4e8be648a3ddd267beb8d47657861d4ec3dda0,"HADOOP-399.  Fix javadoc warnings.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:35:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/util/StringUtils.java b/src/java/org/apache/hadoop/util/StringUtils.java
index 7154cd8..b13d773 100644
--- a/src/java/org/apache/hadoop/util/StringUtils.java
+++ b/src/java/org/apache/hadoop/util/StringUtils.java
@@ -157,7 +157,6 @@
   /**
    * 
    * @param uris
-   * @return
    */
   public static String uriToString(URI[] uris){
     String ret = null;
@@ -171,7 +170,6 @@
   /**
    * 
    * @param str
-   * @return
    */
   public static URI[] stringToURI(String[] str){
     if (str == null) 
@@ -192,7 +190,6 @@
   /**
    * 
    * @param str
-   * @return
    */
   public static Path[] stringToPath(String[] str){
     Path[] p = new Path[str.length];
@@ -208,8 +205,7 @@
    * If finish time comes before start time then negative valeus of X, Y and Z wil return. 
    * 
    * @param finishTime finish time
-   * @param statTime start time
-   * @return
+   * @param startTime start time
    */
   public static String formatTimeDiff(long finishTime, long startTime){
     StringBuffer buf = new StringBuffer() ;
"
hadoop,f35ef67df375488a20c263df79c728962dcec4aa,"HADOOP-553.  Change main() routines in datanode and namenode to log exceptions rather than letting the JVM print them to standard error.  Also change the hadoop-daemon.sh script to rotate standard i/o log files.  Contributed by Raghu Angadi.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469683 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:29:51,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index ac4b0f5..ede7fa4 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -1105,8 +1105,13 @@
     /**
      */
     public static void main(String args[]) throws IOException {
+      try {
         Configuration conf = new Configuration();
         runAndWait(conf);
+      } catch ( Throwable e ) {
+        LOG.error( StringUtils.stringifyException( e ) );
+        System.exit(-1);
+      }
     }
 
 }
"
hadoop,f35ef67df375488a20c263df79c728962dcec4aa,"HADOOP-553.  Change main() routines in datanode and namenode to log exceptions rather than letting the JVM print them to standard error.  Also change the hadoop-daemon.sh script to rotate standard i/o log files.  Contributed by Raghu Angadi.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469683 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:29:51,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/NameNode.java b/src/java/org/apache/hadoop/dfs/NameNode.java
index 24aa4b3..29b5f6f 100644
--- a/src/java/org/apache/hadoop/dfs/NameNode.java
+++ b/src/java/org/apache/hadoop/dfs/NameNode.java
@@ -23,6 +23,7 @@
 import org.apache.hadoop.io.*;
 import org.apache.hadoop.ipc.*;
 import org.apache.hadoop.conf.*;
+import org.apache.hadoop.util.StringUtils;
 
 import java.io.*;
 
@@ -568,6 +569,7 @@
     /**
      */
     public static void main(String argv[]) throws Exception {
+      try {
         Configuration conf = new Configuration();
 
         if (argv.length == 1 && argv[0].equals(""-format"")) {
@@ -591,5 +593,10 @@
         
         NameNode namenode = new NameNode(conf);
         namenode.join();
+        
+      } catch ( Throwable e ) {
+        LOG.error( StringUtils.stringifyException( e ) );
+        System.exit(-1);
+      }
     }
 }
"
hadoop,f35ef67df375488a20c263df79c728962dcec4aa,"HADOOP-553.  Change main() routines in datanode and namenode to log exceptions rather than letting the JVM print them to standard error.  Also change the hadoop-daemon.sh script to rotate standard i/o log files.  Contributed by Raghu Angadi.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469683 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:29:51,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index 9bf1ad5..77c1d4e 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -1230,12 +1230,17 @@
      * JobTracker should be run as part of the DFS Namenode process.
      */
     public static void main(String argv[]) throws IOException, InterruptedException {
-        if (argv.length != 0) {
-          System.out.println(""usage: JobTracker"");
-          System.exit(-1);
-        }
-
+      if (argv.length != 0) {
+        System.out.println(""usage: JobTracker"");
+        System.exit(-1);
+      }
+      
+      try {
         Configuration conf=new Configuration();
         startTracker(conf);
+      } catch ( Throwable e ) {
+        LOG.error( StringUtils.stringifyException( e ) );
+        System.exit(-1);
+      }
     }
 }
"
hadoop,f35ef67df375488a20c263df79c728962dcec4aa,"HADOOP-553.  Change main() routines in datanode and namenode to log exceptions rather than letting the JVM print them to standard error.  Also change the hadoop-daemon.sh script to rotate standard i/o log files.  Contributed by Raghu Angadi.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469683 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:29:51,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index f6d2049..c5dbf73 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -1367,10 +1367,10 @@
           ReflectionUtils.setContentionTracing
               (conf.getBoolean(""tasktracker.contention.tracking"", false));
           new TaskTracker(conf).run();
-        } catch (IOException e) {
-            LOG.warn( ""Can not start task tracker because ""+
-                      StringUtils.stringifyException(e));
-            System.exit(-1);
+        } catch ( Throwable e ) {
+          LOG.error( ""Can not start task tracker because ""+
+                     StringUtils.stringifyException(e) );
+          System.exit(-1);
         }
     }
     
"
hadoop,7a2fa3b2b4d9acb783167017986cf695880079f7,"HADOOP-651.  Fix DFSCk to correctly pass parameters to the servlet on the namenode.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469678 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:17:46,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSck.java b/src/java/org/apache/hadoop/dfs/DFSck.java
index 27cc2eb..ca6aa09 100644
--- a/src/java/org/apache/hadoop/dfs/DFSck.java
+++ b/src/java/org/apache/hadoop/dfs/DFSck.java
@@ -99,12 +99,12 @@
       if (!args[idx].startsWith(""-"")) { dir = args[idx]; break; }
     }
     url.append(URLEncoder.encode(dir, ""UTF-8""));
-    for (int idx = 1; idx < args.length; idx++) {
-      if (args[idx].equals(""-move"")) { url.append(""&move""); }
-      if (args[idx].equals(""-delete"")) { url.append(""&delete""); }
-      if (args[idx].equals(""-files"")) { url.append(""&files""); }
-      if (args[idx].equals(""-blocks"")) { url.append(""&blocks""); }
-      if (args[idx].equals(""-locations"")) { url.append(""&locations""); }
+    for (int idx = 0; idx < args.length; idx++) {
+      if (args[idx].equals(""-move"")) { url.append(""&move=1""); }
+      else if (args[idx].equals(""-delete"")) { url.append(""&delete=1""); }
+      else if (args[idx].equals(""-files"")) { url.append(""&files=1""); }
+      else if (args[idx].equals(""-blocks"")) { url.append(""&blocks=1""); }
+      else if (args[idx].equals(""-locations"")) { url.append(""&locations=1""); }
     }
     URL path = new URL(url.toString());
     URLConnection connection = path.openConnection();
"
hadoop,d20db68461c2c2d9166d2c88f3e8c71b26379e52,"HADOOP-90.  Permit dfs.name.dir to list multiple directories where namenode data is to be replicated.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469673 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:11:20,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSDirectory.java b/src/java/org/apache/hadoop/dfs/FSDirectory.java
index 1aa5633..2635a9a 100644
--- a/src/java/org/apache/hadoop/dfs/FSDirectory.java
+++ b/src/java/org/apache/hadoop/dfs/FSDirectory.java
@@ -312,8 +312,8 @@
     private int numFilesDeleted = 0;
     
     /** Access an existing dfs name directory. */
-    public FSDirectory(File dir) throws IOException {
-      this.fsImage = new FSImage( dir );
+    public FSDirectory(File[] dirs) throws IOException {
+      this.fsImage = new FSImage( dirs );
     }
     
     void loadFSImage( Configuration conf ) throws IOException {
@@ -326,11 +326,13 @@
       metricsRecord = Metrics.createRecord(""dfs"", ""namenode"");
     }
 
-    /** Create a new dfs name directory.  Caution: this destroys all files
+    /** Create new dfs name directories.  Caution: this destroys all files
      * in this filesystem.
-     * @deprecated use @link FSImage#format(File, Configuration) instead */
-    public static void format(File dir, Configuration conf) throws IOException {
-      FSImage.format( dir, conf );
+     * @deprecated use @link FSImage#format(File[], Configuration) instead */
+    public static void format(File[] dirs, Configuration conf) throws IOException {
+      for (int idx = 0; idx < dirs.length; idx++) {
+        FSImage.format( dirs[idx] );
+      }
     }
     
     /**
"
hadoop,d20db68461c2c2d9166d2c88f3e8c71b26379e52,"HADOOP-90.  Permit dfs.name.dir to list multiple directories where namenode data is to be replicated.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469673 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:11:20,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSEditLog.java b/src/java/org/apache/hadoop/dfs/FSEditLog.java
index 24d8220..4c9c61b 100644
--- a/src/java/org/apache/hadoop/dfs/FSEditLog.java
+++ b/src/java/org/apache/hadoop/dfs/FSEditLog.java
@@ -21,9 +21,12 @@
 import java.io.DataInputStream;
 import java.io.DataOutputStream;
 import java.io.File;
+import java.io.FileDescriptor;
 import java.io.FileInputStream;
 import java.io.FileOutputStream;
 import java.io.IOException;
+import java.util.Iterator;
+import java.util.Vector;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.io.ArrayWritable;
@@ -44,15 +47,16 @@
   private static final byte OP_DATANODE_ADD = 5;
   private static final byte OP_DATANODE_REMOVE = 6;
   
-  private File editsFile;
-  DataOutputStream editsStream = null;
+  private File[] editFiles;
+  DataOutputStream[] editStreams = null;
+  FileDescriptor[] editDescriptors = null;
   
-  FSEditLog( File edits ) {
-    this.editsFile = edits;
+  FSEditLog( File[] edits ) {
+    this.editFiles = edits;
   }
   
-  File getEditsFile() {
-    return this.editsFile;
+  File[] getEditFiles() {
+    return this.editFiles;
   }
 
   /**
@@ -61,18 +65,62 @@
    * @throws IOException
    */
   void create() throws IOException {
-    editsStream = new DataOutputStream(new FileOutputStream(editsFile));
-    editsStream.writeInt( FSConstants.DFS_CURRENT_VERSION );
+    editStreams = new DataOutputStream[editFiles.length];
+    editDescriptors = new FileDescriptor[editFiles.length];
+    for (int idx = 0; idx < editStreams.length; idx++) {
+      FileOutputStream stream = new FileOutputStream(editFiles[idx]);
+      editStreams[idx] = new DataOutputStream(stream);
+      editDescriptors[idx] = stream.getFD();
+      editStreams[idx].writeInt( FSConstants.DFS_CURRENT_VERSION );
+    }
   }
   
   /**
    * Shutdown the filestore
    */
   void close() throws IOException {
-    editsStream.close();
+    for (int idx = 0; idx < editStreams.length; idx++) {
+      editStreams[idx].flush();
+      editDescriptors[idx].sync();
+      editStreams[idx].close();
+    }
   }
 
   /**
+   * Delete specified editLog
+   */
+  void delete(int idx) throws IOException {
+    if (editStreams != null) {
+      editStreams[idx].close();
+    }
+    editFiles[idx].delete();
+  }
+  
+  /**
+   * Delete all editLogs
+   */
+  void deleteAll() throws IOException {
+    for (int idx = 0; idx < editFiles.length; idx++ ) {
+      if (editStreams != null) {
+        editStreams[idx].close();
+      }
+      editFiles[idx].delete();
+    }
+  }
+  
+  /**
+   * check if ANY edits log exists
+   */
+  boolean exists() throws IOException {
+    for (int idx = 0; idx < editFiles.length; idx++) {
+      if (editFiles[idx].exists()) { 
+        return true;
+      }
+    }
+    return false;
+  }
+  
+  /**
    * Load an edit log, and apply the changes to the in-memory structure
    *
    * This is where we apply edits that we've been writing to disk all
@@ -84,10 +132,29 @@
     int numEdits = 0;
     int logVersion = 0;
     
-    if (editsFile.exists()) {
+    // first check how many editFiles exist
+    // and choose the largest editFile, because it is the most recent
+    Vector<File> files = new Vector<File>();
+    for (int idx = 0; idx < editFiles.length; idx++) {
+      if (editFiles[idx].exists()) {
+        files.add(editFiles[idx]);
+      }
+    }
+    long maxLength = Long.MIN_VALUE;
+    File edits = null;
+    for (Iterator<File> it = files.iterator(); it.hasNext();) {
+      File f = it.next();
+      long length = f.length();
+      if (length > maxLength) {
+        maxLength = length;
+        edits = f;
+      }
+    }
+    
+    if (edits != null) {
       DataInputStream in = new DataInputStream(
           new BufferedInputStream(
-              new FileInputStream(editsFile)));
+              new FileInputStream(edits)));
       // Read log file version. Could be missing. 
       in.mark( 4 );
       if( in.available() > 0 ) {
@@ -228,17 +295,21 @@
    * Write an operation to the edit log
    */
   void logEdit(byte op, Writable w1, Writable w2) {
-    synchronized (editsStream) {
-      try {
-        editsStream.write(op);
-        if (w1 != null) {
-          w1.write(editsStream);
+    for (int idx = 0; idx < editStreams.length; idx++) {
+      synchronized (editStreams[idx]) {
+        try {
+          editStreams[idx].write(op);
+          if (w1 != null) {
+            w1.write(editStreams[idx]);
+          }
+          if (w2 != null) {
+            w2.write(editStreams[idx]);
+          }
+          editStreams[idx].flush();
+          editDescriptors[idx].sync();
+        } catch (IOException ie) {
+          // TODO: Must report an error here
         }
-        if (w2 != null) {
-          w2.write(editsStream);
-        }
-      } catch (IOException ie) {
-        // TODO: Must report an error here
       }
     }
     // TODO: initialize checkpointing if the log is large enough
"
hadoop,d20db68461c2c2d9166d2c88f3e8c71b26379e52,"HADOOP-90.  Permit dfs.name.dir to list multiple directories where namenode data is to be replicated.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469673 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:11:20,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSImage.java b/src/java/org/apache/hadoop/dfs/FSImage.java
index a573870..23507b9 100644
--- a/src/java/org/apache/hadoop/dfs/FSImage.java
+++ b/src/java/org/apache/hadoop/dfs/FSImage.java
@@ -43,20 +43,27 @@
   private static final String FS_IMAGE = ""fsimage"";
   private static final String NEW_FS_IMAGE = ""fsimage.new"";
   private static final String OLD_FS_IMAGE = ""fsimage.old"";
+  private static final String FS_TIME = ""fstime"";
 
-  private File imageDir;  /// directory that contains the image file 
+  private File[] imageDirs;  /// directories that contains the image file 
   private FSEditLog editLog;
   // private int namespaceID = 0;    /// a persistent attribute of the namespace
 
   /**
    * 
    */
-  FSImage( File fsDir ) throws IOException {
-    this.imageDir = new File(fsDir, ""image"");
-    if (! imageDir.exists()) {
-      throw new IOException(""NameNode not formatted: "" + fsDir);
+  FSImage( File[] fsDirs ) throws IOException {
+    this.imageDirs = new File[fsDirs.length];
+    for (int idx = 0; idx < imageDirs.length; idx++) {
+      imageDirs[idx] = new File(fsDirs[idx], ""image"");
+      if (! imageDirs[idx].exists()) {
+        throw new IOException(""NameNode not formatted: "" + imageDirs[idx]);
+      }
     }
-    File edits = new File(fsDir, ""edits"");
+    File[] edits = new File[fsDirs.length];
+    for (int idx = 0; idx < edits.length; idx++) {
+      edits[idx] = new File(fsDirs[idx], ""edits"");
+    }
     this.editLog = new FSEditLog( edits );
   }
   
@@ -72,27 +79,52 @@
   void loadFSImage( Configuration conf ) throws IOException {
     FSNamesystem fsNamesys = FSNamesystem.getFSNamesystem();
     FSDirectory fsDir = fsNamesys.dir;
-    File edits = editLog.getEditsFile();
-    //
-    // Atomic move sequence, to recover from interrupted save
-    //
-    File curFile = new File(imageDir, FS_IMAGE);
-    File newFile = new File(imageDir, NEW_FS_IMAGE);
-    File oldFile = new File(imageDir, OLD_FS_IMAGE);
+    for (int idx = 0; idx < imageDirs.length; idx++) {
+      //
+      // Atomic move sequence, to recover from interrupted save
+      //
+      File curFile = new File(imageDirs[idx], FS_IMAGE);
+      File newFile = new File(imageDirs[idx], NEW_FS_IMAGE);
+      File oldFile = new File(imageDirs[idx], OLD_FS_IMAGE);
 
-    // Maybe we were interrupted between 2 and 4
-    if (oldFile.exists() && curFile.exists()) {
-      oldFile.delete();
-      if (edits.exists()) {
-        edits.delete();
+      // Maybe we were interrupted between 2 and 4
+      if (oldFile.exists() && curFile.exists()) {
+        oldFile.delete();
+        if (editLog.exists()) {
+          editLog.deleteAll();
+        }
+      } else if (oldFile.exists() && newFile.exists()) {
+        // Or maybe between 1 and 2
+        newFile.renameTo(curFile);
+        oldFile.delete();
+      } else if (curFile.exists() && newFile.exists()) {
+        // Or else before stage 1, in which case we lose the edits
+        newFile.delete();
       }
-    } else if (oldFile.exists() && newFile.exists()) {
-      // Or maybe between 1 and 2
-      newFile.renameTo(curFile);
-      oldFile.delete();
-    } else if (curFile.exists() && newFile.exists()) {
-      // Or else before stage 1, in which case we lose the edits
-      newFile.delete();
+    }
+    
+    // Now check all curFiles and see which is the newest
+    File curFile = null;
+    long maxTimeStamp = 0;
+    for (int idx = 0; idx < imageDirs.length; idx++) {
+      File file = new File(imageDirs[idx], FS_IMAGE);
+      if (file.exists()) {
+        long timeStamp = 0;
+        File timeFile = new File(imageDirs[idx], FS_TIME);
+        if (timeFile.exists() && timeFile.canRead()) {
+          DataInputStream in = new DataInputStream(
+              new FileInputStream(timeFile));
+          try {
+            timeStamp = in.readLong();
+          } finally {
+            in.close();
+          }
+        }
+        if (maxTimeStamp < timeStamp) {
+          maxTimeStamp = timeStamp;
+          curFile = file;
+        }
+      }
     }
 
     //
@@ -100,7 +132,7 @@
     //
     boolean needToSave = true;
     int imgVersion = FSConstants.DFS_CURRENT_VERSION;
-    if (curFile.exists()) {
+    if (curFile != null) {
       DataInputStream in = new DataInputStream(
                               new BufferedInputStream(
                                   new FileInputStream(curFile)));
@@ -156,7 +188,7 @@
     if( fsDir.namespaceID == 0 )
       fsDir.namespaceID = newNamespaceID();
     
-    needToSave |= ( edits.exists() && editLog.loadFSEdits(conf) > 0 );
+    needToSave |= ( editLog.exists() && editLog.loadFSEdits(conf) > 0 );
     if( needToSave )
       saveFSImage();
   }
@@ -167,35 +199,51 @@
   void saveFSImage() throws IOException {
     FSNamesystem fsNamesys = FSNamesystem.getFSNamesystem();
     FSDirectory fsDir = fsNamesys.dir;
-    File curFile = new File(imageDir, FS_IMAGE);
-    File newFile = new File(imageDir, NEW_FS_IMAGE);
-    File oldFile = new File(imageDir, OLD_FS_IMAGE);
-    
-    //
-    // Write out data
-    //
-    DataOutputStream out = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(newFile)));
-    try {
-      out.writeInt(FSConstants.DFS_CURRENT_VERSION);
-      out.writeInt(fsDir.namespaceID);
-      out.writeInt(fsDir.rootDir.numItemsInTree() - 1);
-      saveImage( """", fsDir.rootDir, out );
-      saveDatanodes( out );
-    } finally {
-      out.close();
+    for (int idx = 0; idx < imageDirs.length; idx++) {
+      File newFile = new File(imageDirs[idx], NEW_FS_IMAGE);
+      
+      //
+      // Write out data
+      //
+      DataOutputStream out = new DataOutputStream(
+            new BufferedOutputStream(
+            new FileOutputStream(newFile)));
+      try {
+        out.writeInt(FSConstants.DFS_CURRENT_VERSION);
+        out.writeInt(fsDir.namespaceID);
+        out.writeInt(fsDir.rootDir.numItemsInTree() - 1);
+        saveImage( """", fsDir.rootDir, out );
+        saveDatanodes( out );
+      } finally {
+        out.close();
+      }
     }
     
     //
     // Atomic move sequence
     //
-    // 1.  Move cur to old
-    curFile.renameTo(oldFile);
-    // 2.  Move new to cur
-    newFile.renameTo(curFile);
-    // 3.  Remove pending-edits file (it's been integrated with newFile)
-    editLog.getEditsFile().delete();
-    // 4.  Delete old
-    oldFile.delete();
+    for (int idx = 0; idx < imageDirs.length; idx++) {
+      File curFile = new File(imageDirs[idx], FS_IMAGE);
+      File newFile = new File(imageDirs[idx], NEW_FS_IMAGE);
+      File oldFile = new File(imageDirs[idx], OLD_FS_IMAGE);
+      File timeFile = new File(imageDirs[idx], FS_TIME);
+      // 1.  Move cur to old and delete timeStamp
+      curFile.renameTo(oldFile);
+      if (timeFile.exists()) { timeFile.delete(); }
+      // 2.  Move new to cur and write timestamp
+      newFile.renameTo(curFile);
+      DataOutputStream out = new DataOutputStream(
+            new FileOutputStream(timeFile));
+      try {
+        out.writeLong(System.currentTimeMillis());
+      } finally {
+        out.close();
+      }
+      // 3.  Remove pending-edits file (it's been integrated with newFile)
+      editLog.delete(idx);
+      // 4.  Delete old
+      oldFile.delete();
+    }
   }
 
   /**
@@ -219,9 +267,9 @@
     return newID;
   }
   
-  /** Create a new dfs name directory.  Caution: this destroys all files
+  /** Create new dfs name directory.  Caution: this destroys all files
    * in this filesystem. */
-  static void format(File dir, Configuration conf) throws IOException {
+  static void format(File dir) throws IOException {
     File image = new File(dir, ""image"");
     File edits = new File(dir, ""edits"");
     
"
hadoop,d20db68461c2c2d9166d2c88f3e8c71b26379e52,"HADOOP-90.  Permit dfs.name.dir to list multiple directories where namenode data is to be replicated.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469673 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:11:20,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index 4c25bfd..2e98543 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -196,10 +196,10 @@
     private SafeModeInfo safeMode;  // safe mode information
 
     /**
-     * dir is where the filesystem directory state 
+     * dirs is a list oif directories where the filesystem directory state 
      * is stored
      */
-    public FSNamesystem(File dir, NameNode nn, Configuration conf) throws IOException {
+    public FSNamesystem(File[] dirs, NameNode nn, Configuration conf) throws IOException {
         fsNamesystemObject = this;
         InetSocketAddress addr = DataNode.createSocketAddr(conf.get(""fs.default.name"", ""local""));
         this.maxReplication = conf.getInt(""dfs.replication.max"", 512);
@@ -224,7 +224,7 @@
 
         this.localMachine = addr.getHostName();
         this.port = addr.getPort();
-        this.dir = new FSDirectory(dir);
+        this.dir = new FSDirectory(dirs);
         this.dir.loadFSImage( conf );
         this.safeMode = new SafeModeInfo( conf );
         setBlockTotal();
"
hadoop,d20db68461c2c2d9166d2c88f3e8c71b26379e52,"HADOOP-90.  Permit dfs.name.dir to list multiple directories where namenode data is to be replicated.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469673 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:11:20,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/NameNode.java b/src/java/org/apache/hadoop/dfs/NameNode.java
index 465d47c..24aa4b3 100644
--- a/src/java/org/apache/hadoop/dfs/NameNode.java
+++ b/src/java/org/apache/hadoop/dfs/NameNode.java
@@ -82,7 +82,16 @@
     /** Format a new filesystem.  Destroys any filesystem that may already
      * exist at this location.  **/
     public static void format(Configuration conf) throws IOException {
-      FSDirectory.format(getDir(conf), conf);
+      File[] dirs = getDirs(conf);
+      for (int idx = 0; idx < dirs.length; idx++) {
+        FSImage.format(dirs[idx]);
+      }
+    }
+
+    /** Format a new filesystem.  Destroys any filesystem that may already
+     * exist at this location.  **/
+    public static void format(File dir) throws IOException {
+      FSImage.format(dir);
     }
 
     private class NameNodeMetrics {
@@ -121,24 +130,30 @@
      * Create a NameNode at the default location
      */
     public NameNode(Configuration conf) throws IOException {
-       this(getDir(conf),DataNode.createSocketAddr(conf.get(""fs.default.name"", ""local"")).getHostName(),
+       this(getDirs(conf),DataNode.createSocketAddr(conf.get(""fs.default.name"", ""local"")).getHostName(),
                        DataNode.createSocketAddr(conf.get(""fs.default.name"", ""local"")).getPort(), conf);
     }
 
     /**
      * Create a NameNode at the specified location and start it.
      */
-    public NameNode(File dir, String bindAddress, int port, Configuration conf) throws IOException {
-        this.namesystem = new FSNamesystem(dir, this, conf);
+    public NameNode(File[] dirs, String bindAddress, int port, Configuration conf) throws IOException {
+        this.namesystem = new FSNamesystem(dirs, this, conf);
         this.handlerCount = conf.getInt(""dfs.namenode.handler.count"", 10);
         this.server = RPC.getServer(this, bindAddress, port, handlerCount, false, conf);
         this.server.start();
         myMetrics = new NameNodeMetrics();
     }
 
-    /** Return the configured directory where name data is stored. */
-    private static File getDir(Configuration conf) {
-      return new File(conf.get(""dfs.name.dir"", ""/tmp/hadoop/dfs/name""));
+    /** Return the configured directories where name data is stored. */
+    private static File[] getDirs(Configuration conf) {
+      String[] dirNames = conf.getStrings(""dfs.name.dir"");
+      if (dirNames == null) { dirNames = new String[] {""/tmp/hadoop/dfs/name""}; }
+      File[] dirs = new File[dirNames.length];
+      for (int idx = 0; idx < dirs.length; idx++) {
+        dirs[idx] = new File(dirNames[idx]);
+      }
+      return dirs;
     }
 
     /**
@@ -556,17 +571,22 @@
         Configuration conf = new Configuration();
 
         if (argv.length == 1 && argv[0].equals(""-format"")) {
-          File dir = getDir(conf);
-          if (dir.exists()) {
-            System.err.print(""Re-format filesystem in "" + dir +"" ? (Y or N) "");
-            if (!(System.in.read() == 'Y')) {
-              System.err.println(""Format aborted."");
-              System.exit(1);
+          boolean aborted = false;
+          File[] dirs = getDirs(conf);
+          for (int idx = 0; idx < dirs.length; idx++) {
+            if (dirs[idx].exists()) {
+              System.err.print(""Re-format filesystem in "" + dirs[idx] +"" ? (Y or N) "");
+              if (!(System.in.read() == 'Y')) {
+                System.err.println(""Format aborted in ""+ dirs[idx]);
+                aborted = true;
+              } else {
+                format(dirs[idx]);
+                System.err.println(""Formatted ""+dirs[idx]);
+              }
+              System.in.read(); // discard the enter-key
             }
           }
-          format(conf);
-          System.err.println(""Formatted ""+dir);
-          System.exit(0);
+          System.exit(aborted ? 1 : 0);
         }
         
         NameNode namenode = new NameNode(conf);
"
hadoop,d20db68461c2c2d9166d2c88f3e8c71b26379e52,"HADOOP-90.  Permit dfs.name.dir to list multiple directories where namenode data is to be replicated.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469673 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 22:11:20,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java b/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java
index 1098d79..7c34dff 100644
--- a/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java
+++ b/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java
@@ -135,7 +135,8 @@
     this.nameNodeInfoPort = 50080;   // We just want this port to be different from the default. 
     File base_dir = new File(System.getProperty(""test.build.data""),
                              ""dfs/"");
-    conf.set(""dfs.name.dir"", new File(base_dir, ""name"").getPath());
+    conf.set(""dfs.name.dir"", new File(base_dir, ""name1"").getPath()+"",""+
+        new File(base_dir, ""name2"").getPath());
     conf.set(""dfs.data.dir"", new File(base_dir, ""data1"").getPath()+"",""+
         new File(base_dir, ""data2"").getPath());
     conf.setInt(""dfs.replication"", 1);
"
hadoop,440443c4b7c475dbe1449d9581cd4591d2120ed8,"HADOOP-482.  Fix unit tests to work when a cluster is running on the same machine.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469642 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 20:29:16,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index 8e4ae45..9bf1ad5 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -87,11 +87,11 @@
     }
 
     public static void stopTracker() throws IOException {
-      if (tracker == null)
-        throw new IOException(""Trying to stop JobTracker that is not running."");
       runTracker = false;
-      tracker.close();
-      tracker = null;
+      if (tracker != null) {
+        tracker.close();
+        tracker = null;
+      }
     }
     
     public long getProtocolVersion(String protocol, long clientVersion) {
"
hadoop,440443c4b7c475dbe1449d9581cd4591d2120ed8,"HADOOP-482.  Fix unit tests to work when a cluster is running on the same machine.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469642 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 20:29:16,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java b/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java
index a3848db..1098d79 100644
--- a/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java
+++ b/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java
@@ -34,6 +34,12 @@
   private Thread dataNodeThread;
   private NameNodeRunner nameNode;
   private DataNodeRunner dataNode;
+  private int maxRetries = 10;
+  private int MAX_RETRIES  = 10;
+  private int MAX_RETRIES_PER_PORT = 10;
+
+  private int nameNodePort = 0;
+  private int nameNodeInfoPort = 0;
 
   /**
    * An inner class that runs a name node.
@@ -111,17 +117,22 @@
       }
     }
   }
-  
+
   /**
-   * Create the config and start up the servers.
+   * Create the config and start up the servers.  If either the rpc or info port is already 
+   * in use, we will try new ports.
+   * @param namenodePort suggestion for which rpc port to use.  caller should use 
+   *                     getNameNodePort() to get the actual port used.   
    * @param dataNodeFirst should the datanode be brought up before the namenode?
    */
   public MiniDFSCluster(int namenodePort, 
                         Configuration conf,
                         boolean dataNodeFirst) throws IOException {
+
     this.conf = conf;
-    conf.set(""fs.default.name"", 
-             ""localhost:""+ Integer.toString(namenodePort));
+
+    this.nameNodePort = namenodePort;
+    this.nameNodeInfoPort = 50080;   // We just want this port to be different from the default. 
     File base_dir = new File(System.getProperty(""test.build.data""),
                              ""dfs/"");
     conf.set(""dfs.name.dir"", new File(base_dir, ""name"").getPath());
@@ -131,27 +142,66 @@
     // this timeout seems to control the minimum time for the test, so
     // decrease it considerably.
     conf.setInt(""ipc.client.timeout"", 1000);
-    NameNode.format(conf);
-    nameNode = new NameNodeRunner();
-    nameNodeThread = new Thread(nameNode);
-    dataNode = new DataNodeRunner();
-    dataNodeThread = new Thread(dataNode);
-    if (dataNodeFirst) {
-      dataNodeThread.start();      
-      nameNodeThread.start();      
-    } else {
-      nameNodeThread.start();
-      dataNodeThread.start();      
-    }
-    while (!nameNode.isUp()) {
-      try {                                     // let daemons get started
-        System.out.println(""waiting for dfs minicluster to start"");
-        Thread.sleep(1000);
-      } catch(InterruptedException e) {
+
+    // Loops until we find ports that work or we give up because 
+    // too many tries have failed.
+    boolean foundPorts = false;
+    int portsTried = 0;
+    while ((!foundPorts) && (portsTried < MAX_RETRIES)) {
+      conf.set(""fs.default.name"", 
+               ""localhost:""+ Integer.toString(nameNodePort));
+      conf.set(""dfs.info.port"", nameNodeInfoPort);
+      
+      NameNode.format(conf);
+      nameNode = new NameNodeRunner();
+      nameNodeThread = new Thread(nameNode);
+      dataNode = new DataNodeRunner();
+      dataNodeThread = new Thread(dataNode);
+      if (dataNodeFirst) {
+        dataNodeThread.start();      
+        nameNodeThread.start();      
+      } else {
+        nameNodeThread.start();
+        dataNodeThread.start();      
       }
+
+      int retry = 0;
+      while (!nameNode.isUp() && (retry < MAX_RETRIES_PER_PORT)) {
+        try {                                     // let daemons get started
+          System.out.println(""waiting for dfs minicluster to start"");
+          Thread.sleep(1000);
+        } catch(InterruptedException e) {
+        }
+        retry++;
+      }
+      if (retry >= MAX_RETRIES_PER_PORT) {
+        this.nameNodePort += 3;
+        this.nameNodeInfoPort += 7;
+        System.out.println(""Failed to start DFS minicluster in "" + retry + "" attempts.  Trying new ports:"");
+        System.out.println(""\tNameNode RPC port: "" + nameNodePort);
+        System.out.println(""\tNameNode info port: "" + nameNodeInfoPort);
+
+        nameNode.shutdown();
+        dataNode.shutdown();
+        
+      } else {
+        foundPorts = true;
+      }
+      portsTried++;
+    } 
+    if (portsTried >= MAX_RETRIES) {
+        throw new IOException(""Failed to start a DFS minicluster after trying "" + portsTried + "" ports."");
     }
   }
-  
+
+  /**
+   * Returns the rpc port used by the mini cluster, because the caller supplied port is 
+   * not necessarily the actual port used.
+   */     
+  public int getNameNodePort() {
+    return nameNodePort;
+  }
+    
   /**
    * Shut down the servers.
    */
"
hadoop,440443c4b7c475dbe1449d9581cd4591d2120ed8,"HADOOP-482.  Fix unit tests to work when a cluster is running on the same machine.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@469642 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-31 20:29:16,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/mapred/MiniMRCluster.java b/src/test/org/apache/hadoop/mapred/MiniMRCluster.java
index d70086d..a708566 100644
--- a/src/test/org/apache/hadoop/mapred/MiniMRCluster.java
+++ b/src/test/org/apache/hadoop/mapred/MiniMRCluster.java
@@ -32,7 +32,7 @@
     
     private int jobTrackerPort = 0;
     private int taskTrackerPort = 0;
-    
+    private int jobTrackerInfoPort = 0;
     private int numTaskTrackers;
     
     private List taskTrackerList = new ArrayList();
@@ -40,10 +40,17 @@
     
     private String namenode;
     
+    private int MAX_RETRIES_PER_PORT = 10;
+    private int MAX_RETRIES = 10;
+
     /**
      * An inner class that runs a job tracker.
      */
     class JobTrackerRunner implements Runnable {
+
+        public boolean isUp() {
+            return (JobTracker.getTracker() != null);
+        }
         /**
          * Create the job tracker and run it.
          */
@@ -52,6 +59,7 @@
                 JobConf jc = new JobConf();
                 jc.set(""fs.name.node"", namenode);
                 jc.set(""mapred.job.tracker"", ""localhost:""+jobTrackerPort);
+                jc.set(""mapred.job.tracker.info.port"", jobTrackerInfoPort);
                 // this timeout seems to control the minimum time for the test, so
                 // set it down at 2 seconds.
                 jc.setInt(""ipc.client.timeout"", 1000);
@@ -194,9 +202,18 @@
         }
       }
     }
-    
+
+    /** 
+     * Get the actual rpc port used.
+     */
+    public int getJobTrackerPort() {
+        return jobTrackerPort;
+    }
+
     /**
-     * Create the config and start up the servers.
+     * Create the config and start up the servers.  The ports supplied by the user are
+     * just used as suggestions.  If those ports are already in use, new ports
+     * are tried.  The caller should call getJobTrackerPort to get the actual rpc port used.
      */
     public MiniMRCluster(int jobTrackerPort,
                          int taskTrackerPort,
@@ -211,39 +228,65 @@
             int numTaskTrackers,
             String namenode,
             boolean taskTrackerFirst, int numDir) throws IOException {
+        
         this.jobTrackerPort = jobTrackerPort;
         this.taskTrackerPort = taskTrackerPort;
+        this.jobTrackerInfoPort = 50030;
         this.numTaskTrackers = numTaskTrackers;
         this.namenode = namenode;
-        
-        File configDir = new File(""build"", ""minimr"");
-        configDir.mkdirs();
-        File siteFile = new File(configDir, ""hadoop-site.xml"");
-        PrintWriter pw = new PrintWriter(siteFile);
-        pw.print(""<?xml version=\""1.0\""?>\n""+
-                ""<?xml-stylesheet type=\""text/xsl\"" href=\""configuration.xsl\""?>\n""+
-                ""<configuration>\n""+
-                "" <property>\n""+
-                ""   <name>mapred.system.dir</name>\n""+
-                ""   <value>build/test/mapred/system</value>\n""+
-                "" </property>\n""+
-                ""</configuration>\n"");
-        pw.close();
-        jobTracker = new JobTrackerRunner();
-        jobTrackerThread = new Thread(jobTracker);
-        if (!taskTrackerFirst) {
-          jobTrackerThread.start();
-        }
-        for (int idx = 0; idx < numTaskTrackers; idx++) {
+
+        // Loop until we find a set of ports that are all unused or until we
+        // give up because it's taken too many tries.
+        boolean foundPorts = false;
+        int portsTried = 0;
+        while ((!foundPorts) && (portsTried < MAX_RETRIES)) {
+          jobTracker = new JobTrackerRunner();
+          jobTrackerThread = new Thread(jobTracker);
+          if (!taskTrackerFirst) {
+            jobTrackerThread.start();
+          }
+          for (int idx = 0; idx < numTaskTrackers; idx++) {
             TaskTrackerRunner taskTracker = new TaskTrackerRunner(numDir);
             Thread taskTrackerThread = new Thread(taskTracker);
             taskTrackerThread.start();
             taskTrackerList.add(taskTracker);
             taskTrackerThreadList.add(taskTrackerThread);
+          }
+          if (taskTrackerFirst) {
+            jobTrackerThread.start();
+          }
+          int retry = 0;
+          while (!jobTracker.isUp() && (retry < MAX_RETRIES_PER_PORT)) {
+            try {                                     // let daemons get started
+              System.err.println(""waiting for jobtracker to start"");
+              Thread.sleep(1000);
+            } catch(InterruptedException e) {
+            }
+            retry++;
+          }
+          if (retry >= MAX_RETRIES_PER_PORT) {
+              // Try new ports.
+              this.jobTrackerPort += 7;
+              this.jobTrackerInfoPort += 3;
+              this.taskTrackerPort++;
+
+              System.err.println(""Failed to start MR minicluster in "" + retry + 
+                                 "" attempts.  Retrying with new ports:"");
+              System.err.println(""\tJobTracker RPC port = "" + jobTrackerPort);
+              System.err.println(""\tJobTracker info port = "" + jobTrackerInfoPort);
+              System.err.println(""\tTaskTracker RPC port(s) = "" + 
+                                 taskTrackerPort + ""-"" + (taskTrackerPort+numTaskTrackers-1));
+              shutdown();
+              taskTrackerList.clear();
+          } else {
+            foundPorts = true;
+          }
+          portsTried++;
         }
-        if (taskTrackerFirst) {
-          jobTrackerThread.start();
+        if (portsTried >= MAX_RETRIES) {
+            throw new IOException(""Failed to start MR minicluster after trying "" + portsTried + "" ports."");
         }
+        
         waitUntilIdle();
     }
     
"
hadoop,9267453f45c1ebbead6d130e0cbfa71caf51bddf,"HADOOP-624.  Fix servlet path to stop a Jetty warning on startup.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@468123 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-26 20:35:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index c1858e0..89d4a39 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -391,7 +391,7 @@
       server.setAttribute(""local.file.system"", local);
       server.setAttribute(""conf"", conf);
       server.setAttribute(""log"", LOG);
-      server.addServlet(""mapOutput"", ""mapOutput"", MapOutputServlet.class);
+      server.addServlet(""mapOutput"", ""/mapOutput"", MapOutputServlet.class);
       server.start();
       this.httpPort = server.getPort();
       initialize();
"
hadoop,886493569bb8720abc14d0cb57859d11a5cd0bd0,"HADOOP-638.  Fix an unsynchronized access to TaskTracker's internal state.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@468120 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-26 20:32:20,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 724d9d7..c1858e0 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -446,7 +446,7 @@
             killOverflowingTasks();
             
             //we've cleaned up, resume normal operation
-            if (!acceptNewTasks && tasks.isEmpty()) {
+            if (!acceptNewTasks && isIdle()) {
                 acceptNewTasks=true;
             }
           } catch (InterruptedException ie) {
"
hadoop,7af68687385e925b385c09ceb74261f340cf5e4a,"HADOOP-642.  Change IPC client to specify an explicit connect timeout.  Contributed by Konstantin.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@468117 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-26 20:29:02,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/Client.java b/src/java/org/apache/hadoop/ipc/Client.java
index 3bd1bc9..89480aa 100644
--- a/src/java/org/apache/hadoop/ipc/Client.java
+++ b/src/java/org/apache/hadoop/ipc/Client.java
@@ -32,16 +32,14 @@
 
 import java.util.Hashtable;
 import java.util.Iterator;
-import java.util.Collection;
-import java.util.Random;
 
 import org.apache.commons.logging.*;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.conf.Configurable;
+import org.apache.hadoop.dfs.FSConstants;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.io.UTF8;
 import org.apache.hadoop.io.DataOutputBuffer;
 
 /** A client for an IPC service.  IPC calls take a single {@link Writable} as a
@@ -134,7 +132,8 @@
       short failures = 0;
       while (true) {
         try {
-          this.socket = new Socket(address.getAddress(), address.getPort());
+          this.socket = new Socket();
+          this.socket.connect(address, FSConstants.READ_TIMEOUT);
           break;
         } catch (IOException ie) { //SocketTimeoutException is also caught 
           if (failures == maxRetries) {
"
hadoop,7af68687385e925b385c09ceb74261f340cf5e4a,"HADOOP-642.  Change IPC client to specify an explicit connect timeout.  Contributed by Konstantin.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@468117 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-26 20:29:02,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/Server.java b/src/java/org/apache/hadoop/ipc/Server.java
index fbdb65c..afe684d 100644
--- a/src/java/org/apache/hadoop/ipc/Server.java
+++ b/src/java/org/apache/hadoop/ipc/Server.java
@@ -295,9 +295,9 @@
         numConnections++;
       }
       if (LOG.isDebugEnabled())
-        LOG.debug(""Server connection on port "" + port + "" from "" + 
-                c.getHostAddress() +
-                "": starting. Number of active connections: "" + numConnections);
+        LOG.debug(""Server connection from "" + c.toString() +
+                ""; # active connections: "" + numConnections +
+                ""; # queued calls: "" + callQueue.size() );
     }
 
     void doRead(SelectionKey key) {
"
hadoop,5b5d3b6fde21e7af0bf9444b72497e816f56a54d,"HADOOP-641.  Change NameNoide to request a fresh block report from re-discovered DataNodes.  Contributed by Konstantin.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@468115 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-26 20:22:36,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/BlockCommand.java b/src/java/org/apache/hadoop/dfs/BlockCommand.java
index b35ef19..13a4202 100644
--- a/src/java/org/apache/hadoop/dfs/BlockCommand.java
+++ b/src/java/org/apache/hadoop/dfs/BlockCommand.java
@@ -37,54 +37,43 @@
            public Writable newInstance() { return new BlockCommand(); }
          });
     }
-  
-    boolean transferBlocks = false;
-    boolean invalidateBlocks = false;
-    boolean shutdown = false;
+
+    DatanodeProtocol.DataNodeAction action;
     Block blocks[];
     DatanodeInfo targets[][];
 
     public BlockCommand() {
-        this.transferBlocks = false;
-        this.invalidateBlocks = false;
-        this.shutdown = false;
-        this.blocks = new Block[0];
-        this.targets = new DatanodeInfo[0][];
+      this.action = DatanodeProtocol.DataNodeAction.DNA_UNKNOWN;
+      this.blocks = new Block[0];
+      this.targets = new DatanodeInfo[0][];
     }
 
+    /**
+     * Create BlockCommand for transferring blocks to another datanode
+     * @param blocks    blocks to be transferred 
+     * @param targets   nodes to transfer
+     */
     public BlockCommand(Block blocks[], DatanodeInfo targets[][]) {
-        this.transferBlocks = true;
-        this.invalidateBlocks = false;
-        this.shutdown = false;
-        this.blocks = blocks;
-        this.targets = targets;
+      this.action = DatanodeProtocol.DataNodeAction.DNA_TRANSFER;
+      this.blocks = blocks;
+      this.targets = targets;
     }
 
+    /**
+     * Create BlockCommand for block invalidation
+     * @param blocks  blocks to invalidate
+     */
     public BlockCommand(Block blocks[]) {
-        this.transferBlocks = false;
-        this.invalidateBlocks = true;
-        this.shutdown = false;
-        this.blocks = blocks;
-        this.targets = new DatanodeInfo[0][];
+      this.action = DatanodeProtocol.DataNodeAction.DNA_INVALIDATE;
+      this.blocks = blocks;
+      this.targets = new DatanodeInfo[0][];
     }
 
-    public BlockCommand( boolean doShutdown ) {
+    public BlockCommand( DatanodeProtocol.DataNodeAction action ) {
       this();
-      this.shutdown = doShutdown;
+      this.action = action;
     }
 
-    public boolean transferBlocks() {
-        return transferBlocks;
-    }
-
-    public boolean invalidateBlocks() {
-        return invalidateBlocks;
-    }
-    
-    public boolean shutdownNode() {
-      return shutdown;
-  }
-  
     public Block[] getBlocks() {
         return blocks;
     }
@@ -97,8 +86,7 @@
     // Writable
     ///////////////////////////////////////////
     public void write(DataOutput out) throws IOException {
-        out.writeBoolean(transferBlocks);
-        out.writeBoolean(invalidateBlocks);        
+        WritableUtils.writeEnum( out, action );
         out.writeInt(blocks.length);
         for (int i = 0; i < blocks.length; i++) {
             blocks[i].write(out);
@@ -113,8 +101,8 @@
     }
 
     public void readFields(DataInput in) throws IOException {
-        this.transferBlocks = in.readBoolean();
-        this.invalidateBlocks = in.readBoolean();
+        this.action = (DatanodeProtocol.DataNodeAction)
+            WritableUtils.readEnum( in, DatanodeProtocol.DataNodeAction.class );
         this.blocks = new Block[in.readInt()];
         for (int i = 0; i < blocks.length; i++) {
             blocks[i] = new Block();
"
hadoop,5b5d3b6fde21e7af0bf9444b72497e816f56a54d,"HADOOP-641.  Change NameNoide to request a fresh block report from re-discovered DataNodes.  Contributed by Konstantin.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@468115 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-26 20:22:36,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index 615f656..1525993 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -267,7 +267,17 @@
      * @throws IOException
      */
     private void register() throws IOException {
-      dnRegistration = namenode.register( dnRegistration );
+      while( true ) {
+        try {
+          dnRegistration = namenode.register( dnRegistration );
+          break;
+        } catch( SocketTimeoutException e ) {  // namenode is busy
+          LOG.info(""Problem connecting to server: "" + getNameNodeAddr());
+          try {
+            Thread.sleep(1000);
+          } catch (InterruptedException ie) {}
+        }
+      }
       if( storage.getStorageID().equals("""") ) {
         storage.setStorageID( dnRegistration.getStorageID());
         storage.writeAll();
@@ -350,29 +360,14 @@
 
             if( cmd != null ) {
               data.checkDataDir();
-              if (cmd.transferBlocks()) {
+              switch( cmd.action ) {
+              case DNA_TRANSFER:
                 //
                 // Send a copy of a block to another datanode
                 //
-                Block blocks[] = cmd.getBlocks();
-                DatanodeInfo xferTargets[][] = cmd.getTargets();
-                    
-                for (int i = 0; i < blocks.length; i++) {
-                  if (!data.isValidBlock(blocks[i])) {
-                    String errStr = ""Can't send invalid block "" + blocks[i];
-                    LOG.info(errStr);
-                    namenode.errorReport( dnRegistration, 
-                                DatanodeProtocol.INVALID_BLOCK, 
-                                errStr);
-                    break;
-                  } else {
-                    if (xferTargets[i].length > 0) {
-                        LOG.info(""Starting thread to transfer block "" + blocks[i] + "" to "" + xferTargets[i]);
-                        new Daemon(new DataTransfer(xferTargets[i], blocks[i])).start();
-                    }
-                  }
-                }
-              } else if (cmd.invalidateBlocks()) {
+                transferBlocks( cmd.getBlocks(), cmd.getTargets() );
+                break;
+              case DNA_INVALIDATE:
                 //
                 // Some local block(s) are obsolete and can be 
                 // safely garbage-collected.
@@ -380,10 +375,17 @@
                 Block toDelete[] = cmd.getBlocks();
                 data.invalidate(toDelete);
                 myMetrics.removedBlocks(toDelete.length);
-              } else if( cmd.shutdownNode()) {
+                break;
+              case DNA_SHUTDOWN:
                 // shut down the data node
                 this.shutdown();
                 continue;
+              case DNA_REPORT:
+                // namenode requested a block report; sending
+                lastBlockReport = 0;
+                break;
+              default:
+                LOG.warn( ""Unknown BlockCommand action: "" + cmd.action);
               }
             }
           }
@@ -455,7 +457,24 @@
       } // while (shouldRun)
     } // offerService
 
-    
+    private void transferBlocks(  Block blocks[], 
+                                  DatanodeInfo xferTargets[][] 
+                                ) throws IOException {
+      for (int i = 0; i < blocks.length; i++) {
+        if (!data.isValidBlock(blocks[i])) {
+          String errStr = ""Can't send invalid block "" + blocks[i];
+          LOG.info(errStr);
+          namenode.errorReport( dnRegistration, 
+                                DatanodeProtocol.INVALID_BLOCK, 
+                                errStr );
+          break;
+        }
+        if (xferTargets[i].length > 0) {
+          LOG.info(""Starting thread to transfer block "" + blocks[i] + "" to "" + xferTargets[i]);
+          new Daemon(new DataTransfer(xferTargets[i], blocks[i])).start();
+        }
+      }
+    }
     
     /**
      * Server used for receiving/sending a block of data.
"
hadoop,5b5d3b6fde21e7af0bf9444b72497e816f56a54d,"HADOOP-641.  Change NameNoide to request a fresh block report from re-discovered DataNodes.  Contributed by Konstantin.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@468115 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-26 20:22:36,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DatanodeProtocol.java b/src/java/org/apache/hadoop/dfs/DatanodeProtocol.java
index ded781b..a59211b 100644
--- a/src/java/org/apache/hadoop/dfs/DatanodeProtocol.java
+++ b/src/java/org/apache/hadoop/dfs/DatanodeProtocol.java
@@ -29,12 +29,23 @@
  * @author Michael Cafarella
  **********************************************************************/
 interface DatanodeProtocol extends VersionedProtocol {
-  public static final long versionID = 2L;  // infoPort added to DatanodeID
-                                            // affected: DatanodeRegistration
+  public static final long versionID = 3L;  // BlockCommand.action replaced boolean members
+                                            // affected: BlockCommand
   
   // error code
   final static int DISK_ERROR = 1;
   final static int INVALID_BLOCK = 2;
+
+  /**
+   * Determines actions that data node should perform 
+   * when receiving a block command. 
+   */
+  public enum DataNodeAction{ DNA_UNKNOWN,    // unknown action   
+                              DNA_TRANSFER,   // transfer blocks to another datanode
+                              DNA_INVALIDATE, // invalidate blocks
+                              DNA_SHUTDOWN,   // shutdown node
+                              DNA_REPORT; }   // send block report to the namenode
+
   /** 
    * Register Datanode.
    *
"
hadoop,5b5d3b6fde21e7af0bf9444b72497e816f56a54d,"HADOOP-641.  Change NameNoide to request a fresh block report from re-discovered DataNodes.  Contributed by Konstantin.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@468115 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-26 20:22:36,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index 3c7f487..324e344 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -1297,14 +1297,23 @@
      * The given node has reported in.  This method should:
      * 1) Record the heartbeat, so the datanode isn't timed out
      * 2) Adjust usage stats for future block allocation
+     * 
+     * If a substantial amount of time passed since the last datanode 
+     * heartbeat then request an immediate block report.  
+     * 
+     * @return true if block report is required or false otherwise.
+     * @throws IOException
      */
-    public synchronized void gotHeartbeat(DatanodeID nodeID,
-                                          long capacity, 
-                                          long remaining,
-                                          int xceiverCount) throws IOException {
+    public synchronized boolean gotHeartbeat( DatanodeID nodeID,
+                                              long capacity, 
+                                              long remaining,
+                                              int xceiverCount
+                                            ) throws IOException {
+      boolean needBlockReport;
       synchronized (heartbeats) {
         synchronized (datanodeMap) {
           DatanodeDescriptor nodeinfo = getDatanode( nodeID );
+          needBlockReport = nodeinfo.isDead(); 
           
           if (nodeinfo == null) 
             // We do not accept unregistered guests
@@ -1314,6 +1323,7 @@
           addHeartbeat(nodeinfo);
         }
       }
+      return needBlockReport;
     }
 
     /**
@@ -1902,6 +1912,9 @@
                       ""BLOCK* NameSystem.pendingTransfer: "" + ""ask ""
                       + srcNode.getName() + "" to replicate ""
                       + block.getBlockName() + "" to "" + targetList);
+              NameNode.stateChangeLog.debug(
+                  ""BLOCK* neededReplications = "" + neededReplications.size()
+                  + "" pendingReplications = "" + pendingReplications.size() );
             }
           }
 
@@ -2280,7 +2293,7 @@
        */
       synchronized boolean isOn() {
         try {
-          isConsistent();   // SHV this an assert
+          isConsistent();   // SHV this is an assert
         } catch( IOException e ) {
           System.err.print( StringUtils.stringifyException( e ));
         }
"
hadoop,5b5d3b6fde21e7af0bf9444b72497e816f56a54d,"HADOOP-641.  Change NameNoide to request a fresh block report from re-discovered DataNodes.  Contributed by Konstantin.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@468115 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-26 20:22:36,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/NameNode.java b/src/java/org/apache/hadoop/dfs/NameNode.java
index b589a27..52a4117 100644
--- a/src/java/org/apache/hadoop/dfs/NameNode.java
+++ b/src/java/org/apache/hadoop/dfs/NameNode.java
@@ -464,7 +464,10 @@
                                       int xmitsInProgress,
                                       int xceiverCount) throws IOException {
         verifyRequest( nodeReg );
-        namesystem.gotHeartbeat( nodeReg, capacity, remaining, xceiverCount );
+        if( namesystem.gotHeartbeat( nodeReg, capacity, remaining, xceiverCount )) {
+          // request block report from the datanode
+          return new BlockCommand( DataNodeAction.DNA_REPORT );
+        }
         
         //
         // Ask to perform pending transfers, if any
"
hadoop,e56cc797c450ed29ace0f05e073e9402395bb343,"HADOOP-563. Improve NameNode lease policy.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@468107 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-26 19:55:35,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSClient.java b/src/java/org/apache/hadoop/dfs/DFSClient.java
index f321601..7dc8250 100644
--- a/src/java/org/apache/hadoop/dfs/DFSClient.java
+++ b/src/java/org/apache/hadoop/dfs/DFSClient.java
@@ -445,7 +445,7 @@
         public void run() {
             long lastRenewed = 0;
             while (running) {
-                if (System.currentTimeMillis() - lastRenewed > (LEASE_PERIOD / 2)) {
+                if (System.currentTimeMillis() - lastRenewed > (LEASE_SOFTLIMIT_PERIOD / 2)) {
                     try {
                       if( pendingCreates.size() > 0 )
                         namenode.renewLease(clientName);
@@ -1004,7 +1004,7 @@
                   // wait and try again.
                   LOG.info(StringUtils.stringifyException(e));
                   try {
-                    Thread.sleep(LEASE_PERIOD);
+                    Thread.sleep(LEASE_SOFTLIMIT_PERIOD);
                   } catch (InterruptedException ie) {
                   }
                 }
"
hadoop,e56cc797c450ed29ace0f05e073e9402395bb343,"HADOOP-563. Improve NameNode lease policy.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@468107 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-26 19:55:35,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSConstants.java b/src/java/org/apache/hadoop/dfs/FSConstants.java
index 0319685..27af1a2 100644
--- a/src/java/org/apache/hadoop/dfs/FSConstants.java
+++ b/src/java/org/apache/hadoop/dfs/FSConstants.java
@@ -103,7 +103,8 @@
     public static long HEARTBEAT_INTERVAL = 3;
     public static long EXPIRE_INTERVAL = 10 * 60 * 1000;
     public static long BLOCKREPORT_INTERVAL = 60 * 60 * 1000;
-    public static long LEASE_PERIOD = 60 * 1000;
+    public static final long LEASE_SOFTLIMIT_PERIOD = 60 * 1000;
+    public static final long LEASE_HARDLIMIT_PERIOD = 60 * LEASE_SOFTLIMIT_PERIOD;
     public static int READ_TIMEOUT = 60 * 1000;
 
     // We need to limit the length and depth of a path in the filesystem.  HADOOP-438
"
hadoop,e56cc797c450ed29ace0f05e073e9402395bb343,"HADOOP-563. Improve NameNode lease policy.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@468107 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-26 19:55:35,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index ec5ff19..3c7f487 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -414,11 +414,54 @@
         throw new IOException(""Invalid file name: "" + src);      	  
       }
       try {
-        if (pendingCreates.get(src) != null) {
-           throw new AlreadyBeingCreatedException(
-                   ""failed to create file "" + src + "" for "" + holder +
-                   "" on client "" + clientMachine + 
-                   "" because pendingCreates is non-null."");
+        FileUnderConstruction pendingFile = (FileUnderConstruction ) 
+                                              pendingCreates.get(src);
+        if (pendingFile != null) {
+          //
+          // If the file exists in pendingCreate, then it must be in our
+          // leases. Find the appropriate lease record.
+          //
+          Lease lease = (Lease) leases.get(holder);
+          //
+          // We found the lease for this file. And surprisingly the original
+          // holder is trying to recreate this file. This should never occur.
+          //
+          if (lease != null) {
+            throw new AlreadyBeingCreatedException(
+                  ""failed to create file "" + src + "" for "" + holder +
+                  "" on client "" + clientMachine + 
+                  "" because current leaseholder is trying to recreate file."");
+          }
+          //
+          // Find the original holder.
+          //
+          UTF8 oldholder = pendingFile.getClientName();
+          lease = (Lease) leases.get(oldholder);
+          if (lease == null) {
+            throw new AlreadyBeingCreatedException(
+                  ""failed to create file "" + src + "" for "" + holder +
+                  "" on client "" + clientMachine + 
+                  "" because pendingCreates is non-null but no leases found."");
+          }
+          //
+          // If the original holder has not renewed in the last SOFTLIMIT 
+          // period, then reclaim all resources and allow this request 
+          // to proceed. Otherwise, prevent this request from creating file.
+          //
+          if (lease.expiredSoftLimit()) {
+            lease.releaseLocks();
+            leases.remove(lease.holder);
+            LOG.info(""Removing lease "" + lease + "" "");
+            if (!sortedLeases.remove(lease)) {
+              LOG.error(""Unknown failure trying to remove "" + lease + 
+                       "" from lease set."");
+            }
+          } else  {
+            throw new AlreadyBeingCreatedException(
+                  ""failed to create file "" + src + "" for "" + holder +
+                  "" on client "" + clientMachine + 
+                  "" because pendingCreates is non-null."");
+          }
         }
 
         try {
@@ -929,12 +972,23 @@
         public void renew() {
             this.lastUpdate = now();
         }
-        public boolean expired() {
-            if (now() - lastUpdate > LEASE_PERIOD) {
+        /**
+         * Returns true if the Hard Limit Timer has expired
+         */
+        public boolean expiredHardLimit() {
+            if (now() - lastUpdate > LEASE_HARDLIMIT_PERIOD) {
                 return true;
-            } else {
-                return false;
             }
+            return false;
+        }
+        /**
+         * Returns true if the Soft Limit Timer has expired
+         */
+        public boolean expiredSoftLimit() {
+            if (now() - lastUpdate > LEASE_SOFTLIMIT_PERIOD) {
+                return true;
+            }
+            return false;
         }
         public void obtained(UTF8 src) {
             locks.add(src);
@@ -999,7 +1053,7 @@
                         Lease top;
                         while ((sortedLeases.size() > 0) &&
                                ((top = (Lease) sortedLeases.first()) != null)) {
-                            if (top.expired()) {
+                            if (top.expiredHardLimit()) {
                                 top.releaseLocks();
                                 leases.remove(top.holder);
                                 LOG.info(""Removing lease "" + top + "", leases remaining: "" + sortedLeases.size());
"
hadoop,f6b8336657eac585cc29482d0d7ebcfebc0eafec,"HADOOP-627.  Fix some synchronization problems in MiniMRCluster.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@467502 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-24 21:53:08,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/mapred/MiniMRCluster.java b/src/test/org/apache/hadoop/mapred/MiniMRCluster.java
index 0ad4725..25e38eb 100644
--- a/src/test/org/apache/hadoop/mapred/MiniMRCluster.java
+++ b/src/test/org/apache/hadoop/mapred/MiniMRCluster.java
@@ -78,11 +78,11 @@
      * An inner class to run the task tracker.
      */
     class TaskTrackerRunner implements Runnable {
-        TaskTracker tt;
+        volatile TaskTracker tt;
         // the localDirs for this taskTracker
         String[] localDir;
-        boolean isInitialized = false;
-        boolean isDead = false;
+        volatile boolean isInitialized = false;
+        volatile boolean isDead = false;
         int numDir;       
         TaskTrackerRunner(int numDir) {
           this.numDir = numDir;
"
hadoop,14866ceb5d092acd8de156e11e02dc2850ee94ff,"HADOOP-634.  Add missing license to many files.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@467498 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-24 21:49:45,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java b/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java
index ce1e9c2..2b37b99 100644
--- a/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java
+++ b/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java
@@ -1,3 +1,18 @@
+/**
+ * Copyright 2006 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
 package org.apache.hadoop.dfs;
 
 import java.io.*;
"
hadoop,14866ceb5d092acd8de156e11e02dc2850ee94ff,"HADOOP-634.  Add missing license to many files.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@467498 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-24 21:49:45,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/fs/AccumulatingReducer.java b/src/test/org/apache/hadoop/fs/AccumulatingReducer.java
index bed4efb..a8f25da 100644
--- a/src/test/org/apache/hadoop/fs/AccumulatingReducer.java
+++ b/src/test/org/apache/hadoop/fs/AccumulatingReducer.java
@@ -1,3 +1,18 @@
+/**
+ * Copyright 2006 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
 package org.apache.hadoop.fs;
 
 import java.io.IOException;
"
hadoop,14866ceb5d092acd8de156e11e02dc2850ee94ff,"HADOOP-634.  Add missing license to many files.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@467498 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-24 21:49:45,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/fs/IOMapperBase.java b/src/test/org/apache/hadoop/fs/IOMapperBase.java
index 8ab396f..44ba418 100644
--- a/src/test/org/apache/hadoop/fs/IOMapperBase.java
+++ b/src/test/org/apache/hadoop/fs/IOMapperBase.java
@@ -1,3 +1,18 @@
+/**
+ * Copyright 2006 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
 package org.apache.hadoop.fs;
 
 import java.io.IOException;
"
hadoop,14866ceb5d092acd8de156e11e02dc2850ee94ff,"HADOOP-634.  Add missing license to many files.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@467498 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-24 21:49:45,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/mapred/EmptyInputFormat.java b/src/test/org/apache/hadoop/mapred/EmptyInputFormat.java
index d04fedc..7882ebb 100644
--- a/src/test/org/apache/hadoop/mapred/EmptyInputFormat.java
+++ b/src/test/org/apache/hadoop/mapred/EmptyInputFormat.java
@@ -1,3 +1,18 @@
+/**
+ * Copyright 2006 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
 package org.apache.hadoop.mapred;
 
 import java.io.IOException;
"
hadoop,14866ceb5d092acd8de156e11e02dc2850ee94ff,"HADOOP-634.  Add missing license to many files.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@467498 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-24 21:49:45,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/record/test/RecBuffer.java b/src/test/org/apache/hadoop/record/test/RecBuffer.java
index 2794576..c31db7f 100644
--- a/src/test/org/apache/hadoop/record/test/RecBuffer.java
+++ b/src/test/org/apache/hadoop/record/test/RecBuffer.java
@@ -1,3 +1,18 @@
+/**
+ * Copyright 2006 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
 // File generated by hadoop record compiler. Do not edit.
 package org.apache.hadoop.record.test;
 
"
hadoop,14866ceb5d092acd8de156e11e02dc2850ee94ff,"HADOOP-634.  Add missing license to many files.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@467498 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-24 21:49:45,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/record/test/RecInt.java b/src/test/org/apache/hadoop/record/test/RecInt.java
index 3b39eff..a46654f 100644
--- a/src/test/org/apache/hadoop/record/test/RecInt.java
+++ b/src/test/org/apache/hadoop/record/test/RecInt.java
@@ -1,3 +1,18 @@
+/**
+ * Copyright 2006 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
 // File generated by hadoop record compiler. Do not edit.
 package org.apache.hadoop.record.test;
 
"
hadoop,14866ceb5d092acd8de156e11e02dc2850ee94ff,"HADOOP-634.  Add missing license to many files.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@467498 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-24 21:49:45,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/record/test/RecRecord0.java b/src/test/org/apache/hadoop/record/test/RecRecord0.java
index 2f514b3..afe509a 100644
--- a/src/test/org/apache/hadoop/record/test/RecRecord0.java
+++ b/src/test/org/apache/hadoop/record/test/RecRecord0.java
@@ -1,3 +1,18 @@
+/**
+ * Copyright 2006 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
 // File generated by hadoop record compiler. Do not edit.
 package org.apache.hadoop.record.test;
 
"
hadoop,14866ceb5d092acd8de156e11e02dc2850ee94ff,"HADOOP-634.  Add missing license to many files.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@467498 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-24 21:49:45,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/record/test/RecRecord1.java b/src/test/org/apache/hadoop/record/test/RecRecord1.java
index 35f7ed1..9d6026d 100644
--- a/src/test/org/apache/hadoop/record/test/RecRecord1.java
+++ b/src/test/org/apache/hadoop/record/test/RecRecord1.java
@@ -1,3 +1,18 @@
+/**
+ * Copyright 2006 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
 // File generated by hadoop record compiler. Do not edit.
 package org.apache.hadoop.record.test;
 
"
hadoop,14866ceb5d092acd8de156e11e02dc2850ee94ff,"HADOOP-634.  Add missing license to many files.  Contributed by Nigel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@467498 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-24 21:49:45,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/record/test/RecString.java b/src/test/org/apache/hadoop/record/test/RecString.java
index 0d79dab..378b56b 100644
--- a/src/test/org/apache/hadoop/record/test/RecString.java
+++ b/src/test/org/apache/hadoop/record/test/RecString.java
@@ -1,3 +1,18 @@
+/**
+ * Copyright 2006 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
 // File generated by hadoop record compiler. Do not edit.
 package org.apache.hadoop.record.test;
 
"
hadoop,470cbce4ea17f818193ae300aacfdc71bfae4963,"HADOOP-554.  Fix DFSShell to return -1 for errors.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@467494 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-24 21:34:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSShell.java b/src/java/org/apache/hadoop/dfs/DFSShell.java
index 0a158fa..cf4feb5 100644
--- a/src/java/org/apache/hadoop/dfs/DFSShell.java
+++ b/src/java/org/apache/hadoop/dfs/DFSShell.java
@@ -264,7 +264,7 @@
     private void ls(Path src, boolean recursive, boolean printHeader ) throws IOException {
         Path items[] = fs.listPaths(src);
         if (items == null) {
-            System.out.println(""Could not get listing for "" + src);
+            throw new IOException(""Could not get listing for "" + src);
         } else {
             if(!recursive && printHeader ) {
             	System.out.println(""Found "" + items.length + "" items"");
@@ -292,7 +292,7 @@
     public void du(String src) throws IOException {
         Path items[] = fs.listPaths( fs.globPaths( new Path(src) ) );
         if (items == null) {
-            System.out.println(""Could not get listing for "" + src);
+            throw new IOException(""Could not get listing for "" + src);
         } else {
             System.out.println(""Found "" + items.length + "" items"");
             for (int i = 0; i < items.length; i++) {
@@ -591,6 +591,10 @@
         // initialize DFSShell
         try {
             init();
+        } catch (RPC.VersionMismatch v) { 
+            System.err.println(""Version Mismatch between client and server"" +
+                               ""... command aborted."");
+            return exitCode;
         } catch (IOException e) {
             System.err.println(""Bad connection to DFS... command aborted."");
             return exitCode;
"
hadoop,858e05f88e0d80671b8c0391ab86885de396f97f,"HADOOP-561.  Fix DFS to write one replica of each block locally, if possible.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@466216 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-20 17:32:44,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index 4cfd8cd..ec5ff19 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -1971,9 +1971,9 @@
             if (clientMachine != null && clientMachine.getLength() > 0) {
                 for (Iterator it = targetList.iterator(); it.hasNext(); ) {
                     DatanodeDescriptor node = (DatanodeDescriptor) it.next();
-                    if (clientMachine.equals(node.getHost())) {
-                        if ((node.getRemaining() > blockSize * MIN_BLOCKS_FOR_WRITE) &&
-                            (node.getXceiverCount() < (2.0 * avgLoad))) {
+                    if (clientMachine.toString().equals(node.getHost())) {
+                        if ((node.getRemaining() >= blockSize * MIN_BLOCKS_FOR_WRITE) &&
+                            (node.getXceiverCount() <= (2.0 * avgLoad))) {
                             return node;
                         }
                     }
@@ -1985,20 +1985,34 @@
             //
             for (Iterator it = targetList.iterator(); it.hasNext(); ) {
                 DatanodeDescriptor node = (DatanodeDescriptor) it.next();
-                if ((node.getRemaining() > blockSize * MIN_BLOCKS_FOR_WRITE) &&
-                    (node.getXceiverCount() < (2.0 * avgLoad))) {
+                if ((node.getRemaining() >= blockSize * MIN_BLOCKS_FOR_WRITE) &&
+                    (node.getXceiverCount() <= (2.0 * avgLoad))) {
                     return node;
                 }
             }
 
             //
+            // If we are still not able to find a good node, then see if
+            // we can pick the clientmachine itself.
+            //
+            if (clientMachine != null && clientMachine.getLength() > 0) {
+                for (Iterator it = targetList.iterator(); it.hasNext(); ) {
+                    DatanodeDescriptor node = (DatanodeDescriptor) it.next();
+                    if (clientMachine.toString().equals(node.getHost()) &&
+                        node.getRemaining() >= blockSize) {
+                        return node;
+                    }
+                }
+            }
+
+            //
             // That should do the trick.  But we might not be able
             // to pick any node if the target was out of bytes.  As
             // a last resort, pick the first valid one we can find.
             //
             for (Iterator it = targetList.iterator(); it.hasNext(); ) {
                 DatanodeDescriptor node = (DatanodeDescriptor) it.next();
-                if (node.getRemaining() > blockSize) {
+                if (node.getRemaining() >= blockSize) {
                     return node;
                 }
             }
"
hadoop,936fc8f39361ddcebe2e0efc3a0b2f2d32b1c84b,"HADOOP-607 & HADOOP-609.  Fix a critical bug where job jars were not found by tasks.  Also add unit tests to check for this and other conditions.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@465465 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-19 03:56:34,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 41bc677..c51ab50 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -817,9 +817,8 @@
         
         private void localizeTask(Task task) throws IOException{
             Path localTaskDir =
-              new Path(this.defaultJobConf.getLocalPath(SUBDIR+ Path.SEPARATOR
-                    + JOBCACHE + Path.SEPARATOR
-                    + task.getJobId()), task.getTaskId());
+              new Path(this.defaultJobConf.getLocalPath(TaskTracker.getJobCacheSubdir()), 
+                (task.getJobId() + Path.SEPARATOR + task.getTaskId()));
            FileSystem localFs = FileSystem.getNamed(""local"", fConf);
            localFs.mkdirs(localTaskDir);
            Path localTaskFile = new Path(localTaskDir, ""job.xml"");
"
hadoop,936fc8f39361ddcebe2e0efc3a0b2f2d32b1c84b,"HADOOP-607 & HADOOP-609.  Fix a critical bug where job jars were not found by tasks.  Also add unit tests to check for this and other conditions.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@465465 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-19 03:56:34,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/mapred/MiniMRCluster.java b/src/test/org/apache/hadoop/mapred/MiniMRCluster.java
index da256cc..0ad4725 100644
--- a/src/test/org/apache/hadoop/mapred/MiniMRCluster.java
+++ b/src/test/org/apache/hadoop/mapred/MiniMRCluster.java
@@ -79,9 +79,16 @@
      */
     class TaskTrackerRunner implements Runnable {
         TaskTracker tt;
-        String localDir;
+        // the localDirs for this taskTracker
+        String[] localDir;
         boolean isInitialized = false;
         boolean isDead = false;
+        int numDir;       
+        TaskTrackerRunner(int numDir) {
+          this.numDir = numDir;
+          // a maximum of 10 local dirs can be specified in MinMRCluster
+          localDir = new String[10];
+        }
         
         /**
          * Create and run the task tracker.
@@ -97,10 +104,19 @@
                 jc.setInt(""mapred.task.tracker.info.port"", taskTrackerPort++);
                 jc.setInt(""mapred.task.tracker.report.port"", taskTrackerPort++);
                 File localDir = new File(jc.get(""mapred.local.dir""));
-                File ttDir = new File(localDir, Integer.toString(taskTrackerPort));
+                String mapredDir = """";
+                File ttDir = new File(localDir, Integer.toString(taskTrackerPort) + ""_"" + 0);
                 ttDir.mkdirs();
-                this.localDir = ttDir.getAbsolutePath();
-                jc.set(""mapred.local.dir"", ttDir.getAbsolutePath());
+                this.localDir[0] = ttDir.getAbsolutePath();
+                mapredDir = ttDir.getAbsolutePath();
+                for (int i = 1; i < numDir; i++){
+                  ttDir = new File(localDir, Integer.toString(taskTrackerPort) + ""_"" + i);
+                  ttDir.mkdirs();
+                  this.localDir[i] = ttDir.getAbsolutePath();
+                  mapredDir = mapredDir + "","" + ttDir.getAbsolutePath();
+                }
+                jc.set(""mapred.local.dir"", mapredDir);
+                System.out.println(""mapred.local.dir is "" +  mapredDir);
                 tt = new TaskTracker(jc);
                 isInitialized = true;
                 tt.run();
@@ -114,12 +130,17 @@
         
         /**
          * Get the local dir for this TaskTracker.
+         * This is there so that we do not break
+         * previous tests. 
          * @return the absolute pathname
          */
         public String getLocalDir() {
-          return localDir;
+          return localDir[0];
         }
-        
+       
+        public String[] getLocalDirs(){
+         return localDir;
+        } 
         /**
          * Shut down the server and wait for it to finish.
          */
@@ -176,10 +197,18 @@
      * Create the config and start up the servers.
      */
     public MiniMRCluster(int jobTrackerPort,
+                         int taskTrackerPort,
+                         int numTaskTrackers,
+                         String namenode,
+                         boolean taskTrackerFirst) throws IOException {
+        this(jobTrackerPort, taskTrackerPort, numTaskTrackers, namenode, taskTrackerFirst, 1);
+    } 
+  
+    public MiniMRCluster(int jobTrackerPort,
             int taskTrackerPort,
             int numTaskTrackers,
             String namenode,
-            boolean taskTrackerFirst) throws IOException {
+            boolean taskTrackerFirst, int numDir) throws IOException {
         this.jobTrackerPort = jobTrackerPort;
         this.taskTrackerPort = taskTrackerPort;
         this.numTaskTrackers = numTaskTrackers;
@@ -204,7 +233,7 @@
           jobTrackerThread.start();
         }
         for (int idx = 0; idx < numTaskTrackers; idx++) {
-            TaskTrackerRunner taskTracker = new TaskTrackerRunner();
+            TaskTrackerRunner taskTracker = new TaskTrackerRunner(numDir);
             Thread taskTrackerThread = new Thread(taskTracker);
             taskTrackerThread.start();
             taskTrackerList.add(taskTracker);
"
hadoop,613b2a665568a89e41577eda64f6a5e4dc6ce720,"HADOOP-462.  Improve command line parsing in DFSShell, so that incorrect numbers of arguments result in informative errors rather than ArrayOutOfBoundsException.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@465354 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-18 19:57:23,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/conf/Configuration.java b/src/java/org/apache/hadoop/conf/Configuration.java
index dbe08f5..767c959 100644
--- a/src/java/org/apache/hadoop/conf/Configuration.java
+++ b/src/java/org/apache/hadoop/conf/Configuration.java
@@ -71,6 +71,7 @@
   private static final Log LOG =
     LogFactory.getLog(""org.apache.hadoop.conf.Configuration"");
 
+  private boolean   quietmode = false;
   private ArrayList defaultResources = new ArrayList();
   private ArrayList finalResources = new ArrayList();
 
@@ -454,7 +455,7 @@
   private synchronized Properties getProps() {
     if (properties == null) {
       Properties newProps = new Properties();
-      loadResources(newProps, defaultResources, false, false);
+      loadResources(newProps, defaultResources, false, quietmode);
       loadResources(newProps, finalResources, true, true);
       properties = newProps;
       if(overlay!=null)
@@ -487,20 +488,26 @@
       if (name instanceof URL) {                  // an URL resource
         URL url = (URL)name;
         if (url != null) {
-          LOG.info(""parsing "" + url);
+          if (!quiet) {
+            LOG.info(""parsing "" + url);
+          }
           doc = builder.parse(url.toString());
         }
       } else if (name instanceof String) {        // a CLASSPATH resource
         URL url = getResource((String)name);
         if (url != null) {
-          LOG.info(""parsing "" + url);
+          if (!quiet) {
+            LOG.info(""parsing "" + url);
+          }
           doc = builder.parse(url.toString());
         }
       } else if (name instanceof Path) {          // a file resource
         Path file = (Path)name;
         FileSystem fs = FileSystem.getNamed(""local"", this);
         if (fs.exists(file)) {
-          LOG.info(""parsing "" + file);
+          if (!quiet) {
+            LOG.info(""parsing "" + file);
+          }
           InputStream in = new BufferedInputStream(fs.openRaw(file));
           try {
             doc = builder.parse(in);
@@ -629,6 +636,13 @@
     }
   }
 
+  /** Make this class quiet. Error and informational
+   *  messages might not be logged.
+   */
+  public void setQuietMode(boolean value) {
+    quietmode = value;
+  }
+
   /** For debugging.  List non-default properties to the terminal and exit. */
   public static void main(String[] args) throws Exception {
     new Configuration().write(System.out);
"
hadoop,613b2a665568a89e41577eda64f6a5e4dc6ce720,"HADOOP-462.  Improve command line parsing in DFSShell, so that incorrect numbers of arguments result in informative errors rather than ArrayOutOfBoundsException.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@465354 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-18 19:57:23,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSShell.java b/src/java/org/apache/hadoop/dfs/DFSShell.java
index 36e52d8..0a158fa 100644
--- a/src/java/org/apache/hadoop/dfs/DFSShell.java
+++ b/src/java/org/apache/hadoop/dfs/DFSShell.java
@@ -16,9 +16,11 @@
 package org.apache.hadoop.dfs;
 
 import java.io.*;
+import java.text.*;
 
 import org.apache.hadoop.conf.*;
 import org.apache.hadoop.fs.*;
+import org.apache.hadoop.ipc.*;
 import org.apache.hadoop.util.ToolBase;
 
 /**************************************************
@@ -36,6 +38,7 @@
     }
 
     public void init() throws IOException {
+        conf.setQuietMode(true);
         this.fs = FileSystem.get(conf);
     }
     /**
@@ -455,7 +458,7 @@
         return;
       }
       if( idx != argv.length-1 ) {
-        System.out.println( safeModeUsage );
+        printUsage(""-safemode"");
         return;
       }
       FSConstants.SafeModeAction action;
@@ -466,7 +469,7 @@
       else if( ""get"".equalsIgnoreCase(argv[idx]) )
         action = FSConstants.SafeModeAction.SAFEMODE_GET;
       else {
-        System.out.println( safeModeUsage );
+        printUsage(""-safemode"");
         return;
       }
       DistributedFileSystem dfs = (DistributedFileSystem)fs;
@@ -475,42 +478,125 @@
     }
 
     /**
+     * Displays format of commands.
+     * 
+     */
+    public void printUsage(String cmd) {
+          if (""-fs"".equals(cmd)) {
+            System.err.println(""Usage: java DFSShell"" + 
+                "" [-fs <local | namenode:port>]"");
+          } else if (""-conf"".equals(cmd)) {
+            System.err.println(""Usage: java DFSShell"" + 
+                "" [-conf <configuration file>]"");
+          } else if (""-D"".equals(cmd)) {
+            System.err.println(""Usage: java DFSShell"" + 
+                "" [-D <[property=value>]"");
+          } else if (""-ls"".equals(cmd) || ""-lsr"".equals(cmd) ||
+                   ""-du"".equals(cmd) || ""-rm"".equals(cmd) ||
+                   ""-rmr"".equals(cmd) || ""-mkdir"".equals(cmd)) {
+            System.err.println(""Usage: java DFSShell"" + 
+                "" ["" + cmd + "" <path>]"");
+          } else if (""-mv"".equals(cmd) || ""-cp"".equals(cmd)) {
+            System.err.println(""Usage: java DFSShell"" + 
+                "" ["" + cmd + "" <src> <dst>]"");
+          } else if (""-put"".equals(cmd) || ""-copyFromLocal"".equals(cmd) ||
+                   ""-moveFromLocal"".equals(cmd)) {
+            System.err.println(""Usage: java DFSShell"" + 
+                "" ["" + cmd + "" <localsrc> <dst>]"");
+          } else if (""-get"".equals(cmd) || ""-copyToLocal"".equals(cmd) ||
+                   ""-moveToLocal"".equals(cmd)) {
+            System.err.println(""Usage: java DFSShell"" + 
+                "" ["" + cmd + "" <src> <localdst>]"");
+          } else if (""-cat"".equals(cmd)) {
+            System.out.println(""Usage: java DFSShell"" + 
+                "" ["" + cmd + "" <src>]"");
+          } else if (""-get"".equals(cmd)) {
+            System.err.println(""Usage: java DFSShell"" + 
+                "" ["" + cmd + "" <src> <localdst> [addnl]]"");
+          } else if (""-report"".equals(cmd)) {
+            System.err.println(""Usage: java DFSShell"" + 
+                "" [report]"");
+          } else if (""-setrep"".equals(cmd)) {
+            System.err.println(""Usage: java DFSShell"" + 
+                "" [-setrep [-R] <rep> <path/file>]"");
+          } else if (""-safemode"".equals(cmd)) {
+            System.err.println(""Usage: java DFSShell"" + 
+                "" [-safemode enter | leave | get]"");
+          } else {
+            System.err.println(""Usage: java DFSShell"");
+            System.err.println(""           [-fs <local | namenode:port>]"");
+            System.err.println(""           [-conf <configuration file>]"");
+            System.err.println(""           [-D <[property=value>]"");
+            System.err.println(""           [-ls <path>]"" );
+            System.err.println(""           [-lsr <path>]"");
+            System.err.println(""           [-du <path>]"");
+            System.err.println(""           [-mv <src> <dst>]"");
+            System.err.println(""           [-cp <src> <dst>]"");
+            System.err.println(""           [-rm <path>]"");
+            System.err.println(""           [-rmr <path>]"");
+            System.err.println(""           [-put <localsrc> <dst>]"");
+            System.err.println(""           [-copyFromLocal <localsrc> <dst>]"");
+            System.err.println(""           [-moveFromLocal <localsrc> <dst>]"");
+            System.err.println(""           [-get <src> <localdst>]"");
+            System.err.println(""           [-getmerge <src> <localdst> [addnl]]"");
+            System.err.println(""           [-cat <src>]"");
+            System.err.println(""           [-copyToLocal <src> <localdst>]"");
+            System.err.println(""           [-moveToLocal <src> <localdst>]"");
+            System.err.println(""           [-mkdir <path>]"");
+            System.err.println(""           [-report]"");
+            System.err.println(""           [-setrep [-R] <rep> <path/file>]"");
+            System.err.println(""           [-safemode enter | leave | get]"");
+          }
+    }
+
+    /**
      * run
      */
     public int run( String argv[] ) throws Exception {
+
         if (argv.length < 1) {
-            System.out.println(""Usage: java DFSShell"" + 
-                "" [-fs <local | namenode:port>]"" +
-                "" [-conf <configuration file>]"" +
-                "" [-D <[property=value>]""+
-                "" [-ls <path>]""+
-                "" [-lsr <path>]""+
-                "" [-du <path>]""+
-                "" [-mv <src> <dst>]""+
-                "" [-cp <src> <dst>]""+
-                "" [-rm <path>]"" +
-                "" [-rmr <path>]"" +
-                "" [-put <localsrc> <dst>]""+
-                "" [-copyFromLocal <localsrc> <dst>]""+
-                "" [-moveFromLocal <localsrc> <dst>]"" + 
-                "" [-get <src> <localdst>]""+
-                "" [-getmerge <src> <localdst> [addnl]]""+
-                "" [-cat <src>]""+
-                "" [-copyToLocal <src> <localdst>]"" +
-                "" [-moveToLocal <src> <localdst>]""+
-                "" [-mkdir <path>]""+
-                "" [-report]""+
-                "" [-setrep [-R] <rep> <path/file>]"" +
-                "" [-safemode enter | leave | get]"");
+            printUsage(""""); 
             return -1;
         }
 
-        // initialize DFSShell
-        init();
-        
         int exitCode = -1;
         int i = 0;
         String cmd = argv[i++];
+
+        //
+        // verify that we have enough command line parameters
+        //
+        if (""-put"".equals(cmd) || ""-get"".equals(cmd) || 
+            ""-copyFromLocal"".equals(cmd) || ""-moveFromLocal"".equals(cmd) || 
+            ""-copyToLocal"".equals(cmd) || ""-moveToLocal"".equals(cmd) || 
+            ""-mv"".equals(cmd) || ""-cp"".equals(cmd)) {
+                if (argv.length != 3) {
+                  printUsage(cmd);
+                  return exitCode;
+                }
+        } else if (""-rm"".equals(cmd) || ""-rmr"".equals(cmd) ||
+                 ""-cat"".equals(cmd) || ""-mkdir"".equals(cmd) ||
+                 ""-safemode"".equals(cmd)) {
+                if (argv.length != 2) {
+                  printUsage(cmd);
+                  return exitCode;
+                }
+        } else if ( ""-report"".equals(cmd)) {
+                if (argv.length != 1) {
+                  printUsage(cmd);
+                  return exitCode;
+                }
+        }
+
+        // initialize DFSShell
+        try {
+            init();
+        } catch (IOException e) {
+            System.err.println(""Bad connection to DFS... command aborted."");
+            return exitCode;
+        }
+
+        exitCode = 0;
         try {
             if (""-put"".equals(cmd) || ""-copyFromLocal"".equals(cmd)) {
                 copyFromLocal(new Path(argv[i++]), argv[i++]);
@@ -523,7 +609,6 @@
                     copyMergeToLocal(argv[i++], new Path(argv[i++]), Boolean.parseBoolean(argv[i++]));
                 else
                     copyMergeToLocal(argv[i++], new Path(argv[i++]));
-                
             } else if (""-cat"".equals(cmd)) {
                 cat(argv[i++]);
             } else if (""-moveToLocal"".equals(cmd)) {
@@ -553,10 +638,32 @@
                 report();
             } else if (""-safemode"".equals(cmd)) {
                 setSafeMode(argv,i);
+            } else {
+                exitCode = -1;
+                System.err.println(cmd.substring(1) + "": Unknown command"");
+                printUsage("""");
             }
-            exitCode = 0;;
+        } catch (RemoteException e) {
+          //
+          // This is a error returned by hadoop server. Print
+          // out the first line of the error mesage, ignore the stack trace.
+          exitCode = -1;
+          try {
+            String[] content;
+            content = e.getLocalizedMessage().split(""\n"");
+            System.err.println(cmd.substring(1) + "": "" + 
+                               content[0]);
+          } catch (Exception ex) {
+            System.err.println(cmd.substring(1) + "": "" + 
+                               ex.getLocalizedMessage());  
+          }
         } catch (IOException e ) {
-          System.err.println( cmd.substring(1) + "": "" + e.getLocalizedMessage() );  
+          //
+          // IO exception encountered locally.
+          // 
+          exitCode = -1;
+          System.err.println(cmd.substring(1) + "": "" + 
+                             e.getLocalizedMessage());  
         } finally {
             fs.close();
         }
"
hadoop,613b2a665568a89e41577eda64f6a5e4dc6ce720,"HADOOP-462.  Improve command line parsing in DFSShell, so that incorrect numbers of arguments result in informative errors rather than ArrayOutOfBoundsException.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@465354 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-18 19:57:23,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSDirectory.java b/src/java/org/apache/hadoop/dfs/FSDirectory.java
index 71d622d..f10c7bf 100644
--- a/src/java/org/apache/hadoop/dfs/FSDirectory.java
+++ b/src/java/org/apache/hadoop/dfs/FSDirectory.java
@@ -190,8 +190,9 @@
           }
            // check whether the parent already has a node with that name
           String name = newNode.name = target.getName();
-          if( parentNode.getChild( name ) != null )
+          if( parentNode.getChild( name ) != null ) {
             return null;
+          }
           // insert into the parent children list
           parentNode.children.put(name, newNode);
           newNode.parent = parentNode;
@@ -700,7 +701,13 @@
                    NameNode.stateChangeLog.debug(""DIR* FSDirectory.mkdirs: ""
                         +""created directory ""+cur );
                    fsImage.getEditLog().logMkDir( inserted );
-               } // otherwise cur exists, continue
+               } else { // otherwise cur exists, verify that it is a directory
+                 if (!isDir(new UTF8(cur))) {
+                   NameNode.stateChangeLog.debug(""DIR* FSDirectory.mkdirs: ""
+                        +""path "" + cur + "" is not a directory "");
+                   return false;
+                 } 
+               }
             } catch (FileNotFoundException e ) {
                 NameNode.stateChangeLog.debug(""DIR* FSDirectory.mkdirs: ""
                         +""failed to create directory ""+src);
"
hadoop,613b2a665568a89e41577eda64f6a5e4dc6ce720,"HADOOP-462.  Improve command line parsing in DFSShell, so that incorrect numbers of arguments result in informative errors rather than ArrayOutOfBoundsException.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@465354 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-18 19:57:23,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index 00934aa..4cfd8cd 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -833,13 +833,18 @@
      * Create all the necessary directories
      */
     public boolean mkdirs( String src ) throws IOException {
+        boolean    success;
         NameNode.stateChangeLog.debug(""DIR* NameSystem.mkdirs: "" + src );
         if( isInSafeMode() )
           throw new SafeModeException( ""Cannot create directory "" + src, safeMode );
         if (!isValidName(src)) {
           throw new IOException(""Invalid directory name: "" + src);
         }
-        return dir.mkdirs(src);
+        success = dir.mkdirs(src);
+        if (!success) {
+          throw new IOException(""Invalid directory name: "" + src);
+        }
+        return success;
     }
 
     /**
"
hadoop,613b2a665568a89e41577eda64f6a5e4dc6ce720,"HADOOP-462.  Improve command line parsing in DFSShell, so that incorrect numbers of arguments result in informative errors rather than ArrayOutOfBoundsException.  Contributed by Dhruba.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@465354 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-18 19:57:23,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FileUtil.java b/src/java/org/apache/hadoop/fs/FileUtil.java
index a172894..a39bb96 100644
--- a/src/java/org/apache/hadoop/fs/FileUtil.java
+++ b/src/java/org/apache/hadoop/fs/FileUtil.java
@@ -78,6 +78,8 @@
       } finally {
         in.close();
       } 
+    } else {
+      throw new IOException(src.toString() + "": No such file or directory"");
     }
     if (deleteSource) {
       return srcFS.delete(src);
"
hadoop,4d2820e96d62ad82b77d083916946a6f4dfc1585,"HADOOP-588.  Fix logging and accounting of failed tasks.  Contributed by Sanjay.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@464718 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-16 23:15:00,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobInProgress.java b/src/java/org/apache/hadoop/mapred/JobInProgress.java
index 101ca9c..9063ac6 100644
--- a/src/java/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/JobInProgress.java
@@ -563,9 +563,8 @@
             for (int i = 0; i < reduces.length; i++) {
                 reduces[i].kill();
             }
-            JobHistory.JobInfo.logFinished(this.status.getJobId(), finishTime, 
-                this.finishedMapTasks, this.finishedReduceTasks, failedMapTasks, 
-                failedReduceTasks);
+            JobHistory.JobInfo.logFailed(this.status.getJobId(), finishTime, 
+                this.finishedMapTasks, this.finishedReduceTasks);
             garbageCollect();
         }
     }
@@ -638,15 +637,15 @@
         //
         if (tip.isFailed()) {
             LOG.info(""Aborting job "" + profile.getJobId());
+            JobHistory.Task.logFailed(profile.getJobId(), tip.getTIPId(), 
+                tip.isMapTask() ? Values.MAP.name():Values.REDUCE.name(),  
+                System.currentTimeMillis(), status.getDiagnosticInfo());
             JobHistory.JobInfo.logFailed(profile.getJobId(), 
                 System.currentTimeMillis(), this.finishedMapTasks, this.finishedReduceTasks);
             kill();
         }
 
         jobtracker.removeTaskEntry(taskid);
-        JobHistory.Task.logFailed(profile.getJobId(), tip.getTIPId(), 
-            tip.isMapTask() ? Values.MAP.name():Values.REDUCE.name(),  
-            System.currentTimeMillis(), status.getDiagnosticInfo());
  }
 
     /**
"
hadoop,a54a0b963a07f77d0ce1a5ded85d081339b0480e,"HADOOP-514.  Make DFS heartbeat interval configurable.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@464716 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-16 23:11:22,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index 74dc2e0..615f656 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -95,6 +95,7 @@
     int xmitsInProgress = 0;
     Daemon dataXceiveServer = null;
     long blockReportInterval;
+    long heartBeatInterval;
     private DataStorage storage = null;
     private StatusHttpServer infoServer;
     private static InetSocketAddress nameNodeAddr;
@@ -231,6 +232,7 @@
         conf.getLong(""dfs.blockreport.intervalMsec"", BLOCKREPORT_INTERVAL);
       this.blockReportInterval =
         blockReportIntervalBasis - new Random().nextInt((int)(blockReportIntervalBasis/10));
+      this.heartBeatInterval = conf.getLong(""dfs.heartbeat.interval"", HEARTBEAT_INTERVAL) * 1000L;
       this.nameNodeAddr = nameNodeAddr;
     }
 
@@ -330,7 +332,7 @@
           //
           // Every so often, send heartbeat or block-report
           //
-          if (now - lastHeartbeat > HEARTBEAT_INTERVAL) {
+          if (now - lastHeartbeat > heartBeatInterval) {
             //
             // All heartbeat messages include following info:
             // -- Datanode name
@@ -426,7 +428,7 @@
           // There is no work to do;  sleep until hearbeat timer elapses, 
           // or work arrives, and then iterate again.
           //
-          long waitTime = HEARTBEAT_INTERVAL - (System.currentTimeMillis() - lastHeartbeat);
+          long waitTime = heartBeatInterval - (System.currentTimeMillis() - lastHeartbeat);
           synchronized( receivedBlockList ) {
             if (waitTime > 0 && receivedBlockList.size() == 0) {
               try {
"
hadoop,a54a0b963a07f77d0ce1a5ded85d081339b0480e,"HADOOP-514.  Make DFS heartbeat interval configurable.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@464716 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-16 23:11:22,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSConstants.java b/src/java/org/apache/hadoop/dfs/FSConstants.java
index f85f8af..0319685 100644
--- a/src/java/org/apache/hadoop/dfs/FSConstants.java
+++ b/src/java/org/apache/hadoop/dfs/FSConstants.java
@@ -100,7 +100,7 @@
     //
     // Timeouts, constants
     //
-    public static long HEARTBEAT_INTERVAL = 3 * 1000;
+    public static long HEARTBEAT_INTERVAL = 3;
     public static long EXPIRE_INTERVAL = 10 * 60 * 1000;
     public static long BLOCKREPORT_INTERVAL = 60 * 60 * 1000;
     public static long LEASE_PERIOD = 60 * 1000;
"
hadoop,bffcec9315c5d4cdd58d753f32cad294bd6899c9,"HADOOP-477.  Extend contrib/streaming to scan the PATH environment variable when resolving executable program names.  Contributed by Dhruba Borthakur.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@464685 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-16 21:47:23,Doug Cutting,"diff --git a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java
index 10621e3..87dbb30 100644
--- a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java
+++ b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java
@@ -273,11 +273,23 @@
         sideEffectOut_ = getURIOutputStream(sideEffectURI_, allowSocket);
       }
 
+      // 
       // argvSplit[0]:
       // An absolute path should be a preexisting valid path on all TaskTrackers
-      // A  relative path should match in the unjarred Job data
-      // In this case, force an absolute path to make sure exec finds it.
-      argvSplit[0] = new File(argvSplit[0]).getAbsolutePath();
+      // A relative path is converted into an absolute pathname by looking
+      // up the PATH env variable. If it still fails, look it up in the
+      // tasktracker's local working directory
+      //
+      if (!new File(argvSplit[0]).isAbsolute()) {
+          PathFinder finder = new PathFinder(""PATH"");
+          finder.prependPathComponent(""."");
+          File f = finder.getAbsolutePath(argvSplit[0]);
+          if (f != null) {
+              argvSplit[0] = f.getAbsolutePath();
+          }
+          f = null;
+      }
+      System.out.println(""XXX2 argvSplit[0] = "" + argvSplit[0]);
       logprintln(""PipeMapRed exec "" + Arrays.asList(argvSplit));
       logprintln(""sideEffectURI_="" + sideEffectURI_);
 
"
hadoop,a8135bb91d14e2e74196548bbaa47f2825a7afc8,"HADOOP-597.  Fix TaskTracker to not discard map outputs for errors in transmission to reduce node.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@462918 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-11 19:45:33,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 17fdcb5..41bc677 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -1365,30 +1365,39 @@
         Path filename = conf.getLocalPath(mapId+""/part-""+reduce+"".out"");
         response.setContentLength((int) fileSys.getLength(filename));
         InputStream inStream = null;
+        // true iff IOException was caused by attempt to access input
+        boolean isInputException = true;
         try {
           inStream = fileSys.open(filename);
           try {
             int len = inStream.read(buffer);
             while (len > 0) {
-              outStream.write(buffer, 0, len);
+              try {
+                outStream.write(buffer, 0, len);
+              } catch (IOException ie) {
+                isInputException = false;
+                throw ie;
+              }
               len = inStream.read(buffer);
             }
           } finally {
             inStream.close();
-            outStream.close();
           }
         } catch (IOException ie) {
           TaskTracker tracker = 
             (TaskTracker) context.getAttribute(""task.tracker"");
           Log log = (Log) context.getAttribute(""log"");
-          String errorMsg = ""getMapOutput("" + mapId + "","" + reduceId + 
-          "") failed :\n""+
-          StringUtils.stringifyException(ie);
+          String errorMsg = (""getMapOutput("" + mapId + "","" + reduceId + 
+                             "") failed :\n""+
+                             StringUtils.stringifyException(ie));
           log.warn(errorMsg);
-          tracker.mapOutputLost(mapId, errorMsg);
+          if (isInputException) {
+            tracker.mapOutputLost(mapId, errorMsg);
+          }
           response.sendError(HttpServletResponse.SC_GONE, errorMsg);
           throw ie;
         } 
+        outStream.close();
       }
     }
 }
"
hadoop,a65d912634a21abcb6a0c99085338e79af5c47b7,"HADOOP-598.  Fix tasks to retry when reporting completion.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@462911 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-11 19:32:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/Task.java b/src/java/org/apache/hadoop/mapred/Task.java
index 30f335f..7024b9b 100644
--- a/src/java/org/apache/hadoop/mapred/Task.java
+++ b/src/java/org/apache/hadoop/mapred/Task.java
@@ -176,10 +176,26 @@
     }
   }
 
-  public void done(TaskUmbilicalProtocol umbilical)
-    throws IOException {
-    umbilical.progress(getTaskId(),               // send a final status report
-                       taskProgress.get(), taskProgress.toString(), phase);
-    umbilical.done(getTaskId());
+  public void done(TaskUmbilicalProtocol umbilical) throws IOException {
+    int retries = 10;
+    boolean needProgress = true;
+    while (true) {
+      try {
+        if (needProgress) {
+          // send a final status report
+          umbilical.progress(getTaskId(), taskProgress.get(), 
+                             taskProgress.toString(), phase);
+          needProgress = false;
+        }
+        umbilical.done(getTaskId());
+        return;
+      } catch (IOException ie) {
+        LOG.warn(""Failure signalling completion: "" + 
+                 StringUtils.stringifyException(ie));
+        if (--retries == 0) {
+          throw ie;
+        }
+      }
+    }
   }
 }
"
hadoop,b16e047aa30e2e769d6d9a897ad629d5f6b5272b,"HADOOP-592.  Fix an NPE in IPC Server.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@462885 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-11 17:56:44,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/Server.java b/src/java/org/apache/hadoop/ipc/Server.java
index e9f2bea..686b7f1 100644
--- a/src/java/org/apache/hadoop/ipc/Server.java
+++ b/src/java/org/apache/hadoop/ipc/Server.java
@@ -30,6 +30,7 @@
 import java.nio.channels.ServerSocketChannel;
 import java.nio.channels.SocketChannel;
 
+import java.net.InetAddress;
 import java.net.InetSocketAddress;
 import java.net.Socket;
 
@@ -351,6 +352,10 @@
     private long lastContact;
     private int dataLength;
     private Socket socket;
+    // Cache the remote host & port info so that even if the socket is 
+    // disconnected, we can say where it used to connect to.
+    private String hostAddress;
+    private int remotePort;
 
     public Connection(SelectionKey key, SocketChannel channel, 
     long lastContact) {
@@ -363,14 +368,21 @@
       this.out = new DataOutputStream
         (new BufferedOutputStream(
          this.channelOut = new SocketChannelOutputStream(channel, 4096)));
+      InetAddress addr = socket.getInetAddress();
+      if (addr == null) {
+        this.hostAddress = ""*Unknown*"";
+      } else {
+        this.hostAddress = addr.getHostAddress();
+      }
+      this.remotePort = socket.getPort();
     }   
 
     public String toString() {
-      return getHostAddress() + "":"" + socket.getPort(); 
+      return getHostAddress() + "":"" + remotePort; 
     }
     
     public String getHostAddress() {
-      return socket.getInetAddress().getHostAddress();
+      return hostAddress;
     }
 
     public void setLastContact(long lastContact) {
@@ -431,7 +443,8 @@
       Call call = new Call(id, param, this);
       synchronized (callQueue) {
         if (callQueue.size() >= maxQueueSize) {
-          callQueue.removeFirst();
+          Call oldCall = (Call) callQueue.removeFirst();
+          LOG.warn(""Call queue overflow discarding oldest call "" + oldCall);
         }
         callQueue.addLast(call);              // queue the call
         callQueue.notify();                   // wake up a waiting handler
@@ -484,7 +497,7 @@
           // throw the message away if it is too old
           if (System.currentTimeMillis() - call.receivedTime > 
               maxCallStartAge) {
-            LOG.info(""Call "" + call.toString() + 
+            LOG.warn(""Call "" + call.toString() + 
                      "" discarded for being too old ("" +
                      (System.currentTimeMillis() - call.receivedTime) + "")"");
             continue;
@@ -492,7 +505,7 @@
           
           if (LOG.isDebugEnabled())
             LOG.debug(getName() + "": has #"" + call.id + "" from "" +
-                     call.connection.socket.getInetAddress().getHostAddress());
+                     call.connection);
           
           String errorClass = null;
           String error = null;
"
hadoop,b4de7fb33363b34aea652e77de78c9c90f5c329b,"HADOOP-593.  Fix an NPE in JobTracker.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@462877 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-11 17:48:28,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobInProgress.java b/src/java/org/apache/hadoop/mapred/JobInProgress.java
index 112a908..101ca9c 100644
--- a/src/java/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/JobInProgress.java
@@ -20,7 +20,6 @@
 import org.apache.hadoop.fs.*;
 import org.apache.hadoop.conf.*;
 import org.apache.hadoop.mapred.JobTracker.JobTrackerMetrics;
-import org.apache.hadoop.mapred.JobHistory.Keys ; 
 import org.apache.hadoop.mapred.JobHistory.Values ; 
 import java.io.*;
 import java.net.*;
@@ -33,7 +32,7 @@
 // doing bookkeeping of its Tasks.
 ///////////////////////////////////////////////////////
 class JobInProgress {
-    public static final Log LOG = LogFactory.getLog(""org.apache.hadoop.mapred.JobInProgress"");
+    private static final Log LOG = LogFactory.getLog(""org.apache.hadoop.mapred.JobInProgress"");
 
     JobProfile profile;
     JobStatus status;
@@ -473,25 +472,24 @@
                    "" successfully."");          
 
           String taskTrackerName = status.getTaskTracker();
-          TaskTrackerStatus taskTracker = this.jobtracker.getTaskTracker(taskTrackerName);
           
           if(status.getIsMap()){
             JobHistory.MapAttempt.logStarted(profile.getJobId(), 
                 tip.getTIPId(), status.getTaskId(), status.getStartTime(), 
-                taskTracker.getHost()); 
+                taskTrackerName); 
             JobHistory.MapAttempt.logFinished(profile.getJobId(), 
                 tip.getTIPId(), status.getTaskId(), status.getFinishTime(), 
-                taskTracker.getHost()); 
+                taskTrackerName); 
             JobHistory.Task.logFinished(profile.getJobId(), tip.getTIPId(), 
                 Values.MAP.name(), status.getFinishTime()); 
           }else{
               JobHistory.ReduceAttempt.logStarted(profile.getJobId(), 
                   tip.getTIPId(), status.getTaskId(), status.getStartTime(), 
-                  taskTracker.getHost()); 
+                  taskTrackerName); 
               JobHistory.ReduceAttempt.logFinished(profile.getJobId(), 
                   tip.getTIPId(), status.getTaskId(), status.getShuffleFinishTime(),
                   status.getSortFinishTime(), status.getFinishTime(), 
-                  taskTracker.getHost()); 
+                  taskTrackerName); 
               JobHistory.Task.logFinished(profile.getJobId(), tip.getTIPId(), 
                   Values.REDUCE.name(), status.getFinishTime()); 
           }
@@ -609,21 +607,20 @@
         
         // update job history
         String taskTrackerName = status.getTaskTracker();
-        TaskTrackerStatus taskTracker = this.jobtracker.getTaskTracker(taskTrackerName);
-        if(status.getIsMap()){
+        if (status.getIsMap()) {
           JobHistory.MapAttempt.logStarted(profile.getJobId(), 
               tip.getTIPId(), status.getTaskId(), status.getStartTime(), 
-              taskTracker.getHost()); 
+              taskTrackerName); 
           JobHistory.MapAttempt.logFailed(profile.getJobId(), 
               tip.getTIPId(), status.getTaskId(), System.currentTimeMillis(),
-              taskTracker.getHost(), status.getDiagnosticInfo()); 
-        }else{
+              taskTrackerName, status.getDiagnosticInfo()); 
+        } else {
           JobHistory.ReduceAttempt.logStarted(profile.getJobId(), 
               tip.getTIPId(), status.getTaskId(), status.getStartTime(), 
-              taskTracker.getHost()); 
+              taskTrackerName); 
           JobHistory.ReduceAttempt.logFailed(profile.getJobId(), 
               tip.getTIPId(), status.getTaskId(), System.currentTimeMillis(),
-              taskTracker.getHost(), status.getDiagnosticInfo()); 
+              taskTrackerName, status.getDiagnosticInfo()); 
         }
         
         // After this, try to assign tasks with the one after this, so that
"
hadoop,b4de7fb33363b34aea652e77de78c9c90f5c329b,"HADOOP-593.  Fix an NPE in JobTracker.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@462877 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-11 17:48:28,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index 77f633a..bb55e83 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -21,6 +21,7 @@
 import org.apache.hadoop.fs.*;
 import org.apache.hadoop.ipc.*;
 import org.apache.hadoop.conf.*;
+import org.apache.hadoop.util.StringUtils;
 
 import java.io.*;
 import java.net.*;
@@ -114,8 +115,8 @@
       private Map launchingTasks = new LinkedHashMap();
       
       public void run() {
-        try {
-          while (shouldRun) {
+        while (shouldRun) {
+          try {
             // Every 3 minutes check for any tasks that are overdue
             Thread.sleep(TASKTRACKER_EXPIRY_INTERVAL/3);
             long now = System.currentTimeMillis();
@@ -151,9 +152,13 @@
                 }
               }
             }
+          } catch (InterruptedException ie) {
+            // all done
+            return;
+          } catch (Exception e) {
+            LOG.error(""Expire Launching Task Thread got exception: "" +
+                      StringUtils.stringifyException(e));
           }
-        } catch (InterruptedException ie) {
-          // all done
         }
       }
       
@@ -188,15 +193,13 @@
          */
         public void run() {
             while (shouldRun) {
+              try {
                 //
                 // Thread runs periodically to check whether trackers should be expired.
                 // The sleep interval must be no more than half the maximum expiry time
                 // for a task tracker.
                 //
-                try {
-                    Thread.sleep(TASKTRACKER_EXPIRY_INTERVAL / 3);
-                } catch (InterruptedException ie) {
-                }
+                Thread.sleep(TASKTRACKER_EXPIRY_INTERVAL / 3);
 
                 //
                 // Loop through all expired items in the queue
@@ -232,6 +235,10 @@
                         }
                     }
                 }
+              } catch (Exception t) {
+                LOG.error(""Tracker Expiry Thread got exception: "" +
+                          StringUtils.stringifyException(t));
+              }
             }
         }
         
"
hadoop,a7ce55edc27ed7578ac5c81fc64617f0a0bffb2f,"HADOOP-581.  Fix datanode to not reset itself on commmunications errors with namenode.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@453776 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-06 21:21:00,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index 9314f89..74dc2e0 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -323,8 +323,8 @@
       // Now loop for a long time....
       //
 
-      try {
-        while (shouldRun) {
+      while (shouldRun) {
+        try {
           long now = System.currentTimeMillis();
 
           //
@@ -411,11 +411,15 @@
               // Send newly-received blockids to namenode
               //
               blockArray = (Block[]) receivedBlockList.toArray(new Block[receivedBlockList.size()]);
-              receivedBlockList.removeAllElements();
             }
           }
           if( blockArray != null ) {
             namenode.blockReceived( dnRegistration, blockArray );
+            synchronized (receivedBlockList) {
+              for(Block b: blockArray) {
+                receivedBlockList.remove(b);
+              }
+            }
           }
             
           //
@@ -431,19 +435,22 @@
               }
             }
           } // synchronized
-        } // while (shouldRun)
-      } catch(DiskErrorException e) {
-        handleDiskError(e.getLocalizedMessage());
-      } catch( RemoteException re ) {
-        String reClass = re.getClassName();
-        if( UnregisteredDatanodeException.class.getName().equals( reClass )) {
-          LOG.warn( ""DataNode is shutting down: "" + 
-                    StringUtils.stringifyException(re));
-          shutdown();
+        } catch(DiskErrorException e) {
+          handleDiskError(e.getLocalizedMessage());
           return;
+        } catch( RemoteException re ) {
+          String reClass = re.getClassName();
+          if( UnregisteredDatanodeException.class.getName().equals( reClass )) {
+            LOG.warn( ""DataNode is shutting down: "" + 
+                      StringUtils.stringifyException(re));
+            shutdown();
+            return;
+          }
+          LOG.warn(StringUtils.stringifyException(re));
+        } catch (IOException e) {
+          LOG.warn(StringUtils.stringifyException(e));
         }
-        throw re;
-      }
+      } // while (shouldRun)
     } // offerService
 
     
@@ -968,9 +975,8 @@
             try {
                 offerService();
             } catch (Exception ex) {
-                LOG.info(""Exception: "" + ex);
+              LOG.error(""Exception: "" + StringUtils.stringifyException(ex));
               if (shouldRun) {
-                LOG.info(""Lost connection to namenode.  Retrying..."");
                 try {
                   Thread.sleep(5000);
                 } catch (InterruptedException ie) {
"
hadoop,5d4210425e4baa8522fdd81584827deab1a041ce,"HADOOP-255.  Discard stale queued IPC calls.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@453766 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-06 21:12:08,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/Server.java b/src/java/org/apache/hadoop/ipc/Server.java
index 2ede557..e9f2bea 100644
--- a/src/java/org/apache/hadoop/ipc/Server.java
+++ b/src/java/org/apache/hadoop/ipc/Server.java
@@ -17,7 +17,6 @@
 package org.apache.hadoop.ipc;
 
 import java.io.IOException;
-import java.io.EOFException;
 import java.io.DataInputStream;
 import java.io.DataOutputStream;
 import java.io.BufferedOutputStream;
@@ -30,7 +29,6 @@
 import java.nio.channels.Selector;
 import java.nio.channels.ServerSocketChannel;
 import java.nio.channels.SocketChannel;
-import java.nio.BufferUnderflowException;
 
 import java.net.InetSocketAddress;
 import java.net.Socket;
@@ -57,6 +55,18 @@
  * @see Client
  */
 public abstract class Server {
+  /**
+   * How much time should be allocated for actually running the handler?
+   * Calls that are older than ipc.timeout * MAX_CALL_QUEUE_TIME
+   * are ignored when the handler takes them off the queue.
+   */
+  private static final float MAX_CALL_QUEUE_TIME = 0.6f;
+  
+  /**
+   * How many calls/handler are allowed in the queue.
+   */
+  private static final int MAX_QUEUE_SIZE_PER_HANDLER = 100;
+  
   public static final Log LOG =
     LogFactory.getLog(""org.apache.hadoop.ipc.Server"");
 
@@ -72,7 +82,6 @@
   private String bindAddress; 
   private int port;                               // port we listen on
   private int handlerCount;                       // number of handler threads
-  private int maxQueuedCalls;                     // max number of queued calls
   private Class paramClass;                       // class of call parameters
   private int maxIdleTime;                        // the maximum idle time after 
                                                   // which a client may be disconnected
@@ -87,6 +96,8 @@
   private Configuration conf;
 
   private int timeout;
+  private long maxCallStartAge;
+  private int maxQueueSize;
 
   private boolean running = true;                 // true while server runs
   private LinkedList callQueue = new LinkedList(); // queued calls
@@ -103,11 +114,17 @@
     private int id;                               // the client's call id
     private Writable param;                       // the parameter passed
     private Connection connection;                // connection to client
+    private long receivedTime;                    // the time received
 
     public Call(int id, Writable param, Connection connection) {
       this.id = id;
       this.param = param;
       this.connection = connection;
+      this.receivedTime = System.currentTimeMillis();
+    }
+    
+    public String toString() {
+      return param.toString() + "" from "" + connection.toString();
     }
   }
 
@@ -348,6 +365,10 @@
          this.channelOut = new SocketChannelOutputStream(channel, 4096)));
     }   
 
+    public String toString() {
+      return getHostAddress() + "":"" + socket.getPort(); 
+    }
+    
     public String getHostAddress() {
       return socket.getInetAddress().getHostAddress();
     }
@@ -409,15 +430,13 @@
         
       Call call = new Call(id, param, this);
       synchronized (callQueue) {
+        if (callQueue.size() >= maxQueueSize) {
+          callQueue.removeFirst();
+        }
         callQueue.addLast(call);              // queue the call
         callQueue.notify();                   // wake up a waiting handler
       }
         
-      while (running && callQueue.size() >= maxQueuedCalls) {
-        synchronized (callDequeued) {         // queue is full
-          callDequeued.wait(timeout);         // wait for a dequeue
-        }
-      }
     }
 
     private void close() throws IOException {
@@ -462,6 +481,15 @@
             callDequeued.notify();
           }
 
+          // throw the message away if it is too old
+          if (System.currentTimeMillis() - call.receivedTime > 
+              maxCallStartAge) {
+            LOG.info(""Call "" + call.toString() + 
+                     "" discarded for being too old ("" +
+                     (System.currentTimeMillis() - call.receivedTime) + "")"");
+            continue;
+          }
+          
           if (LOG.isDebugEnabled())
             LOG.debug(getName() + "": has #"" + call.id + "" from "" +
                      call.connection.socket.getInetAddress().getHostAddress());
@@ -526,8 +554,9 @@
     this.port = port;
     this.paramClass = paramClass;
     this.handlerCount = handlerCount;
-    this.maxQueuedCalls = handlerCount;
     this.timeout = conf.getInt(""ipc.client.timeout"",10000);
+    maxCallStartAge = (long) (timeout * MAX_CALL_QUEUE_TIME);
+    maxQueueSize = handlerCount * MAX_QUEUE_SIZE_PER_HANDLER;
     this.maxIdleTime = conf.getInt(""ipc.client.maxidletime"", 120000);
     this.maxConnectionsToNuke = conf.getInt(""ipc.client.kill.max"", 10);
     this.thresholdIdleConnections = conf.getInt(""ipc.client.idlethreshold"", 4000);
"
hadoop,25d18014247bf2f3b3e98f41749959c400d595e8,"HADOOP-506.  Ignore heartbeats from stale tasktrackers.  Contributed by Sanjay.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@453392 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-05 21:31:48,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index f743d98..77f633a 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -751,6 +751,8 @@
                 } else {
                     // If not first contact, there should be some record of the tracker
                     if (!seenBefore) {
+                        LOG.warn(""Status from unknown Tracker : "" + trackerName);
+                        taskTrackers.remove(trackerName); 
                         return InterTrackerProtocol.UNKNOWN_TASKTRACKER;
                     }
                 }
"
hadoop,5d0b936eb07799feacc30cbfd4d7938938944acc,"HADOOP-239.  Add a persistent job history mechanism.  Contributed by Sanjay.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@453041 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-04 21:46:20,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobInProgress.java b/src/java/org/apache/hadoop/mapred/JobInProgress.java
index 4efa56d..112a908 100644
--- a/src/java/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/JobInProgress.java
@@ -20,7 +20,8 @@
 import org.apache.hadoop.fs.*;
 import org.apache.hadoop.conf.*;
 import org.apache.hadoop.mapred.JobTracker.JobTrackerMetrics;
-
+import org.apache.hadoop.mapred.JobHistory.Keys ; 
+import org.apache.hadoop.mapred.JobHistory.Values ; 
 import java.io.*;
 import java.net.*;
 import java.util.*;
@@ -47,6 +48,8 @@
     int runningReduceTasks = 0;
     int finishedMapTasks = 0;
     int finishedReduceTasks = 0;
+    int failedMapTasks = 0 ; 
+    int failedReduceTasks = 0 ; 
     JobTracker jobtracker = null;
     HashMap hostToMaps = new HashMap();
 
@@ -93,6 +96,9 @@
 
         this.numMapTasks = conf.getNumMapTasks();
         this.numReduceTasks = conf.getNumReduceTasks();
+        JobHistory.JobInfo.logSubmitted(jobid, conf.getJobName(), conf.getUser(), 
+            System.currentTimeMillis(), jobFile); 
+        
     }
 
     /**
@@ -183,6 +189,8 @@
 
         this.status = new JobStatus(status.getJobId(), 0.0f, 0.0f, JobStatus.RUNNING);
         tasksInited = true;
+        
+        JobHistory.JobInfo.logStarted(profile.getJobId(), System.currentTimeMillis(), numMapTasks, numReduceTasks);
     }
 
     /////////////////////////////////////////////////////
@@ -331,6 +339,9 @@
       Task result = maps[target].getTaskToRun(tts.getTrackerName());
       if (!wasRunning) {
         runningMapTasks += 1;
+        JobHistory.Task.logStarted(profile.getJobId(), 
+            maps[target].getTIPId(), Values.MAP.name(),
+            System.currentTimeMillis());
       }
       return result;
     }    
@@ -357,6 +368,9 @@
         Task result = reduces[target].getTaskToRun(tts.getTrackerName());
         if (!wasRunning) {
           runningReduceTasks += 1;
+          JobHistory.Task.logStarted(profile.getJobId(), 
+              reduces[target].getTIPId(), Values.REDUCE.name(),
+              System.currentTimeMillis());
         }
         return result;
     }
@@ -457,6 +471,30 @@
         } else {
           LOG.info(""Task '"" + taskid + ""' has completed "" + tip.getTIPId() + 
                    "" successfully."");          
+
+          String taskTrackerName = status.getTaskTracker();
+          TaskTrackerStatus taskTracker = this.jobtracker.getTaskTracker(taskTrackerName);
+          
+          if(status.getIsMap()){
+            JobHistory.MapAttempt.logStarted(profile.getJobId(), 
+                tip.getTIPId(), status.getTaskId(), status.getStartTime(), 
+                taskTracker.getHost()); 
+            JobHistory.MapAttempt.logFinished(profile.getJobId(), 
+                tip.getTIPId(), status.getTaskId(), status.getFinishTime(), 
+                taskTracker.getHost()); 
+            JobHistory.Task.logFinished(profile.getJobId(), tip.getTIPId(), 
+                Values.MAP.name(), status.getFinishTime()); 
+          }else{
+              JobHistory.ReduceAttempt.logStarted(profile.getJobId(), 
+                  tip.getTIPId(), status.getTaskId(), status.getStartTime(), 
+                  taskTracker.getHost()); 
+              JobHistory.ReduceAttempt.logFinished(profile.getJobId(), 
+                  tip.getTIPId(), status.getTaskId(), status.getShuffleFinishTime(),
+                  status.getSortFinishTime(), status.getFinishTime(), 
+                  taskTracker.getHost()); 
+              JobHistory.Task.logFinished(profile.getJobId(), tip.getTIPId(), 
+                  Values.REDUCE.name(), status.getFinishTime()); 
+          }
         }
         
         tip.completed(taskid);
@@ -503,6 +541,8 @@
             garbageCollect();
             LOG.info(""Job "" + this.status.getJobId() + 
                      "" has completed successfully."");
+            JobHistory.JobInfo.logFinished(this.status.getJobId(), finishTime, 
+                this.finishedMapTasks, this.finishedReduceTasks, failedMapTasks, failedReduceTasks);
             metrics.completeJob();
         }
     }
@@ -525,7 +565,9 @@
             for (int i = 0; i < reduces.length; i++) {
                 reduces[i].kill();
             }
-
+            JobHistory.JobInfo.logFinished(this.status.getJobId(), finishTime, 
+                this.finishedMapTasks, this.finishedReduceTasks, failedMapTasks, 
+                failedReduceTasks);
             garbageCollect();
         }
     }
@@ -565,12 +607,33 @@
           }
         }
         
+        // update job history
+        String taskTrackerName = status.getTaskTracker();
+        TaskTrackerStatus taskTracker = this.jobtracker.getTaskTracker(taskTrackerName);
+        if(status.getIsMap()){
+          JobHistory.MapAttempt.logStarted(profile.getJobId(), 
+              tip.getTIPId(), status.getTaskId(), status.getStartTime(), 
+              taskTracker.getHost()); 
+          JobHistory.MapAttempt.logFailed(profile.getJobId(), 
+              tip.getTIPId(), status.getTaskId(), System.currentTimeMillis(),
+              taskTracker.getHost(), status.getDiagnosticInfo()); 
+        }else{
+          JobHistory.ReduceAttempt.logStarted(profile.getJobId(), 
+              tip.getTIPId(), status.getTaskId(), status.getStartTime(), 
+              taskTracker.getHost()); 
+          JobHistory.ReduceAttempt.logFailed(profile.getJobId(), 
+              tip.getTIPId(), status.getTaskId(), System.currentTimeMillis(),
+              taskTracker.getHost(), status.getDiagnosticInfo()); 
+        }
+        
         // After this, try to assign tasks with the one after this, so that
         // the failed task goes to the end of the list.
         if (tip.isMapTask()) {
           firstMapToTry = (tip.getIdWithinJob() + 1) % maps.length;
+          failedMapTasks++; 
         } else {
           firstReduceToTry = (tip.getIdWithinJob() + 1) % reduces.length;
+          failedReduceTasks++; 
         }
             
         //
@@ -578,10 +641,15 @@
         //
         if (tip.isFailed()) {
             LOG.info(""Aborting job "" + profile.getJobId());
+            JobHistory.JobInfo.logFailed(profile.getJobId(), 
+                System.currentTimeMillis(), this.finishedMapTasks, this.finishedReduceTasks);
             kill();
         }
 
         jobtracker.removeTaskEntry(taskid);
+        JobHistory.Task.logFailed(profile.getJobId(), tip.getTIPId(), 
+            tip.isMapTask() ? Values.MAP.name():Values.REDUCE.name(),  
+            System.currentTimeMillis(), status.getDiagnosticInfo());
  }
 
     /**
@@ -604,6 +672,9 @@
                                           reason,
                                           trackerName, phase);
        updateTaskStatus(tip, status, metrics);
+       JobHistory.Task.logFailed(profile.getJobId(), tip.getTIPId(), 
+           tip.isMapTask() ? Values.MAP.name() : Values.REDUCE.name(), 
+           System.currentTimeMillis(), reason); 
     }
        
                            
"
hadoop,e65689b77c59340f82a6d06e9ea680a9cac164a8,"HADOOP-343.  Fix mapred copying so that a failed tasktracker does not slow other copies.  Contributed by Sameer.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@452945 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-04 17:25:10,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java b/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
index a255335..83fc440 100644
--- a/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
+++ b/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
@@ -455,6 +455,20 @@
             LOG.warn(reduceTask.getTaskId() + "" adding host "" +
                      cr.getHost() + "" to penalty box, next contact in "" +
                      ((nextContact-currentTime)/1000) + "" seconds"");
+
+            // other outputs from the failed host may be present in the
+            // knownOutputs cache, purge them. This is important in case
+            // the failure is due to a lost tasktracker (causes many
+            // unnecessary backoffs). If not, we only take a small hit
+            // polling the jobtracker a few more times
+            ListIterator locIt = knownOutputs.listIterator();
+            while (locIt.hasNext()) {
+              MapOutputLocation loc = (MapOutputLocation)locIt.next();
+              if (cr.getHost().equals(loc.getHost())) {
+                locIt.remove();
+                neededOutputs.add(new Integer(loc.getMapId()));
+              }
+            }
           }
           uniqueHosts.remove(cr.getHost());
           numInFlight--;
"
hadoop,3be71187ef4cfbf0e3f72f0d8a1573b0bbebd6d6,"HADOOP-550.  Disable automatic UTF-8 validation in Text.  Contributed by Hairong & Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@452941 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-04 17:16:52,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/Text.java b/src/java/org/apache/hadoop/io/Text.java
index b229731..612a4a9 100644
--- a/src/java/org/apache/hadoop/io/Text.java
+++ b/src/java/org/apache/hadoop/io/Text.java
@@ -64,10 +64,8 @@
   }
 
   /** Construct from a string. 
-   * @exception CharacterCodingExcetpion if the string contains 
-   *            invalid codepoints or unpaired surrogates
    */
-  public Text(String string) throws CharacterCodingException {
+  public Text(String string) {
     set(string);
   }
 
@@ -77,9 +75,8 @@
   }
 
   /** Construct from a byte array.
-   * @exception CharacterCodingExcetpion if the array has invalid UTF8 bytes 
    */
-  public Text(byte[] utf8) throws CharacterCodingException {
+  public Text(byte[] utf8)  {
     set(utf8);
   }
   
@@ -160,29 +157,26 @@
     }
   }  
   /** Set to contain the contents of a string. 
-   * @exception CharacterCodingException if the string contains 
-   *       invalid codepoints or unpaired surrogate
    */
-  public void set(String string) throws CharacterCodingException {
-    ByteBuffer bb = encode(string);
-    bytes = bb.array();
-    length = bb.limit();
+  public void set(String string) {
+    try {
+      ByteBuffer bb = encode(string, true);
+      bytes = bb.array();
+      length = bb.limit();
+    }catch(CharacterCodingException e) {
+      throw new RuntimeException(""Should not have happened "" + e.toString()); 
+    }
   }
 
   /** Set to a utf8 byte array
-   * @exception CharacterCodingException if the array contains invalid UTF8 code  
    */
-  public void set(byte[] utf8) throws CharacterCodingException {
+  public void set(byte[] utf8) {
     set(utf8, 0, utf8.length);
   }
   
   /** copy a text. */
   public void set(Text other) {
-    try {
-      set(other.bytes, 0, other.length);
-    } catch (CharacterCodingException e) {
-      throw new RuntimeException(""bad Text UTF8 encoding"", e);
-    }
+    set(other.bytes, 0, other.length);
   }
 
   /**
@@ -191,9 +185,7 @@
    * @param start the first position of the new string
    * @param len the number of bytes of the new string
    */
-  public void set(byte[] utf8, int start, int len 
-                  ) throws CharacterCodingException{
-    validateUTF8(utf8, start, len);
+  public void set(byte[] utf8, int start, int len) {
     setCapacity(len);
     System.arraycopy(utf8, start, bytes, 0, len);
     this.length = len;
@@ -221,22 +213,16 @@
     try {
       return decode(bytes, 0, length);
     } catch (CharacterCodingException e) { 
-      //bytes is supposed to contain valid utf8, therefore, 
-      // this should never happen
       return null;
     }
   }
   
   /** deserialize 
-   * check if the received bytes are valid utf8 code. 
-   * if not throws MalformedInputException
-   * @see Writable#readFields(DataInput)
    */
   public void readFields(DataInput in) throws IOException {
     length = WritableUtils.readVInt(in);
     setCapacity(length);
     in.readFully(bytes, 0, length);
-    validateUTF8(bytes);
   }
 
   /** Skips over one Text in the input. */
@@ -251,7 +237,7 @@
    * @see Writable#write(DataOutput)
    */
   public void write(DataOutput out) throws IOException {
-    WritableUtils.writeVInt(out, length); // out.writeInt(length);
+    WritableUtils.writeVInt(out, length);
     out.write(bytes, 0, length);
   }
 
@@ -313,15 +299,15 @@
   /**
    * Converts the provided byte array to a String using the
    * UTF-8 encoding. If the input is malformed,
-   * throws a MalformedInputException.
+   * replace by a default value.
    */
   public static String decode(byte[] utf8) throws CharacterCodingException {
-    return decode(ByteBuffer.wrap(utf8), false);
+    return decode(ByteBuffer.wrap(utf8), true);
   }
   
   public static String decode(byte[] utf8, int start, int length) 
       throws CharacterCodingException {
-      return decode(ByteBuffer.wrap(utf8, start, length), false);
+      return decode(ByteBuffer.wrap(utf8, start, length), true);
   }
   
   /**
@@ -358,14 +344,14 @@
   /**
    * Converts the provided String to bytes using the
    * UTF-8 encoding. If the input is malformed,
-   * throws a MalformedInputException.
+   * invalid chars are replaced by a default value.
    * @return ByteBuffer: bytes stores at ByteBuffer.array() 
    *                     and length is ByteBuffer.limit()
    */
 
   public static ByteBuffer encode(String string)
     throws CharacterCodingException {
-    return encode(string, false);
+    return encode(string, true);
   }
 
   /**
@@ -399,7 +385,6 @@
     int length = WritableUtils.readVInt(in);
     byte [] bytes = new byte[length];
     in.readFully(bytes, 0, length);
-    validateUTF8(bytes);
     return decode(bytes);
   }
 
"
hadoop,3be71187ef4cfbf0e3f72f0d8a1573b0bbebd6d6,"HADOOP-550.  Disable automatic UTF-8 validation in Text.  Contributed by Hairong & Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@452941 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-04 17:16:52,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/Utils.java b/src/java/org/apache/hadoop/record/Utils.java
index 5447dec..3336041 100644
--- a/src/java/org/apache/hadoop/record/Utils.java
+++ b/src/java/org/apache/hadoop/record/Utils.java
@@ -255,12 +255,8 @@
             sb.append(ch);
           }
         }
-        try {
-          return new Text(sb.toString());
-        } catch (CharacterCodingException ex) {
-          ex.printStackTrace();
-          return new Text();
-        }
+        
+        return new Text(sb.toString());
     }
     
     /**
"
hadoop,dd20cfa6cf8c7864f2204a9dfbf09c010d467df6,"HADOOP-568.  Fix so that errors while initializing tasks mark the task failed.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@452645 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-03 21:56:40,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 2ea94bc..17fdcb5 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -674,10 +674,19 @@
           reduceTotal++;
         }
       }
-      try{
+      try {
     	  localizeJob(tip);
-      }catch(IOException ie){
-    	  LOG.warn(""Error initializing Job "" + tip.getTask().getJobId());
+      } catch (IOException ie) {
+        String msg = (""Error initializing "" + tip.getTask().getTaskId() + 
+                      "":\n"" + StringUtils.stringifyException(ie));
+        LOG.warn(msg);
+        tip.reportDiagnosticInfo(msg);
+        try {
+          tip.killAndCleanup(true);
+        } catch (IOException ie2) {
+          LOG.info(""Error cleaning up "" + tip.getTask().getTaskId() + "":\n"" +
+                   StringUtils.stringifyException(ie2));          
+        }
       }
     }
     
@@ -995,7 +1004,8 @@
         }
 
         /**
-         * This task has run on too long, and should be killed.
+         * Something went wrong and the task must be killed.
+         * @param wasFailure was it a failure (versus a kill request)?
          */
         public synchronized void killAndCleanup(boolean wasFailure
                                                 ) throws IOException {
@@ -1005,6 +1015,13 @@
                   failures += 1;
                 }
                 runner.kill();
+            } else if (runstate == TaskStatus.State.UNASSIGNED) {
+              if (wasFailure) {
+                failures += 1;
+                runstate = TaskStatus.State.FAILED;
+              } else {
+                runstate = TaskStatus.State.KILLED;
+              }
             }
         }
 
"
hadoop,7f87a0138751c6191e0ce18879c61ffbdecbe56a,"HADOOP-552.  Improved error checking when copying map output files to reduce nodes.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@452156 13f79535-47bb-0310-9956-ffa450edef68
",2006-10-02 18:16:37,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/MapOutputLocation.java b/src/java/org/apache/hadoop/mapred/MapOutputLocation.java
index c07d65c..a7598e8 100644
--- a/src/java/org/apache/hadoop/mapred/MapOutputLocation.java
+++ b/src/java/org/apache/hadoop/mapred/MapOutputLocation.java
@@ -19,7 +19,7 @@
 import java.io.IOException;
 
 import java.io.*;
-import java.net.URL;
+import java.net.*;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.io.*;
@@ -104,24 +104,46 @@
                       Path localFilename, 
                       int reduce,
                       Progressable pingee) throws IOException {
-    URL path = new URL(toString() + ""&reduce="" + reduce);
-    InputStream input = path.openConnection().getInputStream();
-    OutputStream output = fileSys.create(localFilename);
+    boolean good = false;
     long totalBytes = 0;
+    URL path = new URL(toString() + ""&reduce="" + reduce);
     try {
-      byte[] buffer = new byte[64 * 1024];
-      int len = input.read(buffer);
-      while (len > 0) {
-        totalBytes += len;
-        output.write(buffer, 0 ,len);
-        if (pingee != null) {
-          pingee.progress();
+      URLConnection connection = path.openConnection();
+      InputStream input = connection.getInputStream();
+      try {
+        OutputStream output = fileSys.create(localFilename);
+        try {
+          byte[] buffer = new byte[64 * 1024];
+          int len = input.read(buffer);
+          while (len > 0) {
+            totalBytes += len;
+            output.write(buffer, 0 ,len);
+            if (pingee != null) {
+              pingee.progress();
+            }
+            len = input.read(buffer);
+          }
+        } finally {
+          output.close();
         }
-        len = input.read(buffer);
+      } finally {
+        input.close();
+      }
+      good = ((int) totalBytes) == connection.getContentLength();
+      if (!good) {
+        throw new IOException(""Incomplete map output received for "" + path +
+                              "" ("" + totalBytes + "" instead of "" + 
+                              connection.getContentLength() + "")"");
       }
     } finally {
-      input.close();
-      output.close();
+      if (!good) {
+        try {
+          fileSys.delete(localFilename);
+          totalBytes = 0;
+        } catch (Throwable th) {
+          // IGNORED because we are cleaning up
+        }
+      }
     }
     return totalBytes;
   }
"
hadoop,155fddd073d0a31bf79b651b5b53903ec14e9496,"HADOOP-513.  Replace map output handling with a servlet.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@451451 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-29 21:25:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index 85dc135..79022bb 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -161,10 +161,7 @@
         String infoServerBindAddress = conf.get(""dfs.datanode.info.bindAddress"", ""0.0.0.0"");
         this.infoServer = new StatusHttpServer(""datanode"", infoServerBindAddress, infoServerPort, true);
         //create a servlet to serve full-file content
-        try {
-          this.infoServer.addServlet(null, ""/streamFile/*"",
-                ""org.apache.hadoop.dfs.StreamFile"", null);
-        } catch (Exception e) {LOG.warn(""addServlet threw exception"", e);}
+        this.infoServer.addServlet(null, ""/streamFile/*"", StreamFile.class);
         this.infoServer.start();
         this.dnRegistration.infoPort = this.infoServer.getPort();
         datanodeObject = this;
"
hadoop,155fddd073d0a31bf79b651b5b53903ec14e9496,"HADOOP-513.  Replace map output handling with a servlet.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@451451 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-29 21:25:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/MapOutputLocation.java b/src/java/org/apache/hadoop/mapred/MapOutputLocation.java
index 00ad32a..c07d65c 100644
--- a/src/java/org/apache/hadoop/mapred/MapOutputLocation.java
+++ b/src/java/org/apache/hadoop/mapred/MapOutputLocation.java
@@ -87,7 +87,7 @@
   }
 
   public String toString() {
-    return ""http://"" + host + "":"" + port + ""/getMapOutput.jsp?map="" + 
+    return ""http://"" + host + "":"" + port + ""/mapOutput?map="" + 
             mapTaskId;
   }
   
"
hadoop,155fddd073d0a31bf79b651b5b53903ec14e9496,"HADOOP-513.  Replace map output handling with a servlet.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@451451 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-29 21:25:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/StatusHttpServer.java b/src/java/org/apache/hadoop/mapred/StatusHttpServer.java
index c01c6fa..6de18c1 100644
--- a/src/java/org/apache/hadoop/mapred/StatusHttpServer.java
+++ b/src/java/org/apache/hadoop/mapred/StatusHttpServer.java
@@ -21,11 +21,12 @@
 import java.net.URL;
 import java.net.URLDecoder;
 
+import javax.servlet.http.HttpServlet;
+
 import org.mortbay.http.HttpContext;
 import org.mortbay.http.handler.ResourceHandler;
 import org.mortbay.http.SocketListener;
 import org.mortbay.jetty.servlet.WebApplicationContext;
-import org.mortbay.jetty.servlet.ServletHttpContext;
 
 /**
  * Create a Jetty embedded server to answer http requests. The primary goal
@@ -97,19 +98,33 @@
    * @param name The name of the servlet (can be passed as null)
    * @param pathSpec The path spec for the servlet
    * @param classname The class name for the servlet
-   * @param contextPath The context path (can be null, defaults to ""/"")
    */
-  public void addServlet(String name, String pathSpec, String classname,
-     String contextPath) 
- throws ClassNotFoundException, InstantiationException, IllegalAccessException {
-    String tmpContextPath = contextPath;
-    if (tmpContextPath == null) tmpContextPath = ""/"";
-    ServletHttpContext context = 
-                    (ServletHttpContext)webServer.getContext(tmpContextPath);
-    if (name == null)
-      context.addServlet(pathSpec, classname);
-    else
-      context.addServlet(name, pathSpec, classname);
+  public <T extends HttpServlet> 
+  void addServlet(String name, String pathSpec, 
+                  Class<T> servletClass) {
+    WebApplicationContext context = webAppContext;
+    try {
+      if (name == null) {
+        context.addServlet(pathSpec, servletClass.getName());
+      } else {
+        context.addServlet(name, pathSpec, servletClass.getName());
+      } 
+    } catch (ClassNotFoundException ex) {
+      throw makeRuntimeException(""Problem instantiating class"", ex);
+    } catch (InstantiationException ex) {
+      throw makeRuntimeException(""Problem instantiating class"", ex);
+    } catch (IllegalAccessException ex) {
+      throw makeRuntimeException(""Problem instantiating class"", ex);
+    }
+  }
+  
+  private static RuntimeException makeRuntimeException(String msg, 
+                                                       Throwable cause) {
+    RuntimeException result = new RuntimeException(msg);
+    if (cause != null) {
+      result.initCause(cause);
+    }
+    return result;
   }
   
   /**
"
hadoop,155fddd073d0a31bf79b651b5b53903ec14e9496,"HADOOP-513.  Replace map output handling with a servlet.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@451451 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-29 21:25:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index fc0298d..2ea94bc 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -28,8 +28,12 @@
 import java.util.*;
 import java.util.regex.Pattern;
 
-import org.apache.hadoop.metrics.ContextFactory;
-import org.apache.hadoop.metrics.MetricsContext;
+import javax.servlet.ServletContext;
+import javax.servlet.ServletException;
+import javax.servlet.http.HttpServlet;
+import javax.servlet.http.HttpServletRequest;
+import javax.servlet.http.HttpServletResponse;
+
 import org.apache.hadoop.metrics.MetricsRecord;
 import org.apache.hadoop.net.DNS;
 
@@ -361,8 +365,6 @@
       this.server = new StatusHttpServer(""task"", httpBindAddress, httpPort, true);
       int workerThreads = conf.getInt(""tasktracker.http.threads"", 40);
       server.setThreads(1, workerThreads);
-      server.start();
-      this.httpPort = server.getPort();
       // let the jsp pages get to the task tracker, config, and other relevant
       // objects
       FileSystem local = FileSystem.getNamed(""local"", conf);
@@ -370,6 +372,9 @@
       server.setAttribute(""local.file.system"", local);
       server.setAttribute(""conf"", conf);
       server.setAttribute(""log"", LOG);
+      server.addServlet(""mapOutput"", ""mapOutput"", MapOutputServlet.class);
+      server.start();
+      this.httpPort = server.getPort();
       initialize();
     }
 
@@ -1318,4 +1323,55 @@
             System.exit(-1);
         }
     }
+    
+    /**
+     * This class is used in TaskTracker's Jetty to serve the map outputs
+     * to other nodes.
+     * @author Owen O'Malley
+     */
+    public static class MapOutputServlet extends HttpServlet {
+      public void doGet(HttpServletRequest request, 
+                        HttpServletResponse response
+                       ) throws ServletException, IOException {
+        String mapId = request.getParameter(""map"");
+        String reduceId = request.getParameter(""reduce"");
+        if (mapId == null || reduceId == null) {
+          throw new IOException(""map and reduce parameters are required"");
+        }
+        ServletContext context = getServletContext();
+        int reduce = Integer.parseInt(reduceId);
+        byte[] buffer = new byte[64*1024];
+        OutputStream outStream = response.getOutputStream();
+        JobConf conf = (JobConf) context.getAttribute(""conf"");
+        FileSystem fileSys = 
+          (FileSystem) context.getAttribute(""local.file.system"");
+        Path filename = conf.getLocalPath(mapId+""/part-""+reduce+"".out"");
+        response.setContentLength((int) fileSys.getLength(filename));
+        InputStream inStream = null;
+        try {
+          inStream = fileSys.open(filename);
+          try {
+            int len = inStream.read(buffer);
+            while (len > 0) {
+              outStream.write(buffer, 0, len);
+              len = inStream.read(buffer);
+            }
+          } finally {
+            inStream.close();
+            outStream.close();
+          }
+        } catch (IOException ie) {
+          TaskTracker tracker = 
+            (TaskTracker) context.getAttribute(""task.tracker"");
+          Log log = (Log) context.getAttribute(""log"");
+          String errorMsg = ""getMapOutput("" + mapId + "","" + reduceId + 
+          "") failed :\n""+
+          StringUtils.stringifyException(ie);
+          log.warn(errorMsg);
+          tracker.mapOutputLost(mapId, errorMsg);
+          response.sendError(HttpServletResponse.SC_GONE, errorMsg);
+          throw ie;
+        } 
+      }
+    }
 }
"
hadoop,8a009d5f1e1527359c9a9aeaca5a1f776f8c6d0e,"HADOOP-423.  Normalize paths containing directories named '.' or '..'.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@451421 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-29 20:15:09,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index 3c507d1..cd298d2 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -21,6 +21,7 @@
 import org.apache.hadoop.conf.*;
 import org.apache.hadoop.util.*;
 import org.apache.hadoop.mapred.StatusHttpServer;
+import org.apache.hadoop.fs.Path;
 
 import java.io.*;
 import java.net.InetSocketAddress;
@@ -398,6 +399,9 @@
             +src+"" for ""+holder+"" at ""+clientMachine);
       if( isInSafeMode() )
         throw new SafeModeException( ""Cannot create file"" + src, safeMode );
+      if (!isValidName(src.toString())) {
+        throw new IOException(""Invalid file name: "" + src);      	  
+      }
       try {
         if (pendingCreates.get(src) != null) {
            throw new AlreadyBeingCreatedException(
@@ -730,6 +734,9 @@
         NameNode.stateChangeLog.debug(""DIR* NameSystem.renameTo: "" + src + "" to "" + dst );
         if( isInSafeMode() )
           throw new SafeModeException( ""Cannot rename "" + src, safeMode );
+        if (!isValidName(dst.toString())) {
+          throw new IOException(""Invalid name: "" + dst);
+        }
         return dir.renameTo(src, dst);
     }
 
@@ -785,12 +792,42 @@
     }
 
     /**
+     * Whether the pathname is valid.  Currently prohibits relative paths, 
+     * and names which contain a "":"" or ""/"" 
+     */
+    private boolean isValidName(String src) {
+      
+      // Path must be absolute.
+      if (!src.startsWith(Path.SEPARATOR)) {
+        return false;
+      }
+      
+      // Check for "".."" ""."" "":"" ""/""
+      Enumeration tokens = new StringTokenizer(src, Path.SEPARATOR);    
+      ArrayList list = Collections.list(tokens);      
+      for (int i = 0; i < list.size(); i++) {
+        String element = (String)list.get(i);
+        if (element.equals("".."") || 
+            element.equals(""."")  ||
+            (element.indexOf("":"") >= 0)  ||
+            (element.indexOf(""/"") >= 0)) {
+          return false;
+        }
+      }
+      
+      return true;
+    }
+    
+    /**
      * Create all the necessary directories
      */
     public boolean mkdirs( String src ) throws IOException {
         NameNode.stateChangeLog.debug(""DIR* NameSystem.mkdirs: "" + src );
         if( isInSafeMode() )
           throw new SafeModeException( ""Cannot create directory "" + src, safeMode );
+        if (!isValidName(src)) {
+          throw new IOException(""Invalid directory name: "" + src);
+        }
         return dir.mkdirs(src);
     }
 
"
hadoop,8a009d5f1e1527359c9a9aeaca5a1f776f8c6d0e,"HADOOP-423.  Normalize paths containing directories named '.' or '..'.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@451421 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-29 20:15:09,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/Path.java b/src/java/org/apache/hadoop/fs/Path.java
index 7586bca..cf3307b 100644
--- a/src/java/org/apache/hadoop/fs/Path.java
+++ b/src/java/org/apache/hadoop/fs/Path.java
@@ -57,13 +57,15 @@
       this.elements = child.elements;
     } else {
       this.isAbsolute = parent.isAbsolute;
-      this.elements = new String[parent.elements.length+child.elements.length];
+      ArrayList list = new ArrayList(parent.elements.length+child.elements.length);
       for (int i = 0; i < parent.elements.length; i++) {
-        elements[i] = parent.elements[i];
+        list.add(parent.elements[i]);
       }
       for (int i = 0; i < child.elements.length; i++) {
-        elements[i+parent.elements.length] = child.elements[i];
+        list.add(child.elements[i]);
       }
+      normalize(list);
+      this.elements = (String[])list.toArray(new String[list.size()]);
     }
     this.drive = child.drive == null ? parent.drive : child.drive;
   }
@@ -82,10 +84,10 @@
     // determine whether the path is absolute
     this.isAbsolute = pathString.startsWith(SEPARATOR);
 
-
     // tokenize the path into elements
-    Enumeration tokens = new StringTokenizer(pathString, SEPARATOR);
+    Enumeration tokens = new StringTokenizer(pathString, SEPARATOR);    
     ArrayList list = Collections.list(tokens);
+    normalize(list);
     this.elements = (String[])list.toArray(new String[list.size()]);
   }
 
@@ -180,5 +182,56 @@
     return elements.length;
   }
 
+  /* 
+   * Removes '.' and '..' 
+   */
+  private void normalize(ArrayList list) {
+    boolean canNormalize = this.isAbsolute;
+    boolean changed = false;    // true if we have detected any . or ..
+    int index = 0;
+    int listSize = list.size();
+    for (int i = 0; i < listSize; i++) {
+      // Invariant: (index >= 0) && (index <= i)
+      if (list.get(i).equals(""."")) {
+        changed = true;
+      } else {
+        if (canNormalize) {
+          if (list.get(i).equals("".."")) {
+            if ((index > 0) && !list.get(index-1).equals("".."")) {
+              index--;    // effectively deletes the last element currently in list.
+              changed = true;
+            } else { // index == 0
+              // the first element is now going to be '..'
+              canNormalize = false;
+              list.set(index, "".."");
+              isAbsolute = false;
+              index++; 
+            }
+          } else { // list.get(i) != "".."" or "".""
+            if (changed) {
+              list.set(index, list.get(i));
+            }
+            index++;
+          }
+        } else { // !canNormalize
+          if (changed) {
+            list.set(index, list.get(i));
+          }
+          index++;
+          if (!list.get(i).equals("".."")) {
+           canNormalize = true;
+          }
+        }  // else !canNormalize
+      } 
+    }  // for
+    
+    // Remove the junk at the end of the list.
+    for (int j = listSize-1; j >= index; j--) {
+      list.remove(j);
+    }
+
+  }
+  
+  
 }
 
"
hadoop,fed3303541e24722856240ed46eef1036fd530b2,"HADOOP-508.  Fix a bug in FSDataInputStream where incorrect data was sometimes returned after seeking to a random location.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@450586 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-27 21:44:08,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FSDataInputStream.java b/src/java/org/apache/hadoop/fs/FSDataInputStream.java
index 1180125..edfd439 100644
--- a/src/java/org/apache/hadoop/fs/FSDataInputStream.java
+++ b/src/java/org/apache/hadoop/fs/FSDataInputStream.java
@@ -193,7 +193,8 @@
     public void seek(long desired) throws IOException {
       long end = ((PositionCache)in).getPos();
       long start = end - this.count;
-      if (desired >= start && desired < end) {
+      int avail = this.count - this.pos;
+      if (desired >= start && desired < end && avail > 0) {
         this.pos = (int)(desired - start);        // can position within buffer
       } else {
         this.count = 0;                           // invalidate buffer
"
hadoop,dd5825a9b68cea62ccd901ed3a302dcc92d3d374,"HADOOP-547.  Fix reduce tasks to ping tasktracker while copying data, rather than only between copies, avoiding task timeouts.  Contributed by Sanjay.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@449853 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-25 23:06:27,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java b/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
index 5a76c7c..a255335 100644
--- a/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
+++ b/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
@@ -116,7 +116,7 @@
     public MapOutputLocation getLocation() { return loc; }
   }
 
-  private static class PingTimer implements Progressable {
+  private class PingTimer implements Progressable {
     private long pingTime;
     
     public synchronized void reset() {
@@ -130,6 +130,7 @@
     public void progress() {
       synchronized (this) {
         pingTime = System.currentTimeMillis();
+        getTask().reportProgress(getTracker());
       }
     }
   }
"
hadoop,a2cf1f69229f43d5e95f30cfc2e1b5f60c7ba92a,"HADOOP-293.  Report the full list of task error messages in the web ui, not just the most recent.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@448339 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-20 21:06:27,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index 3bcaa73..23b32d8 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -403,7 +403,7 @@
     Vector jobsByArrival = new Vector();
 
     // All the known TaskInProgress items, mapped to by taskids (taskid->TIP)
-    TreeMap taskidToTIPMap = new TreeMap();
+    Map<String, TaskInProgress> taskidToTIPMap = new TreeMap();
 
     // (taskid --> trackerID) 
     TreeMap taskidToTrackerMap = new TreeMap();
@@ -1081,6 +1081,27 @@
         }
     }
 
+    /**
+     * Get the diagnostics for a given task
+     * @param jobId the id of the job
+     * @param tipId the id of the tip 
+     * @param taskId the id of the task
+     * @return a list of the diagnostic messages
+     */
+    public synchronized List<String> getTaskDiagnostics(String jobId,
+                                                        String tipId,
+                                                        String taskId) {
+      JobInProgress job = (JobInProgress) jobs.get(jobId);
+      if (job == null) {
+        throw new IllegalArgumentException(""Job "" + jobId + "" not found."");
+      }
+      TaskInProgress tip = job.getTaskInProgress(tipId);
+      if (tip == null) {
+        throw new IllegalArgumentException(""TIP "" + tipId + "" not found."");
+      }
+      return tip.getDiagnosticInfo(taskId);
+    }
+    
     /** Get all the TaskStatuses from the tipid. */
     TaskStatus[] getTaskStatuses(String jobid, String tipid){
 	JobInProgress job = (JobInProgress) jobs.get(jobid);
"
hadoop,a2cf1f69229f43d5e95f30cfc2e1b5f60c7ba92a,"HADOOP-293.  Report the full list of task error messages in the web ui, not just the most recent.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@448339 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-20 21:06:27,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskInProgress.java b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
index 6bb2769..e54cc07 100644
--- a/src/java/org/apache/hadoop/mapred/TaskInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
@@ -71,7 +71,7 @@
     private TreeSet recentTasks = new TreeSet();
     private JobConf conf;
     private boolean runSpeculative;
-    private TreeMap taskDiagnosticData = new TreeMap();
+    private Map<String,List<String>> taskDiagnosticData = new TreeMap();
     /**
      * Map from taskId -> TaskStatus
      */
@@ -242,6 +242,15 @@
          (String[])diagnostics.toArray(new String[diagnostics.size()]));
     }
 
+    /**
+     * Get the diagnostic messages for a given task within this tip.
+     * @param taskId the id of the required task
+     * @return the list of diagnostics for that task
+     */
+    synchronized List<String> getDiagnosticInfo(String taskId) {
+      return taskDiagnosticData.get(taskId);
+    }
+    
     ////////////////////////////////////////////////
     // Update methods, usually invoked by the owning
     // job.
"
hadoop,615a47a0077629c91eea5be5b368fabc57ac59b3,"HADOOP-261.  Record an error message when map outputs are lost.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@448334 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-20 20:42:43,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index e56b87c..8aa1ae1 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -357,7 +357,7 @@
       this.mapOutputFile = new MapOutputFile();
       this.mapOutputFile.setConf(conf);
       int httpPort = conf.getInt(""tasktracker.http.port"", 50060);
-      String httpBindAddress = conf.get(""tasktracker.http.bindAddress"", ""0.0.0.0"");;
+      String httpBindAddress = conf.get(""tasktracker.http.bindAddress"", ""0.0.0.0"");
       this.server = new StatusHttpServer(""task"", httpBindAddress, httpPort, true);
       int workerThreads = conf.getInt(""tasktracker.http.threads"", 40);
       server.setThreads(1, workerThreads);
@@ -990,11 +990,14 @@
         /**
          * The map output has been lost.
          */
-        public synchronized void mapOutputLost() throws IOException {
+        public synchronized void mapOutputLost(String failure
+                                               ) throws IOException {
             if (runstate == TaskStatus.SUCCEEDED) {
               LOG.info(""Reporting output lost:""+task.getTaskId());
               runstate = TaskStatus.FAILED;       // change status to failure
               progress = 0.0f;
+              reportDiagnosticInfo(""Map output lost, rescheduling: "" + 
+                                   failure);
               runningTasks.put(task.getTaskId(), this);
               mapTotal++;
             } else {
@@ -1120,10 +1123,11 @@
     /**
      * A completed map task's output has been lost.
      */
-    public synchronized void mapOutputLost(String taskid) throws IOException {
+    public synchronized void mapOutputLost(String taskid,
+                                           String errorMsg) throws IOException {
         TaskInProgress tip = (TaskInProgress) tasks.get(taskid);
         if (tip != null) {
-          tip.mapOutputLost();
+          tip.mapOutputLost(errorMsg);
         } else {
           LOG.warn(""Unknown child with bad map output: ""+taskid+"". Ignored."");
         }
"
hadoop,d423005465265fe7b439d15fd161fde4bfa52378,"HADOOP-545.  Remove an unused config file parameter.  Contributed by Philippe Gassmann.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@448332 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-20 20:33:50,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/mapred/MiniMRCluster.java b/src/test/org/apache/hadoop/mapred/MiniMRCluster.java
index 365a65e..da256cc 100644
--- a/src/test/org/apache/hadoop/mapred/MiniMRCluster.java
+++ b/src/test/org/apache/hadoop/mapred/MiniMRCluster.java
@@ -95,7 +95,6 @@
                 // set it down at 2 seconds.
                 jc.setInt(""ipc.client.timeout"", 1000);
                 jc.setInt(""mapred.task.tracker.info.port"", taskTrackerPort++);
-                jc.setInt(""mapred.task.tracker.output.port"", taskTrackerPort++);
                 jc.setInt(""mapred.task.tracker.report.port"", taskTrackerPort++);
                 File localDir = new File(jc.get(""mapred.local.dir""));
                 File ttDir = new File(localDir, Integer.toString(taskTrackerPort));
"
hadoop,175994a11a31d4d14cbd55b064fe2726d43340ca,"HADOOP-536.  Fix a bug with unit tests on Windows.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@447600 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-18 22:22:31,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/mapred/MRCaching.java b/src/test/org/apache/hadoop/mapred/MRCaching.java
index 47900cd..678139c 100644
--- a/src/test/org/apache/hadoop/mapred/MRCaching.java
+++ b/src/test/org/apache/hadoop/mapred/MRCaching.java
@@ -65,7 +65,7 @@
         Path file = new Path(""/tmp"");
         fs.mkdirs(file);
         Path fileOut = new Path(file, ""test.txt"");
-        fs.delete(file);
+        fs.delete(fileOut);
         DataOutputStream out = fs.create(fileOut);
 
         for (int i = 0; i < localArchives.length; i++) {
"
hadoop,c12af2bd8cba5c574a6594f3cc6d2cf5866da7dd,"HADOOP-534.  Change the default value classes in JobConf to be Text, not the now-deprecated UTF8.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@446695 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-15 19:14:50,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobConf.java b/src/java/org/apache/hadoop/mapred/JobConf.java
index 2f5188e..6302194 100644
--- a/src/java/org/apache/hadoop/mapred/JobConf.java
+++ b/src/java/org/apache/hadoop/mapred/JobConf.java
@@ -36,7 +36,7 @@
 import org.apache.hadoop.io.WritableComparable;
 import org.apache.hadoop.io.WritableComparator;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.UTF8;
+import org.apache.hadoop.io.Text;
 import org.apache.hadoop.io.compress.CompressionCodec;
 
 import org.apache.hadoop.mapred.lib.IdentityMapper;
@@ -314,7 +314,7 @@
 
   /** @deprecated Call {@link RecordReader#createValue()}. */
   public Class getInputValueClass() {
-    return getClass(""mapred.input.value.class"", UTF8.class, Writable.class);
+    return getClass(""mapred.input.value.class"", Text.class, Writable.class);
   }
 
   /** @deprecated Not used */
@@ -444,7 +444,7 @@
   }
 
   public Class getOutputValueClass() {
-    return getClass(""mapred.output.value.class"", UTF8.class, Writable.class);
+    return getClass(""mapred.output.value.class"", Text.class, Writable.class);
   }
   public void setOutputValueClass(Class theClass) {
     setClass(""mapred.output.value.class"", theClass, Writable.class);
"
hadoop,364347190552334e90b9af957ced9a699ff23a4b,"HADOOP-532.  Fix a bug reading value-compressed sequence files, where an exception was thrown reporting that the full value had not been read.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@443532 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-15 00:13:32,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/SequenceFile.java b/src/java/org/apache/hadoop/io/SequenceFile.java
index 2deee24..9e26390 100644
--- a/src/java/org/apache/hadoop/io/SequenceFile.java
+++ b/src/java/org/apache/hadoop/io/SequenceFile.java
@@ -976,10 +976,14 @@
 
       if (version > 2) {                          // if version > 2
         this.decompress = in.readBoolean();       // is compressed?
+      } else {
+        decompress = false;
       }
 
       if (version >= BLOCK_COMPRESS_VERSION) {    // if version >= 4
         this.blockCompressed = in.readBoolean();  // is block-compressed?
+      } else {
+        blockCompressed = false;
       }
       
       // if version >= 5
@@ -1008,9 +1012,9 @@
       valBuffer = new DataInputBuffer();
       if (decompress) {
         valInFilter = this.codec.createInputStream(valBuffer);
-        valIn = new DataInputStream(new BufferedInputStream(valInFilter));
+        valIn = new DataInputStream(valInFilter);
       } else {
-        valIn = new DataInputStream(new BufferedInputStream(valBuffer));
+        valIn = valBuffer;
       }
       
       if (blockCompressed) {
@@ -1113,10 +1117,11 @@
      * corresponding to the 'current' key 
      */
     private synchronized void seekToCurrentValue() throws IOException {
-      if (version < BLOCK_COMPRESS_VERSION || blockCompressed == false) {
+      if (!blockCompressed) {
         if (decompress) {
           valInFilter.resetState();
         }
+        valBuffer.reset();
       } else {
         // Check if this is the first value in the 'block' to be read
         if (lazyDecompress && !valuesDecompressed) {
@@ -1160,13 +1165,15 @@
       // Position stream to 'current' value
       seekToCurrentValue();
 
-      if (version < BLOCK_COMPRESS_VERSION || blockCompressed == false) {
+      if (!blockCompressed) {
         val.readFields(valIn);
         
-        if (valBuffer.getPosition() != valBuffer.getLength())
+        if (valIn.read() > 0) {
+          LOG.info(""available bytes: "" + valIn.available());
           throw new IOException(val+"" read ""+(valBuffer.getPosition()-keyLength)
               + "" bytes, should read "" +
               (valBuffer.getLength()-keyLength));
+        }
       } else {
         // Get the value
         int valLength = WritableUtils.readVInt(valLenIn);
@@ -1190,7 +1197,7 @@
         throw new IOException(""wrong key class: ""+key.getClass().getName()
             +"" is not ""+keyClass);
 
-      if (version < BLOCK_COMPRESS_VERSION || blockCompressed == false) {
+      if (!blockCompressed) {
         outBuf.reset();
         
         keyLength = next(outBuf);
@@ -1200,6 +1207,7 @@
         valBuffer.reset(outBuf.getData(), outBuf.getLength());
         
         key.readFields(valBuffer);
+        valBuffer.mark(0);
         if (valBuffer.getPosition() != keyLength)
           throw new IOException(key + "" read "" + valBuffer.getPosition()
               + "" bytes, should read "" + keyLength);
@@ -1271,7 +1279,7 @@
     /** @deprecated Call {@link #nextRaw(DataOutputBuffer,SequenceFile.ValueBytes)}. */
     public synchronized int next(DataOutputBuffer buffer) throws IOException {
       // Unsupported for block-compressed sequence files
-      if (version >= BLOCK_COMPRESS_VERSION && blockCompressed) {
+      if (blockCompressed) {
         throw new IOException(""Unsupported call for block-compressed"" +
             "" SequenceFiles - use SequenceFile.Reader.next(DataOutputStream, ValueBytes)"");
       }
@@ -1308,7 +1316,7 @@
      */
     public int nextRaw(DataOutputBuffer key, ValueBytes val) 
     throws IOException {
-      if (version < BLOCK_COMPRESS_VERSION || blockCompressed == false) {
+      if (!blockCompressed) {
         if (in.getPos() >= end) 
           return -1;
 
"
hadoop,364347190552334e90b9af957ced9a699ff23a4b,"HADOOP-532.  Fix a bug reading value-compressed sequence files, where an exception was thrown reporting that the full value had not been read.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@443532 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-15 00:13:32,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/io/RandomDatum.java b/src/test/org/apache/hadoop/io/RandomDatum.java
index d9a2eef..d4c1378 100644
--- a/src/test/org/apache/hadoop/io/RandomDatum.java
+++ b/src/test/org/apache/hadoop/io/RandomDatum.java
@@ -26,11 +26,15 @@
   public RandomDatum() {}
 
   public RandomDatum(Random random) {
-    length = 10 + random.nextInt(100);
+    length = 10 + (int) Math.pow(10.0, random.nextFloat() * 3.0);
     data = new byte[length];
     random.nextBytes(data);
   }
 
+  public int getLength() {
+    return length;
+  }
+  
   public void write(DataOutput out) throws IOException {
     out.writeInt(length);
     out.write(data);
"
hadoop,53f9f2ab3a1de8a663b237ad1c56189094ca1c04,"HADOOP-288.  Add a file caching system and use it in MapReduce to cache job jar files on slave nodes.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@443497 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-14 22:13:43,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FileUtil.java b/src/java/org/apache/hadoop/fs/FileUtil.java
index 47d7ea8..a172894 100644
--- a/src/java/org/apache/hadoop/fs/FileUtil.java
+++ b/src/java/org/apache/hadoop/fs/FileUtil.java
@@ -17,6 +17,9 @@
 package org.apache.hadoop.fs;
 
 import java.io.*;
+import java.util.Enumeration;
+import java.util.zip.ZipEntry;
+import java.util.zip.ZipFile;
 
 import org.apache.hadoop.conf.Configuration;
 
@@ -230,5 +233,68 @@
     }
     return dst;
   }
-
+  
+  /**
+   * Takes an input dir and returns the du on that local directory. Very basic
+   * implementation.
+   * 
+   * @param dir
+   *          The input dir to get the disk space of this local dir
+   * @return The total disk space of the input local directory
+   */
+  public static long getDU(File dir) {
+    long size = 0;
+    if (!dir.exists())
+      return 0;
+    if (!dir.isDirectory()) {
+      return dir.length();
+    } else {
+      size = dir.length();
+      File[] allFiles = dir.listFiles();
+      for (int i = 0; i < allFiles.length; i++) {
+        size = size + getDU(allFiles[i]);
+      }
+      return size;
+    }
+  }
+    
+	/**
+   * Given a File input it will unzip the file in a the unzip directory
+   * passed as the second parameter
+   * @param inFile The zip file as input
+   * @param unzipDir The unzip directory where to unzip the zip file.
+   * @throws IOException
+   */
+  public static void unZip(File inFile, File unzipDir) throws IOException {
+    Enumeration entries;
+    ZipFile zipFile = new ZipFile(inFile);
+    ;
+    try {
+      entries = zipFile.entries();
+      while (entries.hasMoreElements()) {
+        ZipEntry entry = (ZipEntry) entries.nextElement();
+        if (!entry.isDirectory()) {
+          InputStream in = zipFile.getInputStream(entry);
+          try {
+            File file = new File(unzipDir, entry.getName());
+            file.getParentFile().mkdirs();
+            OutputStream out = new FileOutputStream(file);
+            try {
+              byte[] buffer = new byte[8192];
+              int i;
+              while ((i = in.read(buffer)) != -1) {
+                out.write(buffer, 0, i);
+              }
+            } finally {
+              out.close();
+            }
+          } finally {
+            in.close();
+          }
+        }
+      }
+    } finally {
+      zipFile.close();
+    }
+  }
 }
"
hadoop,53f9f2ab3a1de8a663b237ad1c56189094ca1c04,"HADOOP-288.  Add a file caching system and use it in MapReduce to cache job jar files on slave nodes.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@443497 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-14 22:13:43,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobClient.java b/src/java/org/apache/hadoop/mapred/JobClient.java
index 4840137..63b03f4 100644
--- a/src/java/org/apache/hadoop/mapred/JobClient.java
+++ b/src/java/org/apache/hadoop/mapred/JobClient.java
@@ -21,7 +21,7 @@
 import org.apache.hadoop.ipc.*;
 import org.apache.hadoop.conf.*;
 import org.apache.hadoop.util.*;
-
+import org.apache.hadoop.filecache.*;
 import java.io.*;
 import java.net.*;
 import java.util.*;
@@ -227,7 +227,8 @@
         JobConf job = new JobConf(jobFile);
         return submitJob(job);
     }
-
+    
+   
     /**
      * Submit a job to the MR system
      */
@@ -244,11 +245,39 @@
         Path submitJobDir = new Path(job.getSystemDir(), ""submit_"" + Integer.toString(Math.abs(r.nextInt()), 36));
         Path submitJobFile = new Path(submitJobDir, ""job.xml"");
         Path submitJarFile = new Path(submitJobDir, ""job.jar"");
-
-        String originalJarPath = job.getJar();
-
         FileSystem fs = getFs();
-
+        // try getting the md5 of the archives
+        URI[] tarchives = DistributedCache.getCacheArchives(job);
+        URI[] tfiles = DistributedCache.getCacheFiles(job);
+        if ((tarchives != null) || (tfiles != null)) {
+          // prepare these archives for md5 checksums
+          if (tarchives != null) {
+            String md5Archives = StringUtils.byteToHexString(DistributedCache
+                .createMD5(tarchives[0], job));
+            for (int i = 1; i < tarchives.length; i++) {
+              md5Archives = md5Archives
+                  + "",""
+                  + StringUtils.byteToHexString(DistributedCache
+                      .createMD5(tarchives[i], job));
+            }
+            DistributedCache.setArchiveMd5(job, md5Archives);
+            //job.set(""mapred.cache.archivemd5"", md5Archives);
+          }
+          if (tfiles != null) {
+            String md5Files = StringUtils.byteToHexString(DistributedCache
+                .createMD5(tfiles[0], job));
+            for (int i = 1; i < tfiles.length; i++) {
+              md5Files = md5Files
+                  + "",""
+                  + StringUtils.byteToHexString(DistributedCache
+                      .createMD5(tfiles[i], job));
+            }
+            DistributedCache.setFileMd5(job, md5Files);
+            //""mapred.cache.filemd5"", md5Files);
+          }
+        }
+       
+        String originalJarPath = job.getJar();
         short replication = (short)job.getInt(""mapred.submit.replication"", 10);
 
         if (originalJarPath != null) {           // copy jar to JobTracker's fs
"
hadoop,53f9f2ab3a1de8a663b237ad1c56189094ca1c04,"HADOOP-288.  Add a file caching system and use it in MapReduce to cache job jar files on slave nodes.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@443497 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-14 22:13:43,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobConf.java b/src/java/org/apache/hadoop/mapred/JobConf.java
index 0071c23..2f5188e 100644
--- a/src/java/org/apache/hadoop/mapred/JobConf.java
+++ b/src/java/org/apache/hadoop/mapred/JobConf.java
@@ -171,6 +171,7 @@
     String dirs = get(""mapred.input.dir"");
     set(""mapred.input.dir"", dirs == null ? dir.toString() : dirs + "","" + dir);
   }
+
   public Path[] getInputPaths() {
     String dirs = get(""mapred.input.dir"", """");
     ArrayList list = Collections.list(new StringTokenizer(dirs, "",""));
@@ -197,8 +198,10 @@
     set(""user.name"", user);
   }
 
+
+  
   /**
-   * Set whether the framework shoul keep the intermediate files for 
+   * Set whether the framework should keep the intermediate files for 
    * failed tasks.
    */
   public void setKeepFailedTaskFiles(boolean keep) {
"
hadoop,53f9f2ab3a1de8a663b237ad1c56189094ca1c04,"HADOOP-288.  Add a file caching system and use it in MapReduce to cache job jar files on slave nodes.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@443497 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-14 22:13:43,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskRunner.java b/src/java/org/apache/hadoop/mapred/TaskRunner.java
index 6a515d1..7d79e08 100644
--- a/src/java/org/apache/hadoop/mapred/TaskRunner.java
+++ b/src/java/org/apache/hadoop/mapred/TaskRunner.java
@@ -20,11 +20,12 @@
 import org.apache.hadoop.conf.*;
 import org.apache.hadoop.fs.*;
 import org.apache.hadoop.util.*;
-
+import org.apache.hadoop.filecache.*;
 import java.io.*;
 import java.util.jar.*;
 import java.util.Vector;
 import java.util.Enumeration;
+import java.net.URI;
 
 /** Base class that runs a task in a separate process.  Tasks are run in a
  * separate process in order to isolate the map/reduce system code from bugs in
@@ -61,25 +62,71 @@
   */
   public void close() throws IOException {}
 
+  private String stringifyPathArray(Path[] p){
+	  if (p == null){
+      return null;
+    }
+    String str = p[0].toString();
+    for (int i = 1; i < p.length; i++){
+      str = str + "","" + p[i].toString();
+    }
+    return str;
+  }
+  
   public final void run() {
     try {
-
+      
+      //before preparing the job localize 
+      //all the archives
+      
+      URI[] archives = DistributedCache.getCacheArchives(conf);
+      URI[] files = DistributedCache.getCacheFiles(conf);
+      if ((archives != null) || (files != null)) {
+        if (archives != null) {
+          String[] md5 = DistributedCache.getArchiveMd5(conf);
+          Path[] p = new Path[archives.length];
+          for (int i = 0; i < archives.length;i++){
+            p[i] = DistributedCache.getLocalCache(archives[i], conf, conf.getLocalPath(TaskTracker.getCacheSubdir()), true, md5[i]);
+          }
+          DistributedCache.setLocalArchives(conf, stringifyPathArray(p));
+        }
+        if ((files != null)) {
+          String[] md5 = DistributedCache.getFileMd5(conf);
+          Path[] p = new Path[files.length];
+          for (int i = 0; i < files.length;i++){
+           p[i] = DistributedCache.getLocalCache(files[i], conf, conf.getLocalPath(TaskTracker
+              .getCacheSubdir()), false, md5[i]);
+          }
+          DistributedCache.setLocalFiles(conf, stringifyPathArray(p));
+        }
+        
+        // sets the paths to local archives and paths
+        Path localTaskFile = new Path(t.getJobFile());
+        FileSystem localFs = FileSystem.getNamed(""local"", conf);
+        localFs.delete(localTaskFile);
+        OutputStream out = localFs.create(localTaskFile);
+        try {
+          conf.write(out);
+        } finally {
+          out.close();
+        }
+      }
+      
       if (! prepare()) {
         return;
       }
 
       String sep = System.getProperty(""path.separator"");
-      File workDir = new File(new File(t.getJobFile()).getParent(), ""work"");
-      workDir.mkdirs();
-               
       StringBuffer classPath = new StringBuffer();
       // start with same classpath as parent process
       classPath.append(System.getProperty(""java.class.path""));
       classPath.append(sep);
-
+      File workDir = new File(new File(t.getJobFile()).getParentFile().getParent(), ""work"");
+      workDir.mkdirs();
+	  
       String jar = conf.getJar();
-      if (jar != null) {                      // if jar exists, it into workDir
-        RunJar.unJar(new File(jar), workDir);
+      if (jar != null) {       
+    	  // if jar exists, it into workDir
         File[] libs = new File(workDir, ""lib"").listFiles();
         if (libs != null) {
           for (int i = 0; i < libs.length; i++) {
@@ -160,10 +207,27 @@
         LOG.warn(t.getTaskId()+"" Reporting Diagnostics"", e);
       }
     } finally {
+      try{
+        URI[] archives = DistributedCache.getCacheArchives(conf);
+        URI[] files = DistributedCache.getCacheFiles(conf);
+        if (archives != null){
+          for (int i = 0; i < archives.length; i++){
+            DistributedCache.releaseCache(archives[i], conf);
+          }
+        }
+        if (files != null){
+          for(int i = 0; i < files.length; i++){
+            DistributedCache.releaseCache(files[i], conf);
+          }
+        }
+      }catch(IOException ie){
+        LOG.warn(""Error releasing caches : Cache files might not have been cleaned up"");
+      }
       tracker.reportTaskFinished(t.getTaskId());
     }
   }
 
+  
   /**
    * Handle deprecated mapred.child.heap.size.
    * If present, interpolate into mapred.child.java.opts value with
@@ -238,6 +302,7 @@
       logStream(process.getInputStream());        // normally empty
       
       int exit_code = process.waitFor();
+     
       if (!killed && exit_code != 0) {
         throw new IOException(""Task process exit with nonzero status of "" +
                               exit_code + ""."");
"
hadoop,53f9f2ab3a1de8a663b237ad1c56189094ca1c04,"HADOOP-288.  Add a file caching system and use it in MapReduce to cache job jar files on slave nodes.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@443497 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-14 22:13:43,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 5963c65..af06cb3 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -71,6 +71,7 @@
      * Map from taskId -> TaskInProgress.
      */
     TreeMap runningTasks = null;
+    Map runningJobs = null;
     int mapTotal = 0;
     int reduceTotal = 0;
     boolean justStarted = true;
@@ -89,11 +90,11 @@
 
     static Random r = new Random();
     FileSystem fs = null;
-    static final String SUBDIR = ""taskTracker"";
-
+    private static final String SUBDIR = ""taskTracker"";
+    private static final String CACHEDIR = ""archive"";
+    private static final String JOBCACHE = ""jobcache"";
     private JobConf fConf;
     private MapOutputFile mapOutputFile;
-
     private int maxCurrentTasks;
     private int failures;
     private int finishedCount[] = new int[1];
@@ -145,6 +146,14 @@
       taskCleanupThread.start();
     }
     
+    static String getCacheSubdir() {
+      return TaskTracker.SUBDIR + Path.SEPARATOR + TaskTracker.CACHEDIR;
+    }
+
+    static String getJobCacheSubdir() {
+      return TaskTracker.SUBDIR + Path.SEPARATOR + TaskTracker.JOBCACHE;
+    }
+    
     public long getProtocolVersion(String protocol, long clientVersion) {
       return TaskUmbilicalProtocol.versionID;
     }
@@ -167,6 +176,7 @@
         // Clear out state tables
         this.tasks = new TreeMap();
         this.runningTasks = new TreeMap();
+        this.runningJobs = new TreeMap();
         this.mapTotal = 0;
         this.reduceTotal = 0;
         this.acceptNewTasks = true;
@@ -207,8 +217,84 @@
         
         this.running = true;
     }
+        
+    // intialize the job directory
+    private void localizeJob(TaskInProgress tip) throws IOException {
+      Path localJarFile = null;
+      Task t = tip.getTask();
+      Path localJobFile = new Path(fConf.getLocalPath(getJobCacheSubdir()), (t
+          .getJobId()
+          + Path.SEPARATOR + ""job.xml""));
+      RunningJob rjob = null;
+      synchronized (runningJobs) {
+        if (!runningJobs.containsKey(t.getJobId())) {
+          rjob = new RunningJob();
+          rjob.localized = false;
+          rjob.tasks = new ArrayList();
+          rjob.jobFile = localJobFile;
+          rjob.tasks.add(tip);
+          runningJobs.put(t.getJobId(), rjob);
+        } else {
+          rjob = (RunningJob) runningJobs.get(t.getJobId());
+          // keep this for later use when we just get a jobid to delete
+          // the data for
+          rjob.tasks.add(tip);
+        }
+      }
+      synchronized (rjob) {
+        if (!rjob.localized) {
+          localJarFile = new Path(fConf.getLocalPath(getJobCacheSubdir()), (t
+              .getJobId())
+              + Path.SEPARATOR + ""job.jar"");
+  
+          String jobFile = t.getJobFile();
+          fs.copyToLocalFile(new Path(jobFile), localJobFile);
+          JobConf localJobConf = new JobConf(localJobFile);
+          String jarFile = localJobConf.getJar();
+          if (jarFile != null) {
+            fs.copyToLocalFile(new Path(jarFile), localJarFile);
+            localJobConf.setJar(localJarFile.toString());
+            FileSystem localFs = FileSystem.getNamed(""local"", fConf);
+            OutputStream out = localFs.create(localJobFile);
+            try {
+              localJobConf.write(out);
+            } finally {
+              out.close();
+            }
 
-      public synchronized void shutdown() throws IOException {
+            // also unjar the job.jar files in workdir
+            File workDir = new File(
+                                    new File(localJobFile.toString()).getParent(),
+                                    ""work"");
+            workDir.mkdirs();
+            RunJar.unJar(new File(localJarFile.toString()), workDir);
+          }
+          rjob.localized = true;
+        }
+      }
+      launchTaskForJob(tip, new JobConf(rjob.jobFile)); 
+    }
+    
+    private void launchTaskForJob(TaskInProgress tip, JobConf jobConf) throws IOException{
+      synchronized (tip) {
+      try {
+        tip.setJobConf(jobConf);
+        tip.launchTask();
+      } catch (Throwable ie) {
+        tip.runstate = TaskStatus.FAILED;
+        try {
+          tip.cleanup();
+        } catch (Throwable ie2) {
+          // Ignore it, we are just trying to cleanup.
+        }
+        String error = StringUtils.stringifyException(ie);
+        tip.reportDiagnosticInfo(error);
+        LOG.info(error);
+      }
+      }
+     }
+    
+    public synchronized void shutdown() throws IOException {
           shuttingDown = true;
           close();
           if (this.server != null) {
@@ -312,6 +398,12 @@
         } catch (InterruptedException ie) {}
       }
     }
+    /**Return the DFS filesystem
+     * @return
+     */
+    public FileSystem getFileSystem(){
+      return fs;
+    }
     
     /**
      * Main service loop.  Will stay in this loop forever.
@@ -448,6 +540,10 @@
               synchronized (this) {
                 for (int i = 0; i < toCloseIds.length; i++) {
                   Object tip = tasks.get(toCloseIds[i]);
+                  synchronized(runningJobs){
+                    runningJobs.remove(((TaskInProgress)
+                	 	  tasks.get(toCloseIds[i])).getTask().getJobId());
+                  }
                   if (tip != null) {
                     tasksToCleanup.put(tip);
                   } else {
@@ -551,8 +647,8 @@
 
       return true;
     }
-
-	/**
+    
+    /**
      * Start a new task.
      * All exceptions are handled locally, so that we don't mess up the
      * task tracker.
@@ -569,20 +665,10 @@
           reduceTotal++;
         }
       }
-      synchronized (tip) {
-        try {
-          tip.launchTask();
-        } catch (Throwable ie) {
-          tip.runstate = TaskStatus.FAILED;
-          try {
-            tip.cleanup();
-          } catch (Throwable ie2) {
-            // Ignore it, we are just trying to cleanup.
-          }
-          String error = StringUtils.stringifyException(ie);
-          tip.reportDiagnosticInfo(error);
-          LOG.info(error);
-        }
+      try{
+    	  localizeJob(tip);
+      }catch(IOException ie){
+    	  LOG.warn(""Error initializing Job "" + tip.getTask().getJobId());
       }
     }
     
@@ -691,6 +777,7 @@
         private JobConf localJobConf;
         private boolean keepFailedTaskFiles;
         private boolean alwaysKeepTaskFiles;
+        private boolean keepJobFiles;
 
         /**
          */
@@ -702,60 +789,52 @@
             this.lastProgressReport = System.currentTimeMillis();
             this.defaultJobConf = conf;
             localJobConf = null;
+            keepJobFiles = false;
         }
-
-        /**
-         * Some fields in the Task object need to be made machine-specific.
-         * So here, edit the Task's fields appropriately.
-         */
-        private void localizeTask(Task t) throws IOException {
-            this.defaultJobConf.deleteLocalFiles(SUBDIR + ""/"" + 
-                                                 task.getTaskId());
-            Path localJobFile =
-              this.defaultJobConf.getLocalPath(SUBDIR+""/""+t.getTaskId()+""/""+""job.xml"");
-            Path localJarFile =
-              this.defaultJobConf.getLocalPath(SUBDIR+""/""+t.getTaskId()+""/""+""job.jar"");
-
-            String jobFile = t.getJobFile();
-            fs.copyToLocalFile(new Path(jobFile), localJobFile);
-            t.setJobFile(localJobFile.toString());
-
-            localJobConf = new JobConf(localJobFile);
-            localJobConf.set(""mapred.local.dir"",
-                    this.defaultJobConf.get(""mapred.local.dir""));
-            String jarFile = localJobConf.getJar();
-            if (jarFile != null) {
-              fs.copyToLocalFile(new Path(jarFile), localJarFile);
-              localJobConf.setJar(localJarFile.toString());
-            }
-            task.localizeConfiguration(localJobConf);
-
-            FileSystem localFs = FileSystem.getNamed(""local"", fConf);
-            OutputStream out = localFs.create(localJobFile);
-            try {
-              localJobConf.write(out);
-            } finally {
-              out.close();
-            }
-            // set the task's configuration to the local job conf
-            // rather than the default.
-            t.setConf(localJobConf);
-            keepFailedTaskFiles = localJobConf.getKeepFailedTaskFiles();
+        
+        private void localizeTask(Task task) throws IOException{
+            Path localTaskDir =
+              new Path(this.defaultJobConf.getLocalPath(SUBDIR+ Path.SEPARATOR
+                    + JOBCACHE + Path.SEPARATOR
+                    + task.getJobId()), task.getTaskId());
+           FileSystem localFs = FileSystem.getNamed(""local"", fConf);
+           localFs.mkdirs(localTaskDir);
+           Path localTaskFile = new Path(localTaskDir, ""job.xml"");
+           task.setJobFile(localTaskFile.toString());
+           localJobConf.set(""mapred.local.dir"",
+                    fConf.get(""mapred.local.dir""));
+            
+           localJobConf.set(""mapred.task.id"", task.getTaskId());
+           keepFailedTaskFiles = localJobConf.getKeepFailedTaskFiles();
+           task.localizeConfiguration(localJobConf);
+           OutputStream out = localFs.create(localTaskFile);
+           try {
+             localJobConf.write(out);
+           } finally {
+             out.close();
+           }
+            task.setConf(localJobConf);
             String keepPattern = localJobConf.getKeepTaskFilesPattern();
             if (keepPattern != null) {
-              alwaysKeepTaskFiles = 
+                keepJobFiles = true;
+                alwaysKeepTaskFiles = 
                 Pattern.matches(keepPattern, task.getTaskId());
             } else {
               alwaysKeepTaskFiles = false;
             }
         }
-
+        
         /**
          */
         public Task getTask() {
             return task;
         }
 
+        public void setJobConf(JobConf lconf){
+            this.localJobConf = lconf;
+            keepFailedTaskFiles = localJobConf.getKeepFailedTaskFiles();
+        }
+        
         /**
          */
         public synchronized TaskStatus createStatus() {
@@ -876,11 +955,18 @@
          * finished.  If the task is still running, kill it (and clean up
          */
         public synchronized void jobHasFinished() throws IOException {
+        	 
             if (getRunState() == TaskStatus.RUNNING) {
                 killAndCleanup(false);
             } else {
                 cleanup();
             }
+            if (keepJobFiles)
+              return;
+            // delete the job diretory for this task 
+            // since the job is done/failed
+            this.defaultJobConf.deleteLocalFiles(SUBDIR + Path.SEPARATOR + 
+                    JOBCACHE + Path.SEPARATOR +  task.getJobId());
         }
 
         /**
@@ -934,10 +1020,13 @@
                  }
                }
             }
-            this.defaultJobConf.deleteLocalFiles(SUBDIR + ""/"" + taskId);
-        }
+            this.defaultJobConf.deleteLocalFiles(SUBDIR + Path.SEPARATOR + 
+                    JOBCACHE + Path.SEPARATOR + task.getJobId() + Path.SEPARATOR +
+                    taskId);
+            }
     }
 
+    
     // ///////////////////////////////////////////////////////////////
     // TaskUmbilicalProtocol
     /////////////////////////////////////////////////////////////////
@@ -1035,6 +1124,16 @@
           LOG.warn(""Unknown child with bad map output: ""+taskid+"". Ignored."");
         }
     }
+    
+    /**
+     *  The datastructure for initializing a job
+     */
+    static class RunningJob{
+      Path jobFile;
+      // keep this for later use
+      ArrayList tasks;
+      boolean localized;
+    }
 
     /** 
      * The main() for child processes. 
"
hadoop,53f9f2ab3a1de8a663b237ad1c56189094ca1c04,"HADOOP-288.  Add a file caching system and use it in MapReduce to cache job jar files on slave nodes.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@443497 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-14 22:13:43,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/util/StringUtils.java b/src/java/org/apache/hadoop/util/StringUtils.java
index ca072ba..a00cba4 100644
--- a/src/java/org/apache/hadoop/util/StringUtils.java
+++ b/src/java/org/apache/hadoop/util/StringUtils.java
@@ -18,7 +18,10 @@
 
 import java.io.PrintWriter;
 import java.io.StringWriter;
+import java.net.URI;
+import java.net.URISyntaxException;
 import java.text.DecimalFormat;
+import org.apache.hadoop.fs.*;
 
 /**
  * General string utils
@@ -105,4 +108,81 @@
     }
     return sbuf.toString();
   }
+
+  /**
+   * Given an array of bytes it will convert the bytes to a hex string
+   * representation of the bytes
+   * @param bytes
+   * @return hex string representation of the byte array
+   */
+  public static String byteToHexString(byte bytes[]) {
+    StringBuffer retString = new StringBuffer();
+    for (int i = 0; i < bytes.length; ++i) {
+      retString.append(Integer.toHexString(0x0100 + (bytes[i] & 0x00FF))
+          .substring(1));
+    }
+    return retString.toString();
+  }
+
+  /**
+   * Given a hexstring this will return the byte array corresponding to the
+   * string
+   * @param hex the hex String array
+   * @return a byte array that is a hex string representation of the given
+   *         string. The size of the byte array is therefore hex.length/2
+   */
+  public static byte[] hexStringToByte(String hex) {
+    byte[] bts = new byte[hex.length() / 2];
+    for (int i = 0; i < bts.length; i++) {
+      bts[i] = (byte) Integer.parseInt(hex.substring(2 * i, 2 * i + 2), 16);
+    }
+    return bts;
+  }
+  /**
+   * 
+   * @param uris
+   * @return
+   */
+  public static String uriToString(URI[] uris){
+    String ret = null;
+    ret = uris[0].toString();
+    for(int i = 1; i < uris.length;i++){
+      ret = ret + "","" + uris[i].toString();
+    }
+    return ret;
+  }
+  
+  /**
+   * 
+   * @param str
+   * @return
+   */
+  public static URI[] stringToURI(String[] str){
+    if (str == null) 
+      return null;
+    URI[] uris = new URI[str.length];
+    for (int i = 0; i < str.length;i++){
+      try{
+        uris[i] = new URI(str[i]);
+      }catch(URISyntaxException ur){
+        System.out.println(""Exception in specified URI's "" + StringUtils.stringifyException(ur));
+        //making sure its asssigned to null in case of an error
+        uris[i] = null;
+      }
+    }
+    return uris;
+  }
+  
+  /**
+   * 
+   * @param str
+   * @return
+   */
+  public static Path[] stringToPath(String[] str){
+    Path[] p = new Path[str.length];
+    for (int i = 0; i < str.length;i++){
+      p[i] = new Path(str[i]);
+    }
+    return p;
+  }
 }
"
hadoop,68ad09db3868c7813fd3e9f8b71951f81de54e7a,"HADOOP-530.  Improve error messages in SequenceFile when keys or values are of the wrong type.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@443467 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-14 20:54:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/SequenceFile.java b/src/java/org/apache/hadoop/io/SequenceFile.java
index 5f76a2b..2deee24 100644
--- a/src/java/org/apache/hadoop/io/SequenceFile.java
+++ b/src/java/org/apache/hadoop/io/SequenceFile.java
@@ -528,9 +528,11 @@
     public synchronized void append(Writable key, Writable val)
       throws IOException {
       if (key.getClass() != keyClass)
-        throw new IOException(""wrong key class: ""+key+"" is not ""+keyClass);
+        throw new IOException(""wrong key class: ""+key.getClass().getName()
+            +"" is not ""+keyClass);
       if (val.getClass() != valClass)
-        throw new IOException(""wrong value class: ""+val+"" is not ""+valClass);
+        throw new IOException(""wrong value class: ""+val.getClass().getName()
+            +"" is not ""+valClass);
 
       buffer.reset();
 
@@ -643,9 +645,11 @@
     public synchronized void append(Writable key, Writable val)
       throws IOException {
       if (key.getClass() != keyClass)
-        throw new IOException(""wrong key class: ""+key+"" is not ""+keyClass);
+        throw new IOException(""wrong key class: ""+key.getClass().getName()
+            +"" is not ""+keyClass);
       if (val.getClass() != valClass)
-        throw new IOException(""wrong value class: ""+val+"" is not ""+valClass);
+        throw new IOException(""wrong value class: ""+val.getClass().getName()
+            +"" is not ""+valClass);
 
       buffer.reset();
 
@@ -1183,7 +1187,8 @@
      * value.  True if another entry exists, and false at end of file. */
     public synchronized boolean next(Writable key) throws IOException {
       if (key.getClass() != keyClass)
-        throw new IOException(""wrong key class: ""+key+"" is not ""+keyClass);
+        throw new IOException(""wrong key class: ""+key.getClass().getName()
+            +"" is not ""+keyClass);
 
       if (version < BLOCK_COMPRESS_VERSION || blockCompressed == false) {
         outBuf.reset();
"
hadoop,a68599d879c130855d24b26751f350115b1896cb,"HADOOP-438.  Limit the length of paths permitted by DFS.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@443455 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-14 20:16:19,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSConstants.java b/src/java/org/apache/hadoop/dfs/FSConstants.java
index c8734c6..84ec2c8 100644
--- a/src/java/org/apache/hadoop/dfs/FSConstants.java
+++ b/src/java/org/apache/hadoop/dfs/FSConstants.java
@@ -106,6 +106,11 @@
     public static long LEASE_PERIOD = 60 * 1000;
     public static int READ_TIMEOUT = 60 * 1000;
 
+    // We need to limit the length and depth of a path in the filesystem.  HADOOP-438
+    // Currently we set the maximum length to 8k characters and the maximum depth to 1k.  
+    public static int MAX_PATH_LENGTH = 8000;
+    public static int MAX_PATH_DEPTH = 1000;
+    
     //TODO mb@media-style.com: should be conf injected?
     public static final int BUFFER_SIZE = new Configuration().getInt(""io.file.buffer.size"", 4096);
 
"
hadoop,a68599d879c130855d24b26751f350115b1896cb,"HADOOP-438.  Limit the length of paths permitted by DFS.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@443455 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-14 20:16:19,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/NameNode.java b/src/java/org/apache/hadoop/dfs/NameNode.java
index 2cf5fb6..09c13cb 100644
--- a/src/java/org/apache/hadoop/dfs/NameNode.java
+++ b/src/java/org/apache/hadoop/dfs/NameNode.java
@@ -17,6 +17,7 @@
 
 import org.apache.commons.logging.*;
 
+import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.*;
 import org.apache.hadoop.ipc.*;
 import org.apache.hadoop.conf.*;
@@ -199,6 +200,10 @@
     ) throws IOException {
        stateChangeLog.debug(""*DIR* NameNode.create: file ""
             +src+"" for ""+clientName+"" at ""+clientMachine);
+       if (!checkPathLength(src)) {
+           throw new IOException(""create: Pathname too long.  Limit "" 
+               + MAX_PATH_LENGTH + "" characters, "" + MAX_PATH_DEPTH + "" levels."");
+       }
        Object results[] = namesystem.startFile(new UTF8(src), 
                                                 new UTF8(clientName), 
                                                 new UTF8(clientMachine), 
@@ -304,6 +309,10 @@
      */
     public boolean rename(String src, String dst) throws IOException {
         stateChangeLog.debug(""*DIR* NameNode.rename: "" + src + "" to "" + dst );
+        if (!checkPathLength(dst)) {
+            throw new IOException(""rename: Pathname too long.  Limit "" 
+                + MAX_PATH_LENGTH + "" characters, "" + MAX_PATH_DEPTH + "" levels."");
+        }
         boolean ret = namesystem.renameTo(new UTF8(src), new UTF8(dst));
         if (ret) {
             myMetrics.renameFile();
@@ -331,9 +340,25 @@
     }
 
     /**
+     * Check path length does not exceed maximum.  Returns true if
+     * length and depth are okay.  Returns false if length is too long 
+     * or depth is too great.
+     * 
+     */
+    private boolean checkPathLength(String src) {
+        Path srcPath = new Path(src);
+        return (src.length() <= MAX_PATH_LENGTH &&
+                srcPath.depth() <= MAX_PATH_DEPTH);
+    }
+    
+    /**
      */
     public boolean mkdirs(String src) throws IOException {
         stateChangeLog.debug(""*DIR* NameNode.mkdirs: "" + src );
+        if (!checkPathLength(src)) {
+            throw new IOException(""mkdirs: Pathname too long.  Limit "" 
+                + MAX_PATH_LENGTH + "" characters, "" + MAX_PATH_DEPTH + "" levels."");
+        }
         return namesystem.mkdirs(new UTF8(src));
     }
 
"
hadoop,a68599d879c130855d24b26751f350115b1896cb,"HADOOP-438.  Limit the length of paths permitted by DFS.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@443455 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-14 20:16:19,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/Path.java b/src/java/org/apache/hadoop/fs/Path.java
index 9cfbb24..7586bca 100644
--- a/src/java/org/apache/hadoop/fs/Path.java
+++ b/src/java/org/apache/hadoop/fs/Path.java
@@ -174,5 +174,11 @@
     Path that = (Path)o;
     return this.toString().compareTo(that.toString());
   }
+  
+  /** Return the number of elements in this path. */
+  public int depth() {
+    return elements.length;
+  }
 
 }
+
"
hadoop,d77b1b9ea93d2e758791c2b65598044d86ad55e7,"HADOOP-243.  Fix rounding in the display of task and job progress so that things are not shown as 100% complete until they are in fact finished.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@443452 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-14 19:59:08,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobClient.java b/src/java/org/apache/hadoop/mapred/JobClient.java
index 8876315..4840137 100644
--- a/src/java/org/apache/hadoop/mapred/JobClient.java
+++ b/src/java/org/apache/hadoop/mapred/JobClient.java
@@ -342,9 +342,10 @@
               break;
             }
             running = jc.getJob(jobId);
-            String report = ("" map "" + Math.round(running.mapProgress()*100)+
-                             ""%  reduce "" + 
-                             Math.round(running.reduceProgress()*100) + ""%"");
+            String report = 
+              ("" map "" + StringUtils.formatPercent(running.mapProgress())+
+               "" reduce "" + 
+               StringUtils.formatPercent(running.reduceProgress()));
             if (!report.equals(lastReport)) {
               LOG.info(report);
               lastReport = report;
@@ -402,7 +403,6 @@
         init();
         
         // Process args
-        String jobTrackerSpec = null;
         String submitJobFile = null;
         String jobid = null;
         boolean getStatus = false;
"
hadoop,d77b1b9ea93d2e758791c2b65598044d86ad55e7,"HADOOP-243.  Fix rounding in the display of task and job progress so that things are not shown as 100% complete until they are in fact finished.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@443452 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-14 19:59:08,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobInProgress.java b/src/java/org/apache/hadoop/mapred/JobInProgress.java
index 1ce689c..777e7df 100644
--- a/src/java/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/JobInProgress.java
@@ -293,13 +293,16 @@
           if (maps.length == 0) {
             this.status.setMapProgress(1.0f);
           } else {
-            this.status.mapProgress += (progressDelta / maps.length);
+            this.status.setMapProgress((float) (this.status.mapProgress() +
+                                                progressDelta / maps.length));
           }
         } else {
           if (reduces.length == 0) {
             this.status.setReduceProgress(1.0f);
           } else {
-            this.status.reduceProgress += (progressDelta / reduces.length);
+            this.status.setReduceProgress
+                 ((float) (this.status.reduceProgress() +
+                           (progressDelta / reduces.length)));
           }
         }
     }   
@@ -477,6 +480,9 @@
             }
         }
         if (allDone) {
+            if (tip.isMapTask()) {
+              this.status.setMapProgress(1.0f);              
+            }
             for (int i = 0; i < reduces.length; i++) {
                 if (! reduces[i].isComplete()) {
                     allDone = false;
@@ -489,7 +495,8 @@
         // If all tasks are complete, then the job is done!
         //
         if (status.getRunState() == JobStatus.RUNNING && allDone) {
-            this.status.runState = JobStatus.SUCCEEDED;
+            this.status.setRunState(JobStatus.SUCCEEDED);
+            this.status.setReduceProgress(1.0f);
             this.finishTime = System.currentTimeMillis();
             garbageCollect();
             LOG.info(""Job "" + this.status.getJobId() + 
"
hadoop,d77b1b9ea93d2e758791c2b65598044d86ad55e7,"HADOOP-243.  Fix rounding in the display of task and job progress so that things are not shown as 100% complete until they are in fact finished.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@443452 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-14 19:59:08,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobStatus.java b/src/java/org/apache/hadoop/mapred/JobStatus.java
index 9eb1e45..e2b0f6d 100644
--- a/src/java/org/apache/hadoop/mapred/JobStatus.java
+++ b/src/java/org/apache/hadoop/mapred/JobStatus.java
@@ -41,12 +41,12 @@
     public static final int FAILED = 3;
     public static final int PREP = 4;
 
-    String jobid;
-    float mapProgress;
-    float reduceProgress;
-    int runState;
-    long startTime;
-    String user;
+    private String jobid;
+    private float mapProgress;
+    private float reduceProgress;
+    private int runState;
+    private long startTime;
+    private String user;
     /**
      */
     public JobStatus() {
@@ -81,7 +81,10 @@
      * Sets the map progress of this job
      * @param p The value of map progress to set to
      */
-    void setMapProgress(float p) { this.mapProgress = p; }
+    void setMapProgress(float p) { 
+      this.mapProgress = (float) Math.min(1.0, Math.max(0.0, p)); 
+    
+    }
     
     /**
      * @return Percentage of progress in reduce 
@@ -92,13 +95,22 @@
      * Sets the reduce progress of this Job
      * @param p The value of reduce progress to set to
      */
-    void setReduceProgress(float p) { this.reduceProgress = p; }
+    void setReduceProgress(float p) { 
+      this.reduceProgress = (float) Math.min(1.0, Math.max(0.0, p)); 
+    }
     
     /**
      * @return running state of the job
      */
     public int getRunState() { return runState; }
     
+    /**
+     * Change the current run state of the job.
+     */
+    public void setRunState(int state) {
+      this.runState = state;
+    }
+    
     /** 
      * Set the start time of the job
      * @param startTime The startTime of the job
"
hadoop,d77b1b9ea93d2e758791c2b65598044d86ad55e7,"HADOOP-243.  Fix rounding in the display of task and job progress so that things are not shown as 100% complete until they are in fact finished.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@443452 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-14 19:59:08,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/LocalJobRunner.java b/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
index 02ede75..b7562a8 100644
--- a/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
+++ b/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
@@ -49,7 +49,7 @@
     private JobConf job;
     private Random random = new Random();
 
-    private JobStatus status = new JobStatus();
+    private JobStatus status;
     private ArrayList mapIds = new ArrayList();
     private MapOutputFile mapoutputFile;
     private JobProfile profile;
@@ -73,8 +73,7 @@
       this.job = new JobConf(localFile);
       profile = new JobProfile(job.getUser(), id, file, 
                                ""http://localhost:8080/"", job.getJobName());
-      this.status.jobid = id;
-      this.status.runState = JobStatus.RUNNING;
+      status = new JobStatus(id, 0.0f, 0.0f, JobStatus.RUNNING);
 
       jobs.put(id, this);
 
@@ -134,10 +133,10 @@
         }
         this.mapoutputFile.removeAll(reduceId);
         
-        this.status.runState = JobStatus.SUCCEEDED;
+        this.status.setRunState(JobStatus.SUCCEEDED);
 
       } catch (Throwable t) {
-        this.status.runState = JobStatus.FAILED;
+        this.status.setRunState(JobStatus.FAILED);
         LOG.warn(id, t);
 
       } finally {
@@ -163,9 +162,9 @@
       float taskIndex = mapIds.indexOf(taskId);
       if (taskIndex >= 0) {                       // mapping
         float numTasks = mapIds.size();
-        status.mapProgress = (taskIndex/numTasks)+(progress/numTasks);
+        status.setMapProgress(taskIndex/numTasks + progress/numTasks);
       } else {
-        status.reduceProgress = progress;
+        status.setReduceProgress(progress);
       }
     }
 
@@ -180,9 +179,9 @@
     public void done(String taskId) throws IOException {
       int taskIndex = mapIds.indexOf(taskId);
       if (taskIndex >= 0) {                       // mapping
-        status.mapProgress = 1.0f;
+        status.setMapProgress(1.0f);
       } else {
-        status.reduceProgress = 1.0f;
+        status.setReduceProgress(1.0f);
       }
     }
 
"
hadoop,d77b1b9ea93d2e758791c2b65598044d86ad55e7,"HADOOP-243.  Fix rounding in the display of task and job progress so that things are not shown as 100% complete until they are in fact finished.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@443452 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-14 19:59:08,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/util/StringUtils.java b/src/java/org/apache/hadoop/util/StringUtils.java
index bdc4ab9..ca072ba 100644
--- a/src/java/org/apache/hadoop/util/StringUtils.java
+++ b/src/java/org/apache/hadoop/util/StringUtils.java
@@ -52,7 +52,7 @@
     return fullHostname;
   }
 
-  private static DecimalFormat numFormat = new DecimalFormat(""0.0"");
+  private static DecimalFormat oneDecimal = new DecimalFormat(""0.0"");
   
   /**
    * Given an integer, return a string that is in an approximate, but human 
@@ -77,7 +77,15 @@
       result = number / (1024.0 * 1024 * 1024);
       suffix = ""g"";
     }
-    return numFormat.format(result) + suffix;
+    return oneDecimal.format(result) + suffix;
+  }
+  
+  private static DecimalFormat percentFormat = new DecimalFormat(""0.00%"");
+  
+  public static String formatPercent(double done) {
+    final int scale = 10000;
+    double rounded = Math.floor(done * scale);
+    return percentFormat.format(rounded / scale);
   }
   
   /**
"
hadoop,d77b1b9ea93d2e758791c2b65598044d86ad55e7,"HADOOP-243.  Fix rounding in the display of task and job progress so that things are not shown as 100% complete until they are in fact finished.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@443452 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-14 19:59:08,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/util/ToolBase.java b/src/java/org/apache/hadoop/util/ToolBase.java
index 49a9276..5bffd11 100644
--- a/src/java/org/apache/hadoop/util/ToolBase.java
+++ b/src/java/org/apache/hadoop/util/ToolBase.java
@@ -80,7 +80,7 @@
  *
  */
 public abstract class ToolBase implements Tool {
-    public static final Log LOG = LogFactory.getLog(
+    private static final Log LOG = LogFactory.getLog(
             ""org.apache.hadoop.util.ToolBase"");
     public Configuration conf;
 
"
hadoop,a74171eeb6b9b6f49787f11c46b0e03fa49293e3,"HADOOP-529.  Fix a NullPointerException when opening value-compressed sequence files generated by pre-0.6.0 Hadoop.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@443026 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-13 17:07:30,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/SequenceFile.java b/src/java/org/apache/hadoop/io/SequenceFile.java
index 1e39a18..5f76a2b 100644
--- a/src/java/org/apache/hadoop/io/SequenceFile.java
+++ b/src/java/org/apache/hadoop/io/SequenceFile.java
@@ -980,14 +980,19 @@
       
       // if version >= 5
       // setup the compression codec
-      if (version >= CUSTOM_COMPRESS_VERSION && this.decompress) {    
-        try {
-          this.codec = (CompressionCodec)
-          ReflectionUtils.newInstance(conf.getClassByName(Text.readString(in)),
-              conf);
-        } catch (ClassNotFoundException cnfe) {
-          cnfe.printStackTrace();
-          throw new IllegalArgumentException(""Unknown codec: "" + cnfe);
+      if (decompress) {
+        if (version >= CUSTOM_COMPRESS_VERSION) {
+          String codecClassname = Text.readString(in);
+          try {
+            Class codecClass = conf.getClassByName(codecClassname);
+            this.codec = (CompressionCodec)
+                 ReflectionUtils.newInstance(codecClass, conf);
+          } catch (ClassNotFoundException cnfe) {
+            throw new IllegalArgumentException(""Unknown codec: "" + 
+                                               codecClassname, cnfe);
+          }
+        } else {
+          codec = new DefaultCodec();
         }
       }
       
"
hadoop,5acaf70e82847fbfe4ae5b4ab8ec051f60829ab7,"HADOOP-526.  Fix a NullPointerException when attempting to start two datanodes in the same directory.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@442759 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-12 23:47:56,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataStorage.java b/src/java/org/apache/hadoop/dfs/DataStorage.java
index d011bbc..e5958db 100644
--- a/src/java/org/apache/hadoop/dfs/DataStorage.java
+++ b/src/java/org/apache/hadoop/dfs/DataStorage.java
@@ -35,6 +35,9 @@
   private ArrayList storageFiles = new ArrayList();
   private ArrayList storageLocks = new ArrayList();
   
+  // cache away the names of all passed in dirs
+  private File[] origDirs = null;
+  
   // cache away the names of locked dirs
   private File[] dirs = null;
   
@@ -65,6 +68,7 @@
    */
   public DataStorage( int curVersion, File[] dataDirs ) throws IOException {
     this.version = curVersion;
+    this.origDirs = dataDirs;
     for (int idx = 0; idx < dataDirs.length; idx++) {
       storageFiles.add(idx, new RandomAccessFile( 
                           new File(dataDirs[idx], STORAGE_INFO_FILE_NAME ), 
@@ -129,7 +133,7 @@
     FileLock lock = file.getChannel().tryLock();
     if (lock == null) {
       // log a warning
-      LOG.warn(""Cannot lock storage file in directory ""+dirs[idx].getName());
+      LOG.warn(""Cannot lock storage file in directory ""+origDirs[idx].getName());
       // remove the file from fileList, and close it
       storageFiles.add(idx, null);
       file.close();
"
hadoop,cde1723df82064fdb6daa8fb0117e015bcc4c47c,"HADOOP-521.  Fix another NullPointerException finding the ClassLoader when using libhdfs.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@442689 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-12 19:57:12,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/ObjectWritable.java b/src/java/org/apache/hadoop/io/ObjectWritable.java
index ffb2bf0..b774f31 100644
--- a/src/java/org/apache/hadoop/io/ObjectWritable.java
+++ b/src/java/org/apache/hadoop/io/ObjectWritable.java
@@ -216,9 +216,7 @@
     } else {                                      // Writable
       Class instanceClass = null;
       try {
-        instanceClass = 
-          Class.forName(UTF8.readString(in), true, 
-                        Thread.currentThread().getContextClassLoader());
+        instanceClass = conf.getClassByName(UTF8.readString(in));
       } catch (ClassNotFoundException e) {
         throw new RuntimeException(""readObject can't find class"", e);
       }
"
hadoop,52969c276f0b4d3db619d1c312b56260e46d72dc,"HADOOP-523.  Fix a NullPointerException when TextInputFormat is explicitly specified.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@442673 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-12 18:59:44,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobInProgress.java b/src/java/org/apache/hadoop/mapred/JobInProgress.java
index 00ce4f4..1ce689c 100644
--- a/src/java/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/JobInProgress.java
@@ -109,24 +109,15 @@
         //
         String jobFile = profile.getJobFile();
 
-        JobConf jd = new JobConf(localJobFile);
         FileSystem fs = FileSystem.get(conf);
-        String ifClassName = jd.get(""mapred.input.format.class"");
-        InputFormat inputFormat;
-        if (ifClassName != null && localJarFile != null) {
-          try {
+        if (localJarFile != null) {
             ClassLoader loader =
               new URLClassLoader(new URL[]{ localFs.pathToFile(localJarFile).toURL() });
-            Class inputFormatClass = Class.forName(ifClassName, true, loader);
-            inputFormat = (InputFormat)inputFormatClass.newInstance();
-          } catch (Exception e) {
-            throw new IOException(e.toString());
-          }
-        } else {
-          inputFormat = jd.getInputFormat();
+            conf.setClassLoader(loader);
         }
+        InputFormat inputFormat = conf.getInputFormat();
 
-        FileSplit[] splits = inputFormat.getSplits(fs, jd, numMapTasks);
+        FileSplit[] splits = inputFormat.getSplits(fs, conf, numMapTasks);
 
         //
         // sort splits by decreasing length, to reduce job's tail
"
hadoop,21b8d315fe41865e67acb8395e6ed18d732100d9,"HADOOP-517.  Fix bug in contrib/streaming's end of line detection.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@441645 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-08 20:45:03,Doug Cutting,"diff --git a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java
index ba46003..f083095 100644
--- a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java
+++ b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java
@@ -397,7 +397,7 @@
             Text key = new Text();
             Text val = new Text();
             // 3/4 Tool to Hadoop
-            while((answer=UTF8ByteArrayUtils.readLine(clientIn_))!= null) {
+            while((answer=UTF8ByteArrayUtils.readLine((InputStream)clientIn_))!= null) {
                 // 4/4 Hadoop out
                 if(optSideEffect_) {
                     sideEffectOut_.write(answer);
@@ -434,7 +434,7 @@
       byte [] line;
       try {
         long num = 0;
-        while((line=UTF8ByteArrayUtils.readLine(clientErr_)) != null) {
+        while((line=UTF8ByteArrayUtils.readLine((InputStream)clientErr_)) != null) {
           num++;
           String lineStr = new String(line, ""UTF-8""); 
           logprintln(lineStr);
"
hadoop,21b8d315fe41865e67acb8395e6ed18d732100d9,"HADOOP-517.  Fix bug in contrib/streaming's end of line detection.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@441645 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-08 20:45:03,Doug Cutting,"diff --git a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamLineRecordReader.java b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamLineRecordReader.java
index e4d08da..6c85ea3 100644
--- a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamLineRecordReader.java
+++ b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamLineRecordReader.java
@@ -99,7 +99,7 @@
                 return false;
         }
         
-        line = UTF8ByteArrayUtils.readLine(in_);
+        line = UTF8ByteArrayUtils.readLine((InputStream)in_);
         if(line==null)
             return false;
         try {
"
hadoop,21b8d315fe41865e67acb8395e6ed18d732100d9,"HADOOP-517.  Fix bug in contrib/streaming's end of line detection.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@441645 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-08 20:45:03,Doug Cutting,"diff --git a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/UTF8ByteArrayUtils.java b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/UTF8ByteArrayUtils.java
index b707e51..12bb71a 100644
--- a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/UTF8ByteArrayUtils.java
+++ b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/UTF8ByteArrayUtils.java
@@ -16,8 +16,9 @@
 
 package org.apache.hadoop.streaming;
 
-import java.io.DataInputStream;
 import java.io.IOException;
+import java.io.InputStream;
+import java.io.PushbackInputStream;
 
 import org.apache.hadoop.io.Text;
 
@@ -70,7 +71,7 @@
      * @return a byte array containing the line 
      * @throws IOException
      */
-    public static byte[] readLine(DataInputStream in) throws IOException {
+    public static byte[] readLine(InputStream in) throws IOException {
       byte [] buf = new byte[128];
       byte [] lineBuffer = buf;
       int room = 128;
@@ -84,9 +85,23 @@
         }
 
         char c = (char)b;
-        if (c == '\r' || c == '\n')
+        if (c == '\n')
           break;
 
+        if (c == '\r') {
+          in.mark(1);
+          int c2 = in.read();
+          if(c2 == -1) {
+              isEOF = true;
+              break;
+          }
+          if (c2 != '\n') {
+            // push it back
+            in.reset();
+          }
+          break;
+        }
+        
         if (--room < 0) {
             buf = new byte[offset + 128];
             room = buf.length - offset - 1;
"
hadoop,271c118817891fe701eb29645f9936adf69ff09a,"HADOOP-286.  Avoid pinging the NameNode with renewLease() calls when no files are being written.  Contributed by Konstantin.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@440892 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-06 22:39:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSClient.java b/src/java/org/apache/hadoop/dfs/DFSClient.java
index 9509007..17b20ac 100644
--- a/src/java/org/apache/hadoop/dfs/DFSClient.java
+++ b/src/java/org/apache/hadoop/dfs/DFSClient.java
@@ -436,8 +436,9 @@
             while (running) {
                 if (System.currentTimeMillis() - lastRenewed > (LEASE_PERIOD / 2)) {
                     try {
+                      if( pendingCreates.size() > 0 )
                         namenode.renewLease(clientName);
-                        lastRenewed = System.currentTimeMillis();
+                      lastRenewed = System.currentTimeMillis();
                     } catch (IOException ie) {
                       String err = StringUtils.stringifyException(ie);
                       LOG.warn(""Problem renewing lease for "" + clientName +
"
hadoop,7fcc24c3fa9c5ab80aa28700a07a1cfb2f482b82,"HADOOP-320.  Fix so that checksum files are correctly copied when the destination of a file copy is a directory.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@440879 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-06 21:32:12,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FileSystem.java b/src/java/org/apache/hadoop/fs/FileSystem.java
index 4f4aa28..3446df1 100644
--- a/src/java/org/apache/hadoop/fs/FileSystem.java
+++ b/src/java/org/apache/hadoop/fs/FileSystem.java
@@ -388,10 +388,17 @@
       } else {
 
         boolean value = renameRaw(src, dst);
+        if (!value)
+          return false;
 
         Path checkFile = getChecksumFile(src);
-        if (exists(checkFile))
-          renameRaw(checkFile, getChecksumFile(dst)); // try to rename checksum
+        if (exists(checkFile)) { //try to rename checksum
+          if(isDirectory(dst)) {
+            renameRaw(checkFile, dst);
+          } else {
+            renameRaw(checkFile, getChecksumFile(dst)); 
+          }
+        }
 
         return value;
       }
"
hadoop,9c0c6ce04e93455159ae0e11ae10ecf6b568b335,"HADOOP-507.  Fix an IllegalAccessException in DFS.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@440866 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-06 20:56:51,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/ObjectWritable.java b/src/java/org/apache/hadoop/io/ObjectWritable.java
index 63f4dc7..ffb2bf0 100644
--- a/src/java/org/apache/hadoop/io/ObjectWritable.java
+++ b/src/java/org/apache/hadoop/io/ObjectWritable.java
@@ -223,10 +223,7 @@
         throw new RuntimeException(""readObject can't find class"", e);
       }
       
-      Writable writable = WritableFactories.newInstance(instanceClass);
-      if(writable instanceof Configurable) {
-        ((Configurable) writable).setConf(conf);
-      }
+      Writable writable = WritableFactories.newInstance(instanceClass, conf);
       writable.readFields(in);
       instance = writable;
 
"
hadoop,9c0c6ce04e93455159ae0e11ae10ecf6b568b335,"HADOOP-507.  Fix an IllegalAccessException in DFS.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@440866 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-06 20:56:51,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/WritableFactories.java b/src/java/org/apache/hadoop/io/WritableFactories.java
index ea1f3ea..140f261 100644
--- a/src/java/org/apache/hadoop/io/WritableFactories.java
+++ b/src/java/org/apache/hadoop/io/WritableFactories.java
@@ -16,6 +16,8 @@
 
 package org.apache.hadoop.io;
 
+import org.apache.hadoop.conf.*;
+import org.apache.hadoop.util.ReflectionUtils;
 import java.util.HashMap;
 
 /** Factories for non-public writables.  Defining a factory permits {@link
@@ -36,20 +38,23 @@
   }
 
   /** Create a new instance of a class with a defined factory. */
-  public static Writable newInstance(Class c) {
+  public static Writable newInstance(Class c, Configuration conf) {
     WritableFactory factory = WritableFactories.getFactory(c);
     if (factory != null) {
-      return factory.newInstance();
-    } else {
-      try {
-        return (Writable)c.newInstance();
-      } catch (InstantiationException e) {
-        throw new RuntimeException(e);
-      } catch (IllegalAccessException e) {
-        throw new RuntimeException(e);
+      Writable result = factory.newInstance();
+      if (result instanceof Configurable) {
+        ((Configurable) result).setConf(conf);
       }
+      return result;
+    } else {
+      return (Writable)ReflectionUtils.newInstance(c, conf);
     }
   }
+  
+  /** Create a new instance of a class with a defined factory. */
+  public static Writable newInstance(Class c) {
+    return newInstance(c, null);
+  }
 
 }
 
"
hadoop,e80eed6345e24bf803f932c8f151cdaddc3c6675,"HADOOP-424.  Fix MapReduce so that jobs with zero splits do not fail.  Contributed by Frdric Bertin.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@440826 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-06 18:45:26,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobInProgress.java b/src/java/org/apache/hadoop/mapred/JobInProgress.java
index 949142b..00ce4f4 100644
--- a/src/java/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/JobInProgress.java
@@ -143,6 +143,14 @@
         // adjust number of map tasks to actual number of splits
         //
         this.numMapTasks = splits.length;
+        
+        // if no split is returned, job is considered completed and successful
+        if (numMapTasks == 0) {
+            this.status = new JobStatus(status.getJobId(), 1.0f, 1.0f, JobStatus.SUCCEEDED);
+            tasksInited = true;
+            return;
+        }
+        
         // create a map task for each split
         this.maps = new TaskInProgress[numMapTasks];
         for (int i = 0; i < numMapTasks; i++) {
"
hadoop,dced04bb17fc184bd658c1471d2ed2e383b9bc8f,"HADOOP-501.  Fix Configuration.toString() to handle URL resources.  Contributed by Thomas Friol.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@440444 13f79535-47bb-0310-9956-ffa450edef68
",2006-09-05 19:11:03,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/conf/Configuration.java b/src/java/org/apache/hadoop/conf/Configuration.java
index 4421b2f..dbe08f5 100644
--- a/src/java/org/apache/hadoop/conf/Configuration.java
+++ b/src/java/org/apache/hadoop/conf/Configuration.java
@@ -625,12 +625,7 @@
       if (i.nextIndex() != 0) {
         sb.append("" , "");
       }
-      Object obj = i.next();
-      if (obj instanceof Path) {
-        sb.append((Path)obj);
-      } else {
-        sb.append((String)obj);
-      }
+      sb.append(i.next());
     }
   }
 
"
hadoop,2ee19084169dc6e4fb01bc3971a6c2b17bc99276,"HADOOP-196.  Fix Configuration(Configuration) constructor to work correctly.  Contributed by Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@439046 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-31 21:01:05,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/conf/Configuration.java b/src/java/org/apache/hadoop/conf/Configuration.java
index 724bdda..4421b2f 100644
--- a/src/java/org/apache/hadoop/conf/Configuration.java
+++ b/src/java/org/apache/hadoop/conf/Configuration.java
@@ -75,6 +75,7 @@
   private ArrayList finalResources = new ArrayList();
 
   private Properties properties;
+  private Properties overlay;
   private ClassLoader classLoader;
   {
     classLoader = Thread.currentThread().getContextClassLoader();
@@ -102,6 +103,8 @@
     this.finalResources = (ArrayList)other.finalResources.clone();
     if (other.properties != null)
       this.properties = (Properties)other.properties.clone();
+    if(other.overlay!=null)
+      this.overlay = (Properties)other.overlay.clone();
   }
 
   /** Add a default resource. */
@@ -197,9 +200,17 @@
 
   /** Sets the value of the <code>name</code> property. */
   public void set(String name, Object value) {
+    getOverlay().setProperty(name, value.toString());
     getProps().setProperty(name, value.toString());
   }
   
+  private synchronized Properties getOverlay() {
+    if(overlay==null){
+      overlay=new Properties();
+    }
+    return overlay;
+  }
+
   /** Returns the value of the <code>name</code> property.  If no such property
    * exists, then <code>defaultValue</code> is returned.
    */
@@ -446,6 +457,8 @@
       loadResources(newProps, defaultResources, false, false);
       loadResources(newProps, finalResources, true, true);
       properties = newProps;
+      if(overlay!=null)
+        properties.putAll(overlay);
     }
     return properties;
   }
"
hadoop,0f7496653cfcdb657c16910f54a5bb7eba7b3ae2,"HADOOP-460.  Fix contrib/smallJobsBenchmark to use Text instead of UTF8.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@438242 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-29 21:50:09,Doug Cutting,"diff --git a/src/contrib/smallJobsBenchmark/src/java/org/apache/hadoop/benchmarks/mapred/BenchmarkMapper.java b/src/contrib/smallJobsBenchmark/src/java/org/apache/hadoop/benchmarks/mapred/BenchmarkMapper.java
index 08a72f5..d2838ef 100644
--- a/src/contrib/smallJobsBenchmark/src/java/org/apache/hadoop/benchmarks/mapred/BenchmarkMapper.java
+++ b/src/contrib/smallJobsBenchmark/src/java/org/apache/hadoop/benchmarks/mapred/BenchmarkMapper.java
@@ -23,7 +23,7 @@
   public void map(WritableComparable key, Writable value,
       OutputCollector output, Reporter reporter) throws IOException {
     
-    String line = ((UTF8)value).toString();
+    String line = value.toString();
     output.collect(new UTF8(process(line)), new UTF8(""""));		
   }
   
"
hadoop,0f7496653cfcdb657c16910f54a5bb7eba7b3ae2,"HADOOP-460.  Fix contrib/smallJobsBenchmark to use Text instead of UTF8.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@438242 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-29 21:50:09,Doug Cutting,"diff --git a/src/contrib/smallJobsBenchmark/src/java/org/apache/hadoop/benchmarks/mapred/MultiJobRunner.java b/src/contrib/smallJobsBenchmark/src/java/org/apache/hadoop/benchmarks/mapred/MultiJobRunner.java
index 1d78160..5b5310b 100644
--- a/src/contrib/smallJobsBenchmark/src/java/org/apache/hadoop/benchmarks/mapred/MultiJobRunner.java
+++ b/src/contrib/smallJobsBenchmark/src/java/org/apache/hadoop/benchmarks/mapred/MultiJobRunner.java
@@ -345,7 +345,7 @@
     int numMaps = 2; 
     int numReduces = 1 ; 
     int dataLines = 1 ; 
-    int inputType = GenData.RANDOM ; 
+    int inputType = GenData.ASCENDING ; 
     boolean ignoreOutput = false ; 
     boolean verbose = false ; 
     
"
hadoop,8689fba665bd629dcb3c630dec1b4cb364715116,"HADOOP-419.  Fix a NullPointerException finding the ClassLoader when using libhdfs.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@438143 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-29 18:23:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/conf/Configuration.java b/src/java/org/apache/hadoop/conf/Configuration.java
index 7f3875e..724bdda 100644
--- a/src/java/org/apache/hadoop/conf/Configuration.java
+++ b/src/java/org/apache/hadoop/conf/Configuration.java
@@ -75,9 +75,14 @@
   private ArrayList finalResources = new ArrayList();
 
   private Properties properties;
-  private ClassLoader classLoader = 
-    Thread.currentThread().getContextClassLoader();
-
+  private ClassLoader classLoader;
+  {
+    classLoader = Thread.currentThread().getContextClassLoader();
+    if (classLoader == null) {
+      classLoader = Configuration.class.getClassLoader();
+    }
+  }
+  
   /** A new configuration. */
   public Configuration() {
     if (LOG.isDebugEnabled()) {
@@ -293,6 +298,16 @@
     return (String[])values.toArray(new String[values.size()]);
   }
 
+  /**
+   * Load a class by name.
+   * @param name the class name
+   * @return the class object
+   * @throws ClassNotFoundException if the class is not found
+   */
+  public Class getClassByName(String name) throws ClassNotFoundException {
+    return Class.forName(name, true, classLoader);
+  }
+  
   /** Returns the value of the <code>name</code> property as a Class.  If no
    * such property is specified, then <code>defaultValue</code> is returned.
    */
@@ -301,7 +316,7 @@
     if (valueString == null)
       return defaultValue;
     try {
-      return Class.forName(valueString, true, classLoader);
+      return getClassByName(valueString);
     } catch (ClassNotFoundException e) {
       throw new RuntimeException(e);
     }
@@ -566,6 +581,14 @@
   }
 
   /**
+   * Get the class loader for this job.
+   * @return the correct class loader
+   */
+  public ClassLoader getClassLoader() {
+    return classLoader;
+  }
+  
+  /**
    * Set the class loader that will be used to load the various objects.
    * @param classLoader the new class loader
    */
"
hadoop,8689fba665bd629dcb3c630dec1b4cb364715116,"HADOOP-419.  Fix a NullPointerException finding the ClassLoader when using libhdfs.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@438143 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-29 18:23:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/ObjectWritable.java b/src/java/org/apache/hadoop/io/ObjectWritable.java
index 1e0583c..63f4dc7 100644
--- a/src/java/org/apache/hadoop/io/ObjectWritable.java
+++ b/src/java/org/apache/hadoop/io/ObjectWritable.java
@@ -21,8 +21,7 @@
 import java.io.*;
 import java.util.*;
 
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.conf.Configurable;
+import org.apache.hadoop.conf.*;
 
 /** A polymorphic Writable that writes an instance with it's class name.
  * Handles arrays, strings and primitive types without a Writable wrapper.
@@ -61,7 +60,7 @@
   }
   
   public void write(DataOutput out) throws IOException {
-    writeObject(out, instance, declaredClass);
+    writeObject(out, instance, declaredClass, conf);
   }
 
   private static final Map PRIMITIVE_NAMES = new HashMap();
@@ -77,10 +76,11 @@
     PRIMITIVE_NAMES.put(""void"", Void.TYPE);
   }
 
-  private static class NullInstance implements Writable {
+  private static class NullInstance extends Configured implements Writable {
     private Class declaredClass;
-    public NullInstance() {}
-    public NullInstance(Class declaredClass) {
+    public NullInstance() { super(null); }
+    public NullInstance(Class declaredClass, Configuration conf) {
+      super(conf);
       this.declaredClass = declaredClass;
     }
     public void readFields(DataInput in) throws IOException {
@@ -88,8 +88,7 @@
       declaredClass = (Class)PRIMITIVE_NAMES.get(className);
       if (declaredClass == null) {
         try {
-          declaredClass =
-            Thread.currentThread().getContextClassLoader().loadClass(className);
+          declaredClass = getConf().getClassByName(className);
         } catch (ClassNotFoundException e) {
           throw new RuntimeException(e.toString());
         }
@@ -103,10 +102,11 @@
   /** Write a {@link Writable}, {@link String}, primitive type, or an array of
    * the preceding. */
   public static void writeObject(DataOutput out, Object instance,
-                                 Class declaredClass) throws IOException {
+                                 Class declaredClass, 
+                                 Configuration conf) throws IOException {
 
     if (instance == null) {                       // null
-      instance = new NullInstance(declaredClass);
+      instance = new NullInstance(declaredClass, conf);
       declaredClass = Writable.class;
     }
 
@@ -117,7 +117,7 @@
       out.writeInt(length);
       for (int i = 0; i < length; i++) {
         writeObject(out, Array.get(instance, i),
-                    declaredClass.getComponentType());
+                    declaredClass.getComponentType(), conf);
       }
       
     } else if (declaredClass == String.class) {   // String
@@ -171,9 +171,7 @@
     Class declaredClass = (Class)PRIMITIVE_NAMES.get(className);
     if (declaredClass == null) {
       try {
-        declaredClass =
-          Class.forName(className, true, 
-                        Thread.currentThread().getContextClassLoader());
+        declaredClass = conf.getClassByName(className);
       } catch (ClassNotFoundException e) {
         throw new RuntimeException(""readObject can't find class"", e);
       }
"
hadoop,8689fba665bd629dcb3c630dec1b4cb364715116,"HADOOP-419.  Fix a NullPointerException finding the ClassLoader when using libhdfs.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@438143 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-29 18:23:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/SequenceFile.java b/src/java/org/apache/hadoop/io/SequenceFile.java
index 4d17311..2972955 100644
--- a/src/java/org/apache/hadoop/io/SequenceFile.java
+++ b/src/java/org/apache/hadoop/io/SequenceFile.java
@@ -853,13 +853,13 @@
         UTF8 className = new UTF8();
         
         className.readFields(in);                   // read key class name
-        this.keyClass = WritableName.getClass(className.toString());
+        this.keyClass = WritableName.getClass(className.toString(), conf);
         
         className.readFields(in);                   // read val class name
-        this.valClass = WritableName.getClass(className.toString());
+        this.valClass = WritableName.getClass(className.toString(), conf);
       } else {
-        this.keyClass = WritableName.getClass(Text.readString(in));
-        this.valClass = WritableName.getClass(Text.readString(in));
+        this.keyClass = WritableName.getClass(Text.readString(in), conf);
+        this.valClass = WritableName.getClass(Text.readString(in), conf);
       }
 
       if (version > 2) {                          // if version > 2
"
hadoop,8689fba665bd629dcb3c630dec1b4cb364715116,"HADOOP-419.  Fix a NullPointerException finding the ClassLoader when using libhdfs.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@438143 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-29 18:23:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/WritableName.java b/src/java/org/apache/hadoop/io/WritableName.java
index e8cc119..d45beff 100644
--- a/src/java/org/apache/hadoop/io/WritableName.java
+++ b/src/java/org/apache/hadoop/io/WritableName.java
@@ -19,6 +19,8 @@
 import java.util.HashMap;
 import java.io.IOException;
 
+import org.apache.hadoop.conf.Configuration;
+
 /** Utility to permit renaming of Writable implementation classes without
  * invalidiating files that contain their class name.
  * @author Doug Cutting
@@ -57,13 +59,14 @@
   }
 
   /** Return the class for a name.  Default is {@link Class#forName(String)}.*/
-  public static synchronized Class getClass(String name) throws IOException {
+  public static synchronized Class getClass(String name,
+                                            Configuration conf
+                                            ) throws IOException {
     Class writableClass = (Class)NAME_TO_CLASS.get(name);
     if (writableClass != null)
       return writableClass;
     try {
-      return Class.forName(name, true, 
-                           Thread.currentThread().getContextClassLoader());
+      return conf.getClassByName(name);
     } catch (ClassNotFoundException e) {
       IOException newE = new IOException(""WritableName can't load class"");
       newE.initCause(e);
"
hadoop,8689fba665bd629dcb3c630dec1b4cb364715116,"HADOOP-419.  Fix a NullPointerException finding the ClassLoader when using libhdfs.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@438143 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-29 18:23:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/RPC.java b/src/java/org/apache/hadoop/ipc/RPC.java
index cc1d630..5f5a294 100644
--- a/src/java/org/apache/hadoop/ipc/RPC.java
+++ b/src/java/org/apache/hadoop/ipc/RPC.java
@@ -97,7 +97,8 @@
       UTF8.writeString(out, methodName);
       out.writeInt(parameterClasses.length);
       for (int i = 0; i < parameterClasses.length; i++) {
-        ObjectWritable.writeObject(out, parameters[i], parameterClasses[i]);
+        ObjectWritable.writeObject(out, parameters[i], parameterClasses[i],
+                                   conf);
       }
     }
 
"
hadoop,8689fba665bd629dcb3c630dec1b4cb364715116,"HADOOP-419.  Fix a NullPointerException finding the ClassLoader when using libhdfs.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@438143 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-29 18:23:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/util/ReflectionUtils.java b/src/java/org/apache/hadoop/util/ReflectionUtils.java
index 06d69fa..9ba54e8 100644
--- a/src/java/org/apache/hadoop/util/ReflectionUtils.java
+++ b/src/java/org/apache/hadoop/util/ReflectionUtils.java
@@ -40,7 +40,7 @@
         try {
             Constructor meth = theClass.getDeclaredConstructor(emptyArray);
             meth.setAccessible(true);
-            result = meth.newInstance(emptyArray);
+            result = meth.newInstance();
         } catch (Exception e) {
             throw new RuntimeException(e);
         }
"
hadoop,9df31b43da16cb3bc21abab3395faef02ad56f1b,"HADOOP-281.  Prohibit DFS files that are also directories.  Contributed by Wendy.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@437821 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-28 20:41:12,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSDirectory.java b/src/java/org/apache/hadoop/dfs/FSDirectory.java
index 9fab5e5..71d622d 100644
--- a/src/java/org/apache/hadoop/dfs/FSDirectory.java
+++ b/src/java/org/apache/hadoop/dfs/FSDirectory.java
@@ -184,6 +184,10 @@
               throw new FileNotFoundException(
                       ""Parent path does not exist: ""+path);
           }
+          if (!parentNode.isDir()) {
+        	  throw new FileNotFoundException(
+        			  ""Parent path is not a directory: ""+path);
+          }
            // check whether the parent already has a node with that name
           String name = newNode.name = target.getName();
           if( parentNode.getChild( name ) != null )
"
hadoop,4585c5af5b8e7a0f6228419d75508f10f5eeb60d,"HADOOP-473.  Fix TextInputFormat to correctly handle more EOL formats.  Contributed by Dennis Kubes & James White.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@436916 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-25 19:31:41,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TextInputFormat.java b/src/java/org/apache/hadoop/mapred/TextInputFormat.java
index 54a6ae7..be23913 100644
--- a/src/java/org/apache/hadoop/mapred/TextInputFormat.java
+++ b/src/java/org/apache/hadoop/mapred/TextInputFormat.java
@@ -44,7 +44,16 @@
       in.seek(start-1);
       while (in.getPos() < end) {    // scan to the next newline in the file
         char c = (char)in.read();
-        if (c == '\r' || c == '\n') {
+        if (c == '\n')
+          break;
+          
+        if (c == '\r') {       
+          long curPos = in.getPos();
+          char nextC = (char)in.read();
+          if (nextC != '\n') {
+            in.seek(curPos);
+          }
+
           break;
         }
       }
@@ -90,8 +99,18 @@
         break;
 
       char c = (char)b;              // bug: this assumes eight-bit characters.
-      if (c == '\r' || c == '\n')
+      if (c == '\n')
         break;
+        
+      if (c == '\r') {       
+        long curPos = in.getPos();
+        char nextC = (char)in.read();
+        if (nextC != '\n') {
+          in.seek(curPos);
+        }
+
+        break;
+      }
 
       buffer.append(c);
     }
"
hadoop,b69e4b0b7ec3bdd80aeb4257b5e4ff53bb7c4123,"HADOOP-421.  Replace uses of String in recordio with Text.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@433322 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-21 18:52:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/BinaryInputArchive.java b/src/java/org/apache/hadoop/record/BinaryInputArchive.java
index c435281..dc37ec03 100644
--- a/src/java/org/apache/hadoop/record/BinaryInputArchive.java
+++ b/src/java/org/apache/hadoop/record/BinaryInputArchive.java
@@ -21,6 +21,7 @@
 import java.io.ByteArrayOutputStream;
 import java.io.DataInputStream;
 import java.io.InputStream;
+import org.apache.hadoop.io.Text;
 
 import org.apache.hadoop.io.WritableUtils;
 
@@ -78,11 +79,10 @@
         return in.readDouble();
     }
     
-    public String readString(String tag) throws IOException {
-        int len = readInt(tag);
-        byte[] chars = new byte[len];
-        in.readFully(chars);
-        return new String(chars, ""UTF-8"");
+    public Text readString(String tag) throws IOException {
+        Text text = new Text();
+        text.readFields(in);
+        return text;
     }
     
     public ByteArrayOutputStream readBuffer(String tag) throws IOException {
"
hadoop,b69e4b0b7ec3bdd80aeb4257b5e4ff53bb7c4123,"HADOOP-421.  Replace uses of String in recordio with Text.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@433322 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-21 18:52:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/BinaryOutputArchive.java b/src/java/org/apache/hadoop/record/BinaryOutputArchive.java
index bd8d3e3..bea8479 100644
--- a/src/java/org/apache/hadoop/record/BinaryOutputArchive.java
+++ b/src/java/org/apache/hadoop/record/BinaryOutputArchive.java
@@ -23,6 +23,7 @@
 import java.io.DataOutput;
 import java.io.DataOutputStream;
 import java.io.OutputStream;
+import org.apache.hadoop.io.Text;
 
 import org.apache.hadoop.io.WritableUtils;
 
@@ -67,10 +68,8 @@
         out.writeDouble(d);
     }
     
-    public void writeString(String s, String tag) throws IOException {
-        byte[] chars = s.getBytes(""UTF-8"");
-        writeInt(chars.length, tag);
-        out.write(chars);
+    public void writeString(Text s, String tag) throws IOException {
+        s.write(out);
     }
     
     public void writeBuffer(ByteArrayOutputStream buf, String tag)
"
hadoop,b69e4b0b7ec3bdd80aeb4257b5e4ff53bb7c4123,"HADOOP-421.  Replace uses of String in recordio with Text.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@433322 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-21 18:52:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/CsvInputArchive.java b/src/java/org/apache/hadoop/record/CsvInputArchive.java
index 3b52a17..b77411b 100644
--- a/src/java/org/apache/hadoop/record/CsvInputArchive.java
+++ b/src/java/org/apache/hadoop/record/CsvInputArchive.java
@@ -20,9 +20,9 @@
 import java.io.InputStream;
 import java.io.IOException;
 import java.io.ByteArrayOutputStream;
-import java.io.PushbackInputStream;
 import java.io.PushbackReader;
 import java.io.UnsupportedEncodingException;
+import org.apache.hadoop.io.Text;
 
 /**
  *
@@ -119,7 +119,7 @@
         }
     }
     
-    public String readString(String tag) throws IOException {
+    public Text readString(String tag) throws IOException {
         String sval = readField(tag);
         return Utils.fromCSVString(sval);
         
"
hadoop,b69e4b0b7ec3bdd80aeb4257b5e4ff53bb7c4123,"HADOOP-421.  Replace uses of String in recordio with Text.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@433322 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-21 18:52:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/CsvOutputArchive.java b/src/java/org/apache/hadoop/record/CsvOutputArchive.java
index f43b88f..eec2e49 100644
--- a/src/java/org/apache/hadoop/record/CsvOutputArchive.java
+++ b/src/java/org/apache/hadoop/record/CsvOutputArchive.java
@@ -23,6 +23,7 @@
 import java.io.PrintStream;
 import java.io.OutputStream;
 import java.io.UnsupportedEncodingException;
+import org.apache.hadoop.io.Text;
 
 /**
  *
@@ -88,7 +89,7 @@
         throwExceptionOnError(tag);
     }
     
-    public void writeString(String s, String tag) throws IOException {
+    public void writeString(Text s, String tag) throws IOException {
         printCommaUnlessFirst();
         stream.print(Utils.toCSVString(s));
         throwExceptionOnError(tag);
"
hadoop,b69e4b0b7ec3bdd80aeb4257b5e4ff53bb7c4123,"HADOOP-421.  Replace uses of String in recordio with Text.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@433322 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-21 18:52:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/InputArchive.java b/src/java/org/apache/hadoop/record/InputArchive.java
index 5d14ed7..42ddc1c 100644
--- a/src/java/org/apache/hadoop/record/InputArchive.java
+++ b/src/java/org/apache/hadoop/record/InputArchive.java
@@ -18,8 +18,7 @@
 
 import java.io.IOException;
 import java.io.ByteArrayOutputStream;
-import java.util.TreeMap;
-import java.util.ArrayList;
+import org.apache.hadoop.io.Text;
 
 /**
  * Interface that all the Deserializers have to implement.
@@ -33,7 +32,7 @@
     public long readLong(String tag) throws IOException;
     public float readFloat(String tag) throws IOException;
     public double readDouble(String tag) throws IOException;
-    public String readString(String tag) throws IOException;
+    public Text readString(String tag) throws IOException;
     public ByteArrayOutputStream readBuffer(String tag) throws IOException;
     public void readRecord(Record r, String tag) throws IOException;
     public void startRecord(String tag) throws IOException;
"
hadoop,b69e4b0b7ec3bdd80aeb4257b5e4ff53bb7c4123,"HADOOP-421.  Replace uses of String in recordio with Text.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@433322 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-21 18:52:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/OutputArchive.java b/src/java/org/apache/hadoop/record/OutputArchive.java
index 667e2e6..1c8eb38 100644
--- a/src/java/org/apache/hadoop/record/OutputArchive.java
+++ b/src/java/org/apache/hadoop/record/OutputArchive.java
@@ -20,6 +20,7 @@
 import java.io.ByteArrayOutputStream;
 import java.util.TreeMap;
 import java.util.ArrayList;
+import org.apache.hadoop.io.Text;
 
 /**
  * Interface that alll the serializers have to implement.
@@ -33,7 +34,7 @@
     public void writeLong(long l, String tag) throws IOException;
     public void writeFloat(float f, String tag) throws IOException;
     public void writeDouble(double d, String tag) throws IOException;
-    public void writeString(String s, String tag) throws IOException;
+    public void writeString(Text s, String tag) throws IOException;
     public void writeBuffer(ByteArrayOutputStream buf, String tag)
         throws IOException;
     public void writeRecord(Record r, String tag) throws IOException;
"
hadoop,b69e4b0b7ec3bdd80aeb4257b5e4ff53bb7c4123,"HADOOP-421.  Replace uses of String in recordio with Text.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@433322 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-21 18:52:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/Utils.java b/src/java/org/apache/hadoop/record/Utils.java
index c909cfd..5447dec 100644
--- a/src/java/org/apache/hadoop/record/Utils.java
+++ b/src/java/org/apache/hadoop/record/Utils.java
@@ -21,6 +21,8 @@
 import java.io.DataOutput;
 import java.io.IOException;
 import java.io.UnsupportedEncodingException;
+import java.nio.charset.CharacterCodingException;
+import org.apache.hadoop.io.Text;
 
 /**
  * Various utility functions for Hadooop record I/O runtime.
@@ -194,19 +196,45 @@
         return true;
     }
     
+    public static final byte[] hexchars = { '0', '1', '2', '3', '4', '5',
+                                            '6', '7', '8', '9', 'A', 'B',
+                                            'C', 'D', 'E', 'F' };
     /**
      * 
      * @param s 
      * @return 
      */
-    static String toXMLString(String s) {
-        String rets = """";
-        try {
-            rets = java.net.URLEncoder.encode(s, ""UTF-8"");
-        } catch (UnsupportedEncodingException ex) {
-            ex.printStackTrace();
+    static String toXMLString(Text t) {
+        String s = t.toString();
+        StringBuffer sb = new StringBuffer();
+        for (int idx = 0; idx < s.length(); idx++) {
+          char ch = s.charAt(idx);
+          if (ch == '<') {
+            sb.append(""&lt;"");
+          } else if (ch == '&') {
+            sb.append(""&amp;"");
+          } else if (ch == '%') {
+            sb.append(""%25"");
+          } else if (ch < 0x20) {
+            sb.append(""%"");
+            sb.append(hexchars[ch/16]);
+            sb.append(hexchars[ch%16]);
+          } else {
+            sb.append(ch);
+          }
         }
-        return rets;
+        return sb.toString();
+    }
+    
+    static private int h2c(char ch) {
+      if (ch >= '0' && ch <= '9') {
+        return ch - '0';
+      } else if (ch >= 'A' && ch <= 'F') {
+        return ch - 'A';
+      } else if (ch >= 'a' && ch <= 'f') {
+        return ch - 'a';
+      }
+      return 0;
     }
     
     /**
@@ -214,14 +242,25 @@
      * @param s 
      * @return 
      */
-    static String fromXMLString(String s) {
-        String rets = """";
-        try {
-            rets = java.net.URLDecoder.decode(s, ""UTF-8"");
-        } catch (UnsupportedEncodingException ex) {
-            ex.printStackTrace();
+    static Text fromXMLString(String s) {
+        StringBuffer sb = new StringBuffer();
+        for (int idx = 0; idx < s.length();) {
+          char ch = s.charAt(idx++);
+          if (ch == '%') {
+            char ch1 = s.charAt(idx++);
+            char ch2 = s.charAt(idx++);
+            char res = (char)(h2c(ch1)*16 + h2c(ch2));
+            sb.append(res);
+          } else {
+            sb.append(ch);
+          }
         }
-        return rets;
+        try {
+          return new Text(sb.toString());
+        } catch (CharacterCodingException ex) {
+          ex.printStackTrace();
+          return new Text();
+        }
     }
     
     /**
@@ -229,7 +268,8 @@
      * @param s 
      * @return 
      */
-    static String toCSVString(String s) {
+    static String toCSVString(Text t) {
+        String s = t.toString();
         StringBuffer sb = new StringBuffer(s.length()+1);
         sb.append('\'');
         int len = s.length();
@@ -267,7 +307,7 @@
      * @throws java.io.IOException 
      * @return 
      */
-    static String fromCSVString(String s) throws IOException {
+    static Text fromCSVString(String s) throws IOException {
         if (s.charAt(0) != '\'') {
             throw new IOException(""Error deserializing string."");
         }
@@ -290,7 +330,7 @@
                 sb.append(c);
             }
         }
-        return sb.toString();
+        return new Text(sb.toString());
     }
     
     /**
"
hadoop,b69e4b0b7ec3bdd80aeb4257b5e4ff53bb7c4123,"HADOOP-421.  Replace uses of String in recordio with Text.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@433322 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-21 18:52:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/XmlInputArchive.java b/src/java/org/apache/hadoop/record/XmlInputArchive.java
index 5ad3104..f3df82f 100644
--- a/src/java/org/apache/hadoop/record/XmlInputArchive.java
+++ b/src/java/org/apache/hadoop/record/XmlInputArchive.java
@@ -20,13 +20,13 @@
 import java.io.IOException;
 import java.io.ByteArrayOutputStream;
 import java.util.ArrayList;
-import java.util.Iterator;
 
 import org.xml.sax.*;
 import org.xml.sax.helpers.DefaultHandler;
 import javax.xml.parsers.SAXParserFactory;
 import javax.xml.parsers.ParserConfigurationException;
 import javax.xml.parsers.SAXParser;
+import org.apache.hadoop.io.Text;
 /**
  *
  * @author Milind Bhandarkar
@@ -197,7 +197,7 @@
         return Double.parseDouble(v.getValue());
     }
     
-    public String readString(String tag) throws IOException {
+    public Text readString(String tag) throws IOException {
         Value v = next();
         if (!""string"".equals(v.getType())) {
             throw new IOException(""Error deserializing ""+tag+""."");
"
hadoop,b69e4b0b7ec3bdd80aeb4257b5e4ff53bb7c4123,"HADOOP-421.  Replace uses of String in recordio with Text.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@433322 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-21 18:52:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/XmlOutputArchive.java b/src/java/org/apache/hadoop/record/XmlOutputArchive.java
index 28a2ad7..77d6a2b 100644
--- a/src/java/org/apache/hadoop/record/XmlOutputArchive.java
+++ b/src/java/org/apache/hadoop/record/XmlOutputArchive.java
@@ -23,6 +23,7 @@
 import java.io.PrintStream;
 import java.io.OutputStream;
 import java.util.Stack;
+import org.apache.hadoop.io.Text;
 
 /**
  *
@@ -188,7 +189,7 @@
         printEndEnvelope(tag);
     }
     
-    public void writeString(String s, String tag) throws IOException {
+    public void writeString(Text s, String tag) throws IOException {
         printBeginEnvelope(tag);
         stream.print(""<string>"");
         stream.print(Utils.toXMLString(s));
"
hadoop,b69e4b0b7ec3bdd80aeb4257b5e4ff53bb7c4123,"HADOOP-421.  Replace uses of String in recordio with Text.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@433322 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-21 18:52:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/JRecord.java b/src/java/org/apache/hadoop/record/compiler/JRecord.java
index b872bf5..1f9b6db 100644
--- a/src/java/org/apache/hadoop/record/compiler/JRecord.java
+++ b/src/java/org/apache/hadoop/record/compiler/JRecord.java
@@ -116,15 +116,15 @@
             JField jf = (JField) i.next();
             hh.write(jf.genCppDecl());
         }
-        hh.write(""  std::bitset<""+mFields.size()+""> bs_;\n"");
+        hh.write(""  mutable std::bitset<""+mFields.size()+""> bs_;\n"");
         hh.write(""public:\n"");
-        hh.write(""  virtual void serialize(::hadoop::OArchive& a_, const char* tag);\n"");
+        hh.write(""  virtual void serialize(::hadoop::OArchive& a_, const char* tag) const;\n"");
         hh.write(""  virtual void deserialize(::hadoop::IArchive& a_, const char* tag);\n"");
         hh.write(""  virtual const ::std::string& type() const;\n"");
         hh.write(""  virtual const ::std::string& signature() const;\n"");
         hh.write(""  virtual bool validate() const;\n"");
-        hh.write(""  virtual bool operator<(const ""+getName()+""& peer_);\n"");
-        hh.write(""  virtual bool operator==(const ""+getName()+""& peer_);\n"");
+        hh.write(""  virtual bool operator<(const ""+getName()+""& peer_) const;\n"");
+        hh.write(""  virtual bool operator==(const ""+getName()+""& peer_) const;\n"");
         hh.write(""  virtual ~""+getName()+""() {};\n"");
         int fIdx = 0;
         for (Iterator i = mFields.iterator(); i.hasNext(); fIdx++) {
@@ -135,7 +135,7 @@
         for (int i=ns.length-1; i>=0; i--) {
             hh.write(""} // end namespace ""+ns[i]+""\n"");
         }
-        cc.write(""void ""+getCppFQName()+""::serialize(::hadoop::OArchive& a_, const char* tag) {\n"");
+        cc.write(""void ""+getCppFQName()+""::serialize(::hadoop::OArchive& a_, const char* tag) const {\n"");
         cc.write(""  if (!validate()) throw new ::hadoop::IOException(\""All fields not set.\"");\n"");
         cc.write(""  a_.startRecord(*this,tag);\n"");
         fIdx = 0;
@@ -182,7 +182,7 @@
         cc.write(""  return true;\n"");
         cc.write(""}\n"");
         
-        cc.write(""bool ""+getCppFQName()+""::operator< (const ""+getCppFQName()+""& peer_) {\n"");
+        cc.write(""bool ""+getCppFQName()+""::operator< (const ""+getCppFQName()+""& peer_) const {\n"");
         cc.write(""  return (1\n"");
         for (Iterator i = mFields.iterator(); i.hasNext();) {
             JField jf = (JField) i.next();
@@ -192,7 +192,7 @@
         cc.write(""  );\n"");
         cc.write(""}\n"");
         
-        cc.write(""bool ""+getCppFQName()+""::operator== (const ""+getCppFQName()+""& peer_) {\n"");
+        cc.write(""bool ""+getCppFQName()+""::operator== (const ""+getCppFQName()+""& peer_) const {\n"");
         cc.write(""  return (1\n"");
         for (Iterator i = mFields.iterator(); i.hasNext();) {
             JField jf = (JField) i.next();
@@ -234,6 +234,7 @@
         FileWriter jj = new FileWriter(jfile);
         jj.write(""// File generated by hadoop record compiler. Do not edit.\n"");
         jj.write(""package ""+getJavaPackage()+"";\n\n"");
+        jj.write(""import org.apache.hadoop.io.Text;\n\n"");
         jj.write(""public class ""+getName()+"" implements org.apache.hadoop.record.Record, org.apache.hadoop.io.WritableComparable {\n"");
         for (Iterator i = mFields.iterator(); i.hasNext();) {
             JField jf = (JField) i.next();
"
hadoop,b69e4b0b7ec3bdd80aeb4257b5e4ff53bb7c4123,"HADOOP-421.  Replace uses of String in recordio with Text.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@433322 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-21 18:52:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/JString.java b/src/java/org/apache/hadoop/record/compiler/JString.java
index 14c561b..4382d8b 100644
--- a/src/java/org/apache/hadoop/record/compiler/JString.java
+++ b/src/java/org/apache/hadoop/record/compiler/JString.java
@@ -24,7 +24,7 @@
     
     /** Creates a new instance of JString */
     public JString() {
-        super("" ::std::string"", ""String"", ""String"", ""String"");
+        super("" ::std::string"", ""Text"", ""String"", ""Text"");
     }
     
     public String getSignature() {
@@ -34,7 +34,7 @@
     public String genJavaReadWrapper(String fname, String tag, boolean decl) {
         String ret = """";
         if (decl) {
-            ret = ""    String ""+fname+"";\n"";
+            ret = ""    Text ""+fname+"";\n"";
         }
         return ret + ""        ""+fname+""=a_.readString(\""""+tag+""\"");\n"";
     }
"
hadoop,b69e4b0b7ec3bdd80aeb4257b5e4ff53bb7c4123,"HADOOP-421.  Replace uses of String in recordio with Text.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@433322 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-21 18:52:47,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/record/test/FromCpp.java b/src/test/org/apache/hadoop/record/test/FromCpp.java
index a38bc19..66fc8b9 100644
--- a/src/test/org/apache/hadoop/record/test/FromCpp.java
+++ b/src/test/org/apache/hadoop/record/test/FromCpp.java
@@ -17,15 +17,14 @@
 package org.apache.hadoop.record.test;
 
 import org.apache.hadoop.record.RecordReader;
-import org.apache.hadoop.record.RecordWriter;
 import java.io.ByteArrayOutputStream;
 import java.io.File;
 import java.io.FileInputStream;
-import java.io.FileOutputStream;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.TreeMap;
 import junit.framework.*;
+import org.apache.hadoop.io.Text;
 
 /**
  *
@@ -54,7 +53,7 @@
             r1.setDoubleVal(1.5234);
             r1.setIntVal(4567);
             r1.setLongVal(0x5a5a5a5a5a5aL);
-            r1.setStringVal(""random text"");
+            r1.setStringVal(new Text(""random text""));
             r1.setBufferVal(new ByteArrayOutputStream(20));
             r1.setVectorVal(new ArrayList());
             r1.setMapVal(new TreeMap());
@@ -80,7 +79,7 @@
             r1.setDoubleVal(1.5234);
             r1.setIntVal(4567);
             r1.setLongVal(0x5a5a5a5a5a5aL);
-            r1.setStringVal(""random text"");
+            r1.setStringVal(new Text(""random text""));
             r1.setBufferVal(new ByteArrayOutputStream(20));
             r1.setVectorVal(new ArrayList());
             r1.setMapVal(new TreeMap());
@@ -106,7 +105,7 @@
             r1.setDoubleVal(1.5234);
             r1.setIntVal(4567);
             r1.setLongVal(0x5a5a5a5a5a5aL);
-            r1.setStringVal(""random text"");
+            r1.setStringVal(new Text(""random text""));
             r1.setBufferVal(new ByteArrayOutputStream(20));
             r1.setVectorVal(new ArrayList());
             r1.setMapVal(new TreeMap());
"
hadoop,b69e4b0b7ec3bdd80aeb4257b5e4ff53bb7c4123,"HADOOP-421.  Replace uses of String in recordio with Text.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@433322 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-21 18:52:47,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/record/test/RecBuffer.java b/src/test/org/apache/hadoop/record/test/RecBuffer.java
index dd2d3cb..2794576 100644
--- a/src/test/org/apache/hadoop/record/test/RecBuffer.java
+++ b/src/test/org/apache/hadoop/record/test/RecBuffer.java
@@ -1,6 +1,8 @@
 // File generated by hadoop record compiler. Do not edit.
 package org.apache.hadoop.record.test;
 
+import org.apache.hadoop.io.Text;
+
 public class RecBuffer implements org.apache.hadoop.record.Record, org.apache.hadoop.io.WritableComparable {
   private java.io.ByteArrayOutputStream mData;
   private java.util.BitSet bs_;
@@ -56,7 +58,7 @@
     org.apache.hadoop.record.BinaryInputArchive archive = new org.apache.hadoop.record.BinaryInputArchive(in);
     deserialize(archive, """");
   }
-  private boolean validate() {
+  public boolean validate() {
     if (bs_.cardinality() != bs_.length()) return false;
     return true;
 }
"
hadoop,b69e4b0b7ec3bdd80aeb4257b5e4ff53bb7c4123,"HADOOP-421.  Replace uses of String in recordio with Text.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@433322 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-21 18:52:47,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/record/test/RecInt.java b/src/test/org/apache/hadoop/record/test/RecInt.java
index fb64986..3b39eff 100644
--- a/src/test/org/apache/hadoop/record/test/RecInt.java
+++ b/src/test/org/apache/hadoop/record/test/RecInt.java
@@ -1,6 +1,8 @@
 // File generated by hadoop record compiler. Do not edit.
 package org.apache.hadoop.record.test;
 
+import org.apache.hadoop.io.Text;
+
 public class RecInt implements org.apache.hadoop.record.Record, org.apache.hadoop.io.WritableComparable {
   private int mData;
   private java.util.BitSet bs_;
@@ -56,7 +58,7 @@
     org.apache.hadoop.record.BinaryInputArchive archive = new org.apache.hadoop.record.BinaryInputArchive(in);
     deserialize(archive, """");
   }
-  private boolean validate() {
+  public boolean validate() {
     if (bs_.cardinality() != bs_.length()) return false;
     return true;
 }
"
hadoop,b69e4b0b7ec3bdd80aeb4257b5e4ff53bb7c4123,"HADOOP-421.  Replace uses of String in recordio with Text.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@433322 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-21 18:52:47,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/record/test/RecRecord0.java b/src/test/org/apache/hadoop/record/test/RecRecord0.java
index e53b2f9..2f514b3 100644
--- a/src/test/org/apache/hadoop/record/test/RecRecord0.java
+++ b/src/test/org/apache/hadoop/record/test/RecRecord0.java
@@ -1,23 +1,25 @@
 // File generated by hadoop record compiler. Do not edit.
 package org.apache.hadoop.record.test;
 
+import org.apache.hadoop.io.Text;
+
 public class RecRecord0 implements org.apache.hadoop.record.Record, org.apache.hadoop.io.WritableComparable {
-  private String mStringVal;
+  private Text mStringVal;
   private java.util.BitSet bs_;
   public RecRecord0() {
     bs_ = new java.util.BitSet(2);
     bs_.set(1);
   }
   public RecRecord0(
-        String m0) {
+        Text m0) {
     bs_ = new java.util.BitSet(2);
     bs_.set(1);
     mStringVal=m0; bs_.set(0);
   }
-  public String getStringVal() {
+  public Text getStringVal() {
     return mStringVal;
   }
-  public void setStringVal(String m_) {
+  public void setStringVal(Text m_) {
     mStringVal=m_; bs_.set(0);
   }
   public void serialize(org.apache.hadoop.record.OutputArchive a_, String tag) throws java.io.IOException {
"
hadoop,b69e4b0b7ec3bdd80aeb4257b5e4ff53bb7c4123,"HADOOP-421.  Replace uses of String in recordio with Text.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@433322 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-21 18:52:47,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/record/test/RecRecord1.java b/src/test/org/apache/hadoop/record/test/RecRecord1.java
index c26625b..35f7ed1 100644
--- a/src/test/org/apache/hadoop/record/test/RecRecord1.java
+++ b/src/test/org/apache/hadoop/record/test/RecRecord1.java
@@ -1,6 +1,8 @@
 // File generated by hadoop record compiler. Do not edit.
 package org.apache.hadoop.record.test;
 
+import org.apache.hadoop.io.Text;
+
 public class RecRecord1 implements org.apache.hadoop.record.Record, org.apache.hadoop.io.WritableComparable {
   private boolean mBoolVal;
   private byte mByteVal;
@@ -8,7 +10,7 @@
   private long mLongVal;
   private float mFloatVal;
   private double mDoubleVal;
-  private String mStringVal;
+  private Text mStringVal;
   private java.io.ByteArrayOutputStream mBufferVal;
   private java.util.ArrayList mVectorVal;
   private java.util.TreeMap mMapVal;
@@ -25,7 +27,7 @@
         long m3,
         float m4,
         double m5,
-        String m6,
+        Text m6,
         java.io.ByteArrayOutputStream m7,
         java.util.ArrayList m8,
         java.util.TreeMap m9,
@@ -80,10 +82,10 @@
   public void setDoubleVal(double m_) {
     mDoubleVal=m_; bs_.set(5);
   }
-  public String getStringVal() {
+  public Text getStringVal() {
     return mStringVal;
   }
-  public void setStringVal(String m_) {
+  public void setStringVal(Text m_) {
     mStringVal=m_; bs_.set(6);
   }
   public java.io.ByteArrayOutputStream getBufferVal() {
@@ -133,7 +135,7 @@
       a_.startVector(mVectorVal,""VectorVal"");
       int len1 = mVectorVal.size();
       for(int vidx1 = 0; vidx1<len1; vidx1++) {
-        String e1 = (String) mVectorVal.get(vidx1);
+        Text e1 = (Text) mVectorVal.get(vidx1);
         a_.writeString(e1,""e1"");
       }
       a_.endVector(mVectorVal,""VectorVal"");
@@ -144,8 +146,8 @@
       java.util.Set es1 = mMapVal.entrySet();
       for(java.util.Iterator midx1 = es1.iterator(); midx1.hasNext(); ) {
         java.util.Map.Entry me1 = (java.util.Map.Entry) midx1.next();
-        String k1 = (String) me1.getKey();
-        String v1 = (String) me1.getValue();
+        Text k1 = (Text) me1.getKey();
+        Text v1 = (Text) me1.getValue();
         a_.writeString(k1,""k1"");
         a_.writeString(v1,""v1"");
       }
@@ -178,7 +180,7 @@
       org.apache.hadoop.record.Index vidx1 = a_.startVector(""VectorVal"");
       mVectorVal=new java.util.ArrayList();
       for (; !vidx1.done(); vidx1.incr()) {
-    String e1;
+    Text e1;
         e1=a_.readString(""e1"");
         mVectorVal.add(e1);
       }
@@ -189,9 +191,9 @@
       org.apache.hadoop.record.Index midx1 = a_.startMap(""MapVal"");
       mMapVal=new java.util.TreeMap();
       for (; !midx1.done(); midx1.incr()) {
-    String k1;
+    Text k1;
         k1=a_.readString(""k1"");
-    String v1;
+    Text v1;
         v1=a_.readString(""v1"");
         mMapVal.put(k1,v1);
       }
@@ -222,7 +224,7 @@
       a_.startVector(mVectorVal,""VectorVal"");
       int len1 = mVectorVal.size();
       for(int vidx1 = 0; vidx1<len1; vidx1++) {
-        String e1 = (String) mVectorVal.get(vidx1);
+        Text e1 = (Text) mVectorVal.get(vidx1);
         a_.writeString(e1,""e1"");
       }
       a_.endVector(mVectorVal,""VectorVal"");
@@ -232,8 +234,8 @@
       java.util.Set es1 = mMapVal.entrySet();
       for(java.util.Iterator midx1 = es1.iterator(); midx1.hasNext(); ) {
         java.util.Map.Entry me1 = (java.util.Map.Entry) midx1.next();
-        String k1 = (String) me1.getKey();
-        String v1 = (String) me1.getValue();
+        Text k1 = (Text) me1.getKey();
+        Text v1 = (Text) me1.getValue();
         a_.writeString(k1,""k1"");
         a_.writeString(v1,""v1"");
       }
"
hadoop,b69e4b0b7ec3bdd80aeb4257b5e4ff53bb7c4123,"HADOOP-421.  Replace uses of String in recordio with Text.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@433322 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-21 18:52:47,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/record/test/RecString.java b/src/test/org/apache/hadoop/record/test/RecString.java
index c926e9c..0d79dab 100644
--- a/src/test/org/apache/hadoop/record/test/RecString.java
+++ b/src/test/org/apache/hadoop/record/test/RecString.java
@@ -1,23 +1,25 @@
 // File generated by hadoop record compiler. Do not edit.
 package org.apache.hadoop.record.test;
 
+import org.apache.hadoop.io.Text;
+
 public class RecString implements org.apache.hadoop.record.Record, org.apache.hadoop.io.WritableComparable {
-  private String mData;
+  private Text mData;
   private java.util.BitSet bs_;
   public RecString() {
     bs_ = new java.util.BitSet(2);
     bs_.set(1);
   }
   public RecString(
-        String m0) {
+        Text m0) {
     bs_ = new java.util.BitSet(2);
     bs_.set(1);
     mData=m0; bs_.set(0);
   }
-  public String getData() {
+  public Text getData() {
     return mData;
   }
-  public void setData(String m_) {
+  public void setData(Text m_) {
     mData=m_; bs_.set(0);
   }
   public void serialize(org.apache.hadoop.record.OutputArchive a_, String tag) throws java.io.IOException {
@@ -56,7 +58,7 @@
     org.apache.hadoop.record.BinaryInputArchive archive = new org.apache.hadoop.record.BinaryInputArchive(in);
     deserialize(archive, """");
   }
-  private boolean validate() {
+  public boolean validate() {
     if (bs_.cardinality() != bs_.length()) return false;
     return true;
 }
"
hadoop,b69e4b0b7ec3bdd80aeb4257b5e4ff53bb7c4123,"HADOOP-421.  Replace uses of String in recordio with Text.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@433322 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-21 18:52:47,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/record/test/ToCpp.java b/src/test/org/apache/hadoop/record/test/ToCpp.java
index 2963f47..c13c6bc 100644
--- a/src/test/org/apache/hadoop/record/test/ToCpp.java
+++ b/src/test/org/apache/hadoop/record/test/ToCpp.java
@@ -19,13 +19,12 @@
 import java.io.IOException;
 import junit.framework.*;
 import org.apache.hadoop.record.RecordWriter;
-import org.apache.hadoop.record.RecordReader;
 import java.io.ByteArrayOutputStream;
 import java.io.File;
-import java.io.FileInputStream;
 import java.io.FileOutputStream;
 import java.util.ArrayList;
 import java.util.TreeMap;
+import org.apache.hadoop.io.Text;
 
 /**
  *
@@ -56,7 +55,7 @@
             r1.setDoubleVal(1.5234);
             r1.setIntVal(4567);
             r1.setLongVal(0x5a5a5a5a5a5aL);
-            r1.setStringVal(""random text"");
+            r1.setStringVal(new Text(""random text""));
             r1.setBufferVal(new ByteArrayOutputStream(20));
             r1.setVectorVal(new ArrayList());
             r1.setMapVal(new TreeMap());
@@ -80,7 +79,7 @@
             r1.setDoubleVal(1.5234);
             r1.setIntVal(4567);
             r1.setLongVal(0x5a5a5a5a5a5aL);
-            r1.setStringVal(""random text"");
+            r1.setStringVal(new Text(""random text""));
             r1.setBufferVal(new ByteArrayOutputStream(20));
             r1.setVectorVal(new ArrayList());
             r1.setMapVal(new TreeMap());
@@ -104,7 +103,7 @@
             r1.setDoubleVal(1.5234);
             r1.setIntVal(4567);
             r1.setLongVal(0x5a5a5a5a5a5aL);
-            r1.setStringVal(""random text"");
+            r1.setStringVal(new Text(""random text""));
             r1.setBufferVal(new ByteArrayOutputStream(20));
             r1.setVectorVal(new ArrayList());
             r1.setMapVal(new TreeMap());
"
hadoop,98ce06270476933a33fd8227fd2ac1e0580ad820,"HADOOP-176.  Fix a bug in IntWritable.Comparator.  Contributed by Dick King.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@432385 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-17 20:45:15,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/IntWritable.java b/src/java/org/apache/hadoop/io/IntWritable.java
index b458065..158863a 100644
--- a/src/java/org/apache/hadoop/io/IntWritable.java
+++ b/src/java/org/apache/hadoop/io/IntWritable.java
@@ -56,7 +56,7 @@
   public int compareTo(Object o) {
     int thisValue = this.value;
     int thatValue = ((IntWritable)o).value;
-    return thisValue - thatValue;
+    return (thisValue<thatValue ? -1 : (thisValue==thatValue ? 0 : 1));
   }
 
   public String toString() {
@@ -73,7 +73,7 @@
                        byte[] b2, int s2, int l2) {
       int thisValue = readInt(b1, s1);
       int thatValue = readInt(b2, s2);
-      return thisValue - thatValue;
+      return (thisValue<thatValue ? -1 : (thisValue==thatValue ? 0 : 1));
     }
   }
 
"
hadoop,e4a2d6d9f2bfcf47e66c763dc122a395c6ca6373,"HADOOP-455.  Fix a bug in Text, where DEL was not permitted.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@431709 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-15 21:36:02,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/Text.java b/src/java/org/apache/hadoop/io/Text.java
index ec7e62b..c2adb23 100644
--- a/src/java/org/apache/hadoop/io/Text.java
+++ b/src/java/org/apache/hadoop/io/Text.java
@@ -435,7 +435,7 @@
 
         switch (length) {
         case 0: // check for ASCII
-          if (leadByte > 0x7E)
+          if (leadByte > 0x7F)
             throw new MalformedInputException(count);
           break;
         case 1:
"
hadoop,784ea20b8ec9646b3689a55b49f893548dcd2aa0,"HADOOP-453.  Fix a bug in Text.setCapacity().  Contributed by Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@431426 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-14 20:11:49,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/Text.java b/src/java/org/apache/hadoop/io/Text.java
index 01e1147..ec7e62b 100644
--- a/src/java/org/apache/hadoop/io/Text.java
+++ b/src/java/org/apache/hadoop/io/Text.java
@@ -198,8 +198,8 @@
    * (if any) are deleted.
    */
   private void setCapacity( int len ) {
-    if (bytes == null || bytes.length < length)
-      bytes = new byte[length];      
+    if (bytes == null || bytes.length < len)
+      bytes = new byte[len];      
   }
    
   /** 
"
hadoop,4f01ee11d948c45ef049244b59d41590798672c0,"HADOOP-434.  Change smallJobsBenchmark to user standard Hadoop scripts.  Contributed by Sanjay Dahiya.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@430838 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-11 18:04:58,Doug Cutting,"diff --git a/src/contrib/smallJobsBenchmark/src/java/org/apache/hadoop/benchmarks/mapred/MultiJobRunner.java b/src/contrib/smallJobsBenchmark/src/java/org/apache/hadoop/benchmarks/mapred/MultiJobRunner.java
index f635705..1d78160 100644
--- a/src/contrib/smallJobsBenchmark/src/java/org/apache/hadoop/benchmarks/mapred/MultiJobRunner.java
+++ b/src/contrib/smallJobsBenchmark/src/java/org/apache/hadoop/benchmarks/mapred/MultiJobRunner.java
@@ -26,10 +26,10 @@
  */
 public class MultiJobRunner {
   
-  private String jarFile = ""MRBenchmark.jar"" ;
+  private String jarFile = null ; // ""MRBenchmark.jar"" ;
   private String input ; 
   private String output ; 
-  private int numJobs = 2000 ; // default value
+  private int numJobs = 1 ; // default value
   private static final Log LOG = LogFactory.getLog(MultiJobRunner.class);
   private int numMaps = 2; 
   private int numReduces = 1;
@@ -130,7 +130,9 @@
     
     job.setOutputPath(new Path(output));
     
-    job.setJar(jarFile);
+    if( null != jarFile ){
+      job.setJar(jarFile);
+    }
     job.setMapperClass(BenchmarkMapper.class);
     job.setReducerClass(BenchmarkReducer.class);
     
@@ -338,8 +340,8 @@
     }
     
     String output = """";
-    String jarFile = ""MRBenchmark.jar"" ; 
-    int numJobs = 0 ; 
+    String jarFile = null; //""MRBenchmark.jar"" ; 
+    int numJobs = 1 ; 
     int numMaps = 2; 
     int numReduces = 1 ; 
     int dataLines = 1 ; 
"
hadoop,e2abfbeac13ee8444d71af0cce5ee04df591f50e,"HADOOP-324.  Fix datanode to not exit when a disk is full, but simply to fail writes.  Contributed by Wendy Chien.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@430085 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-09 16:25:58,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index 2a796d4..84908a9 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -22,6 +22,7 @@
 import org.apache.hadoop.metrics.Metrics;
 import org.apache.hadoop.util.*;
 import org.apache.hadoop.util.DiskChecker.DiskErrorException;
+import org.apache.hadoop.util.DiskChecker.DiskOutOfSpaceException;
 import org.apache.hadoop.mapred.StatusHttpServer;
 
 import java.io.*;
@@ -726,8 +727,12 @@
                       out.write(buf, 0, bytesRead);
                       myMetrics.wroteBytes(bytesRead);
                     } catch (IOException iex) {
-                      shutdown();
-                      throw iex;
+                      if (iex.getMessage().startsWith(""No space left on device"")) {
+                    	  throw new DiskOutOfSpaceException(""No space left on device"");
+                      } else {
+                        shutdown();
+                        throw iex;
+                      }
                     }
                     if (out2 != null) {
                       try {
"
hadoop,e2abfbeac13ee8444d71af0cce5ee04df591f50e,"HADOOP-324.  Fix datanode to not exit when a disk is full, but simply to fail writes.  Contributed by Wendy Chien.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@430085 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-09 16:25:58,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSDataset.java b/src/java/org/apache/hadoop/dfs/FSDataset.java
index 15f5382..7c7bcdc 100644
--- a/src/java/org/apache/hadoop/dfs/FSDataset.java
+++ b/src/java/org/apache/hadoop/dfs/FSDataset.java
@@ -21,6 +21,7 @@
 import org.apache.hadoop.fs.*;
 import org.apache.hadoop.util.DiskChecker;
 import org.apache.hadoop.util.DiskChecker.DiskErrorException;
+import org.apache.hadoop.util.DiskChecker.DiskOutOfSpaceException;
 import org.apache.hadoop.conf.*;
 
 /**************************************************
@@ -302,7 +303,7 @@
             // Check if we have too little space
             //
             if (getRemaining() < blockSize) {
-                throw new IOException(""Insufficient space for an additional block"");
+                throw new DiskOutOfSpaceException(""Insufficient space for an additional block"");
             }
 
             //
"
hadoop,e2abfbeac13ee8444d71af0cce5ee04df591f50e,"HADOOP-324.  Fix datanode to not exit when a disk is full, but simply to fail writes.  Contributed by Wendy Chien.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@430085 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-09 16:25:58,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/util/DiskChecker.java b/src/java/org/apache/hadoop/util/DiskChecker.java
index d94e05d..22b194b 100644
--- a/src/java/org/apache/hadoop/util/DiskChecker.java
+++ b/src/java/org/apache/hadoop/util/DiskChecker.java
@@ -16,6 +16,12 @@
       }
     }
     
+    public static class DiskOutOfSpaceException extends IOException {
+        public DiskOutOfSpaceException(String msg) {
+          super(msg);
+        }
+      }
+      
     public static void checkDir( File dir ) throws DiskErrorException {
         if( !dir.exists() && !dir.mkdirs() )
             throw new DiskErrorException( ""can not create directory: "" 
"
hadoop,8ccf4891c101d5faeb3fe4cb16c825a7fa17f174,"HADOOP-400.  Improvements to task assignment.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@430052 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-09 13:48:10,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobInProgress.java b/src/java/org/apache/hadoop/mapred/JobInProgress.java
index b28d2c1..949142b 100644
--- a/src/java/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/JobInProgress.java
@@ -311,167 +311,132 @@
     /**
      * Return a MapTask, if appropriate, to run on the given tasktracker
      */
-    public Task obtainNewMapTask(String taskTracker, TaskTrackerStatus tts) {
-        if (! tasksInited) {
-            LOG.info(""Cannot create task split for "" + profile.getJobId());
-            return null;
-        }
-
-        Task t = null;
-        int cacheTarget = -1;
-        int stdTarget = -1;
-        int specTarget = -1;
-        int failedTarget = -1;
-
-        //
-        // We end up creating two tasks for the same bucket, because
-        // we call obtainNewMapTask() really fast, twice in a row.
-        // There's not enough time for the ""recentTasks""
-        //
-
-        //
-        // Compute avg progress through the map tasks
-        //
-        double avgProgress = status.mapProgress() / maps.length;
-
-        //
-        // See if there is a split over a block that is stored on
-        // the TaskTracker checking in.  That means the block
-        // doesn't have to be transmitted from another node.
-        //
-        ArrayList hostMaps = (ArrayList)hostToMaps.get(tts.getHost());
-        if (hostMaps != null) {
-          Iterator i = hostMaps.iterator();
-          while (i.hasNext()) {
-            TaskInProgress tip = (TaskInProgress)i.next();
-            if (tip.hasTask() && !tip.hasFailedOnMachine(taskTracker)) {
-              LOG.info(""Found task with local split for ""+tts.getHost());
-              cacheTarget = tip.getIdWithinJob();
-              i.remove();
-              break;
-            }
-          }
-        }
-
-        //
-        // If there's no cached target, see if there's
-        // a std. task to run.
-        //
-        if (cacheTarget < 0) {
-            for (int i = 0; i < maps.length; i++) {
-                int realIdx = (i + firstMapToTry) % maps.length; 
-                if (maps[realIdx].hasTask()) {
-                    if (stdTarget < 0) {
-                      if (maps[realIdx].hasFailedOnMachine(taskTracker)) {
-                        if (failedTarget < 0) {
-                          failedTarget = realIdx;
-                        }
-                      } else {
-                        stdTarget = realIdx;
-                        break;
-                      }
-                    }
-                }
-            }
-        }
-
-        //
-        // If no cached-target and no std target, see if
-        // there's a speculative task to run.
-        //
-        if (cacheTarget < 0 && stdTarget < 0) {
-            for (int i = 0; i < maps.length; i++) {        
-                int realIdx = (i + firstMapToTry) % maps.length; 
-                if (maps[realIdx].hasSpeculativeTask(avgProgress)) {
-                      if (!maps[realIdx].hasFailedOnMachine(taskTracker)) {
-                        specTarget = realIdx;
-                        break;
-                      }
-                }
-            }
-        }
-
-        //
-        // Run whatever we found
-        //
-        if (cacheTarget >= 0) {
-            t = maps[cacheTarget].getTaskToRun(taskTracker, tts, avgProgress);
-            runningMapTasks += 1;
-        } else if (stdTarget >= 0) {
-            t = maps[stdTarget].getTaskToRun(taskTracker, tts, avgProgress);
-            runningMapTasks += 1;
-	} else if (specTarget >= 0) {
-	    //should always be true, but being paranoid
-            boolean isRunning = maps[specTarget].isRunning(); 
-            t = maps[specTarget].getTaskToRun(taskTracker, tts, avgProgress);
-            if (!isRunning){
-                runningMapTasks += 1;
-            }
-        } else if (failedTarget >= 0) {
-           //should always be false, but being paranoid again
-            boolean isRunning = maps[failedTarget].isRunning(); 
-            t = maps[failedTarget].getTaskToRun(taskTracker, tts, avgProgress);
-            if (!isRunning) {
-                runningMapTasks += 1;
-	    }
-        }
-        return t;
-    }
+    public Task obtainNewMapTask(TaskTrackerStatus tts, int clusterSize) {
+      if (! tasksInited) {
+        LOG.info(""Cannot create task split for "" + profile.getJobId());
+        return null;
+      }
+      ArrayList mapCache = (ArrayList)hostToMaps.get(tts.getHost());
+      double avgProgress = status.mapProgress() / maps.length;
+      int target = findNewTask(tts, clusterSize, avgProgress, 
+                                  maps, firstMapToTry, mapCache);
+      if (target == -1) {
+        return null;
+      }
+      boolean wasRunning = maps[target].isRunning();
+      Task result = maps[target].getTaskToRun(tts.getTrackerName());
+      if (!wasRunning) {
+        runningMapTasks += 1;
+      }
+      return result;
+    }    
 
     /**
      * Return a ReduceTask, if appropriate, to run on the given tasktracker.
      * We don't have cache-sensitivity for reduce tasks, as they
      *  work on temporary MapRed files.  
      */
-    public Task obtainNewReduceTask(String taskTracker, TaskTrackerStatus tts) {
+    public Task obtainNewReduceTask(TaskTrackerStatus tts,
+                                    int clusterSize) {
         if (! tasksInited) {
             LOG.info(""Cannot create task split for "" + profile.getJobId());
             return null;
         }
 
-        Task t = null;
-        int stdTarget = -1;
-        int specTarget = -1;
-        int failedTarget = -1;
         double avgProgress = status.reduceProgress() / reduces.length;
+        int target = findNewTask(tts, clusterSize, avgProgress, 
+                                    reduces, firstReduceToTry, null);
+        if (target == -1) {
+          return null;
+        }
+        boolean wasRunning = reduces[target].isRunning();
+        Task result = reduces[target].getTaskToRun(tts.getTrackerName());
+        if (!wasRunning) {
+          runningReduceTasks += 1;
+        }
+        return result;
+    }
+    
+    /**
+     * Find a new task to run.
+     * @param tts The task tracker that is asking for a task
+     * @param clusterSize The number of task trackers in the cluster
+     * @param avgProgress The average progress of this kind of task in this job
+     * @param tasks The list of potential tasks to try
+     * @param firstTaskToTry The first index in tasks to check
+     * @param cachedTasks A list of tasks that would like to run on this node
+     * @return the index in tasks of the selected task (or -1 for no task)
+     */
+    private int findNewTask(TaskTrackerStatus tts, 
+                            int clusterSize,
+                            double avgProgress,
+                            TaskInProgress[] tasks,
+                            int firstTaskToTry,
+                            List cachedTasks) {
+        String taskTracker = tts.getTrackerName();
+        //
+        // See if there is a split over a block that is stored on
+        // the TaskTracker checking in.  That means the block
+        // doesn't have to be transmitted from another node.
+        //
+        if (cachedTasks != null) {
+          Iterator i = cachedTasks.iterator();
+          while (i.hasNext()) {
+            TaskInProgress tip = (TaskInProgress)i.next();
+            i.remove();
+            if (tip.isRunnable() && 
+                !tip.isRunning() &&
+                !tip.hasFailedOnMachine(taskTracker)) {
+              LOG.info(""Choosing cached task "" + tip.getTIPId());
+              int cacheTarget = tip.getIdWithinJob();
+              return cacheTarget;
+            }
+          }
+        }
 
-        for (int i = 0; i < reduces.length; i++) {
-            int realIdx = (i + firstReduceToTry) % reduces.length;
-            if (reduces[realIdx].hasTask()) {
-                if (reduces[realIdx].hasFailedOnMachine(taskTracker)) {
-                  if (failedTarget < 0) {
-                    failedTarget = realIdx;
-                  }
-                } else if (stdTarget < 0) {
-                    stdTarget = realIdx;
-                }
-            } else if (reduces[realIdx].hasSpeculativeTask(avgProgress)) {
-                if (specTarget < 0 &&
-                    !reduces[realIdx].hasFailedOnMachine(taskTracker)) {
-                    specTarget = realIdx;
-                }
+
+        //
+        // If there's no cached target, see if there's
+        // a std. task to run.
+        //
+        int failedTarget = -1;
+        int specTarget = -1;
+        for (int i = 0; i < tasks.length; i++) {
+          int realIdx = (i + firstTaskToTry) % tasks.length; 
+          TaskInProgress task = tasks[realIdx];
+          if (task.isRunnable()) {
+            // if it failed here and we haven't tried every machine, we
+            // don't schedule it here.
+            boolean hasFailed = task.hasFailedOnMachine(taskTracker);
+            if (hasFailed && (task.getNumberOfFailedMachines() < clusterSize)) {
+              continue;
             }
+            boolean isRunning = task.isRunning();
+            if (hasFailed) {
+              // failed tasks that aren't running can be scheduled as a last
+              // resort
+              if (!isRunning && failedTarget == -1) {
+                failedTarget = realIdx;
+              }
+            } else {
+              if (!isRunning) {
+                LOG.info(""Choosing normal task "" + tasks[realIdx].getTIPId());
+                return realIdx;
+              } else if (specTarget == -1 &&
+                         task.hasSpeculativeTask(avgProgress)) {
+                specTarget = realIdx;
+              }
+            }
+          }
         }
-        
-        if (stdTarget >= 0) {
-            t = reduces[stdTarget].getTaskToRun(taskTracker, tts, avgProgress);
-            runningReduceTasks += 1;
-	} else if (specTarget >= 0) {
-            //should be false
-            boolean isRunning = reduces[specTarget].isRunning();
-            t = reduces[specTarget].getTaskToRun(taskTracker, tts, avgProgress);
-            if (!isRunning){
-               runningReduceTasks += 1;
-            }
-        } else if (failedTarget >= 0) {
-            boolean isRunning = reduces[failedTarget].isRunning();
-            t = reduces[failedTarget].getTaskToRun(taskTracker, tts, 
-                                                   avgProgress);
-            if (!isRunning){
-                runningReduceTasks += 1;
-            }
+        if (specTarget != -1) {
+          LOG.info(""Choosing speculative task "" + 
+                    tasks[specTarget].getTIPId());
+        } else if (failedTarget != -1) {
+          LOG.info(""Choosing failed task "" + 
+                    tasks[failedTarget].getTIPId());          
         }
-        return t;
+        return specTarget != -1 ? specTarget : failedTarget;
     }
 
     /**
"
hadoop,8ccf4891c101d5faeb3fe4cb16c825a7fa17f174,"HADOOP-400.  Improvements to task assignment.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@430052 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-09 13:48:10,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index b4fc5b0..52c0762 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -779,13 +779,17 @@
         int remainingMapLoad = 0;
         int numTaskTrackers;
         TaskTrackerStatus tts;
-        int avgMapLoad = 0;
-        int avgReduceLoad = 0;
 	
         synchronized (taskTrackers) {
           numTaskTrackers = taskTrackers.size();
           tts = (TaskTrackerStatus) taskTrackers.get(taskTracker);
         }
+        if (tts == null) {
+          LOG.warn(""Unknown task tracker polling; ignoring: "" + taskTracker);
+          return null;
+        }
+        int totalCapacity = numTaskTrackers * maxCurrentTasks;
+
         synchronized(jobsByArrival){
             for (Iterator it = jobsByArrival.iterator(); it.hasNext(); ) {
                     JobInProgress job = (JobInProgress) it.next();
@@ -797,19 +801,23 @@
                     }
             }   
         }
-        
+
+        // find out the maximum number of maps or reduces that we are willing
+        // to run on any node.
+        int maxMapLoad = 0;
+        int maxReduceLoad = 0;
         if (numTaskTrackers > 0) {
-          avgMapLoad = remainingMapLoad / numTaskTrackers;
-          avgReduceLoad = remainingReduceLoad / numTaskTrackers;
+          maxMapLoad = Math.min(maxCurrentTasks,
+                                (int) Math.ceil((double) remainingMapLoad / 
+                                                numTaskTrackers));
+          maxReduceLoad = Math.min(maxCurrentTasks,
+                                   (int) Math.ceil((double) remainingReduceLoad
+                                                   / numTaskTrackers));
         }
-        int totalCapacity = numTaskTrackers * maxCurrentTasks;
+        
         //
         // Get map + reduce counts for the current tracker.
         //
-        if (tts == null) {
-          LOG.warn(""Unknown task tracker polling; ignoring: "" + taskTracker);
-          return null;
-        }
 
         int numMaps = tts.countMapTasks();
         int numReduces = tts.countReduceTasks();
@@ -823,18 +831,12 @@
 
         //
         // We hand a task to the current taskTracker if the given machine 
-        // has a workload that's equal to or less than the pendingMaps average.
-        // This way the maps are launched if the TaskTracker has running tasks 
-        // less than the pending average 
-        // +/- TASK_ALLOC_EPSILON.  (That epsilon is in place in case
-        // there is an odd machine that is failing for some reason but 
-        // has not yet been removed from the pool, making capacity seem
-        // larger than it really is.)
+        // has a workload that's less than the maximum load of that kind of
+        // task.
         //
        
         synchronized (jobsByArrival) {
-            if ((numMaps < maxCurrentTasks) &&
-                (numMaps <= avgMapLoad + 1 + TASK_ALLOC_EPSILON)) {
+            if (numMaps < maxMapLoad) {
 
                 int totalNeededMaps = 0;
                 for (Iterator it = jobsByArrival.iterator(); it.hasNext(); ) {
@@ -843,7 +845,7 @@
                         continue;
                     }
 
-                    Task t = job.obtainNewMapTask(taskTracker, tts);
+                    Task t = job.obtainNewMapTask(tts, numTaskTrackers);
                     if (t != null) {
                       expireLaunchingTasks.addNewTask(t.getTaskId());
                       myMetrics.launchMap();
@@ -870,17 +872,17 @@
             //
             // Same thing, but for reduce tasks
             //
-            if ((numReduces < maxCurrentTasks) &&
-                (numReduces <= avgReduceLoad + 1 + TASK_ALLOC_EPSILON)) {
+            if (numReduces < maxReduceLoad) {
 
                 int totalNeededReduces = 0;
                 for (Iterator it = jobsByArrival.iterator(); it.hasNext(); ) {
                     JobInProgress job = (JobInProgress) it.next();
-                    if (job.getStatus().getRunState() != JobStatus.RUNNING) {
+                    if (job.getStatus().getRunState() != JobStatus.RUNNING ||
+                        job.numReduceTasks == 0) {
                         continue;
                     }
 
-                    Task t = job.obtainNewReduceTask(taskTracker, tts);
+                    Task t = job.obtainNewReduceTask(tts, numTaskTrackers);
                     if (t != null) {
                       expireLaunchingTasks.addNewTask(t.getTaskId());
                       myMetrics.launchReduce();
"
hadoop,8ccf4891c101d5faeb3fe4cb16c825a7fa17f174,"HADOOP-400.  Improvements to task assignment.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@430052 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-09 13:48:10,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskInProgress.java b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
index 23f8d43..6bb2769 100644
--- a/src/java/org/apache/hadoop/mapred/TaskInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
@@ -16,8 +16,10 @@
 package org.apache.hadoop.mapred;
 
 import org.apache.commons.logging.*;
+import org.apache.hadoop.util.*;
 
 import java.text.NumberFormat;
+import java.io.*;
 import java.util.*;
 
 
@@ -391,21 +393,12 @@
     /////////////////////////////////////////////////
 
     /**
-     * Return whether this TIP has a non-speculative task to run
+     * Return whether this TIP still needs to run
      */
-    boolean hasTask() {
-        if (failed || isComplete() || recentTasks.size() > 0) {
-            return false;
-        } else {
-            for (Iterator it = taskStatuses.values().iterator(); it.hasNext(); ) {
-                TaskStatus ts = (TaskStatus) it.next();
-                if (ts.getRunState() == TaskStatus.RUNNING) {
-                    return false;
-                }
-            }
-            return true;
-        }
+    boolean isRunnable() {
+      return !failed && (completes == 0);
     }
+    
     /**
      * Return whether the TIP has a speculative task to run.  We
      * only launch a speculative task if the current TIP is really
@@ -430,27 +423,24 @@
     /**
      * Return a Task that can be sent to a TaskTracker for execution.
      */
-    public Task getTaskToRun(String taskTracker, TaskTrackerStatus tts, double avgProgress) {
+    public Task getTaskToRun(String taskTracker) {
         Task t = null;
-        if (hasTask() || 
-            hasSpeculativeTask(avgProgress)) {
 
-            String taskid = (String) usableTaskIds.first();
-            usableTaskIds.remove(taskid);
-            String jobId = job.getProfile().getJobId();
+        String taskid = (String) usableTaskIds.first();
+        usableTaskIds.remove(taskid);
+        String jobId = job.getProfile().getJobId();
 
-            if (isMapTask()) {
-                t = new MapTask(jobId, jobFile, taskid, partition, split);
-            } else {
-                t = new ReduceTask(jobId, jobFile, taskid, partition, numMaps);
-            }
-            t.setConf(conf);
-
-            recentTasks.add(taskid);
-
-            // Ask JobTracker to note that the task exists
-            jobtracker.createTaskEntry(taskid, taskTracker, this);
+        if (isMapTask()) {
+          t = new MapTask(jobId, jobFile, taskid, partition, split);
+        } else {
+          t = new ReduceTask(jobId, jobFile, taskid, partition, numMaps);
         }
+        t.setConf(conf);
+
+        recentTasks.add(taskid);
+
+        // Ask JobTracker to note that the task exists
+        jobtracker.createTaskEntry(taskid, taskTracker, this);
         return t;
     }
     
@@ -464,6 +454,14 @@
     }
     
     /**
+     * Get the number of machines where this task has failed.
+     * @return the size of the failed machine set
+     */
+    public int getNumberOfFailedMachines() {
+      return machinesWhereFailed.size();
+    }
+    
+    /**
      * Get the id of this map or reduce task.
      * @return The index of this tip in the maps/reduces lists.
      */
"
hadoop,d4facd83031093673dd5629652aee17226e2e1fb,"HADOOP-426.  Fix streaming contrib module to work correctly on Solaris.  This was causing nightly builds to fail.  Contributed by Michel.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@429868 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-08 22:40:52,Doug Cutting,"diff --git a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/Environment.java b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/Environment.java
index 8297ed9..4480128 100644
--- a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/Environment.java
+++ b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/Environment.java
@@ -30,12 +30,16 @@
    {
       // Extend this code to fit all operating
       // environments that you expect to run in
+      // http://lopica.sourceforge.net/os.html
       String command = null;
       String OS = System.getProperty(""os.name"");
       String lowerOs = OS.toLowerCase();
       if (OS.indexOf(""Windows"") > -1) {
          command = ""cmd /C set"";
-      } else if (OS.indexOf(""ix"") > -1 || OS.indexOf(""inux"") > -1) {
+      } else if (lowerOs.indexOf(""ix"") > -1 || lowerOs.indexOf(""linux"") > -1 
+        || lowerOs.indexOf(""freebsd"") > -1
+        || lowerOs.indexOf(""sunos"") > -1 || lowerOs.indexOf(""solaris"") > -1
+        || lowerOs.indexOf(""hp-ux"") > -1) {
          command = ""env"";
       } else if(lowerOs.startsWith(""mac os x"")) {
          command = ""env"";
"
hadoop,3fae4c40230917e072b506e1d36bf34cc95d0ab8,"HADOOP-427.  New bug number for this patch.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@429858 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-08 22:24:01,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobInProgress.java b/src/java/org/apache/hadoop/mapred/JobInProgress.java
index b28d2c1..8bb77dd 100644
--- a/src/java/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/JobInProgress.java
@@ -280,7 +280,7 @@
           } else if (status.getRunState() == TaskStatus.FAILED) {
             // Tell the job to fail the relevant task
             failedTask(tip, status.getTaskId(), status, status.getTaskTracker(),
-                       wasRunning, wasComplete);
+                       wasRunning, wasComplete, metrics);
           }          
         }
 
@@ -520,7 +520,7 @@
                 }
             }
         }
-
+        
         //
         // If all tasks are complete, then the job is done!
         //
@@ -571,7 +571,8 @@
      */
     private void failedTask(TaskInProgress tip, String taskid, 
                             TaskStatus status, String trackerName,
-                            boolean wasRunning, boolean wasComplete) {
+                            boolean wasRunning, boolean wasComplete,
+                            JobTrackerMetrics metrics) {
         tip.failedSubTask(taskid, trackerName);
         boolean isRunning = tip.isRunning();
         boolean isComplete = tip.isComplete();
@@ -596,8 +597,10 @@
         // the failed task goes to the end of the list.
         if (tip.isMapTask()) {
           firstMapToTry = (tip.getIdWithinJob() + 1) % maps.length;
+          metrics.failedMap();
         } else {
           firstReduceToTry = (tip.getIdWithinJob() + 1) % reduces.length;
+          metrics.failedReduce();
         }
             
         //
@@ -695,4 +698,5 @@
        }
        return null;
     }
+    
 }
"
hadoop,3fae4c40230917e072b506e1d36bf34cc95d0ab8,"HADOOP-427.  New bug number for this patch.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@429858 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-08 22:24:01,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index b4fc5b0..f57370e 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -219,6 +219,20 @@
                             // tracker has already been destroyed.
                             if (newProfile != null) {
                                 if (now - newProfile.getLastSeen() > TASKTRACKER_EXPIRY_INTERVAL) {
+                                    // But save the state so that if at a later
+                                    // point of time, we happen to hear from the
+                                    // same TaskTracker, we can reinstate 
+                                    // the state
+                                    ExpiredTaskTrackerState 
+                                        expTaskTrackerState = 
+                                        new ExpiredTaskTrackerState(
+                                           leastRecent.getTrackerName());
+                                    if (LOG.isDebugEnabled())
+                                      LOG.debug(""Saving state of TaskTracker "" +
+                                             leastRecent.getTrackerName());
+                                    expiredTaskTrackerStates.put(
+                                            leastRecent.getTrackerName(), 
+                                            expTaskTrackerState);
                                     // Remove completely
                                     updateTaskTrackerStatus(trackerName, null);
                                     lostTaskTracker(leastRecent.getTrackerName(),
@@ -347,6 +361,11 @@
         Metrics.report(metricsRecord, ""maps-completed"",
             ++numMapTasksCompleted);
       }
+
+      synchronized void failedMap() {
+        Metrics.report(metricsRecord, ""maps-completed"",
+            --numMapTasksCompleted);
+      }
       
       synchronized void launchReduce() {
         Metrics.report(metricsRecord, ""reduces-launched"",
@@ -357,6 +376,11 @@
         Metrics.report(metricsRecord, ""reduces-completed"",
             ++numReduceTasksCompleted);
       }
+
+      synchronized void failedReduce() {
+        Metrics.report(metricsRecord, ""reduces-completed"",
+            --numReduceTasksCompleted);
+      }
       
       synchronized void submitJob() {
         Metrics.report(metricsRecord, ""jobs-submitted"",
@@ -427,6 +451,7 @@
     Thread initJobsThread = null;
     ExpireLaunchingTasks expireLaunchingTasks = new ExpireLaunchingTasks();
     Thread expireLaunchingTaskThread = new Thread(expireLaunchingTasks);
+    private TreeMap expiredTaskTrackerStates = new TreeMap();
     
     /**
      * It might seem like a bug to maintain a TreeSet of status objects,
@@ -599,6 +624,36 @@
         LOG.info(""stopped all jobtracker services"");
         return;
     }
+
+    boolean reinstateStateOfTaskTracker(String trackerName) {
+      if (LOG.isDebugEnabled())
+        LOG.debug(""Going to reinstate state of tasktracker "" + trackerName);
+      ExpiredTaskTrackerState e = (ExpiredTaskTrackerState)
+                                   expiredTaskTrackerStates.get(trackerName);
+      if (e == null) return false;
+      Set taskset = e.getTaskSet();
+      if (taskset == null) return true;
+      for (Iterator it = taskset.iterator(); it.hasNext(); ) {
+        String taskId = (String) it.next();
+        TaskInProgress tip = e.getTIP(taskId);
+        if (LOG.isDebugEnabled())
+          LOG.debug(""Going to recreate task entry for task "" + taskId);
+        //check whether the job is still running
+        if (tip != null && 
+                tip.getJob().getStatus().getRunState() == JobStatus.RUNNING)
+          createTaskEntry(taskId, trackerName, tip);
+      }
+      ArrayList completedTasks = e.getCompletedTasks();
+      for (int i = 0; i < completedTasks.size(); i++) {
+        TaskStatus ts = (TaskStatus)completedTasks.get(i);
+        TaskInProgress tip = (TaskInProgress)taskidToTIPMap.get(ts.getTaskId());
+        if (tip == null) continue;
+        JobInProgress j = tip.getJob();
+        if (j != null && j.getStatus().getRunState() == JobStatus.RUNNING)
+          j.updateTaskStatus(tip, ts, myMetrics); 
+      }
+      return true;
+    }
     
     ///////////////////////////////////////////////////////
     // Maintain lookup tables; called by JobInProgress
@@ -748,7 +803,11 @@
                 } else {
                     // If not first contact, there should be some record of the tracker
                     if (!seenBefore) {
-                        return InterTrackerProtocol.UNKNOWN_TASKTRACKER;
+                        if (!reinstateStateOfTaskTracker(trackerName))
+                          return InterTrackerProtocol.UNKNOWN_TASKTRACKER;
+                        else 
+                          trackerExpiryQueue.add(trackerStatus);
+                          
                     }
                 }
 
@@ -1197,4 +1256,67 @@
         Configuration conf=new Configuration();
         startTracker(conf);
     }
+
+    private class ExpiredTaskTrackerState {
+      //Map from taskId (assigned to a given tasktracker) to the taskId's TIP
+      private TreeMap trackerTaskIdToTIPMap = new TreeMap();
+      //completedTasks is an array list that contains the list of tasks that a
+      //tasktracker successfully completed
+      ArrayList completedTasks = new ArrayList();
+
+      public ExpiredTaskTrackerState(String trackerId) {
+        trackerTaskIdToTIPMap.clear();
+        completedTasks.clear();
+        TreeSet tasks = (TreeSet) trackerToTaskMap.get(trackerId);
+        if (tasks == null) {
+          if (LOG.isDebugEnabled())
+            LOG.debug(""This tasktracker has no tasks"");
+          return;
+        }
+        if (LOG.isDebugEnabled())
+          LOG.debug(""Task IDs that this tasktracker has: "");
+        //We save the status of completed tasks only since TaskTrackers don't
+        //send updates about completed tasks. We don't need to save the status
+        //of other tasks since the TaskTracker will send the update along
+        //with the heartbeat (whenever that happens).
+        //Saving the status of completed tasks is required since the JobTracker
+        //will mark all tasks that belonged to a given TaskTracker as failed
+        //if that TaskTracker is lost. Now, if that same TaskTracker reports 
+        //in later on, we can simply re-mark the completed tasks (TIPs really) 
+        //it reported earlier about as ""completed"" and avoid unnecessary 
+        //re-run of those tasks.
+        for (Iterator it = tasks.iterator(); it.hasNext(); ) {
+          String taskId = (String) it.next();
+          if (LOG.isDebugEnabled())
+            LOG.debug(taskId);
+          TaskInProgress tip = (TaskInProgress) taskidToTIPMap.get(taskId);
+          if (tip !=null && 
+                  tip.getJob().getStatus().getRunState() == JobStatus.RUNNING)
+            trackerTaskIdToTIPMap.put(taskId, tip);
+          else continue;
+          TaskStatus ts = tip.getTaskStatus(taskId);
+          //ts could be null for a recently assigned task, in the case where,
+          //the tasktracker hasn't yet reported status about that task
+          if (ts == null) continue;
+          if (tip.isComplete()) {
+            TaskStatus saveTS = null;
+            try {
+              saveTS = (TaskStatus)ts.clone();
+            } catch (Exception e) {
+              LOG.fatal(""Could not save TaskTracker state"",e);
+            }
+            completedTasks.add(saveTS);
+          }
+        } 
+      }
+      public Set getTaskSet() {
+        return trackerTaskIdToTIPMap.keySet();
+      }
+      public TaskInProgress getTIP(String taskId) {
+        return (TaskInProgress)trackerTaskIdToTIPMap.get(taskId);
+      }
+      public ArrayList getCompletedTasks() {
+        return completedTasks;
+      }
+    }
 }
"
hadoop,3fae4c40230917e072b506e1d36bf34cc95d0ab8,"HADOOP-427.  New bug number for this patch.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@429858 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-08 22:24:01,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskInProgress.java b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
index 23f8d43..35cc136 100644
--- a/src/java/org/apache/hadoop/mapred/TaskInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
@@ -283,6 +283,12 @@
         
         taskStatuses.put(taskid, status);
 
+        //since if this task was declared failed due to tasktracker getting 
+        //lost, but now that same tasktracker reports in with this taskId as 
+        //running, we update recentTasks
+        if (status.getRunState() == TaskStatus.RUNNING)
+          recentTasks.add(taskid);
+
         // Recompute progress
         recomputeProgress();
         return changed;
@@ -470,4 +476,11 @@
     public int getIdWithinJob() {
       return partition;
     }
+
+    /**
+     * Get the TaskStatus associated with a given taskId
+     */
+    public TaskStatus getTaskStatus(String taskId) {
+      return (TaskStatus)taskStatuses.get(taskId);
+    }
 }
"
hadoop,3fae4c40230917e072b506e1d36bf34cc95d0ab8,"HADOOP-427.  New bug number for this patch.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@429858 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-08 22:24:01,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskStatus.java b/src/java/org/apache/hadoop/mapred/TaskStatus.java
index 0920646..e4e3616 100644
--- a/src/java/org/apache/hadoop/mapred/TaskStatus.java
+++ b/src/java/org/apache/hadoop/mapred/TaskStatus.java
@@ -25,7 +25,7 @@
  *
  * @author Mike Cafarella
  **************************************************/
-class TaskStatus implements Writable {
+class TaskStatus implements Writable, Cloneable {
     public static final int RUNNING = 0;
     public static final int SUCCEEDED = 1;
     public static final int FAILED = 2;
@@ -53,6 +53,16 @@
         this.taskTracker = taskTracker;
     }
     
+    //Implementing the clone method so that we can save the status of tasks
+    public Object clone() throws CloneNotSupportedException {
+        TaskStatus ts = (TaskStatus)super.clone();
+        if (this.diagnosticInfo != null)
+          ts.diagnosticInfo = new String(this.diagnosticInfo);
+        if (this.stateString != null)
+          ts.stateString = new String(this.stateString);
+        return ts;
+    }
+    
     public String getTaskId() { return taskid; }
     public boolean getIsMap() { return isMap; }
     public float getProgress() { return progress; }
"
hadoop,b85901efc8ff496953b543bf38fc546ae8fa4564,"HADOOP-415.  Replace some uses of DatanodeDescriptor in the DFS web ui code with DatanodeInfo, the preferred public class.  Contributed by Devaraj.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@429851 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-08 22:14:04,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/JspHelper.java b/src/java/org/apache/hadoop/dfs/JspHelper.java
index 7ca345d..180fc79 100644
--- a/src/java/org/apache/hadoop/dfs/JspHelper.java
+++ b/src/java/org/apache/hadoop/dfs/JspHelper.java
@@ -143,7 +143,7 @@
       TreeMap nodesSortedByName = new TreeMap();
       fsn.DFSNodesStatus(live, dead);
       for (int num = 0; num < live.size(); num++) {
-        DatanodeDescriptor d = (DatanodeDescriptor)live.elementAt(num);
+        DatanodeInfo d = (DatanodeInfo)live.elementAt(num);
         nodesSortedByName.put(d.getName(), d);
       }
       live.clear();
"
hadoop,82d046f732396018ef089b85686a3230b2c4a87f,"HADOOP-415.  Fix a potential namenode performance issue introduced by HADOOP-392.  Contributed by Devaraj.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@428860 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-04 19:58:10,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index 16fd4d9..9d8bb4a 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -61,11 +61,6 @@
     //
     TreeMap datanodeMap = new TreeMap();
 
-    // 
-    // Stores the datanode.name-->datanodeInfo map. Used for getting a sorted
-    // list of datanodes sorted by their names
-    TreeMap datanodeMapByName = new TreeMap();
-
     //
     // Stores the set of dead datanodes
     TreeMap deaddatanodeMap = new TreeMap();
@@ -1088,10 +1083,8 @@
               + ""new storageID "" + nodeReg.getStorageID() + "" assigned."" );
         }
         // register new datanode
-        DatanodeDescriptor dinfo;
         datanodeMap.put(nodeReg.getStorageID(), 
-                        (dinfo = new DatanodeDescriptor( nodeReg )));
-        datanodeMapByName.put(nodeReg.getName(), dinfo);
+                        new DatanodeDescriptor( nodeReg ));
         NameNode.stateChangeLog.debug(
             ""BLOCK* NameSystem.registerDatanode: ""
             + ""node registered."" );
@@ -1159,7 +1152,6 @@
                     +""brand-new heartbeat from ""+nodeID.getName() );
             nodeinfo = new DatanodeDescriptor(nodeID, capacity, remaining, xceiverCount);
             datanodeMap.put(nodeinfo.getStorageID(), nodeinfo);
-            datanodeMapByName.put(nodeinfo.getName(), nodeinfo);
             capacityDiff = capacity;
             remainingDiff = remaining;
           } else {
@@ -1216,7 +1208,6 @@
     private void removeDatanode( DatanodeDescriptor nodeInfo ) {
       heartbeats.remove(nodeInfo);
       datanodeMap.remove(nodeInfo.getStorageID());
-      datanodeMapByName.remove(nodeInfo.getName());
       deaddatanodeMap.put(nodeInfo.getName(), nodeInfo);
       NameNode.stateChangeLog.debug(""BLOCK* NameSystem.removeDatanode: ""
               + nodeInfo.getName() + "" is removed from datanodeMap"");
@@ -1552,7 +1543,7 @@
     public void DFSNodesStatus(Vector live, Vector dead) {
         synchronized (heartbeats) {
             synchronized (datanodeMap) {
-                live.addAll(datanodeMapByName.values());
+                live.addAll(datanodeMap.values());
                 dead.addAll(deaddatanodeMap.values());
             }
         }
"
hadoop,82d046f732396018ef089b85686a3230b2c4a87f,"HADOOP-415.  Fix a potential namenode performance issue introduced by HADOOP-392.  Contributed by Devaraj.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@428860 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-04 19:58:10,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/JspHelper.java b/src/java/org/apache/hadoop/dfs/JspHelper.java
index 3518790..7ca345d 100644
--- a/src/java/org/apache/hadoop/dfs/JspHelper.java
+++ b/src/java/org/apache/hadoop/dfs/JspHelper.java
@@ -27,6 +27,7 @@
 import org.apache.hadoop.conf.*;
 
 public class JspHelper {
+    static FSNamesystem fsn = null;
     static InetSocketAddress nameNodeAddr;
     static Configuration conf = new Configuration();
 
@@ -39,7 +40,7 @@
         nameNodeAddr = DataNode.getDataNode().getNameNodeAddr();
       }
       else {
-        FSNamesystem fsn = FSNamesystem.getFSNamesystem();
+        fsn = FSNamesystem.getFSNamesystem();
         nameNodeAddr = new InetSocketAddress(fsn.getDFSNameNodeMachine(),
                   fsn.getDFSNameNodePort()); 
       }      
@@ -137,6 +138,17 @@
       in.close();
       out.print(new String(buf));
     }
+    public void DFSNodesStatus(Vector live, Vector dead) {
+      if (fsn == null) return;
+      TreeMap nodesSortedByName = new TreeMap();
+      fsn.DFSNodesStatus(live, dead);
+      for (int num = 0; num < live.size(); num++) {
+        DatanodeDescriptor d = (DatanodeDescriptor)live.elementAt(num);
+        nodesSortedByName.put(d.getName(), d);
+      }
+      live.clear();
+      live.addAll(nodesSortedByName.values());
+    }
     public void addTableHeader(JspWriter out) throws IOException {
       out.print(""<table border=\""1\""""+
                 "" cellpadding=\""2\"" cellspacing=\""2\"">"");
"
hadoop,abf504b5114a0b42927c587987eba8e9557f710f,"HADOOP-226.  Fix fsck to properly handle replication counts, now that these can vary per file.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@428854 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-04 19:37:34,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSck.java b/src/java/org/apache/hadoop/dfs/DFSck.java
index dd771d1..27ac13b 100644
--- a/src/java/org/apache/hadoop/dfs/DFSck.java
+++ b/src/java/org/apache/hadoop/dfs/DFSck.java
@@ -151,8 +151,9 @@
       Block block = blocks[i].getBlock();
       long id = block.getBlockId();
       DatanodeInfo[] locs = blocks[i].getLocations();
-      if (locs.length > res.replication) res.overReplicatedBlocks += (locs.length - res.replication);
-      if (locs.length < res.replication && locs.length > 0) res.underReplicatedBlocks += (res.replication - locs.length);
+      short targetFileReplication = file.getReplication();
+      if (locs.length > targetFileReplication) res.overReplicatedBlocks += (locs.length - targetFileReplication);
+      if (locs.length < targetFileReplication && locs.length > 0) res.underReplicatedBlocks += (targetFileReplication - locs.length);
       report.append(i + "". "" + id + "" len="" + block.getNumBytes());
       if (locs == null || locs.length == 0) {
         report.append("" MISSING!"");
"
hadoop,a463e2c836b2265d4196d8139ad5f495936daaba,"HADOOP-377.  Permit one to add URL resources to a Configuration.  Contributed by Jean-Baptiste Quenot.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@427683 13f79535-47bb-0310-9956-ffa450edef68
",2006-08-01 19:19:13,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/conf/Configuration.java b/src/java/org/apache/hadoop/conf/Configuration.java
index f716362..0f0fd30 100644
--- a/src/java/org/apache/hadoop/conf/Configuration.java
+++ b/src/java/org/apache/hadoop/conf/Configuration.java
@@ -88,6 +88,11 @@
   }
 
   /** Add a default resource. */
+  public void addDefaultResource(URL url) {
+    addResource(defaultResources, url);
+  }
+
+  /** Add a default resource. */
   public void addDefaultResource(Path file) {
     addResource(defaultResources, file);
   }
@@ -98,6 +103,11 @@
   }
 
   /** Add a final resource. */
+  public void addFinalResource(URL url) {
+    addResource(finalResources, url);
+  }
+
+  /** Add a final resource. */
   public void addFinalResource(Path file) {
     addResource(finalResources, file);
   }
@@ -391,7 +401,14 @@
         DocumentBuilderFactory.newInstance().newDocumentBuilder();
       Document doc = null;
 
-      if (name instanceof String) {               // a CLASSPATH resource
+
+      if (name instanceof URL) {                  // an URL resource
+        URL url = (URL)name;
+        if (url != null) {
+          LOG.info(""parsing "" + url);
+          doc = builder.parse(url.toString());
+        }
+      } else if (name instanceof String) {        // a CLASSPATH resource
         URL url = getResource((String)name);
         if (url != null) {
           LOG.info(""parsing "" + url);
"
hadoop,65cc9fd5edd54ffbcddfae06669c0d707304539c,"HADOOP-393.  Fix a bug in Text validation.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@427269 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-31 21:04:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/Text.java b/src/java/org/apache/hadoop/io/Text.java
index a299c27..01e1147 100644
--- a/src/java/org/apache/hadoop/io/Text.java
+++ b/src/java/org/apache/hadoop/io/Text.java
@@ -415,13 +415,17 @@
    * @param utf8: byte array
    * @exception MalformedInputException if the byte array contains invalid utf-8
    */
-  public static void validateUTF8(byte[] utf8) 
+  public static void validateUTF8(byte[] utf8) throws MalformedInputException {
+     validateUTF(utf8, 0, utf8.length);     
+  }
+  
+  public static void validateUTF(byte[] utf8, int start, int len)
     throws MalformedInputException {
-    int count = 0;
+    int count = start;
     int leadByte = 0;
     int length = 0;
     int state = LEAD_BYTE;
-    while (count < utf8.length) {
+    while (count < start+len) {
       int aByte = ((int) utf8[count] & 0xFF);
 
       switch (state) {
@@ -433,7 +437,6 @@
         case 0: // check for ASCII
           if (leadByte > 0x7E)
             throw new MalformedInputException(count);
-          state = TRAIL_BYTE;
           break;
         case 1:
           if (leadByte < 0xC2 || leadByte > 0xDF)
"
hadoop,cb14d7b53c999a88048f7c6db03e5038f5e1d626,"HADOOP-362.  Fix a problem where jobs hung when status messages were recieved out of order.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@427239 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-31 20:08:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobInProgress.java b/src/java/org/apache/hadoop/mapred/JobInProgress.java
index 4671b9f..b28d2c1 100644
--- a/src/java/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/JobInProgress.java
@@ -19,6 +19,7 @@
 
 import org.apache.hadoop.fs.*;
 import org.apache.hadoop.conf.*;
+import org.apache.hadoop.mapred.JobTracker.JobTrackerMetrics;
 
 import java.io.*;
 import java.net.*;
@@ -266,15 +267,28 @@
     // Status update methods
     ////////////////////////////////////////////////////
     public synchronized void updateTaskStatus(TaskInProgress tip, 
-                                              TaskStatus status) {
+                                              TaskStatus status,
+                                              JobTrackerMetrics metrics) {
+
         double oldProgress = tip.getProgress();   // save old progress
-        tip.updateStatus(status);                 // update tip
-        LOG.debug(""Taking progress for "" + tip.getTIPId() + "" from "" + 
-                 oldProgress + "" to "" + tip.getProgress());
+        boolean wasRunning = tip.isRunning();
+        boolean wasComplete = tip.isComplete();
+        boolean change = tip.updateStatus(status);
+        if (change) {
+          if (status.getRunState() == TaskStatus.SUCCEEDED) {
+            completedTask(tip, status, metrics);
+          } else if (status.getRunState() == TaskStatus.FAILED) {
+            // Tell the job to fail the relevant task
+            failedTask(tip, status.getTaskId(), status, status.getTaskTracker(),
+                       wasRunning, wasComplete);
+          }          
+        }
 
         //
         // Update JobInProgress status
         //
+        LOG.debug(""Taking progress for "" + tip.getTIPId() + "" from "" + 
+                  oldProgress + "" to "" + tip.getProgress());
         double progressDelta = tip.getProgress() - oldProgress;
         if (tip.isMapTask()) {
           if (maps.length == 0) {
@@ -464,25 +478,28 @@
      * A taskid assigned to this JobInProgress has reported in successfully.
      */
     public synchronized void completedTask(TaskInProgress tip, 
-                                           TaskStatus status) {
+                                           TaskStatus status,
+                                           JobTrackerMetrics metrics) {
         String taskid = status.getTaskId();
-        boolean oldDone = tip.isComplete();
-        updateTaskStatus(tip, status);
-        LOG.info(""Taskid '"" + taskid + ""' has finished successfully."");
+        if (tip.isComplete()) {
+          LOG.info(""Already complete TIP "" + tip.getTIPId() + 
+                   "" has completed task "" + taskid);
+          return;
+        } else {
+          LOG.info(""Task '"" + taskid + ""' has completed "" + tip.getTIPId() + 
+                   "" successfully."");          
+        }
+        
         tip.completed(taskid);
-        boolean newDone = tip.isComplete();
         // updating the running/finished map/reduce counts
-        if (oldDone != newDone) {
-            if (newDone) {  
-                if (tip.isMapTask()){
-                    runningMapTasks -= 1;
-                    finishedMapTasks += 1;
-                }
-                else{
-                    runningReduceTasks -= 1;
-                    finishedReduceTasks += 1;
-                }    
-            }
+        if (tip.isMapTask()){
+          runningMapTasks -= 1;
+          finishedMapTasks += 1;
+          metrics.completeMap();
+        } else{
+          runningReduceTasks -= 1;
+          finishedReduceTasks += 1;
+          metrics.completeReduce();
         }
         
         //
@@ -508,10 +525,12 @@
         // If all tasks are complete, then the job is done!
         //
         if (status.getRunState() == JobStatus.RUNNING && allDone) {
-            this.status = new JobStatus(this.status.getJobId(), 1.0f, 1.0f, 
-                                        JobStatus.SUCCEEDED);
+            this.status.runState = JobStatus.SUCCEEDED;
             this.finishTime = System.currentTimeMillis();
             garbageCollect();
+            LOG.info(""Job "" + this.status.getJobId() + 
+                     "" has completed successfully."");
+            metrics.completeJob();
         }
     }
 
@@ -550,33 +569,29 @@
      * we need to schedule reexecution so that downstream reduce tasks can 
      * obtain the map task's output.
      */
-    public synchronized void failedTask(TaskInProgress tip, String taskid, 
-                                        TaskStatus status, String trackerName) {
-        boolean oldStatus = tip.isRunning();
-        boolean oldRun = tip.isComplete();
+    private void failedTask(TaskInProgress tip, String taskid, 
+                            TaskStatus status, String trackerName,
+                            boolean wasRunning, boolean wasComplete) {
         tip.failedSubTask(taskid, trackerName);
-        updateTaskStatus(tip, status);
-        boolean newStatus = tip.isRunning();
-        boolean newRun = tip.isComplete();
+        boolean isRunning = tip.isRunning();
+        boolean isComplete = tip.isComplete();
+        
         //update running  count on task failure.
-        if (oldStatus != newStatus) {
-           if (!newStatus) {
-              if (tip.isMapTask()){
-                  runningMapTasks -= 1;
-              }
-              else{
-                  runningReduceTasks -= 1;
-              }
-           }
+        if (wasRunning && !isRunning) {
+          if (tip.isMapTask()){
+            runningMapTasks -= 1;
+          } else {
+            runningReduceTasks -= 1;
+          }
         }
+        
         // the case when the map was complete but the task tracker went down.
-        if (oldRun != newRun) {
-            if (oldRun){
-                if (tip.isMapTask()){
-                    finishedMapTasks -= 1;
-                }
-            }
+        if (wasComplete && !isComplete) {
+          if (tip.isMapTask()){
+            finishedMapTasks -= 1;
+          }
         }
+        
         // After this, try to assign tasks with the one after this, so that
         // the failed task goes to the end of the list.
         if (tip.isMapTask()) {
@@ -605,7 +620,9 @@
      * @param trackerName The task tracker the task failed on
      */
     public void failedTask(TaskInProgress tip, String taskid, 
-                           String reason, String hostname, String trackerName) {
+                           String reason, String hostname, 
+                           String trackerName,
+                           JobTrackerMetrics metrics) {
        TaskStatus status = new TaskStatus(taskid,
                                           tip.isMapTask(),
                                           0.0f,
@@ -613,7 +630,7 @@
                                           reason,
                                           reason,
                                           trackerName);
-       failedTask(tip, taskid, status, trackerName);
+       updateTaskStatus(tip, status, metrics);
     }
        
                            
"
hadoop,cb14d7b53c999a88048f7c6db03e5038f5e1d626,"HADOOP-362.  Fix a problem where jobs hung when status messages were recieved out of order.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@427239 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-31 20:08:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index 2a27d31..b4fc5b0 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -138,7 +138,8 @@
                       TaskTrackerStatus trackerStatus = 
                         getTaskTracker(trackerName);
                       job.failedTask(tip, taskId, ""Error launching task"", 
-                                     trackerStatus.getHost(), trackerName);
+                                     trackerStatus.getHost(), trackerName,
+                                     myMetrics);
                     }
                     itr.remove();
                   } else {
@@ -1145,25 +1146,7 @@
                 LOG.info(""Serious problem.  While updating status, cannot find taskid "" + report.getTaskId());
             } else {
                 expireLaunchingTasks.removeTask(taskId);
-                JobInProgress job = tip.getJob();
-
-                if (report.getRunState() == TaskStatus.SUCCEEDED) {
-                    job.completedTask(tip, report);
-                    if (tip.isMapTask()) {
-                        myMetrics.completeMap();
-                    } else {
-                        myMetrics.completeReduce();
-                    }
-                    if (job.getStatus().getRunState() == JobStatus.SUCCEEDED) {
-                        myMetrics.completeJob();
-                    }
-                } else if (report.getRunState() == TaskStatus.FAILED) {
-                    // Tell the job to fail the relevant task
-                    job.failedTask(tip, report.getTaskId(), report, 
-                                   status.getTrackerName());
-                } else {
-                    job.updateTaskStatus(tip, report);
-                }
+                tip.getJob().updateTaskStatus(tip, report, myMetrics);
             }
         }
     }
@@ -1190,7 +1173,7 @@
                   // if the job is done, we don't want to change anything
                   if (job.getStatus().getRunState() == JobStatus.RUNNING) {
                     job.failedTask(tip, taskId, ""Lost task tracker"", 
-                                   hostname, trackerName);
+                                   hostname, trackerName, myMetrics);
                   }
                 }
             }
"
hadoop,cb14d7b53c999a88048f7c6db03e5038f5e1d626,"HADOOP-362.  Fix a problem where jobs hung when status messages were recieved out of order.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@427239 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-31 20:08:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskInProgress.java b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
index 3b97f17..23f8d43 100644
--- a/src/java/org/apache/hadoop/mapred/TaskInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
@@ -70,6 +70,9 @@
     private JobConf conf;
     private boolean runSpeculative;
     private TreeMap taskDiagnosticData = new TreeMap();
+    /**
+     * Map from taskId -> TaskStatus
+     */
     private TreeMap taskStatuses = new TreeMap();
 
     private TreeSet machinesWhereFailed = new TreeSet();
@@ -227,10 +230,10 @@
      * task ID and overall status, plus reports for all the
      * component task-threads that have ever been started.
      */
-    TaskReport generateSingleReport() {
+    synchronized TaskReport generateSingleReport() {
       ArrayList diagnostics = new ArrayList();
       for (Iterator i = taskDiagnosticData.values().iterator(); i.hasNext();) {
-        diagnostics.addAll((Vector)i.next());
+        diagnostics.addAll((List)i.next());
       }
       return new TaskReport
         (getTIPId(), (float)progress, state,
@@ -245,23 +248,44 @@
      * A status message from a client has arrived.
      * It updates the status of a single component-thread-task,
      * which might result in an overall TaskInProgress status update.
+     * @return has the task changed its state noticably?
      */
-    public void updateStatus(TaskStatus status) {
+    synchronized boolean updateStatus(TaskStatus status) {
         String taskid = status.getTaskId();
         String diagInfo = status.getDiagnosticInfo();
+        TaskStatus oldStatus = (TaskStatus) taskStatuses.get(taskid);
+        boolean changed = true;
         if (diagInfo != null && diagInfo.length() > 0) {
-            LOG.info(""Error from ""+taskid+"": ""+diagInfo);
-            Vector diagHistory = (Vector) taskDiagnosticData.get(taskid);
-            if (diagHistory == null) {
-                diagHistory = new Vector();
-                taskDiagnosticData.put(taskid, diagHistory);
-            }
-            diagHistory.add(diagInfo);
+          LOG.info(""Error from ""+taskid+"": ""+diagInfo);
+          List diagHistory = (List) taskDiagnosticData.get(taskid);
+          if (diagHistory == null) {
+              diagHistory = new ArrayList();
+              taskDiagnosticData.put(taskid, diagHistory);
+          }
+          diagHistory.add(diagInfo);
         }
+        if (oldStatus != null) {
+          int oldState = oldStatus.getRunState();
+          int newState = status.getRunState();
+          
+          // The task is not allowed to move from completed back to running.
+          // We have seen out of order status messagesmoving tasks from complete
+          // to running. This is a spot fix, but it should be addressed more
+          // globally.
+          if (newState == TaskStatus.RUNNING &&
+              (oldState == TaskStatus.FAILED || 
+               oldState == TaskStatus.SUCCEEDED)) {
+            return false;
+          }
+          
+          changed = oldState != newState;
+        }
+        
         taskStatuses.put(taskid, status);
 
         // Recompute progress
         recomputeProgress();
+        return changed;
     }
 
     /**
"
hadoop,cb14d7b53c999a88048f7c6db03e5038f5e1d626,"HADOOP-362.  Fix a problem where jobs hung when status messages were recieved out of order.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@427239 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-31 20:08:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index c85f75c..bf49692 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -357,15 +357,6 @@
                     TaskInProgress tip = (TaskInProgress) it.next();
                     TaskStatus status = tip.createStatus();
                     taskReports.add(status);
-                    if (status.getRunState() != TaskStatus.RUNNING) {
-                        if (tip.getTask().isMapTask()) {
-                            mapTotal--;
-                        } else {
-                            reduceTotal--;
-                        }
-                        myMetrics.completeTask();
-                        it.remove();
-                    }
                 }
             }
 
@@ -378,6 +369,21 @@
                                     httpPort, taskReports, 
                                     failures); 
             int resultCode = jobClient.emitHeartbeat(status, justStarted);
+            synchronized (this) {
+              for (Iterator it = taskReports.iterator();
+                   it.hasNext(); ) {
+                  TaskStatus taskStatus = (TaskStatus) it.next();
+                  if (taskStatus.getRunState() != TaskStatus.RUNNING) {
+                      if (taskStatus.getIsMap()) {
+                          mapTotal--;
+                      } else {
+                          reduceTotal--;
+                      }
+                      myMetrics.completeTask();
+                      runningTasks.remove(taskStatus.getTaskId());
+                  }
+              }
+            }
             justStarted = false;
               
             if (resultCode == InterTrackerProtocol.UNKNOWN_TASKTRACKER) {
"
hadoop,89ada44b74b9d05919c260089e80adbeaf14726c,"HADOOP-389.  Fix intermitten t failures of mapreduce unit tests.  Also fix some build dependencies.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@427231 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-31 19:45:31,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/mapred/MiniMRCluster.java b/src/test/org/apache/hadoop/mapred/MiniMRCluster.java
index 00bc9f9e..365a65e 100644
--- a/src/test/org/apache/hadoop/mapred/MiniMRCluster.java
+++ b/src/test/org/apache/hadoop/mapred/MiniMRCluster.java
@@ -80,6 +80,8 @@
     class TaskTrackerRunner implements Runnable {
         TaskTracker tt;
         String localDir;
+        boolean isInitialized = false;
+        boolean isDead = false;
         
         /**
          * Create and run the task tracker.
@@ -101,8 +103,10 @@
                 this.localDir = ttDir.getAbsolutePath();
                 jc.set(""mapred.local.dir"", ttDir.getAbsolutePath());
                 tt = new TaskTracker(jc);
+                isInitialized = true;
                 tt.run();
             } catch (Throwable e) {
+                isDead = true;
                 tt = null;
                 System.err.println(""Task tracker crashed:"");
                 e.printStackTrace();
@@ -154,10 +158,14 @@
      */
     public void waitUntilIdle() {
       for(Iterator itr= taskTrackerList.iterator(); itr.hasNext(); ) {
-        TaskTracker tracker = ((TaskTrackerRunner) itr.next()).tt;
-        while (!tracker.isIdle()) {
-          System.out.println(""Waiting for task tracker "" + tracker.getName() +
-                             "" to finish."");
+        TaskTrackerRunner runner = (TaskTrackerRunner) itr.next();
+        while (!runner.isDead && (!runner.isInitialized || !runner.tt.isIdle())) {
+          if (!runner.isInitialized) {
+            System.out.println(""Waiting for task tracker to start."");
+          } else {
+            System.out.println(""Waiting for task tracker "" + runner.tt.getName() +
+                               "" to be idle."");
+          }
           try {
             Thread.sleep(1000);
           } catch (InterruptedException ie) {}
@@ -206,17 +214,15 @@
         if (taskTrackerFirst) {
           jobTrackerThread.start();
         }
-        try {                                     // let taskTrackers get started
-            Thread.sleep(2000);
-        } catch(InterruptedException e) {
-        }
+        waitUntilIdle();
     }
     
     /**
      * Shut down the servers.
      */
     public void shutdown() {
-        try {
+      try {
+        waitUntilIdle();
         for (int idx = 0; idx < numTaskTrackers; idx++) {
             TaskTrackerRunner taskTracker = (TaskTrackerRunner) taskTrackerList.get(idx);
             Thread taskTrackerThread = (Thread) taskTrackerThreadList.get(idx);
"
hadoop,5d9703a2db4099e9282a0f11462011ec56246703,"HADOOP-375.  Permit multiple datanodes to run on a single host.  Contributed by Devaraj.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@425718 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-26 12:15:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index b0fda06..64e64b1 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -96,12 +96,9 @@
     long blockReportInterval;
     private DataStorage storage = null;
     private StatusHttpServer infoServer;
-    private static int infoPort;
-    private static int port;
-    private static String localMachine;
+    private int infoPort;
     private static InetSocketAddress nameNodeAddr;
     private static DataNode datanodeObject = null;
-    static Date startTime = new Date(System.currentTimeMillis());
     private class DataNodeMetrics {
       private MetricsRecord metricsRecord = null;
       
@@ -158,8 +155,6 @@
         this(InetAddress.getLocalHost().getHostName(), 
              new File(datadir),
              createSocketAddr(conf.get(""fs.default.name"", ""local"")), conf);
-        // register datanode
-        register();
         infoPort = conf.getInt(""dfs.datanode.info.port"", 50075);
         this.infoServer = new StatusHttpServer(""datanode"", infoPort, true);
         //create a servlet to serve full-file content
@@ -168,6 +163,9 @@
                 ""org.apache.hadoop.dfs.StreamFile"", null);
         } catch (Exception e) {LOG.warn(""addServlet threw exception"", e);}
         this.infoServer.start();
+        infoPort = this.infoServer.getPort();
+        // register datanode
+        register();
         datanodeObject = this;
     }
     
@@ -215,9 +213,7 @@
         conf.getLong(""dfs.blockreport.intervalMsec"", BLOCKREPORT_INTERVAL);
       this.blockReportInterval =
         blockReportIntervalBasis - new Random().nextInt((int)(blockReportIntervalBasis/10));
-      localMachine = machineName;
       this.nameNodeAddr = nameNodeAddr;
-      port = tmpPort;
     }
 
     /** Return the DataNode object
@@ -227,30 +223,10 @@
         return datanodeObject;
     } 
 
-    public String getDataNodeMachine() {
-      return localMachine;
-    }
-
-    public int getDataNodePort() {
-      return port;
-    }
-
-    public int getDataNodeInfoPort() {
-        return infoPort;
-    }
-
     public InetSocketAddress getNameNodeAddr() {
       return nameNodeAddr;
     }
     
-    public InetSocketAddress getDataNodeAddr() {
-        return new InetSocketAddress(localMachine, port);
-    }
-    
-    public Date getStartTime() {
-      return startTime;
-    }
-    
     /**
      * Return the namenode's identifier
      */
@@ -271,6 +247,7 @@
      * @throws IOException
      */
     private void register() throws IOException {
+      dnRegistration.infoPort = infoPort;
       dnRegistration = namenode.register( dnRegistration );
       if( storage.getStorageID().equals("""") ) {
         storage.setStorageID( dnRegistration.getStorageID());
"
hadoop,5d9703a2db4099e9282a0f11462011ec56246703,"HADOOP-375.  Permit multiple datanodes to run on a single host.  Contributed by Devaraj.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@425718 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-26 12:15:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DatanodeInfo.java b/src/java/org/apache/hadoop/dfs/DatanodeInfo.java
index 0ca2bee..5afafb1 100644
--- a/src/java/org/apache/hadoop/dfs/DatanodeInfo.java
+++ b/src/java/org/apache/hadoop/dfs/DatanodeInfo.java
@@ -36,6 +36,7 @@
   protected long remaining;
   protected long lastUpdate;
   protected int xceiverCount;
+  protected int infoPort; //the port where the infoserver is running
 
   DatanodeInfo() {
     this( new String(), new String() );
@@ -72,6 +73,9 @@
   /** @deprecated Use {@link #getLastUpdate()} instead. */
   public long lastUpdate() { return getLastUpdate(); }
 
+  /** The port at which the http server is running*/
+  public int infoPort() { return infoPort; }
+
   /** A formatted string for reporting the status of the DataNode. */
   public String getDatanodeReport() {
     StringBuffer buffer = new StringBuffer();
@@ -106,6 +110,7 @@
     out.writeLong(remaining);
     out.writeLong(lastUpdate);
     out.writeInt(xceiverCount);
+    out.writeInt(infoPort);
   }
 
   /**
@@ -120,5 +125,6 @@
     this.remaining = in.readLong();
     this.lastUpdate = in.readLong();
     this.xceiverCount = in.readInt();
+    this.infoPort = in.readInt();
   }
 }
"
hadoop,5d9703a2db4099e9282a0f11462011ec56246703,"HADOOP-375.  Permit multiple datanodes to run on a single host.  Contributed by Devaraj.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@425718 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-26 12:15:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DatanodeRegistration.java b/src/java/org/apache/hadoop/dfs/DatanodeRegistration.java
index d723995..71a04a4 100644
--- a/src/java/org/apache/hadoop/dfs/DatanodeRegistration.java
+++ b/src/java/org/apache/hadoop/dfs/DatanodeRegistration.java
@@ -28,6 +28,7 @@
   int version;            /// current Datanode version
   String registrationID;  /// a unique per namenode id; indicates   
                           /// the namenode the datanode is registered with
+  int infoPort;
 
   /**
    * Default constructor.
@@ -70,6 +71,7 @@
     new UTF8( this.name ).write(out);
     new UTF8( this.storageID ).write(out);
     new UTF8( this.registrationID ).write(out);   
+    out.writeInt(this.infoPort);
   }
 
   /**
@@ -83,5 +85,6 @@
     this.storageID = uStr.toString();
     uStr.readFields(in);
     this.registrationID = uStr.toString();   
+    this.infoPort = in.readInt();
   }
 }
"
hadoop,5d9703a2db4099e9282a0f11462011ec56246703,"HADOOP-375.  Permit multiple datanodes to run on a single host.  Contributed by Devaraj.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@425718 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-26 12:15:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index dedf68e..b2e08af 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -106,7 +106,6 @@
     StatusHttpServer infoServer;
     int infoPort;
     Date startTime;
-    int dataNodeInfoPort;
     
     //
     Random r = new Random();
@@ -176,7 +175,6 @@
     public FSNamesystem(File dir, Configuration conf) throws IOException {
         fsNamesystemObject = this;
         this.infoPort = conf.getInt(""dfs.info.port"", 50070);
-        this.dataNodeInfoPort = conf.getInt(""dfs.datanode.info.port"", 50075);
         this.infoServer = new StatusHttpServer(""dfs"", infoPort, false);
         this.infoServer.start();
         InetSocketAddress addr = DataNode.createSocketAddr(conf.get(""fs.default.name"", ""local""));
@@ -1085,8 +1083,10 @@
               + ""new storageID "" + nodeReg.getStorageID() + "" assigned."" );
         }
         // register new datanode
+        DatanodeDescriptor dinfo;
         datanodeMap.put(nodeReg.getStorageID(), 
-                        new DatanodeDescriptor( nodeReg ) ) ;
+                        (dinfo = new DatanodeDescriptor( nodeReg ) ) ) ;
+        dinfo.infoPort = nodeReg.infoPort;
         NameNode.stateChangeLog.debug(
             ""BLOCK* NameSystem.registerDatanode: ""
             + ""node registered."" );
@@ -1153,6 +1153,7 @@
             NameNode.stateChangeLog.debug(""BLOCK* NameSystem.gotHeartbeat: ""
                     +""brand-new heartbeat from ""+nodeID.getName() );
             nodeinfo = new DatanodeDescriptor(nodeID, capacity, remaining, xceiverCount);
+            nodeinfo.infoPort = ((DatanodeRegistration)nodeID).infoPort;
             datanodeMap.put(nodeinfo.getStorageID(), nodeinfo);
             capacityDiff = capacity;
             remainingDiff = remaining;
@@ -1950,7 +1951,7 @@
         index = r.nextInt(size);
         DatanodeInfo d = getDatanodeByIndex(index);
         if (d != null) {
-          return d.getHost();
+          return d.getHost() + "":"" + d.infoPort();
         }
       }
       return null;
@@ -1960,7 +1961,4 @@
       return infoPort;
     }
 
-    public int getDataNodeInfoPort() {
-      return dataNodeInfoPort;
-    }
 }
"
hadoop,5d9703a2db4099e9282a0f11462011ec56246703,"HADOOP-375.  Permit multiple datanodes to run on a single host.  Contributed by Devaraj.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@425718 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-26 12:15:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/JspHelper.java b/src/java/org/apache/hadoop/dfs/JspHelper.java
index b937eb3..e1193a8 100644
--- a/src/java/org/apache/hadoop/dfs/JspHelper.java
+++ b/src/java/org/apache/hadoop/dfs/JspHelper.java
@@ -28,10 +28,6 @@
 
 public class JspHelper {
     static InetSocketAddress nameNodeAddr;
-    static InetSocketAddress dataNodeAddr;
-    static String dataNodeLabel;
-    static int dataNodeInfoPort;
-    static DataNode datanode = null;
     static Configuration conf = new Configuration();
 
     static int defaultChunkSizeToView = 
@@ -39,16 +35,11 @@
     static Random rand = new Random();
 
     public JspHelper() {
-      if ((datanode = DataNode.getDataNode()) != null) {
-        dataNodeInfoPort = datanode.getDataNodeInfoPort();
-        nameNodeAddr = datanode.getNameNodeAddr();
-        dataNodeAddr = datanode.getDataNodeAddr();
-        dataNodeLabel = datanode.getDataNodeMachine() + "":"" +
-                        datanode.getDataNodePort();
+      if (DataNode.getDataNode() != null) {
+        nameNodeAddr = DataNode.getDataNode().getNameNodeAddr();
       }
       else {
         FSNamesystem fsn = FSNamesystem.getFSNamesystem();
-        dataNodeInfoPort = fsn.getDataNodeInfoPort();
         nameNodeAddr = new InetSocketAddress(fsn.getDFSNameNodeMachine(),
                   fsn.getDFSNameNodePort()); 
       }      
@@ -72,7 +63,7 @@
         chosenNode = nodes[index];
 
         //just ping to check whether the node is alive
-        InetSocketAddress targetAddr = DataNode.createSocketAddr(chosenNode.getHost() + "":"" + dataNodeInfoPort);
+        InetSocketAddress targetAddr = DataNode.createSocketAddr(chosenNode.getHost() + "":"" + chosenNode.infoPort());
         
         try {
           s = new Socket();
@@ -125,10 +116,18 @@
         amtToRead = blockSize - offsetIntoBlock;
       byte[] buf = new byte[(int)amtToRead];
       int readOffset = 0;
+      int retries = 2;
       while (true) {
-        int numRead = in.read(buf, readOffset, (int)amtToRead);
-        if (numRead == -1)
-          throw new IOException(""Could not read data from datanode"");
+        int numRead;
+        try {
+          numRead = in.read(buf, readOffset, (int)amtToRead);
+        }
+        catch (IOException e) {
+          retries--;
+          if (retries == 0)
+            throw new IOException(""Could not read data from datanode"");
+          continue;
+        }
         amtToRead -= numRead;
         readOffset += numRead;
         if (amtToRead == 0)
"
hadoop,5d9703a2db4099e9282a0f11462011ec56246703,"HADOOP-375.  Permit multiple datanodes to run on a single host.  Contributed by Devaraj.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@425718 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-26 12:15:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/StreamFile.java b/src/java/org/apache/hadoop/dfs/StreamFile.java
index d22bf99..5a6a5d3 100644
--- a/src/java/org/apache/hadoop/dfs/StreamFile.java
+++ b/src/java/org/apache/hadoop/dfs/StreamFile.java
@@ -30,13 +30,11 @@
 public class StreamFile extends HttpServlet {
 
   static InetSocketAddress nameNodeAddr;
-  static int dataNodeInfoPort;
   static DataNode datanode = null;
   static Configuration conf = new Configuration();
   Random rand = new Random();
   static {
     if ((datanode = DataNode.getDataNode()) != null) {
-      dataNodeInfoPort = datanode.getDataNodeInfoPort();
       nameNodeAddr = datanode.getNameNodeAddr();
     }
   }
@@ -64,6 +62,7 @@
     } finally {
       in.close();
       os.close();
+      dfs.close();
     }
   }
 }
"
hadoop,df385a638e8a9c322b4a2d84b37396897bd5c3ef,"HADOOP-385.  Fix some bugs in record io code generation.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@425654 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-26 07:42:01,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/JRecord.java b/src/java/org/apache/hadoop/record/compiler/JRecord.java
index cfc8b70..b872bf5 100644
--- a/src/java/org/apache/hadoop/record/compiler/JRecord.java
+++ b/src/java/org/apache/hadoop/record/compiler/JRecord.java
@@ -84,8 +84,8 @@
         return ""  ""+mName+"" ""+fname+"";\n"";
     }
     
-    public String genJavaDecl (String fname) {
-        return ""  private ""+mName+"" ""+fname+"";\n"";
+    public String genJavaReadMethod(String fname, String tag) {
+        return genJavaReadWrapper(fname, tag, false);
     }
     
     public String genJavaReadWrapper(String fname, String tag, boolean decl) {
@@ -320,7 +320,7 @@
         jj.write(""    deserialize(archive, \""\"");\n"");
         jj.write(""  }\n"");
         
-        jj.write(""  private boolean validate() {\n"");
+        jj.write(""  public boolean validate() {\n"");
         jj.write(""    if (bs_.cardinality() != bs_.length()) return false;\n"");
         for (Iterator i = mFields.iterator(); i.hasNext(); fIdx++) {
             JField jf = (JField) i.next();
"
hadoop,df385a638e8a9c322b4a2d84b37396897bd5c3ef,"HADOOP-385.  Fix some bugs in record io code generation.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@425654 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-26 07:42:01,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/generated/RccTokenManager.java b/src/java/org/apache/hadoop/record/compiler/generated/RccTokenManager.java
index 8630e69..b8effe9 100644
--- a/src/java/org/apache/hadoop/record/compiler/generated/RccTokenManager.java
+++ b/src/java/org/apache/hadoop/record/compiler/generated/RccTokenManager.java
@@ -31,7 +31,7 @@
   public  void setDebugStream(java.io.PrintStream ds) { debugStream = ds; }
 private final int jjMoveStringLiteralDfa0_1()
 {
-   return jjMoveNfa_1(1, 0);
+   return jjMoveNfa_1(0, 0);
 }
 private final void jjCheckNAdd(int state)
 {
@@ -67,7 +67,7 @@
 {
    int[] nextStates;
    int startsAt = 0;
-   jjnewStateCnt = 5;
+   jjnewStateCnt = 3;
    int i = 1;
    jjstateSet[0] = startState;
    int j, kind = 0x7fffffff;
@@ -82,32 +82,22 @@
          {
             switch(jjstateSet[--i])
             {
-               case 1:
+               case 0:
                   if ((0x2400L & l) != 0L)
                   {
                      if (kind > 6)
                         kind = 6;
                   }
-                  else if (curChar == 47)
-                     jjstateSet[jjnewStateCnt++] = 0;
                   if (curChar == 13)
-                     jjstateSet[jjnewStateCnt++] = 3;
+                     jjstateSet[jjnewStateCnt++] = 1;
                   break;
-               case 0:
-                  if (curChar == 47 && kind > 6)
-                     kind = 6;
-                  break;
-               case 2:
-                  if ((0x2400L & l) != 0L && kind > 6)
-                     kind = 6;
-                  break;
-               case 3:
+               case 1:
                   if (curChar == 10 && kind > 6)
                      kind = 6;
                   break;
-               case 4:
+               case 2:
                   if (curChar == 13)
-                     jjstateSet[jjnewStateCnt++] = 3;
+                     jjstateSet[jjnewStateCnt++] = 1;
                   break;
                default : break;
             }
@@ -143,7 +133,7 @@
          kind = 0x7fffffff;
       }
       ++curPos;
-      if ((i = jjnewStateCnt) == (startsAt = 5 - (jjnewStateCnt = startsAt)))
+      if ((i = jjnewStateCnt) == (startsAt = 3 - (jjnewStateCnt = startsAt)))
          return curPos;
       try { curChar = input_stream.readChar(); }
       catch(java.io.IOException e) { return curPos; }
"
hadoop,df385a638e8a9c322b4a2d84b37396897bd5c3ef,"HADOOP-385.  Fix some bugs in record io code generation.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@425654 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-26 07:42:01,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/record/test/RecRecord0.java b/src/test/org/apache/hadoop/record/test/RecRecord0.java
index e3e6957..e53b2f9 100644
--- a/src/test/org/apache/hadoop/record/test/RecRecord0.java
+++ b/src/test/org/apache/hadoop/record/test/RecRecord0.java
@@ -56,7 +56,7 @@
     org.apache.hadoop.record.BinaryInputArchive archive = new org.apache.hadoop.record.BinaryInputArchive(in);
     deserialize(archive, """");
   }
-  private boolean validate() {
+  public boolean validate() {
     if (bs_.cardinality() != bs_.length()) return false;
     return true;
 }
"
hadoop,df385a638e8a9c322b4a2d84b37396897bd5c3ef,"HADOOP-385.  Fix some bugs in record io code generation.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@425654 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-26 07:42:01,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/record/test/RecRecord1.java b/src/test/org/apache/hadoop/record/test/RecRecord1.java
index 944d4cc..c26625b 100644
--- a/src/test/org/apache/hadoop/record/test/RecRecord1.java
+++ b/src/test/org/apache/hadoop/record/test/RecRecord1.java
@@ -12,10 +12,11 @@
   private java.io.ByteArrayOutputStream mBufferVal;
   private java.util.ArrayList mVectorVal;
   private java.util.TreeMap mMapVal;
+  private org.apache.hadoop.record.test.RecRecord0 mRecordVal;
   private java.util.BitSet bs_;
   public RecRecord1() {
-    bs_ = new java.util.BitSet(11);
-    bs_.set(10);
+    bs_ = new java.util.BitSet(12);
+    bs_.set(11);
   }
   public RecRecord1(
         boolean m0,
@@ -27,9 +28,10 @@
         String m6,
         java.io.ByteArrayOutputStream m7,
         java.util.ArrayList m8,
-        java.util.TreeMap m9) {
-    bs_ = new java.util.BitSet(11);
-    bs_.set(10);
+        java.util.TreeMap m9,
+        org.apache.hadoop.record.test.RecRecord0 m10) {
+    bs_ = new java.util.BitSet(12);
+    bs_.set(11);
     mBoolVal=m0; bs_.set(0);
     mByteVal=m1; bs_.set(1);
     mIntVal=m2; bs_.set(2);
@@ -40,6 +42,7 @@
     mBufferVal=m7; bs_.set(7);
     mVectorVal=m8; bs_.set(8);
     mMapVal=m9; bs_.set(9);
+    mRecordVal=m10; bs_.set(10);
   }
   public boolean getBoolVal() {
     return mBoolVal;
@@ -101,6 +104,12 @@
   public void setMapVal(java.util.TreeMap m_) {
     mMapVal=m_; bs_.set(9);
   }
+  public org.apache.hadoop.record.test.RecRecord0 getRecordVal() {
+    return mRecordVal;
+  }
+  public void setRecordVal(org.apache.hadoop.record.test.RecRecord0 m_) {
+    mRecordVal=m_; bs_.set(10);
+  }
   public void serialize(org.apache.hadoop.record.OutputArchive a_, String tag) throws java.io.IOException {
     if (!validate()) throw new java.io.IOException(""All fields not set:"");
     a_.startRecord(this,tag);
@@ -143,6 +152,8 @@
       a_.endMap(mMapVal,""MapVal"");
     }
     bs_.clear(9);
+    a_.writeRecord(mRecordVal,""RecordVal"");
+    bs_.clear(10);
     a_.endRecord(this,tag);
   }
   public void deserialize(org.apache.hadoop.record.InputArchive a_, String tag) throws java.io.IOException {
@@ -187,6 +198,9 @@
     a_.endMap(""MapVal"");
     }
     bs_.set(9);
+    mRecordVal= new org.apache.hadoop.record.test.RecRecord0();
+    a_.readRecord(mRecordVal,""RecordVal"");
+    bs_.set(10);
     a_.endRecord(tag);
 }
   public String toString() {
@@ -225,6 +239,7 @@
       }
       a_.endMap(mMapVal,""MapVal"");
     }
+    a_.writeRecord(mRecordVal,""RecordVal"");
       a_.endRecord(this,"""");
       return new String(s.toByteArray(), ""UTF-8"");
     } catch (Throwable ex) {
@@ -240,8 +255,9 @@
     org.apache.hadoop.record.BinaryInputArchive archive = new org.apache.hadoop.record.BinaryInputArchive(in);
     deserialize(archive, """");
   }
-  private boolean validate() {
+  public boolean validate() {
     if (bs_.cardinality() != bs_.length()) return false;
+    if (!mRecordVal.validate()) return false;
     return true;
 }
   public int compareTo (Object peer_) throws ClassCastException {
@@ -267,6 +283,8 @@
     if (ret != 0) return ret;
     if (ret != 0) return ret;
     if (ret != 0) return ret;
+    ret = mRecordVal.compareTo(peer.mRecordVal);
+    if (ret != 0) return ret;
      return ret;
   }
   public boolean equals(Object peer_) {
@@ -298,6 +316,8 @@
     if (!ret) return ret;
     ret = mMapVal.equals(peer.mMapVal);
     if (!ret) return ret;
+    ret = mRecordVal.equals(peer.mRecordVal);
+    if (!ret) return ret;
      return ret;
   }
   public int hashCode() {
@@ -323,9 +343,11 @@
     result = 37*result + ret;
     ret = mMapVal.hashCode();
     result = 37*result + ret;
+    ret = mRecordVal.hashCode();
+    result = 37*result + ret;
     return result;
   }
   public static String signature() {
-    return ""LRecRecord1(zbilfdsB[s]{ss})"";
+    return ""LRecRecord1(zbilfdsB[s]{ss}LRecRecord0(s))"";
   }
 }
"
hadoop,8d4dbdfc24841f1e7b115ad00a84052e47622820,"HADOOP-387.  Fix a potential task id collision in LocalTaskRunner.  Contributed by Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@425651 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-26 07:31:15,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/LocalJobRunner.java b/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
index 17d0756..02ede75 100644
--- a/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
+++ b/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
@@ -47,6 +47,7 @@
     private String file;
     private String id;
     private JobConf job;
+    private Random random = new Random();
 
     private JobStatus status = new JobStatus();
     private ArrayList mapIds = new ArrayList();
@@ -150,7 +151,7 @@
     }
 
     private String newId() {
-      return Integer.toString(Math.abs(new Random().nextInt()),36);
+      return Integer.toString(Math.abs(random.nextInt()),36);
     }
 
     // TaskUmbilicalProtocol methods
"
hadoop,94d5b63e475fe01fcbfada067dea319835aeb3d8,"HADOOP-380.  Fix reduce tasks to poll less frequently for map outputs.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@425348 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-25 09:22:56,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java b/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
index 24aa8b8..5a76c7c 100644
--- a/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
+++ b/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
@@ -528,7 +528,7 @@
       } catch (InterruptedException ie) { } // IGNORE
       currentTime = System.currentTimeMillis();
     }
-    lastPollTime = pollTime;
+    lastPollTime = currentTime;
 
     return jobClient.locateMapOutputs(reduceTask.getJobId().toString(), 
                                       neededIds,
"
hadoop,735a2229d6fece36b5ae75f00e7e8219fd6643b0,"HADOOP-383.  Fix a bug in the streaming contrib module on Windows.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@425339 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-25 09:06:18,Doug Cutting,"diff --git a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/Environment.java b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/Environment.java
index 7d75ace..8297ed9 100644
--- a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/Environment.java
+++ b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/Environment.java
@@ -33,7 +33,7 @@
       String command = null;
       String OS = System.getProperty(""os.name"");
       String lowerOs = OS.toLowerCase();
-      if (OS.equals(""Windows NT"")) {
+      if (OS.indexOf(""Windows"") > -1) {
          command = ""cmd /C set"";
       } else if (OS.indexOf(""ix"") > -1 || OS.indexOf(""inux"") > -1) {
          command = ""env"";
"
hadoop,826f7d6727362fad408a3650dd044799508be2fe,"HADOOP-344.  Fix some Windows-related problems with DF.  Contributed by Konstantin.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@425337 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-25 09:01:13,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSDataset.java b/src/java/org/apache/hadoop/dfs/FSDataset.java
index 7254bd3..15f5382 100644
--- a/src/java/org/apache/hadoop/dfs/FSDataset.java
+++ b/src/java/org/apache/hadoop/dfs/FSDataset.java
@@ -213,7 +213,7 @@
     public FSDataset(File dir, Configuration conf) throws IOException {
     		this.reserved = conf.getLong(""dfs.datanode.du.reserved"", 0);
     		this.usableDiskPct = conf.getFloat(""dfs.datanode.du.pct"", (float) USABLE_DISK_PCT_DEFAULT);
-        diskUsage = new DF( dir.getCanonicalPath(), conf); 
+        diskUsage = new DF( dir, conf); 
         this.data = new File(dir, ""data"");
         if (! data.exists()) {
             data.mkdirs();
"
hadoop,826f7d6727362fad408a3650dd044799508be2fe,"HADOOP-344.  Fix some Windows-related problems with DF.  Contributed by Konstantin.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@425337 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-25 09:01:13,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/DF.java b/src/java/org/apache/hadoop/fs/DF.java
index ca1b8e0..9a01fd5 100644
--- a/src/java/org/apache/hadoop/fs/DF.java
+++ b/src/java/org/apache/hadoop/fs/DF.java
@@ -15,6 +15,7 @@
  */
 package org.apache.hadoop.fs;
 
+import java.io.File;
 import java.io.IOException;
 import java.io.InputStreamReader;
 import java.io.BufferedReader;
@@ -39,12 +40,24 @@
   private int percentUsed;
   private String mount;
   
+  /** @deprecated
+   */
   public DF(String path, Configuration conf ) throws IOException {
+    this( new File(path), conf );
+  }
+
+  /** @deprecated
+   */
+  public DF(String path, long dfInterval) throws IOException {
+    this( new File(path), dfInterval );
+  }
+  
+  public DF(File path, Configuration conf ) throws IOException {
     this( path, conf.getLong( ""dfs.df.interval"", DF.DF_INTERVAL_DEFAULT ));
   }
 
-  public DF(String path, long dfInterval) throws IOException {
-    this.dirPath = path;
+  public DF(File path, long dfInterval) throws IOException {
+    this.dirPath = path.getCanonicalPath();
     this.dfInterval = dfInterval;
     lastDF = ( dfInterval < 0 ) ? 0 : -dfInterval;
     this.doDF();
@@ -145,6 +158,6 @@
     if( args.length > 0 )
       path = args[0];
 
-    System.out.println(new DF(path, DF_INTERVAL_DEFAULT).toString());
+    System.out.println(new DF(new File(path), DF_INTERVAL_DEFAULT).toString());
   }
 }
"
hadoop,826f7d6727362fad408a3650dd044799508be2fe,"HADOOP-344.  Fix some Windows-related problems with DF.  Contributed by Konstantin.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@425337 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-25 09:01:13,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/LocalFileSystem.java b/src/java/org/apache/hadoop/fs/LocalFileSystem.java
index ea1af0e..2190855 100644
--- a/src/java/org/apache/hadoop/fs/LocalFileSystem.java
+++ b/src/java/org/apache/hadoop/fs/LocalFileSystem.java
@@ -345,7 +345,7 @@
         File f = pathToFile(p).getCanonicalFile();
       
         // find highest writable parent dir of f on the same device
-        String device = new DF(f.toString(), getConf()).getMount();
+        String device = new DF(f, getConf()).getMount();
         File parent = f.getParentFile();
         File dir;
         do {
"
hadoop,826f7d6727362fad408a3650dd044799508be2fe,"HADOOP-344.  Fix some Windows-related problems with DF.  Contributed by Konstantin.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@425337 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-25 09:01:13,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 9454b57..c85f75c 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -544,7 +544,7 @@
         if (localDirsDf.containsKey(localDirs[i])) {
           df = (DF) localDirsDf.get(localDirs[i]);
         } else {
-          df = new DF(localDirs[i], fConf);
+          df = new DF(new File(localDirs[i]), fConf);
           localDirsDf.put(localDirs[i], df);
         }
 
"
hadoop,81e92063f3f3a98ca6c93ad6e28301d3e1d1968e,"HADOOP-384.  Improved error messages for checksum problems.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@425327 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-25 08:27:41,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FSDataInputStream.java b/src/java/org/apache/hadoop/fs/FSDataInputStream.java
index c08eca7..1180125 100644
--- a/src/java/org/apache/hadoop/fs/FSDataInputStream.java
+++ b/src/java/org/apache/hadoop/fs/FSDataInputStream.java
@@ -22,6 +22,7 @@
 import org.apache.commons.logging.*;
 
 import org.apache.hadoop.conf.*;
+import org.apache.hadoop.util.StringUtils;
 
 /** Utility that wraps a {@link FSInputStream} in a {@link DataInputStream}
  * and buffers input through a {@link BufferedInputStream}. */
@@ -48,7 +49,7 @@
       
       this.fs = fs;
       this.file = file;
-      Path sumFile = fs.getChecksumFile(file);
+      Path sumFile = FileSystem.getChecksumFile(file);
       try {
         this.sums = new FSDataInputStream(fs.openRaw(sumFile), conf);
         byte[] version = new byte[VERSION.length];
@@ -59,7 +60,9 @@
       } catch (FileNotFoundException e) {         // quietly ignore
         stopSumming();
       } catch (IOException e) {                   // loudly ignore
-        LOG.warn(""Problem opening checksum file: ""+ file + "".  Ignoring with exception "" + e + ""."");
+        LOG.warn(""Problem opening checksum file: ""+ file + 
+                 "".  Ignoring exception: "" + 
+                 StringUtils.stringifyException(e));
         stopSumming();
       }
     }
@@ -91,7 +94,15 @@
           int inBuf = read - summed;
           int toSum = inBuf <= goal ? inBuf : goal;
           
-          sum.update(b, off+summed, toSum);
+          try {
+            sum.update(b, off+summed, toSum);
+          } catch (ArrayIndexOutOfBoundsException e) {
+            throw new RuntimeException(""Summer buffer overflow b.len="" + 
+                                       b.length + "", off="" + off + 
+                                       "", summed="" + summed + "", read="" + 
+                                       read + "", bytesPerSum="" + bytesPerSum +
+                                       "", inSum="" + inSum, e);
+          }
           summed += toSum;
           
           inSum += toSum;
"
hadoop,1a06f6bd44873c1d246c6472882c1c4030ba232c,"HADOOP-376.  Fix Datanode's HTTP server to scan for a free port.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@424968 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-24 08:53:42,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index 9401043..b0fda06 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -161,7 +161,7 @@
         // register datanode
         register();
         infoPort = conf.getInt(""dfs.datanode.info.port"", 50075);
-        this.infoServer = new StatusHttpServer(""datanode"", infoPort, false);
+        this.infoServer = new StatusHttpServer(""datanode"", infoPort, true);
         //create a servlet to serve full-file content
         try {
           this.infoServer.addServlet(null, ""/streamFile/*"",
"
hadoop,d2fcbca290b025df7fdc970ea34af6dbf30290cc,"HADOOP-368.  Improvements to DistributedFSCheck and TestDFSIO.  Contributed by Konstantin.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@423054 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-18 11:38:01,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/fs/DistributedFSCheck.java b/src/test/org/apache/hadoop/fs/DistributedFSCheck.java
index 35b87e1..f841659 100644
--- a/src/test/org/apache/hadoop/fs/DistributedFSCheck.java
+++ b/src/test/org/apache/hadoop/fs/DistributedFSCheck.java
@@ -81,10 +81,11 @@
   public void testFSBlocks( String rootName ) throws Exception {
     createInputFile(rootName);
     runDistributedFSCheck();
+    cleanup();  // clean up after all to restore the system state
   }
 
   private void createInputFile( String rootName ) throws IOException {
-    fs.delete(MAP_INPUT_DIR);
+    cleanup();  // clean up if previous run failed
 
     Path inputFile = new Path(MAP_INPUT_DIR, ""in_file"");
     SequenceFile.Writer writer =
@@ -133,18 +134,22 @@
                         long offset 
                         ) throws IOException {
       // open file
-      DataInputStream in;
-      in = new DataInputStream(fs.open(new Path(name)));
+      FSDataInputStream in = null;
+      try {
+        in = fs.open(new Path(name));
+      } catch( IOException e ) {
+        return name + ""@(missing)"";
+      }
+      in.seek( offset );
       long actualSize = 0;
       try {
         long blockSize = fs.getDefaultBlockSize();
-        int curSize = bufferSize;
-        for(  actualSize = 0; 
+        reporter.setStatus( ""reading "" + name + ""@"" + 
+            offset + ""/"" + blockSize );
+        for(  int curSize = bufferSize; 
               curSize == bufferSize && actualSize < blockSize;
               actualSize += curSize) {
-          curSize = in.read( buffer, (int)offset, Math.min(bufferSize, (int)(blockSize - actualSize)) );
-          reporter.setStatus( ""reading "" + name + ""@"" + 
-                              offset + ""/"" + blockSize );
+          curSize = in.read( buffer, 0, bufferSize );
         }
       } catch( IOException e ) {
         LOG.info( ""Corrupted block detected in \"""" + name + ""\"" at "" + offset );
@@ -178,7 +183,6 @@
   }
   
   private void runDistributedFSCheck() throws Exception {
-    fs.delete(READ_DIR);
     JobConf job = new JobConf( fs.getConf(), DistributedFSCheck.class );
 
     job.setInputPath(MAP_INPUT_DIR);
@@ -240,6 +244,7 @@
     long execTime = System.currentTimeMillis() - tStart;
     
     test.analyzeResult( execTime, resFileName, viewStats );
+    // test.cleanup();  // clean up after all to restore the system state
   }
   
   private void analyzeResult( long execTime,
@@ -318,7 +323,7 @@
     }
   }
 
-  private void cleanup() throws Exception {
+  private void cleanup() throws IOException {
     LOG.info( ""Cleaning up test files"" );
     fs.delete(TEST_ROOT_DIR);
   }
"
hadoop,ebe108d4532287f6668539a2b67cc7d71c98a931,"HADOOP-365.  Fix a ClassCastException introduced by HADOOP-354.  Patch contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@423019 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-18 09:15:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index 9f43573..236c6e2 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -904,7 +904,7 @@
 
     //  Wait for sub threads to exit
     for (Iterator iterator = subDataNodeList.entrySet().iterator(); iterator.hasNext();) {
-      Thread threadDataNode = (Thread) iterator.next();
+      Thread threadDataNode = (Thread) ((Map.Entry) iterator.next()).getValue();
       try {
         threadDataNode.join();
       } catch (InterruptedException e) {
"
hadoop,16beb92c8bb58bb460fea162c20cc9bf02280a00,"HADOOP-364.  Fix some problems introduced by HADOOP-252.  In particular, fix things when RPC clients start before daemons, plus other improvements to RPC versioning.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@423017 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-18 09:06:36,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index ada38b0..9f43573 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -118,10 +118,11 @@
       // get storage info and lock the data dir
       storage = new DataStorage( datadir );
       // connect to name node
-      this.namenode = (DatanodeProtocol) RPC.getProxy(DatanodeProtocol.class,
-                                                      DatanodeProtocol.versionID,
-                                                      nameNodeAddr, 
-                                                      conf);
+      this.namenode = (DatanodeProtocol) 
+          RPC.waitForProxy(DatanodeProtocol.class,
+                           DatanodeProtocol.versionID,
+                           nameNodeAddr, 
+                           conf);
       // find free port
       ServerSocket ss = null;
       int tmpPort = conf.getInt(""dfs.datanode.port"", 50010);
@@ -170,20 +171,7 @@
      * @throws IOException
      */
     private void register() throws IOException {
-      while (shouldRun) {
-        try {
-          dnRegistration = namenode.register( dnRegistration );
-          break;
-        } catch( ConnectException se ) {  // namenode has not been started
-          LOG.info(""Namenode not available yet, Zzzzz..."");
-        } catch( SocketTimeoutException te ) {  // namenode is busy
-          LOG.info(""Problem connecting to Namenode: "" + 
-                   StringUtils.stringifyException(te));
-        }
-        try {
-          Thread.sleep(10 * 1000);
-        } catch (InterruptedException ie) {}
-      }
+      dnRegistration = namenode.register( dnRegistration );
       if( storage.getStorageID().equals("""") ) {
         storage.setStorageID( dnRegistration.getStorageID());
         storage.write();
"
hadoop,16beb92c8bb58bb460fea162c20cc9bf02280a00,"HADOOP-364.  Fix some problems introduced by HADOOP-252.  In particular, fix things when RPC clients start before daemons, plus other improvements to RPC versioning.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@423017 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-18 09:06:36,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/RPC.java b/src/java/org/apache/hadoop/ipc/RPC.java
index e7d0bd7..cc1d630 100644
--- a/src/java/org/apache/hadoop/ipc/RPC.java
+++ b/src/java/org/apache/hadoop/ipc/RPC.java
@@ -22,12 +22,15 @@
 import java.lang.reflect.InvocationHandler;
 import java.lang.reflect.InvocationTargetException;
 
+import java.net.ConnectException;
 import java.net.InetSocketAddress;
+import java.net.SocketTimeoutException;
 import java.io.*;
 
 import org.apache.commons.logging.*;
 
 import org.apache.hadoop.io.*;
+import org.apache.hadoop.util.StringUtils;
 import org.apache.hadoop.conf.*;
 
 /** A simple RPC mechanism.
@@ -163,22 +166,89 @@
     }
   }
 
+  /**
+   * A version mismatch for the RPC protocol.
+   * @author Owen O'Malley
+   */
+  public static class VersionMismatch extends IOException {
+    private String interfaceName;
+    private long clientVersion;
+    private long serverVersion;
+    
+    /**
+     * Create a version mismatch exception
+     * @param interfaceName the name of the protocol mismatch
+     * @param clientVersion the client's version of the protocol
+     * @param serverVersion the server's version of the protocol
+     */
+    public VersionMismatch(String interfaceName, long clientVersion,
+                           long serverVersion) {
+      super(""Protocol "" + interfaceName + "" version mismatch. (client = "" +
+            clientVersion + "", server = "" + serverVersion + "")"");
+      this.interfaceName = interfaceName;
+      this.clientVersion = clientVersion;
+      this.serverVersion = serverVersion;
+    }
+    
+    /**
+     * Get the interface name
+     * @return the java class name 
+     *          (eg. org.apache.hadoop.mapred.InterTrackerProtocol)
+     */
+    public String getInterfaceName() {
+      return interfaceName;
+    }
+    
+    /**
+     * Get the client's prefered version
+     */
+    public long getClientVersion() {
+      return clientVersion;
+    }
+    
+    /**
+     * Get the server's agreed to version.
+     */
+    public long getServerVersion() {
+      return serverVersion;
+    }
+  }
+  
+  public static VersionedProtocol waitForProxy(Class protocol,
+                                               long clientVersion,
+                                               InetSocketAddress addr,
+                                               Configuration conf
+                                               ) throws IOException {
+    while (true) {
+      try {
+        return getProxy(protocol, clientVersion, addr, conf);
+      } catch( ConnectException se ) {  // namenode has not been started
+        LOG.info(""Server at "" + addr + "" not available yet, Zzzzz..."");
+      } catch( SocketTimeoutException te ) {  // namenode is busy
+        LOG.info(""Problem connecting to server: "" + addr);
+      }
+      try {
+        Thread.sleep(10*1000);
+      } catch (InterruptedException ie) {
+        // IGNORE
+      }
+    }
+  }
   /** Construct a client-side proxy object that implements the named protocol,
    * talking to a server at the named address. */
   public static VersionedProtocol getProxy(Class protocol, long clientVersion,
-      InetSocketAddress addr, Configuration conf)
-  throws RemoteException {
+                                           InetSocketAddress addr, Configuration conf) throws IOException {
     VersionedProtocol proxy = (VersionedProtocol) Proxy.newProxyInstance(
                                   protocol.getClassLoader(),
                                   new Class[] { protocol },
                                   new Invoker(addr, conf));
-    long serverVersion = proxy.getProtocolVersion(protocol.getName(), clientVersion);
+    long serverVersion = proxy.getProtocolVersion(protocol.getName(), 
+                                                  clientVersion);
     if (serverVersion == clientVersion) {
       return proxy;
     } else {
-      throw new RemoteException(protocol.getName(),
-          ""RPC Server and Client Versions Mismatched. SID:""+serverVersion+
-          "" CID:""+clientVersion);
+      throw new VersionMismatch(protocol.getName(), clientVersion, 
+                                serverVersion);
     }
   }
 
"
hadoop,16beb92c8bb58bb460fea162c20cc9bf02280a00,"HADOOP-364.  Fix some problems introduced by HADOOP-252.  In particular, fix things when RPC clients start before daemons, plus other improvements to RPC versioning.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@423017 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-18 09:06:36,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/VersionedProtocol.java b/src/java/org/apache/hadoop/ipc/VersionedProtocol.java
index e11dbb4..1a8cad2 100644
--- a/src/java/org/apache/hadoop/ipc/VersionedProtocol.java
+++ b/src/java/org/apache/hadoop/ipc/VersionedProtocol.java
@@ -16,7 +16,7 @@
 
 package org.apache.hadoop.ipc;
 
-import org.apache.hadoop.io.UTF8;
+import java.io.IOException;
 
 /**
  * Superclass of all protocols that use Hadoop RPC.
@@ -25,8 +25,13 @@
  * @author milindb
  */
 public interface VersionedProtocol {
+  
   /**
-   * Return protocol version corresponding to protocol interface
+   * Return protocol version corresponding to protocol interface.
+   * @param protocol The classname of the protocol interface
+   * @param clientVersion The version of the protocol that the client speaks
+   * @return the version that the server will speak
    */
-  public long getProtocolVersion(String protocol, long clientVersion);
+  public long getProtocolVersion(String protocol, 
+                                 long clientVersion) throws IOException;
 }
"
hadoop,16beb92c8bb58bb460fea162c20cc9bf02280a00,"HADOOP-364.  Fix some problems introduced by HADOOP-252.  In particular, fix things when RPC clients start before daemons, plus other improvements to RPC versioning.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@423017 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-18 09:06:36,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 5bb9165..abf87d9 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -61,6 +61,9 @@
     boolean shuttingDown = false;
     
     TreeMap tasks = null;
+    /**
+     * Map from taskId -> TaskInProgress.
+     */
     TreeMap runningTasks = null;
     int mapTotal = 0;
     int reduceTotal = 0;
@@ -163,8 +166,10 @@
         this.mapOutputFile.cleanupStorage();
         this.justStarted = true;
 
-        this.jobClient = (InterTrackerProtocol) RPC.getProxy(InterTrackerProtocol.class,
-            InterTrackerProtocol.versionID, jobTrackAddr, this.fConf);
+        this.jobClient = (InterTrackerProtocol) 
+                          RPC.waitForProxy(InterTrackerProtocol.class,
+                                           InterTrackerProtocol.versionID, 
+                                           jobTrackAddr, this.fConf);
         
         this.running = true;
     }
@@ -1139,7 +1144,8 @@
           JobConf conf=new JobConf();
           new TaskTracker(conf).run();
         } catch (IOException e) {
-            LOG.warn( ""Can not start task tracker because ""+e.getMessage());
+            LOG.warn( ""Can not start task tracker because ""+
+                      StringUtils.stringifyException(e));
             System.exit(-1);
         }
     }
"
hadoop,16beb92c8bb58bb460fea162c20cc9bf02280a00,"HADOOP-364.  Fix some problems introduced by HADOOP-252.  In particular, fix things when RPC clients start before daemons, plus other improvements to RPC versioning.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@423017 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-18 09:06:36,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java b/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java
index 473e2b3..839b9e9 100644
--- a/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java
+++ b/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java
@@ -24,6 +24,18 @@
   class NameNodeRunner implements Runnable {
     private NameNode node;
     
+    public boolean isUp() {
+      if (node == null) {
+        return false;
+      }
+      try {
+        long[] sizes = node.getStats();
+        return sizes[0] != 0;
+      } catch (IOException ie) {
+        return false;
+      }
+    }
+    
     /**
      * Create the name node and run it.
      */
@@ -82,8 +94,11 @@
   
   /**
    * Create the config and start up the servers.
+   * @param dataNodeFirst should the datanode be brought up before the namenode?
    */
-  public MiniDFSCluster(int namenodePort, Configuration conf) throws IOException {
+  public MiniDFSCluster(int namenodePort, 
+                        Configuration conf,
+                        boolean dataNodeFirst) throws IOException {
     this.conf = conf;
     conf.set(""fs.default.name"", 
              ""localhost:""+ Integer.toString(namenodePort));
@@ -98,17 +113,21 @@
     NameNode.format(conf);
     nameNode = new NameNodeRunner();
     nameNodeThread = new Thread(nameNode);
-    nameNodeThread.start();
-    try {                                     // let namenode get started
-      Thread.sleep(2000);
-    } catch(InterruptedException e) {
-    }
     dataNode = new DataNodeRunner();
     dataNodeThread = new Thread(dataNode);
-    dataNodeThread.start();
-    try {                                     // let daemons get started
-      Thread.sleep(2000);
-    } catch(InterruptedException e) {
+    if (dataNodeFirst) {
+      dataNodeThread.start();      
+      nameNodeThread.start();      
+    } else {
+      nameNodeThread.start();
+      dataNodeThread.start();      
+    }
+    while (!nameNode.isUp()) {
+      try {                                     // let daemons get started
+        System.out.println(""waiting for dfs minicluster to start"");
+        Thread.sleep(2000);
+      } catch(InterruptedException e) {
+      }
     }
   }
   
"
hadoop,16beb92c8bb58bb460fea162c20cc9bf02280a00,"HADOOP-364.  Fix some problems introduced by HADOOP-252.  In particular, fix things when RPC clients start before daemons, plus other improvements to RPC versioning.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@423017 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-18 09:06:36,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/mapred/MiniMRCluster.java b/src/test/org/apache/hadoop/mapred/MiniMRCluster.java
index 3b2e723..8e192f9 100644
--- a/src/test/org/apache/hadoop/mapred/MiniMRCluster.java
+++ b/src/test/org/apache/hadoop/mapred/MiniMRCluster.java
@@ -130,7 +130,8 @@
     public MiniMRCluster(int jobTrackerPort,
             int taskTrackerPort,
             int numTaskTrackers,
-            String namenode) throws IOException {
+            String namenode,
+            boolean taskTrackerFirst) throws IOException {
         this.jobTrackerPort = jobTrackerPort;
         this.taskTrackerPort = taskTrackerPort;
         this.numTaskTrackers = numTaskTrackers;
@@ -151,10 +152,8 @@
         pw.close();
         jobTracker = new JobTrackerRunner();
         jobTrackerThread = new Thread(jobTracker);
-        jobTrackerThread.start();
-        try {                                     // let jobTracker get started
-            Thread.sleep(2000);
-        } catch(InterruptedException e) {
+        if (!taskTrackerFirst) {
+          jobTrackerThread.start();
         }
         for (int idx = 0; idx < numTaskTrackers; idx++) {
             TaskTrackerRunner taskTracker = new TaskTrackerRunner();
@@ -163,6 +162,9 @@
             taskTrackerList.add(taskTracker);
             taskTrackerThreadList.add(taskTrackerThread);
         }
+        if (taskTrackerFirst) {
+          jobTrackerThread.start();
+        }
         try {                                     // let taskTrackers get started
             Thread.sleep(2000);
         } catch(InterruptedException e) {
@@ -201,7 +203,7 @@
     
     public static void main(String[] args) throws IOException {
         System.out.println(""Bringing up Jobtracker and tasktrackers."");
-        MiniMRCluster mr = new MiniMRCluster(50000, 50002, 4, ""local"");
+        MiniMRCluster mr = new MiniMRCluster(50000, 50002, 4, ""local"", false);
         System.out.println(""JobTracker and TaskTrackers are up."");
         mr.shutdown();
         System.out.println(""JobTracker and TaskTrackers brought down."");
"
hadoop,a60044aacd30752ea5e1b65e2964a6eb962857b4,"HADOOP-354.  Make public methods to stop DFS daemons.  Contributed by Barry Kaplan.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@421837 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-14 08:47:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index 4291a96..6bd838d 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -82,7 +82,7 @@
         return new InetSocketAddress(host, port);
     }
 
-    private static Vector subThreadList = null;
+    private static Map subDataNodeList = null;
     DatanodeProtocol namenode;
     FSDataset data;
     DatanodeRegistration dnRegistration;
@@ -193,7 +193,7 @@
      * Shut down this instance of the datanode.
      * Returns only after shutdown is complete.
      */
-    void shutdown() {
+    public void shutdown() {
         this.shouldRun = false;
         ((DataXceiveServer) this.dataXceiveServer.getRunnable()).kill();
         try {
@@ -202,6 +202,19 @@
         }
     }
 
+    /**
+     * Shut down all datanodes that where started via the run(conf) method.
+     * Returns only after shutdown is complete.
+     */
+    public static void shutdownAll(){
+      if(subDataNodeList != null && !subDataNodeList.isEmpty()){
+        for (Iterator iterator = subDataNodeList.keySet().iterator(); iterator.hasNext();) {
+          DataNode dataNode = (DataNode) iterator.next();
+          dataNode.shutdown();
+        }
+      }
+    }
+
     void handleDiskError( String errMsgr ) {
         LOG.warn( ""DataNode is shutting down.\n"" + errMsgr );
         try {
@@ -880,14 +893,14 @@
      */
     public static void run(Configuration conf) throws IOException {
         String[] dataDirs = conf.getStrings(""dfs.data.dir"");
-        subThreadList = new Vector(dataDirs.length);
+        subDataNodeList = new HashMap(dataDirs.length);
         for (int i = 0; i < dataDirs.length; i++) {
           DataNode dn = makeInstanceForDir(dataDirs[i], conf);
           if (dn != null) {
             Thread t = new Thread(dn, ""DataNode: ""+dataDirs[i]);
             t.setDaemon(true); // needed for JUnit testing
             t.start();
-            subThreadList.add(t);
+            subDataNodeList.put(dn,t);
           }
         }
     }
@@ -901,7 +914,7 @@
     run(conf);
 
     //  Wait for sub threads to exit
-    for (Iterator iterator = subThreadList.iterator(); iterator.hasNext();) {
+    for (Iterator iterator = subDataNodeList.entrySet().iterator(); iterator.hasNext();) {
       Thread threadDataNode = (Thread) iterator.next();
       try {
         threadDataNode.join();
"
hadoop,a60044aacd30752ea5e1b65e2964a6eb962857b4,"HADOOP-354.  Make public methods to stop DFS daemons.  Contributed by Barry Kaplan.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@421837 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-14 08:47:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/NameNode.java b/src/java/org/apache/hadoop/dfs/NameNode.java
index 817d4b2..710462b 100644
--- a/src/java/org/apache/hadoop/dfs/NameNode.java
+++ b/src/java/org/apache/hadoop/dfs/NameNode.java
@@ -114,9 +114,8 @@
 
     /**
      * Stop all NameNode threads and wait for all to finish.
-     * Package-only access since this is intended for JUnit testing.
     */
-    void stop() {
+    public void stop() {
       if (! stopRequested) {
         stopRequested = true;
         namesystem.close();
"
hadoop,a60044aacd30752ea5e1b65e2964a6eb962857b4,"HADOOP-354.  Make public methods to stop DFS daemons.  Contributed by Barry Kaplan.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@421837 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-14 08:47:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/RPC.java b/src/java/org/apache/hadoop/ipc/RPC.java
index d9f0646..e9024ad 100644
--- a/src/java/org/apache/hadoop/ipc/RPC.java
+++ b/src/java/org/apache/hadoop/ipc/RPC.java
@@ -135,6 +135,14 @@
     return CLIENT;
   }
 
+  /**
+   * Stop all RPC client connections
+   */
+  public static synchronized void stopClient(){
+    if(CLIENT != null)
+      CLIENT.stop();
+  }
+
   private static class Invoker implements InvocationHandler {
     private InetSocketAddress address;
     private Client client;
"
hadoop,9f12b30f0ebb2fadb25a568d3a16474b51a0c685,"HADOOP-327.  Fix ToolBase to not call System.exit() when an exception is thrown.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@421189 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-12 08:36:53,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSShell.java b/src/java/org/apache/hadoop/dfs/DFSShell.java
index 632ae8c..d2a1a4b 100644
--- a/src/java/org/apache/hadoop/dfs/DFSShell.java
+++ b/src/java/org/apache/hadoop/dfs/DFSShell.java
@@ -356,7 +356,7 @@
     /**
      * main() has some simple utility methods
      */
-    public static void main(String argv[]) throws IOException {
+    public static void main(String argv[]) throws Exception {
         new DFSShell().doMain(new Configuration(), argv);
     }
 }
"
hadoop,9f12b30f0ebb2fadb25a568d3a16474b51a0c685,"HADOOP-327.  Fix ToolBase to not call System.exit() when an exception is thrown.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@421189 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-12 08:36:53,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobClient.java b/src/java/org/apache/hadoop/mapred/JobClient.java
index 62ac52c..ead2a27 100644
--- a/src/java/org/apache/hadoop/mapred/JobClient.java
+++ b/src/java/org/apache/hadoop/mapred/JobClient.java
@@ -454,7 +454,7 @@
     
     /**
      */
-    public static void main(String argv[]) throws IOException {
+    public static void main(String argv[]) throws Exception {
         new JobClient().doMain(new Configuration(), argv);
     }
 }
"
hadoop,9f12b30f0ebb2fadb25a568d3a16474b51a0c685,"HADOOP-327.  Fix ToolBase to not call System.exit() when an exception is thrown.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@421189 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-12 08:36:53,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/util/CopyFiles.java b/src/java/org/apache/hadoop/util/CopyFiles.java
index be58143..a8e7645 100644
--- a/src/java/org/apache/hadoop/util/CopyFiles.java
+++ b/src/java/org/apache/hadoop/util/CopyFiles.java
@@ -238,7 +238,7 @@
    * input files. The mapper actually copies the files allotted to it. And
    * the reduce is empty.
    */
-  public int run(String[] args) throws IOException {
+  public int run(String[] args) throws Exception {
     String srcPath = null;
     String destPath = null;
     boolean ignoreReadFailures = false;
@@ -392,7 +392,7 @@
     return exitCode;
   }
   
-  public static void main(String[] args) throws IOException {
+  public static void main(String[] args) throws Exception {
       new CopyFiles().doMain(
               new JobConf(new Configuration(), CopyFiles.class), 
               args);
"
hadoop,9f12b30f0ebb2fadb25a568d3a16474b51a0c685,"HADOOP-327.  Fix ToolBase to not call System.exit() when an exception is thrown.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@421189 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-12 08:36:53,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/util/ToolBase.java b/src/java/org/apache/hadoop/util/ToolBase.java
index f3be6f0..2140a8d 100644
--- a/src/java/org/apache/hadoop/util/ToolBase.java
+++ b/src/java/org/apache/hadoop/util/ToolBase.java
@@ -34,35 +34,47 @@
 /*************************************************************
  * This is a base class to support generic commonad options.
  * Generic command options allow a user to specify a namenode,
- * a job tracker etc. Generic options supported are
- * -conf <configuration file>     specify an application configuration file
- * -D <property=value>            use value for given property
- * -fs <local|namenode:port>      specify a namenode
- * -jt <local|jobtracker:port>    specify a job tracker
+ * a job tracker etc. Generic options supported are 
+ * <p>-conf <configuration file>     specify an application configuration file
+ * <p>-D <property=value>            use value for given property
+ * <p>-fs <local|namenode:port>      specify a namenode
+ * <p>-jt <local|jobtracker:port>    specify a job tracker
+ * <br>
+ * <p>The general command line syntax is
+ * <p>bin/hadoop command [genericOptions] [commandOptions]
  * 
- * The general command line syntax is
- * bin/hadoop command [genericOptions] [commandOptions]
- * 
- * For every tool that inherits from ToolBase, generic options are 
+ * <p>For every tool that inherits from ToolBase, generic options are 
  * handled by ToolBase while command options are passed to the tool.
  * Generic options handling is implemented using Common CLI.
  * 
- * Tools that inherit from ToolBase in Hadoop are
+ * <p>Tools that inherit from ToolBase in Hadoop are
  * DFSShell, DFSck, JobClient, and CopyFiles.
- * 
- * Examples using generic options are
- * bin/hadoop dfs -fs darwin:8020 -ls /data
+ * <br>
+ * <p>Examples using generic options are
+ * <p>bin/hadoop dfs -fs darwin:8020 -ls /data
+ * <p><blockquote><pre>
  *     list /data directory in dfs with namenode darwin:8020
- * bin/hadoop dfs -D fs.default.name=darwin:8020 -ls /data
+ * </pre></blockquote>
+ * <p>bin/hadoop dfs -D fs.default.name=darwin:8020 -ls /data
+ * <p><blockquote><pre>
  *     list /data directory in dfs with namenode darwin:8020
- * bin/hadoop dfs -conf hadoop-site.xml -ls /data
+ * </pre></blockquote>
+ * <p>bin/hadoop dfs -conf hadoop-site.xml -ls /data
+ * <p><blockquote><pre>
  *     list /data directory in dfs with conf specified in hadoop-site.xml
- * bin/hadoop job -D mapred.job.tracker=darwin:50020 -submit job.xml
+ * </pre></blockquote>
+ * <p>bin/hadoop job -D mapred.job.tracker=darwin:50020 -submit job.xml
+ * <p><blockquote><pre>
  *     submit a job to job tracker darwin:50020
- * bin/hadoop job -jt darwin:50020 -submit job.xml
+ * </pre></blockquote>
+ * <p>bin/hadoop job -jt darwin:50020 -submit job.xml
+ * <p><blockquote><pre>
  *     submit a job to job tracker darwin:50020
- * bin/hadoop job -jt local -submit job.xml
+ * </pre></blockquote>
+ * <p>bin/hadoop job -jt local -submit job.xml
+ * <p><blockquote><pre>
  *     submit a job to local runner
+ * </pre></blockquote>
  *        
  * @author hairong
  *
@@ -161,31 +173,15 @@
     }
 
     /**
-     * Execute a command
-     * @param conf Application default configuration
-     * @param args User-specified arguments
-     * @return Exit code
-     * @throws Exception
-     */
-    public int executeCommand(Configuration conf, String[] args) throws Exception {
-        String [] commandOptions = parseGeneralOptions(conf, args);
-        setConf(conf);
-        return this.run(commandOptions);
-    }
-    /**
      * Work as a main program: execute a command and handle exception if any
      * @param conf Application default configuration
      * @param args User-specified arguments
+     * @throws Exception
      */
-    public final void doMain(Configuration conf, String[] args) {
-        try {
-            System.exit(executeCommand(conf, args));
-        }
-        catch (Exception e) {
-            LOG.warn(e.getMessage());
-            e.printStackTrace();
-            System.exit(-1);
-        }
+    public final void doMain(Configuration conf, String[] args) throws Exception {
+        String [] commandOptions = parseGeneralOptions(conf, args);
+        setConf(conf);
+        this.run(commandOptions);
     }
 
 }
"
hadoop,93ea3bb5d245fb0c058321d5043eabe2407b37ad,"HADOOP-358.  Fix a NPE in Path.equals().  Contributed by Frdric Bertin.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@420772 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-11 09:25:40,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/Path.java b/src/java/org/apache/hadoop/fs/Path.java
index 2c08ec6..9cfbb24 100644
--- a/src/java/org/apache/hadoop/fs/Path.java
+++ b/src/java/org/apache/hadoop/fs/Path.java
@@ -156,7 +156,7 @@
     return
       this.isAbsolute == that.isAbsolute &&
       Arrays.equals(this.elements, that.elements) &&
-      this.drive == null ? true : this.drive.equals(that.drive);
+      (this.drive == null ? true : this.drive.equals(that.drive));
   }
 
   public int hashCode() {
"
hadoop,07cce97dfd213b45c166f76f35d5d225385f9ebd,"HADOOP-313.  Permit task state to be saved so that single tasks may be manually re-executed when debugging.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@420760 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-11 08:16:28,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/conf/Configuration.java b/src/java/org/apache/hadoop/conf/Configuration.java
index 6d322d2..f716362 100644
--- a/src/java/org/apache/hadoop/conf/Configuration.java
+++ b/src/java/org/apache/hadoop/conf/Configuration.java
@@ -494,6 +494,14 @@
     }
   }
 
+  /**
+   * Set the class loader that will be used to load the various objects.
+   * @param classLoader the new class loader
+   */
+  public void setClassLoader(ClassLoader classLoader) {
+    this.classLoader = classLoader;
+  }
+  
   public String toString() {
     StringBuffer sb = new StringBuffer();
     sb.append(""Configuration: "");
"
hadoop,07cce97dfd213b45c166f76f35d5d225385f9ebd,"HADOOP-313.  Permit task state to be saved so that single tasks may be manually re-executed when debugging.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@420760 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-11 08:16:28,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobConf.java b/src/java/org/apache/hadoop/mapred/JobConf.java
index ac9321f..9865b66 100644
--- a/src/java/org/apache/hadoop/mapred/JobConf.java
+++ b/src/java/org/apache/hadoop/mapred/JobConf.java
@@ -194,6 +194,22 @@
   public void setUser(String user) {
     set(""user.name"", user);
   }
+
+  /**
+   * Set whether the framework shoul keep the intermediate files for 
+   * failed tasks.
+   */
+  public void setKeepFailedTaskFiles(boolean keep) {
+    setBoolean(""keep.failed.task.files"", keep);
+  }
+  
+  /**
+   * Should the temporary files for failed tasks be kept?
+   * @return should the files be kept?
+   */
+  public boolean getKeepFailedTaskFiles() {
+    return getBoolean(""keep.failed.task.files"", false);
+  }
   
   /**
    * Set the current working directory for the default file system
"
hadoop,07cce97dfd213b45c166f76f35d5d225385f9ebd,"HADOOP-313.  Permit task state to be saved so that single tasks may be manually re-executed when debugging.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@420760 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-11 08:16:28,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/LocalJobRunner.java b/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
index 7557c60..cba669e 100644
--- a/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
+++ b/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
@@ -78,13 +78,14 @@
         // split input into minimum number of splits
         FileSplit[] splits;
         splits = job.getInputFormat().getSplits(fs, job, 1);
-
+        String jobId = profile.getJobId();
         
         // run a map task for each split
         job.setNumReduceTasks(1);                 // force a single reduce task
         for (int i = 0; i < splits.length; i++) {
           mapIds.add(""map_"" + newId());
-          MapTask map = new MapTask(file, (String)mapIds.get(i), splits[i]);
+          MapTask map = new MapTask(jobId, file, (String)mapIds.get(i), i,
+                                    splits[i]);
           map.setConf(job);
           map_tasks += 1;
           map.run(job, this);
@@ -104,8 +105,8 @@
         }
 
         // run a single reduce task
-        ReduceTask reduce = new ReduceTask(profile.getJobId(), file, 
-                                           reduceId, mapIds.size(),0);
+        ReduceTask reduce = new ReduceTask(jobId, file, 
+                                           reduceId, 0, mapIds.size());
         reduce.setConf(job);
         reduce_tasks += 1;
         reduce.run(job, this);
"
hadoop,07cce97dfd213b45c166f76f35d5d225385f9ebd,"HADOOP-313.  Permit task state to be saved so that single tasks may be manually re-executed when debugging.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@420760 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-11 08:16:28,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/MapTask.java b/src/java/org/apache/hadoop/mapred/MapTask.java
index aecedfc..3e53ea1 100644
--- a/src/java/org/apache/hadoop/mapred/MapTask.java
+++ b/src/java/org/apache/hadoop/mapred/MapTask.java
@@ -39,8 +39,9 @@
 
   public MapTask() {}
 
-  public MapTask(String jobFile, String taskId, FileSplit split) {
-    super(jobFile, taskId);
+  public MapTask(String jobId, String jobFile, String taskId, 
+                 int partition, FileSplit split) {
+    super(jobId, jobFile, taskId, partition);
     this.split = split;
   }
 
@@ -48,6 +49,13 @@
       return true;
   }
 
+  public void localizeConfiguration(JobConf conf) {
+    super.localizeConfiguration(conf);
+    conf.set(""map.input.file"", split.getPath().toString());
+    conf.setLong(""map.input.start"", split.getStart());
+    conf.setLong(""map.input.length"", split.getLength());
+  }
+  
   public TaskRunner createRunner(TaskTracker tracker) {
     return new MapTaskRunner(this, tracker, this.conf);
   }
"
hadoop,07cce97dfd213b45c166f76f35d5d225385f9ebd,"HADOOP-313.  Permit task state to be saved so that single tasks may be manually re-executed when debugging.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@420760 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-11 08:16:28,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/ReduceTask.java b/src/java/org/apache/hadoop/mapred/ReduceTask.java
index f9f835a..f4f9a8e 100644
--- a/src/java/org/apache/hadoop/mapred/ReduceTask.java
+++ b/src/java/org/apache/hadoop/mapred/ReduceTask.java
@@ -36,9 +36,7 @@
        });
   }
 
-  private UTF8 jobId = new UTF8();
   private int numMaps;
-  private int partition;
   private boolean sortComplete;
 
   { getProgress().setStatus(""reduce""); }
@@ -52,11 +50,9 @@
   public ReduceTask() {}
 
   public ReduceTask(String jobId, String jobFile, String taskId,
-                    int numMaps, int partition) {
-    super(jobFile, taskId);
-    this.jobId.set(jobId);
+                    int partition, int numMaps) {
+    super(jobId, jobFile, taskId, partition);
     this.numMaps = numMaps;
-    this.partition = partition;
   }
 
   public TaskRunner createRunner(TaskTracker tracker) throws IOException {
@@ -67,31 +63,26 @@
       return false;
   }
 
-  /**
-   * Get the job name for this task.
-   * @return the job name
-   */
-  public UTF8 getJobId() {
-    return jobId;
-  }
-  
   public int getNumMaps() { return numMaps; }
-  public int getPartition() { return partition; }
+  
+  /**
+   * Localize the given JobConf to be specific for this task.
+   */
+  public void localizeConfiguration(JobConf conf) {
+    super.localizeConfiguration(conf);
+    conf.setNumMapTasks(numMaps);
+  }
 
   public void write(DataOutput out) throws IOException {
     super.write(out);
 
-    jobId.write(out);
     out.writeInt(numMaps);                        // write the number of maps
-    out.writeInt(partition);                      // write partition
   }
 
   public void readFields(DataInput in) throws IOException {
     super.readFields(in);
 
-    jobId.readFields(in);
     numMaps = in.readInt();
-    this.partition = in.readInt();                // read partition
   }
 
   /** Iterates values while keys match in sorted input. */
@@ -215,7 +206,7 @@
       // sort the input file
       SequenceFile.Sorter sorter =
         new SequenceFile.Sorter(lfs, comparator, valueClass, job);
-      sorter.sort(mapFiles, sortedFile, true);              // sort
+      sorter.sort(mapFiles, sortedFile, !conf.getKeepFailedTaskFiles()); // sort
 
     } finally {
       sortComplete = true;
"
hadoop,07cce97dfd213b45c166f76f35d5d225385f9ebd,"HADOOP-313.  Permit task state to be saved so that single tasks may be manually re-executed when debugging.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@420760 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-11 08:16:28,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/Task.java b/src/java/org/apache/hadoop/mapred/Task.java
index 4b84473..e0769e0 100644
--- a/src/java/org/apache/hadoop/mapred/Task.java
+++ b/src/java/org/apache/hadoop/mapred/Task.java
@@ -35,15 +35,20 @@
 
   private String jobFile;                         // job configuration file
   private String taskId;                          // unique, includes job id
+  private String jobId;                           // unique jobid
+  private int partition;                          // id within job
+  
   ////////////////////////////////////////////
   // Constructors
   ////////////////////////////////////////////
 
   public Task() {}
 
-  public Task(String jobFile, String taskId) {
+  public Task(String jobId, String jobFile, String taskId, int partition) {
     this.jobFile = jobFile;
     this.taskId = taskId;
+    this.jobId = jobId;
+    this.partition = partition;
   }
 
   ////////////////////////////////////////////
@@ -52,6 +57,22 @@
   public void setJobFile(String jobFile) { this.jobFile = jobFile; }
   public String getJobFile() { return jobFile; }
   public String getTaskId() { return taskId; }
+  
+  /**
+   * Get the job name for this task.
+   * @return the job name
+   */
+  public String getJobId() {
+    return jobId;
+  }
+  
+  /**
+   * Get the index of this task within the job.
+   * @return the integer part of the task id
+   */
+  public int getPartition() {
+    return partition;
+  }
 
   ////////////////////////////////////////////
   // Writable methods
@@ -60,14 +81,28 @@
   public void write(DataOutput out) throws IOException {
     UTF8.writeString(out, jobFile);
     UTF8.writeString(out, taskId);
+    UTF8.writeString(out, jobId);
+    out.writeInt(partition);
   }
   public void readFields(DataInput in) throws IOException {
     jobFile = UTF8.readString(in);
     taskId = UTF8.readString(in);
+    jobId = UTF8.readString(in);
+    partition = in.readInt();
   }
 
   public String toString() { return taskId; }
 
+  /**
+   * Localize the given JobConf to be specific for this task.
+   */
+  public void localizeConfiguration(JobConf conf) {
+    conf.set(""mapred.task.id"", taskId);
+    conf.setBoolean(""mapred.task.is.map"",isMapTask());
+    conf.setInt(""mapred.task.partition"", partition);
+    conf.set(""mapred.job.id"", jobId);
+  }
+  
   /** Run this task as a part of the named job.  This method is executed in the
    * child process and is what invokes user-supplied map, reduce, etc. methods.
    * @param umbilical for progress reports
"
hadoop,07cce97dfd213b45c166f76f35d5d225385f9ebd,"HADOOP-313.  Permit task state to be saved so that single tasks may be manually re-executed when debugging.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@420760 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-11 08:16:28,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskInProgress.java b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
index aa29458..3b97f17 100644
--- a/src/java/org/apache/hadoop/mapred/TaskInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
@@ -413,12 +413,12 @@
 
             String taskid = (String) usableTaskIds.first();
             usableTaskIds.remove(taskid);
+            String jobId = job.getProfile().getJobId();
 
             if (isMapTask()) {
-                t = new MapTask(jobFile, taskid, split);
+                t = new MapTask(jobId, jobFile, taskid, partition, split);
             } else {
-                t = new ReduceTask(job.getProfile().getJobId(), jobFile, taskid, 
-                                   numMaps, partition);
+                t = new ReduceTask(jobId, jobFile, taskid, partition, numMaps);
             }
             t.setConf(conf);
 
"
hadoop,07cce97dfd213b45c166f76f35d5d225385f9ebd,"HADOOP-313.  Permit task state to be saved so that single tasks may be manually re-executed when debugging.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@420760 13f79535-47bb-0310-9956-ffa450edef68
",2006-07-11 08:16:28,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 81359b2..ee1e173 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -652,6 +652,7 @@
         boolean wasKilled = false;
         private JobConf defaultJobConf;
         private JobConf localJobConf;
+        private boolean keepFailedTaskFiles;
 
         /**
          */
@@ -682,7 +683,6 @@
             t.setJobFile(localJobFile.toString());
 
             localJobConf = new JobConf(localJobFile);
-            localJobConf.set(""mapred.task.id"", task.getTaskId());
             localJobConf.set(""mapred.local.dir"",
                     this.defaultJobConf.get(""mapred.local.dir""));
             String jarFile = localJobConf.getJar();
@@ -690,6 +690,7 @@
               fs.copyToLocalFile(new Path(jarFile), localJarFile);
               localJobConf.setJar(localJarFile.toString());
             }
+            task.localizeConfiguration(localJobConf);
 
             FileSystem localFs = FileSystem.getNamed(""local"", fConf);
             OutputStream out = localFs.create(localJobFile);
@@ -701,6 +702,7 @@
             // set the task's configuration to the local job conf
             // rather than the default.
             t.setConf(localJobConf);
+            keepFailedTaskFiles = localJobConf.getKeepFailedTaskFiles();
         }
 
         /**
@@ -875,6 +877,9 @@
             LOG.debug(""Cleaning up "" + taskId);
             synchronized (TaskTracker.this) {
                tasks.remove(taskId);
+               if (runstate == TaskStatus.FAILED && keepFailedTaskFiles) {
+                 return;
+               }
                synchronized (this) {
                  try {
                     runner.close();
"
hadoop,b57de6708151912f7e20ca383e2e416629b89f41,"HADOOP-318.  Keep slow DFS output from causing task timeouts.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417874 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-28 21:05:38,Doug Cutting,"diff --git a/src/examples/org/apache/hadoop/examples/ExampleDriver.java b/src/examples/org/apache/hadoop/examples/ExampleDriver.java
index 94f7b1d..65fa9f1 100644
--- a/src/examples/org/apache/hadoop/examples/ExampleDriver.java
+++ b/src/examples/org/apache/hadoop/examples/ExampleDriver.java
@@ -36,6 +36,7 @@
 	    pgd.addClass(""randomwriter"", RandomWriter.class, 
                         ""A random writer benchmark that writes 10GB per node."");
             pgd.addClass(""sort"", Sort.class, ""A sort benchmark that sorts the data written by the random writer."");
+            pgd.addClass(""pi"", PiBenchmark.class, ""A benchmark that estimates Pi using monte-carlo method."");
             pgd.driver(argv);
 	}
 	catch(Throwable e){
"
hadoop,b57de6708151912f7e20ca383e2e416629b89f41,"HADOOP-318.  Keep slow DFS output from causing task timeouts.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417874 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-28 21:05:38,Doug Cutting,"diff --git a/src/examples/org/apache/hadoop/examples/RandomWriter.java b/src/examples/org/apache/hadoop/examples/RandomWriter.java
index 2f88205..ef4596c 100644
--- a/src/examples/org/apache/hadoop/examples/RandomWriter.java
+++ b/src/examples/org/apache/hadoop/examples/RandomWriter.java
@@ -76,7 +76,8 @@
       String filename = ((UTF8) value).toString();
       SequenceFile.Writer writer = 
         new SequenceFile.Writer(fileSys, new Path(filename), 
-                                BytesWritable.class, BytesWritable.class);
+                                BytesWritable.class, BytesWritable.class,
+                                reporter);
       int itemCount = 0;
       while (numBytesToWrite > 0) {
         int keyLength = minKeySize + 
"
hadoop,b57de6708151912f7e20ca383e2e416629b89f41,"HADOOP-318.  Keep slow DFS output from causing task timeouts.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417874 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-28 21:05:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSClient.java b/src/java/org/apache/hadoop/dfs/DFSClient.java
index 44b8601..8066ab1 100644
--- a/src/java/org/apache/hadoop/dfs/DFSClient.java
+++ b/src/java/org/apache/hadoop/dfs/DFSClient.java
@@ -216,7 +216,23 @@
     public FSOutputStream create( UTF8 src, 
                                   boolean overwrite
                                 ) throws IOException {
-      return create( src, overwrite, defaultReplication, defaultBlockSize);
+      return create( src, overwrite, defaultReplication, defaultBlockSize, null);
+    }
+    
+    /**
+     * Create a new dfs file and return an output stream for writing into it
+     * with write-progress reporting. 
+     * 
+     * @param src stream name
+     * @param overwrite do not check for file existence if true
+     * @return output stream
+     * @throws IOException
+     */
+    public FSOutputStream create( UTF8 src, 
+                                  boolean overwrite,
+                                  Progressable progress
+                                ) throws IOException {
+      return create( src, overwrite, defaultReplication, defaultBlockSize, null);
     }
     
     /**
@@ -234,15 +250,34 @@
                                   short replication,
                                   long blockSize
                                 ) throws IOException {
+      return create(src, overwrite, replication, blockSize, null);
+    }
+
+    /**
+     * Create a new dfs file with the specified block replication 
+     * with write-progress reporting and return an output stream for writing
+     * into the file.  
+     * 
+     * @param src stream name
+     * @param overwrite do not check for file existence if true
+     * @param replication block replication
+     * @return output stream
+     * @throws IOException
+     */
+    public FSOutputStream create( UTF8 src, 
+                                  boolean overwrite, 
+                                  short replication,
+                                  long blockSize,
+                                  Progressable progress
+                                ) throws IOException {
       checkOpen();
       FSOutputStream result = new DFSOutputStream(src, overwrite, 
-                                                  replication, blockSize);
+                                                  replication, blockSize, progress);
       synchronized (pendingCreates) {
         pendingCreates.put(src.toString(), result);
       }
       return result;
     }
-
     /**
      * Set replication for an existing file.
      * 
@@ -718,11 +753,13 @@
         private String datanodeName;
         private long blockSize;
 
+        private Progressable progress;
         /**
          * Create a new output stream to the given DataNode.
          */
         public DFSOutputStream(UTF8 src, boolean overwrite, 
-                               short replication, long blockSize
+                               short replication, long blockSize,
+                               Progressable progress
                                ) throws IOException {
             this.src = src;
             this.overwrite = overwrite;
@@ -730,6 +767,10 @@
             this.backupFile = newBackupFile();
             this.blockSize = blockSize;
             this.backupStream = new FileOutputStream(backupFile);
+            this.progress = progress;
+            if (progress != null) {
+                LOG.debug(""Set non-null progress callback on DFSOutputStream ""+src);
+            }
         }
 
         private File newBackupFile() throws IOException {
@@ -980,6 +1021,7 @@
                     while (bytesRead > 0) {
                         blockStream.writeLong((long) bytesRead);
                         blockStream.write(buf, 0, bytesRead);
+                        if (progress != null) { progress.progress(); }
                         bytesRead = in.read(buf);
                     }
                     internalClose();
"
hadoop,b57de6708151912f7e20ca383e2e416629b89f41,"HADOOP-318.  Keep slow DFS output from causing task timeouts.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417874 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-28 21:05:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index 1cf5a04..4291a96 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -212,6 +212,17 @@
         shutdown();
     }
     
+    private static class Count {
+        int value = 0;
+        Count(int init) { value = init; }
+        synchronized void incr() { value++; }
+        synchronized void decr() { value--; }
+        public String toString() { return Integer.toString(value); }
+        public int getValue() { return value; }
+    }
+    
+    Count xceiverCount = new Count(0);
+    
     /**
      * Main loop for the DataNode.  Runs until shutdown,
      * forever calling remote NameNode functions.
@@ -243,7 +254,8 @@
             BlockCommand cmd = namenode.sendHeartbeat(dnRegistration, 
                                                       data.getCapacity(), 
                                                       data.getRemaining(), 
-                                                      xmitsInProgress);
+                                                      xmitsInProgress,
+                                                      xceiverCount.getValue());
             //LOG.info(""Just sent heartbeat, with name "" + localName);
             lastHeartbeat = now;
 
@@ -345,6 +357,8 @@
       }
     } // offerService
 
+    
+    
     /**
      * Server used for receiving/sending a block of data.
      * This is created to listen for requests from clients or 
@@ -366,6 +380,7 @@
                     Socket s = ss.accept();
                     //s.setSoTimeout(READ_TIMEOUT);
                     data.checkDataDir();
+                    xceiverCount.incr();
                     new Daemon(new DataXceiver(s)).start();
                 }
                 ss.close();
@@ -393,6 +408,7 @@
         Socket s;
         public DataXceiver(Socket s) {
             this.s = s;
+            LOG.debug(""Number of active connections is: ""+xceiverCount);
         }
 
         /**
@@ -421,6 +437,8 @@
               LOG.warn(""DataXCeiver"", ie);
             } finally {
                 try {
+                    xceiverCount.decr();
+                    LOG.debug(""Number of active connections is: ""+xceiverCount);
                     s.close();
                 } catch (IOException ie2) {
                 }
"
hadoop,b57de6708151912f7e20ca383e2e416629b89f41,"HADOOP-318.  Keep slow DFS output from causing task timeouts.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417874 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-28 21:05:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DatanodeInfo.java b/src/java/org/apache/hadoop/dfs/DatanodeInfo.java
index 60babaf..0613152 100644
--- a/src/java/org/apache/hadoop/dfs/DatanodeInfo.java
+++ b/src/java/org/apache/hadoop/dfs/DatanodeInfo.java
@@ -28,6 +28,7 @@
  * @author Mike Cafarella
  **************************************************/
 public class DatanodeInfo extends DatanodeID implements Writable, Comparable {
+  private int xceiverCount;
 
     static {                                      // register a ctor
       WritableFactories.setFactory
@@ -36,18 +37,18 @@
            public Writable newInstance() { return new DatanodeInfo(); }
          });
     }
-
+  /** number of active connections */
+  public int getXceiverCount() { return xceiverCount; }
+  
     private long capacityBytes, remainingBytes, lastUpdate;
     private volatile TreeSet blocks;
-
     /** Create an empty DatanodeInfo.
      */
     public DatanodeInfo() {
-        this(new String(), new String(), 0, 0);
+        this(new String(), new String(), 0, 0, 0);
     }
-
     public DatanodeInfo( DatanodeID nodeID ) {
-      this( nodeID.getName(), nodeID.getStorageID(), 0, 0);
+      this( nodeID.getName(), nodeID.getStorageID(), 0, 0, 0);
     }
     
    /**
@@ -55,22 +56,22 @@
     */
     public DatanodeInfo(DatanodeID nodeID, 
                         long capacity, 
-                        long remaining) {
-      this( nodeID.getName(), nodeID.getStorageID(), capacity, remaining );
+                        long remaining,
+                        int xceiverCount) {
+      this( nodeID.getName(), nodeID.getStorageID(), capacity, remaining, xceiverCount );
     }
-
    /**
     * @param name hostname:portNumber as String object.
     */
     public DatanodeInfo(String name, 
                         String storageID, 
                         long capacity, 
-                        long remaining) {
+                        long remaining,
+                        int xceiverCount) {
         super( name, storageID );
         this.blocks = new TreeSet();
-        updateHeartbeat(capacity, remaining);
+        updateHeartbeat(capacity, remaining, xceiverCount);
     }
-
    /**
     */
     public void updateBlocks(Block newBlocks[]) {
@@ -88,9 +89,10 @@
 
     /**
      */
-    public void updateHeartbeat(long capacity, long remaining) {
+    public void updateHeartbeat(long capacity, long remaining, int xceiverCount) {
         this.capacityBytes = capacity;
         this.remainingBytes = remaining;
+        this.xceiverCount = xceiverCount;
         this.lastUpdate = System.currentTimeMillis();
     }
 
@@ -119,7 +121,6 @@
         DatanodeInfo d = (DatanodeInfo) o;
         return name.compareTo(d.getName());
     }
-
     /////////////////////////////////////////////////
     // Writable
     /////////////////////////////////////////////////
@@ -131,6 +132,7 @@
         out.writeLong(capacityBytes);
         out.writeLong(remainingBytes);
         out.writeLong(lastUpdate);
+        out.writeInt(xceiverCount);
 
         /**
         out.writeInt(blocks.length);
@@ -151,7 +153,7 @@
         this.capacityBytes = in.readLong();
         this.remainingBytes = in.readLong();
         this.lastUpdate = in.readLong();
-
+        this.xceiverCount = in.readInt();
         /**
         int numBlocks = in.readInt();
         this.blocks = new Block[numBlocks];
"
hadoop,b57de6708151912f7e20ca383e2e416629b89f41,"HADOOP-318.  Keep slow DFS output from causing task timeouts.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417874 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-28 21:05:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DatanodeProtocol.java b/src/java/org/apache/hadoop/dfs/DatanodeProtocol.java
index f0769ef..ab3bae4 100644
--- a/src/java/org/apache/hadoop/dfs/DatanodeProtocol.java
+++ b/src/java/org/apache/hadoop/dfs/DatanodeProtocol.java
@@ -52,7 +52,8 @@
      */
     public BlockCommand sendHeartbeat(DatanodeRegistration registration,
                                       long capacity, long remaining,
-                                      int xmitsInProgress) throws IOException;
+                                      int xmitsInProgress,
+                                      int xceiverCount) throws IOException;
 
     /**
      * blockReport() tells the NameNode about all the locally-stored blocks.
"
hadoop,b57de6708151912f7e20ca383e2e416629b89f41,"HADOOP-318.  Keep slow DFS output from causing task timeouts.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417874 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-28 21:05:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java b/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
index 39ff609..579b252 100644
--- a/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
+++ b/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
@@ -22,6 +22,7 @@
 import org.apache.hadoop.io.*;
 import org.apache.hadoop.fs.*;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.util.Progressable;
 
 /****************************************************************
  * Implementation of the abstract FileSystem for the DFS system.
@@ -96,6 +97,13 @@
       return dfs.create(getPath(f), overwrite, replication, blockSize);
     }
 
+    public FSOutputStream createRaw(Path f, boolean overwrite, 
+                                    short replication, long blockSize,
+                                    Progressable progress)
+      throws IOException {
+      return dfs.create(getPath(f), overwrite, replication, blockSize, progress);
+    }
+    
     public boolean setReplicationRaw( Path src, 
                                       short replication
                                     ) throws IOException {
"
hadoop,b57de6708151912f7e20ca383e2e416629b89f41,"HADOOP-318.  Keep slow DFS output from causing task timeouts.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417874 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-28 21:05:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index 01d7879..f952285 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -1132,7 +1132,8 @@
      */
     public synchronized void gotHeartbeat(DatanodeID nodeID,
                                           long capacity, 
-                                          long remaining) throws IOException {
+                                          long remaining,
+                                          int xceiverCount) throws IOException {
       synchronized (heartbeats) {
         synchronized (datanodeMap) {
           long capacityDiff = 0;
@@ -1143,7 +1144,8 @@
           if (nodeinfo == null) {
             NameNode.stateChangeLog.debug(""BLOCK* NameSystem.gotHeartbeat: ""
                     +""brand-new heartbeat from ""+nodeID.getName() );
-            nodeinfo = new DatanodeInfo(nodeID, capacity, remaining);
+
+            nodeinfo = new DatanodeInfo(nodeID, capacity, remaining, xceiverCount);
             datanodeMap.put(nodeinfo.getStorageID(), nodeinfo);
             capacityDiff = capacity;
             remainingDiff = remaining;
@@ -1151,7 +1153,7 @@
             capacityDiff = capacity - nodeinfo.getCapacity();
             remainingDiff = remaining - nodeinfo.getRemaining();
             heartbeats.remove(nodeinfo);
-            nodeinfo.updateHeartbeat(capacity, remaining);
+            nodeinfo.updateHeartbeat(capacity, remaining, xceiverCount);
           }
           heartbeats.add(nodeinfo);
           totalCapacity += capacityDiff;
@@ -1771,6 +1773,7 @@
             }
         }
 
+        double avgLoad = 0.0;
         //
         // Build list of machines we can actually choose from
         //
@@ -1779,8 +1782,11 @@
             DatanodeInfo node = (DatanodeInfo) it.next();
             if (! forbiddenMachines.contains(node.getHost())) {
                 targetList.add(node);
+                avgLoad += node.getXceiverCount();
             }
         }
+        if (targetList.size() > 0) { avgLoad = avgLoad/targetList.size(); }
+        
         Collections.shuffle(targetList);
         
         //
@@ -1795,7 +1801,8 @@
                 for (Iterator it = targetList.iterator(); it.hasNext(); ) {
                     DatanodeInfo node = (DatanodeInfo) it.next();
                     if (clientMachine.equals(node.getHost())) {
-                        if (node.getRemaining() > blockSize * MIN_BLOCKS_FOR_WRITE) {
+                        if ((node.getRemaining() > blockSize * MIN_BLOCKS_FOR_WRITE) &&
+                            (node.getXceiverCount() < (2.0 * avgLoad))) {
                             return node;
                         }
                     }
@@ -1807,7 +1814,8 @@
             //
             for (Iterator it = targetList.iterator(); it.hasNext(); ) {
                 DatanodeInfo node = (DatanodeInfo) it.next();
-                if (node.getRemaining() > blockSize * MIN_BLOCKS_FOR_WRITE) {
+                if ((node.getRemaining() > blockSize * MIN_BLOCKS_FOR_WRITE) &&
+                    (node.getXceiverCount() < (2.0 * avgLoad))) {
                     return node;
                 }
             }
"
hadoop,b57de6708151912f7e20ca383e2e416629b89f41,"HADOOP-318.  Keep slow DFS output from causing task timeouts.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417874 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-28 21:05:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/NameNode.java b/src/java/org/apache/hadoop/dfs/NameNode.java
index d5dc183..817d4b2 100644
--- a/src/java/org/apache/hadoop/dfs/NameNode.java
+++ b/src/java/org/apache/hadoop/dfs/NameNode.java
@@ -366,9 +366,10 @@
     public BlockCommand sendHeartbeat(DatanodeRegistration nodeReg,
                                       long capacity, 
                                       long remaining,
-                                      int xmitsInProgress) throws IOException {
+                                      int xmitsInProgress,
+                                      int xceiverCount) throws IOException {
         verifyRequest( nodeReg );
-        namesystem.gotHeartbeat( nodeReg, capacity, remaining );
+        namesystem.gotHeartbeat( nodeReg, capacity, remaining, xceiverCount );
         
         //
         // Only ask datanodes to perform block operations (transfer, delete) 
"
hadoop,b57de6708151912f7e20ca383e2e416629b89f41,"HADOOP-318.  Keep slow DFS output from causing task timeouts.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417874 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-28 21:05:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FSDataOutputStream.java b/src/java/org/apache/hadoop/fs/FSDataOutputStream.java
index 87116a0..73fee54 100644
--- a/src/java/org/apache/hadoop/fs/FSDataOutputStream.java
+++ b/src/java/org/apache/hadoop/fs/FSDataOutputStream.java
@@ -19,6 +19,7 @@
 import java.util.zip.Checksum;
 import java.util.zip.CRC32;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.util.Progressable;
 
 /** Utility that wraps a {@link FSOutputStream} in a {@link DataOutputStream},
  * buffers output through a {@link BufferedOutputStream} and creates a checksum
@@ -41,7 +42,18 @@
                   long blockSize,
                   Configuration conf)
       throws IOException {
-      super(fs.createRaw(file, overwrite, replication, blockSize));
+      this(fs, file, overwrite, replication, blockSize, conf, null);
+    }
+
+    public Summer(FileSystem fs, 
+                  Path file, 
+                  boolean overwrite, 
+                  short replication,
+                  long blockSize,
+                  Configuration conf,
+                  Progressable progress)
+      throws IOException {
+      super(fs.createRaw(file, overwrite, replication, blockSize, progress));
       this.bytesPerSum = conf.getInt(""io.bytes.per.checksum"", 512);
       this.sums = new FSDataOutputStream(
             fs.createRaw(FileSystem.getChecksumFile(file), true, 
@@ -50,7 +62,7 @@
       sums.write(CHECKSUM_VERSION, 0, CHECKSUM_VERSION.length);
       sums.writeInt(this.bytesPerSum);
     }
-
+    
     public void write(byte b[], int off, int len) throws IOException {
       int summed = 0;
       while (summed < len) {
@@ -137,6 +149,17 @@
             bufferSize));
   }
 
+  public FSDataOutputStream(FileSystem fs, Path file,
+                            boolean overwrite, Configuration conf,
+                            int bufferSize, short replication, long blockSize,
+                            Progressable progress)
+  throws IOException {
+    super(new Buffer(
+            new PositionCache(
+                new Summer(fs, file, overwrite, replication, blockSize, conf, progress)), 
+            bufferSize));
+  }
+  
   /** Construct without checksums. */
   private FSDataOutputStream(FSOutputStream out, Configuration conf) throws IOException {
     this(out, conf.getInt(""io.file.buffer.size"", 4096));
"
hadoop,b57de6708151912f7e20ca383e2e416629b89f41,"HADOOP-318.  Keep slow DFS output from causing task timeouts.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417874 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-28 21:05:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FileSystem.java b/src/java/org/apache/hadoop/fs/FileSystem.java
index 8bf1e0e..4f4aa28 100644
--- a/src/java/org/apache/hadoop/fs/FileSystem.java
+++ b/src/java/org/apache/hadoop/fs/FileSystem.java
@@ -23,6 +23,7 @@
 
 import org.apache.hadoop.dfs.*;
 import org.apache.hadoop.conf.*;
+import org.apache.hadoop.util.Progressable;
 
 /****************************************************************
  * An abstract base class for a fairly generic filesystem.  It
@@ -180,6 +181,18 @@
     }
 
     /**
+     * Create an FSDataOutputStream at the indicated Path with write-progress
+     * reporting.
+     * Files are overwritten by default.
+     */
+    public FSDataOutputStream create(Path f, Progressable progress) throws IOException {
+      return create(f, true, 
+                    getConf().getInt(""io.file.buffer.size"", 4096),
+                    getDefaultReplication(),
+                    getDefaultBlockSize(), progress);
+    }
+
+    /**
      * Opens an FSDataOutputStream at the indicated Path.
      * Files are overwritten by default.
      */
@@ -192,6 +205,20 @@
     }
 
     /**
+     * Opens an FSDataOutputStream at the indicated Path with write-progress
+     * reporting.
+     * Files are overwritten by default.
+     */
+    public FSDataOutputStream create(Path f, short replication, Progressable progress)
+      throws IOException {
+      return create(f, true, 
+                    getConf().getInt(""io.file.buffer.size"", 4096),
+                    replication,
+                    getDefaultBlockSize(), progress);
+    }
+
+    
+    /**
      * Opens an FSDataOutputStream at the indicated Path.
      * @param f the file name to open
      * @param overwrite if a file with this name already exists, then if true,
@@ -208,6 +235,25 @@
     }
     
     /**
+     * Opens an FSDataOutputStream at the indicated Path with write-progress
+     * reporting.
+     * @param f the file name to open
+     * @param overwrite if a file with this name already exists, then if true,
+     *   the file will be overwritten, and if false an error will be thrown.
+     * @param bufferSize the size of the buffer to be used.
+     */
+    public FSDataOutputStream create( Path f, 
+                                      boolean overwrite,
+                                      int bufferSize,
+                                      Progressable progress
+                                    ) throws IOException {
+      return create( f, overwrite, bufferSize, 
+                     getDefaultReplication(),
+                     getDefaultBlockSize(), progress);
+    }
+    
+    
+    /**
      * Opens an FSDataOutputStream at the indicated Path.
      * @param f the file name to open
      * @param overwrite if a file with this name already exists, then if true,
@@ -225,6 +271,26 @@
                                     bufferSize, replication, blockSize );
     }
 
+    /**
+     * Opens an FSDataOutputStream at the indicated Path with write-progress
+     * reporting.
+     * @param f the file name to open
+     * @param overwrite if a file with this name already exists, then if true,
+     *   the file will be overwritten, and if false an error will be thrown.
+     * @param bufferSize the size of the buffer to be used.
+     * @param replication required block replication for the file. 
+     */
+    public FSDataOutputStream create( Path f, 
+                                      boolean overwrite,
+                                      int bufferSize,
+                                      short replication,
+                                      long blockSize,
+                                      Progressable progress
+                                    ) throws IOException {
+      return new FSDataOutputStream(this, f, overwrite, getConf(), 
+                                    bufferSize, replication, blockSize, progress );
+    }
+
     /** Opens an OutputStream at the indicated Path.
      * @param f the file name to open
      * @param overwrite if a file with this name already exists, then if true,
@@ -236,6 +302,18 @@
                                              long blockSize)
       throws IOException;
 
+    /** Opens an OutputStream at the indicated Path with write-progress
+     * reporting.
+     * @param f the file name to open
+     * @param overwrite if a file with this name already exists, then if true,
+     *   the file will be overwritten, and if false an error will be thrown.
+     * @param replication required block replication for the file. 
+     */
+    public abstract FSOutputStream createRaw(Path f, boolean overwrite, 
+                                             short replication,
+                                             long blockSize, Progressable progress)
+      throws IOException;
+    
     /** @deprecated Call {@link #createNewFile(Path)} instead. */
     public boolean createNewFile(File f) throws IOException {
       return createNewFile(new Path(f.toString()));
"
hadoop,b57de6708151912f7e20ca383e2e416629b89f41,"HADOOP-318.  Keep slow DFS output from causing task timeouts.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417874 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-28 21:05:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/LocalFileSystem.java b/src/java/org/apache/hadoop/fs/LocalFileSystem.java
index 1824cc0..ea1af0e 100644
--- a/src/java/org/apache/hadoop/fs/LocalFileSystem.java
+++ b/src/java/org/apache/hadoop/fs/LocalFileSystem.java
@@ -19,10 +19,8 @@
 import java.io.*;
 import java.util.*;
 import java.nio.channels.*;
-
-import org.apache.hadoop.fs.DF;
-import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.util.Progressable;
 
 /****************************************************************
  * Implement the FileSystem API for the native filesystem.
@@ -174,6 +172,13 @@
         return new LocalFSFileOutputStream(f);
     }
 
+    public FSOutputStream createRaw(Path f, boolean overwrite, 
+                                    short replication, long blockSize,
+                                    Progressable progress)
+      throws IOException {
+        // ignore write-progress reporter for local files
+        return createRaw(f, overwrite, replication, blockSize);
+    }
     /**
      * Replication is not supported for the local file system.
      */
"
hadoop,b57de6708151912f7e20ca383e2e416629b89f41,"HADOOP-318.  Keep slow DFS output from causing task timeouts.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417874 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-28 21:05:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/SequenceFile.java b/src/java/org/apache/hadoop/io/SequenceFile.java
index e48ba0f..2e76c03 100644
--- a/src/java/org/apache/hadoop/io/SequenceFile.java
+++ b/src/java/org/apache/hadoop/io/SequenceFile.java
@@ -26,6 +26,7 @@
 import org.apache.commons.logging.*;
 import org.apache.hadoop.fs.*;
 import org.apache.hadoop.conf.*;
+import org.apache.hadoop.util.Progressable;
 
 /** Support for flat files of binary key/value pairs. */
 public class SequenceFile {
@@ -88,6 +89,13 @@
       this(fs, name, keyClass, valClass, false);
     }
     
+    /** Create the named file with write-progress reporter. */
+    public Writer(FileSystem fs, Path name, Class keyClass, Class valClass,
+            Progressable progress)
+      throws IOException {
+      this(fs, name, keyClass, valClass, false, progress);
+    }
+    
     /** Create the named file.
      * @param compress if true, values are compressed.
      */
@@ -98,6 +106,17 @@
       init(fs.create(target), keyClass, valClass, compress);
     }
     
+    /** Create the named file with write-progress reporter.
+     * @param compress if true, values are compressed.
+     */
+    public Writer(FileSystem fs, Path name,
+                  Class keyClass, Class valClass, boolean compress,
+                  Progressable progress)
+      throws IOException {
+      this.target = name;
+      init(fs.create(target, progress), keyClass, valClass, compress);
+    }
+    
     /** Write to an arbitrary stream using a specified buffer size. */
     private Writer(FSDataOutputStream out,
                    Class keyClass, Class valClass, boolean compress)
"
hadoop,b57de6708151912f7e20ca383e2e416629b89f41,"HADOOP-318.  Keep slow DFS output from causing task timeouts.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417874 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-28 21:05:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/MapFileOutputFormat.java b/src/java/org/apache/hadoop/mapred/MapFileOutputFormat.java
index 09e78ca..600e13c 100644
--- a/src/java/org/apache/hadoop/mapred/MapFileOutputFormat.java
+++ b/src/java/org/apache/hadoop/mapred/MapFileOutputFormat.java
@@ -27,15 +27,18 @@
 import org.apache.hadoop.io.WritableComparable;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.util.Progressable;
 
 /** An {@link OutputFormat} that writes {@link MapFile}s. */
 public class MapFileOutputFormat extends OutputFormatBase {
 
   public RecordWriter getRecordWriter(FileSystem fs, JobConf job,
-                                      String name) throws IOException {
+                                      String name, Progressable progress)
+                                      throws IOException {
 
     Path file = new Path(job.getOutputPath(), name);
 
+    // ignore the progress parameter, since MapFile is local
     final MapFile.Writer out =
       new MapFile.Writer(fs, file.toString(),
                          job.getMapOutputKeyClass(),
"
hadoop,b57de6708151912f7e20ca383e2e416629b89f41,"HADOOP-318.  Keep slow DFS output from causing task timeouts.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417874 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-28 21:05:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/MapOutputLocation.java b/src/java/org/apache/hadoop/mapred/MapOutputLocation.java
index bba2d1f..00ad32a 100644
--- a/src/java/org/apache/hadoop/mapred/MapOutputLocation.java
+++ b/src/java/org/apache/hadoop/mapred/MapOutputLocation.java
@@ -23,6 +23,7 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.io.*;
+import org.apache.hadoop.util.Progressable;
 
 /** The location of a map output file, as passed to a reduce task via the
  * {@link InterTrackerProtocol}. */ 
@@ -89,14 +90,6 @@
     return ""http://"" + host + "":"" + port + ""/getMapOutput.jsp?map="" + 
             mapTaskId;
   }
-
-  /**
-   * An interface for callbacks when an method makes some progress.
-   * @author Owen O'Malley
-   */
-  public static interface Pingable {
-    void ping();
-  }
   
   /**
    * Get the map output into a local file from the remote server.
@@ -110,7 +103,7 @@
   public long getFile(FileSystem fileSys, 
                       Path localFilename, 
                       int reduce,
-                      Pingable pingee) throws IOException {
+                      Progressable pingee) throws IOException {
     URL path = new URL(toString() + ""&reduce="" + reduce);
     InputStream input = path.openConnection().getInputStream();
     OutputStream output = fileSys.create(localFilename);
@@ -122,7 +115,7 @@
         totalBytes += len;
         output.write(buffer, 0 ,len);
         if (pingee != null) {
-          pingee.ping();
+          pingee.progress();
         }
         len = input.read(buffer);
       }
"
hadoop,b57de6708151912f7e20ca383e2e416629b89f41,"HADOOP-318.  Keep slow DFS output from causing task timeouts.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417874 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-28 21:05:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/OutputFormat.java b/src/java/org/apache/hadoop/mapred/OutputFormat.java
index 6157b94..32e39e9 100644
--- a/src/java/org/apache/hadoop/mapred/OutputFormat.java
+++ b/src/java/org/apache/hadoop/mapred/OutputFormat.java
@@ -19,19 +19,22 @@
 import java.io.IOException;
 
 import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.util.Progressable;
 
 /** An output data format.  Output files are stored in a {@link
  * FileSystem}. */
 public interface OutputFormat {
 
-  /** Construct a {@link RecordWriter}.
+  /** Construct a {@link RecordWriter} with Progressable.
    *
    * @param fs the file system to write to
    * @param job the job whose output is being written
    * @param name the unique name for this part of the output
+   * @param progress mechanism for reporting progress while writing to file
    * @return a {@link RecordWriter}
    */
-  RecordWriter getRecordWriter(FileSystem fs, JobConf job, String name)
+  RecordWriter getRecordWriter(FileSystem fs, JobConf job, String name,
+          Progressable progress)
     throws IOException;
 
   /** Check whether the output specification for a job is appropriate.  Called
"
hadoop,b57de6708151912f7e20ca383e2e416629b89f41,"HADOOP-318.  Keep slow DFS output from causing task timeouts.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417874 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-28 21:05:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/OutputFormatBase.java b/src/java/org/apache/hadoop/mapred/OutputFormatBase.java
index bdbb351..958726a 100644
--- a/src/java/org/apache/hadoop/mapred/OutputFormatBase.java
+++ b/src/java/org/apache/hadoop/mapred/OutputFormatBase.java
@@ -20,11 +20,13 @@
 
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.util.Progressable;
 
 /** A base class for {@link OutputFormat}. */
 public abstract class OutputFormatBase implements OutputFormat {
   public abstract RecordWriter getRecordWriter(FileSystem fs,
-                                               JobConf job, String name)
+                                               JobConf job, String name,
+                                               Progressable progress)
     throws IOException;
 
   public void checkOutputSpecs(FileSystem fs, JobConf job) throws IOException {
"
hadoop,b57de6708151912f7e20ca383e2e416629b89f41,"HADOOP-318.  Keep slow DFS output from causing task timeouts.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417874 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-28 21:05:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/ReduceTask.java b/src/java/org/apache/hadoop/mapred/ReduceTask.java
index db3d476..f9f835a 100644
--- a/src/java/org/apache/hadoop/mapred/ReduceTask.java
+++ b/src/java/org/apache/hadoop/mapred/ReduceTask.java
@@ -223,10 +223,12 @@
 
     sortPhase.complete();                         // sort is complete
 
+    Reporter reporter = getReporter(umbilical, getProgress());
+    
     // make output collector
     String name = getOutputName(getPartition());
     final RecordWriter out =
-      job.getOutputFormat().getRecordWriter(FileSystem.get(job), job, name);
+      job.getOutputFormat().getRecordWriter(FileSystem.get(job), job, name, reporter);
     OutputCollector collector = new OutputCollector() {
         public void collect(WritableComparable key, Writable value)
           throws IOException {
@@ -237,7 +239,6 @@
     
     // apply reduce function
     SequenceFile.Reader in = new SequenceFile.Reader(lfs, sortedFile, job);
-    Reporter reporter = getReporter(umbilical, getProgress());
     long length = lfs.getLength(sortedFile);
     try {
       ValuesIterator values = new ValuesIterator(in, length, comparator,
"
hadoop,b57de6708151912f7e20ca383e2e416629b89f41,"HADOOP-318.  Keep slow DFS output from causing task timeouts.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417874 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-28 21:05:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java b/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
index 75e58ce..24aa8b8 100644
--- a/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
+++ b/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
@@ -22,6 +22,7 @@
 import java.io.*;
 import java.util.*;
 import java.text.DecimalFormat;
+import org.apache.hadoop.util.Progressable;
 
 /** Runs a reduce task. */
 class ReduceTaskRunner extends TaskRunner {
@@ -115,7 +116,7 @@
     public MapOutputLocation getLocation() { return loc; }
   }
 
-  private static class PingTimer implements MapOutputLocation.Pingable {
+  private static class PingTimer implements Progressable {
     private long pingTime;
     
     public synchronized void reset() {
@@ -126,7 +127,7 @@
       return pingTime;
     }
     
-    public void ping() {
+    public void progress() {
       synchronized (this) {
         pingTime = System.currentTimeMillis();
       }
@@ -202,7 +203,7 @@
 
           try {
             start(loc);
-            pingTimer.ping();
+            pingTimer.progress();
             size = copyOutput(loc, pingTimer);
             pingTimer.reset();
           } catch (IOException e) {
@@ -222,7 +223,7 @@
      * @throws IOException if there is an error copying the file
      */
     private long copyOutput(MapOutputLocation loc, 
-                            MapOutputLocation.Pingable pingee)
+                            Progressable pingee)
     throws IOException {
 
       String reduceId = reduceTask.getTaskId();
"
hadoop,b57de6708151912f7e20ca383e2e416629b89f41,"HADOOP-318.  Keep slow DFS output from causing task timeouts.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417874 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-28 21:05:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/Reporter.java b/src/java/org/apache/hadoop/mapred/Reporter.java
index ad9f5b5..e326f18 100644
--- a/src/java/org/apache/hadoop/mapred/Reporter.java
+++ b/src/java/org/apache/hadoop/mapred/Reporter.java
@@ -17,9 +17,10 @@
 package org.apache.hadoop.mapred;
 
 import java.io.IOException;
+import org.apache.hadoop.util.Progressable;
 
 /** Passed to application code to permit alteration of status. */
-public interface Reporter {
+public interface Reporter extends Progressable {
   /** Alter the application's status description.
    *
    * @param status a brief description of the current status
"
hadoop,b57de6708151912f7e20ca383e2e416629b89f41,"HADOOP-318.  Keep slow DFS output from causing task timeouts.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417874 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-28 21:05:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java b/src/java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java
index cf73e02..c8167f3 100644
--- a/src/java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java
+++ b/src/java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java
@@ -27,12 +27,14 @@
 import org.apache.hadoop.io.WritableComparable;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.util.Progressable;
 
 /** An {@link OutputFormat} that writes {@link SequenceFile}s. */
 public class SequenceFileOutputFormat extends OutputFormatBase {
 
   public RecordWriter getRecordWriter(FileSystem fs, JobConf job,
-                                      String name) throws IOException {
+                                      String name, Progressable progress)
+                                      throws IOException {
 
     Path file = new Path(job.getOutputPath(), name);
 
@@ -40,7 +42,8 @@
       new SequenceFile.Writer(fs, file,
                               job.getOutputKeyClass(),
                               job.getOutputValueClass(),
-                              job.getBoolean(""mapred.output.compress"", false));
+                              job.getBoolean(""mapred.output.compress"", false),
+                              progress);
 
     return new RecordWriter() {
 
"
hadoop,b57de6708151912f7e20ca383e2e416629b89f41,"HADOOP-318.  Keep slow DFS output from causing task timeouts.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417874 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-28 21:05:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/Task.java b/src/java/org/apache/hadoop/mapred/Task.java
index 9d94efc..4b84473 100644
--- a/src/java/org/apache/hadoop/mapred/Task.java
+++ b/src/java/org/apache/hadoop/mapred/Task.java
@@ -96,7 +96,10 @@
     return new Reporter() {
         public void setStatus(String status) throws IOException {
           progress.setStatus(status);
-          reportProgress(umbilical);
+          progress();
+        }
+        public void progress() throws IOException {
+            reportProgress(umbilical);
         }
       };
   }
"
hadoop,b57de6708151912f7e20ca383e2e416629b89f41,"HADOOP-318.  Keep slow DFS output from causing task timeouts.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417874 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-28 21:05:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TextOutputFormat.java b/src/java/org/apache/hadoop/mapred/TextOutputFormat.java
index eae5905..a408d5b 100644
--- a/src/java/org/apache/hadoop/mapred/TextOutputFormat.java
+++ b/src/java/org/apache/hadoop/mapred/TextOutputFormat.java
@@ -24,16 +24,17 @@
 
 import org.apache.hadoop.io.WritableComparable;
 import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.util.Progressable;
 
 /** An {@link OutputFormat} that writes plain text files. */
 public class TextOutputFormat extends OutputFormatBase {
 
   public RecordWriter getRecordWriter(FileSystem fs, JobConf job,
-                                      String name) throws IOException {
+                                      String name, Progressable progress) throws IOException {
 
     Path file = new Path(job.getOutputPath(), name);
 
-    final FSDataOutputStream out = fs.create(file);
+    final FSDataOutputStream out = fs.create(file, progress);
 
     return new RecordWriter() {
         public synchronized void write(WritableComparable key, Writable value)
"
hadoop,0dcd678c3e278a78f04cc5aca5525d7cf811f290,"HADOOP-27.  Don't allocate tasks to trackers whose local free space is too low.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417789 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-28 15:28:27,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 005729c..81359b2 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -65,6 +65,18 @@
     int mapTotal = 0;
     int reduceTotal = 0;
     boolean justStarted = true;
+    
+    //dir -> DF
+    Map localDirsDf = new HashMap();
+    long minSpaceStart = 0;
+    //must have this much space free to start new tasks
+    boolean acceptNewTasks = true;
+    long minSpaceKill = 0;
+    //if we run under this limit, kill one task
+    //and make sure we never receive any new jobs
+    //until all the old tasks have been cleaned up.
+    //this is if a machine is so full it's only good
+    //for serving map output to the other nodes
 
     static Random r = new Random();
     FileSystem fs = null;
@@ -119,7 +131,12 @@
         this.runningTasks = new TreeMap();
         this.mapTotal = 0;
         this.reduceTotal = 0;
-
+        this.acceptNewTasks = true;
+        
+        this.minSpaceStart = this.fConf.getLong(""mapred.local.dir.minspacestart"", 0L);
+        this.minSpaceKill = this.fConf.getLong(""mapred.local.dir.minspacekill"", 0L);
+        
+        
         // port numbers
         this.taskReportPort = this.fConf.getInt(""mapred.task.tracker.report.port"", 50050);
 
@@ -331,11 +348,14 @@
             // Check if we should create a new Task
             //
             try {
-              if (mapTotal < maxCurrentTasks || reduceTotal < maxCurrentTasks) {
+              if ((mapTotal < maxCurrentTasks || reduceTotal < maxCurrentTasks) && acceptNewTasks) {
                   checkLocalDirs(fConf.getLocalDirs());
-                  Task t = jobClient.pollForNewTask(taskTrackerName);
-                  if (t != null) {
-                    startNewTask(t);
+                  
+                  if (enoughFreeSpace(minSpaceStart)) {
+                    Task t = jobClient.pollForNewTask(taskTrackerName);
+                    if (t != null) {
+                      startNewTask(t);
+                    }
                   }
               }
             } catch (DiskErrorException de ) {
@@ -403,12 +423,99 @@
               LOG.info(""Problem getting closed tasks: "" +
                        StringUtils.stringifyException(ie));
             }
+            
+            //Check if we're dangerously low on disk space
+            // If so, kill jobs to free up space and make sure
+            // we don't accept any new tasks
+            // Try killing the reduce jobs first, since I believe they
+            // use up most space
+            // Then pick the one with least progress
+            
+            if (!enoughFreeSpace(minSpaceKill)) {
+              acceptNewTasks=false; 
+              //we give up! do not accept new tasks until
+              //all the ones running have finished and they're all cleared up
+              synchronized (this) {
+                TaskInProgress killMe = null;
+
+                for (Iterator it = runningTasks.values().iterator(); it.hasNext(); ) {
+                  TaskInProgress tip = (TaskInProgress) it.next();
+                  if ((tip.getRunState() == TaskStatus.RUNNING) &&
+                      !tip.wasKilled) {
+                        	
+                    if (killMe == null) {
+                      killMe = tip;
+
+                    } else if (!tip.getTask().isMapTask()) {
+                      //reduce task, give priority
+                      if (killMe.getTask().isMapTask() || 
+                          (tip.getTask().getProgress().get() < 
+                           killMe.getTask().getProgress().get())) {
+
+                        killMe = tip;
+                      }
+
+                    } else if (killMe.getTask().isMapTask() &&
+                               tip.getTask().getProgress().get() < 
+                               killMe.getTask().getProgress().get()) {
+                      //map task, only add if the progress is lower
+
+                      killMe = tip;
+                    }
+                  }
+                }
+
+                if (killMe!=null) {
+                  String msg = ""Tasktracker running out of space. Killing task."";
+                  LOG.info(killMe.getTask().getTaskId() + "": "" + msg);
+                  killMe.reportDiagnosticInfo(msg);
+                  try {
+                    killMe.killAndCleanup(true);
+                  } catch (IOException ie) {
+                    LOG.info(""Problem cleaning task up: "" +
+                             StringUtils.stringifyException(ie));
+                  }
+                }
+              }
+            }
+
+
+            //we've cleaned up, resume normal operation
+            if (!acceptNewTasks && tasks.isEmpty()) {
+                acceptNewTasks=true;
+            }
         }
 
         return 0;
     }
 
     /**
+     * Check if all of the local directories have enough
+     * free space
+     * 
+     * If not, do not try to get a new task assigned 
+     * @return
+     * @throws IOException 
+     */
+    private boolean enoughFreeSpace(long minSpace) throws IOException {
+      String[] localDirs = fConf.getLocalDirs();
+      for (int i = 0; i < localDirs.length; i++) {
+        DF df = null;
+        if (localDirsDf.containsKey(localDirs[i])) {
+          df = (DF) localDirsDf.get(localDirs[i]);
+        } else {
+          df = new DF(localDirs[i], fConf);
+          localDirsDf.put(localDirs[i], df);
+        }
+
+        if (df.getAvailable() < minSpace)
+          return false;
+      }
+
+      return true;
+    }
+
+	/**
      * Start a new task.
      * All exceptions are handled locally, so that we don't mess up the
      * task tracker.
"
hadoop,3c9bd64a62dc562f6d15c37204814e725c48767a,"HADOOP-325.  Correctly initialize RPC parameter classes, and remove workaround code.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417569 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-27 19:42:15,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/conf/Configuration.java b/src/java/org/apache/hadoop/conf/Configuration.java
index 87ff09c..6d322d2 100644
--- a/src/java/org/apache/hadoop/conf/Configuration.java
+++ b/src/java/org/apache/hadoop/conf/Configuration.java
@@ -242,7 +242,7 @@
     if (valueString == null)
       return defaultValue;
     try {
-      return classLoader.loadClass(valueString);
+      return Class.forName(valueString, true, classLoader);
     } catch (ClassNotFoundException e) {
       throw new RuntimeException(e);
     }
"
hadoop,3c9bd64a62dc562f6d15c37204814e725c48767a,"HADOOP-325.  Correctly initialize RPC parameter classes, and remove workaround code.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417569 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-27 19:42:15,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSClient.java b/src/java/org/apache/hadoop/dfs/DFSClient.java
index 4669bd0..44b8601 100644
--- a/src/java/org/apache/hadoop/dfs/DFSClient.java
+++ b/src/java/org/apache/hadoop/dfs/DFSClient.java
@@ -53,9 +53,6 @@
     private long defaultBlockSize;
     private short defaultReplication;
     
-    // required for unknown reason to make WritableFactories work distributed
-    static { new DFSFileInfo(); }
-
     /**
      * A map from name -> DFSOutputStream of files that are currently being
      * written by this client.
"
hadoop,3c9bd64a62dc562f6d15c37204814e725c48767a,"HADOOP-325.  Correctly initialize RPC parameter classes, and remove workaround code.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417569 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-27 19:42:15,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSShell.java b/src/java/org/apache/hadoop/dfs/DFSShell.java
index ecf30e3..68853ea 100644
--- a/src/java/org/apache/hadoop/dfs/DFSShell.java
+++ b/src/java/org/apache/hadoop/dfs/DFSShell.java
@@ -28,9 +28,6 @@
  **************************************************/
 public class DFSShell extends ToolBase {
 
-    // required for unknown reason to make WritableFactories work distributed
-    static { new DatanodeInfo(); }
-
     FileSystem fs;
 
     /**
"
hadoop,3c9bd64a62dc562f6d15c37204814e725c48767a,"HADOOP-325.  Correctly initialize RPC parameter classes, and remove workaround code.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417569 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-27 19:42:15,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index 7c22172..1cf5a04 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -67,9 +67,6 @@
     //private static long numGigs = Configuration.get().getLong(""dfs.datanode.maxgigs"", 100);
     //
 
-    // required for unknown reason to make WritableFactories work distributed
-    static { new BlockCommand(); }
-
     /**
      * Util method to build socket addr from string
      */
"
hadoop,3c9bd64a62dc562f6d15c37204814e725c48767a,"HADOOP-325.  Correctly initialize RPC parameter classes, and remove workaround code.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417569 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-27 19:42:15,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/NameNode.java b/src/java/org/apache/hadoop/dfs/NameNode.java
index 40faead..e72f19a 100644
--- a/src/java/org/apache/hadoop/dfs/NameNode.java
+++ b/src/java/org/apache/hadoop/dfs/NameNode.java
@@ -68,12 +68,6 @@
     
     /** only used for testing purposes  */
     private boolean stopRequested = false;
-    // force loading of classes that will be received via RPC
-    // creating an instance will do the static initialization of the class
-    static {
-      new DatanodeRegistration();
-      new Block();
-    }
 
     /** Format a new filesystem.  Destroys any filesystem that may already
      * exist at this location.  **/
"
hadoop,3c9bd64a62dc562f6d15c37204814e725c48767a,"HADOOP-325.  Correctly initialize RPC parameter classes, and remove workaround code.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417569 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-27 19:42:15,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/ObjectWritable.java b/src/java/org/apache/hadoop/io/ObjectWritable.java
index 21e2680..1e0583c 100644
--- a/src/java/org/apache/hadoop/io/ObjectWritable.java
+++ b/src/java/org/apache/hadoop/io/ObjectWritable.java
@@ -172,9 +172,10 @@
     if (declaredClass == null) {
       try {
         declaredClass =
-          Thread.currentThread().getContextClassLoader().loadClass(className);
+          Class.forName(className, true, 
+                        Thread.currentThread().getContextClassLoader());
       } catch (ClassNotFoundException e) {
-        throw new RuntimeException(e.toString());
+        throw new RuntimeException(""readObject can't find class"", e);
       }
     }    
 
@@ -217,10 +218,11 @@
     } else {                                      // Writable
       Class instanceClass = null;
       try {
-        instanceClass = Thread.currentThread().getContextClassLoader()
-          .loadClass(UTF8.readString(in));
+        instanceClass = 
+          Class.forName(UTF8.readString(in), true, 
+                        Thread.currentThread().getContextClassLoader());
       } catch (ClassNotFoundException e) {
-        throw new RuntimeException(e.toString());
+        throw new RuntimeException(""readObject can't find class"", e);
       }
       
       Writable writable = WritableFactories.newInstance(instanceClass);
"
hadoop,3c9bd64a62dc562f6d15c37204814e725c48767a,"HADOOP-325.  Correctly initialize RPC parameter classes, and remove workaround code.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417569 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-27 19:42:15,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/WritableName.java b/src/java/org/apache/hadoop/io/WritableName.java
index f39f631..e8cc119 100644
--- a/src/java/org/apache/hadoop/io/WritableName.java
+++ b/src/java/org/apache/hadoop/io/WritableName.java
@@ -62,9 +62,12 @@
     if (writableClass != null)
       return writableClass;
     try {
-      return Thread.currentThread().getContextClassLoader().loadClass(name);
+      return Class.forName(name, true, 
+                           Thread.currentThread().getContextClassLoader());
     } catch (ClassNotFoundException e) {
-      throw new IOException(e.toString());
+      IOException newE = new IOException(""WritableName can't load class"");
+      newE.initCause(e);
+      throw newE;
     }
   }
 
"
hadoop,3c9bd64a62dc562f6d15c37204814e725c48767a,"HADOOP-325.  Correctly initialize RPC parameter classes, and remove workaround code.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417569 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-27 19:42:15,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobClient.java b/src/java/org/apache/hadoop/mapred/JobClient.java
index 9e9df39..d15fd67 100644
--- a/src/java/org/apache/hadoop/mapred/JobClient.java
+++ b/src/java/org/apache/hadoop/mapred/JobClient.java
@@ -39,9 +39,6 @@
 
     static long MAX_JOBPROFILE_AGE = 1000 * 2;
 
-    // required for unknown reason to make WritableFactories work distributed
-    static { new JobStatus(); new JobProfile(); new ClusterStatus(); }
-
     /**
      * A NetworkedJob is an implementation of RunningJob.  It holds
      * a JobProfile object to provide some info, and interacts with the
"
hadoop,3c9bd64a62dc562f6d15c37204814e725c48767a,"HADOOP-325.  Correctly initialize RPC parameter classes, and remove workaround code.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417569 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-27 19:42:15,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobInProgress.java b/src/java/org/apache/hadoop/mapred/JobInProgress.java
index 8468947..4671b9f 100644
--- a/src/java/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/JobInProgress.java
@@ -116,7 +116,7 @@
           try {
             ClassLoader loader =
               new URLClassLoader(new URL[]{ localFs.pathToFile(localJarFile).toURL() });
-            Class inputFormatClass = loader.loadClass(ifClassName);
+            Class inputFormatClass = Class.forName(ifClassName, true, loader);
             inputFormat = (InputFormat)inputFormatClass.newInstance();
           } catch (Exception e) {
             throw new IOException(e.toString());
"
hadoop,3c9bd64a62dc562f6d15c37204814e725c48767a,"HADOOP-325.  Correctly initialize RPC parameter classes, and remove workaround code.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417569 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-27 19:42:15,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index ad33d24..b125b6c 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -50,9 +50,6 @@
       idFormat.setGroupingUsed(false);
     }
 
-    // required for unknown reason to make WritableFactories work distributed
-    static { new TaskTrackerStatus(); }
-
     private int nextJobId = 1;
 
     public static final Log LOG = LogFactory.getLog(""org.apache.hadoop.mapred.JobTracker"");
"
hadoop,3c9bd64a62dc562f6d15c37204814e725c48767a,"HADOOP-325.  Correctly initialize RPC parameter classes, and remove workaround code.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417569 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-27 19:42:15,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index b503324..0331925 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -41,9 +41,6 @@
 
     static final int STALE_STATE = 1;
 
-    // required for unknown reason to make WritableFactories work distributed
-    static { new MapTask(); new ReduceTask(); new MapOutputLocation(); }
-
     public static final Log LOG =
     LogFactory.getLog(""org.apache.hadoop.mapred.TaskTracker"");
 
"
hadoop,3c9bd64a62dc562f6d15c37204814e725c48767a,"HADOOP-325.  Correctly initialize RPC parameter classes, and remove workaround code.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417569 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-27 19:42:15,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/util/RunJar.java b/src/java/org/apache/hadoop/util/RunJar.java
index 2bfd3e8..3eb6b8a 100644
--- a/src/java/org/apache/hadoop/util/RunJar.java
+++ b/src/java/org/apache/hadoop/util/RunJar.java
@@ -120,7 +120,7 @@
       new URLClassLoader((URL[])classPath.toArray(new URL[0]));
 
     Thread.currentThread().setContextClassLoader(loader);
-    Class mainClass = loader.loadClass(mainClassName);
+    Class mainClass = Class.forName(mainClassName, true, loader);
     Method main = mainClass.getMethod(""main"", new Class[] {
       Array.newInstance(String.class, 0).getClass()
     });
"
hadoop,9c30db870a6bd12111f269323cb1e964ee6d8dbf,"Fix a bug in the fix for HADOOP-278.  Input directories should be
checked against the job's filesystem, not the jobtracker's.
Contributed by Owen.


git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417295 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-26 20:49:55,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobClient.java b/src/java/org/apache/hadoop/mapred/JobClient.java
index 8cbf48f..469630c 100644
--- a/src/java/org/apache/hadoop/mapred/JobClient.java
+++ b/src/java/org/apache/hadoop/mapred/JobClient.java
@@ -242,7 +242,6 @@
 
         String originalJarPath = job.getJar();
 
-        FileSystem localFs = FileSystem.getNamed(""local"", job);
         FileSystem fs = getFs();
 
         short replication = (short)job.getInt(""mapred.submit.replication"", 10);
@@ -260,12 +259,14 @@
           job.setWorkingDirectory(fs.getWorkingDirectory());          
         }
 
+        FileSystem userFileSys = FileSystem.get(job);
         Path[] inputDirs = job.getInputPaths();
         boolean[] validDirs = 
-          job.getInputFormat().areValidInputDirectories(fs, inputDirs);
+          job.getInputFormat().areValidInputDirectories(userFileSys, inputDirs);
         for(int i=0; i < validDirs.length; ++i) {
           if (!validDirs[i]) {
-            String msg = ""Input directory "" + inputDirs[i] + "" is invalid."";
+            String msg = ""Input directory "" + inputDirs[i] + 
+                         "" in "" + userFileSys.getName() + "" is invalid."";
             LOG.error(msg);
             throw new IOException(msg);
           }
"
hadoop,893a4e29ad0320eae0192742b89b7a83808fdb2f,"HADOOP-304.  Improve error message for UnregisteredDatanodeException to include expected node name.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417273 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-26 18:28:53,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/UnregisteredDatanodeException.java b/src/java/org/apache/hadoop/dfs/UnregisteredDatanodeException.java
index b91bf9b..7821545 100644
--- a/src/java/org/apache/hadoop/dfs/UnregisteredDatanodeException.java
+++ b/src/java/org/apache/hadoop/dfs/UnregisteredDatanodeException.java
@@ -18,8 +18,8 @@
   public UnregisteredDatanodeException( DatanodeID nodeID, 
                                         DatanodeInfo storedNode ) {
     super(""Data node "" + nodeID.getName() 
-        + ""is attempting to report storage ID ""
-        + nodeID.getStorageID() + "". Expecting "" 
-        + storedNode.getStorageID() + ""."");
+        + "" is attempting to report storage ID ""
+        + nodeID.getStorageID() + "". Node "" 
+        + storedNode.getName() + "" is expected to serve this storage."");
   }
 }
"
hadoop,8c10090919467ec30377b9b6a716dd002561b216,"HADOOP-278.  Check for the existence of input directories before starting MapReduce jobs, making it easier to debug this common error.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417256 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-26 17:48:11,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/InputFormat.java b/src/java/org/apache/hadoop/mapred/InputFormat.java
index f3f610e..96d690f 100644
--- a/src/java/org/apache/hadoop/mapred/InputFormat.java
+++ b/src/java/org/apache/hadoop/mapred/InputFormat.java
@@ -19,6 +19,7 @@
 import java.io.IOException;
 
 import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
 
 /** An input data format.  Input files are stored in a {@link FileSystem}.
  * The processing of an input file may be split across multiple machines.
@@ -26,6 +27,18 @@
  * RecordReader}.  Files must thus be split on record boundaries. */
 public interface InputFormat {
 
+  /**
+   * Are the input directories valid? This method is used to test the input
+   * directories when a job is submitted so that the framework can fail early
+   * with a useful error message when the input directory does not exist.
+   * @param fileSys the file system to check for the directories
+   * @param inputDirs the list of input directories
+   * @return is each inputDir valid?
+   * @throws IOException
+   */
+  boolean[] areValidInputDirectories(FileSystem fileSys,
+                                     Path[] inputDirs) throws IOException;
+  
   /** Splits a set of input files.  One split is created per map task.
    *
    * @param fs the filesystem containing the files to be split
"
hadoop,8c10090919467ec30377b9b6a716dd002561b216,"HADOOP-278.  Check for the existence of input directories before starting MapReduce jobs, making it easier to debug this common error.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417256 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-26 17:48:11,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/InputFormatBase.java b/src/java/org/apache/hadoop/mapred/InputFormatBase.java
index dfec1a4..992f012 100644
--- a/src/java/org/apache/hadoop/mapred/InputFormatBase.java
+++ b/src/java/org/apache/hadoop/mapred/InputFormatBase.java
@@ -98,6 +98,16 @@
     return (Path[])result.toArray(new Path[result.size()]);
   }
 
+  public boolean[] areValidInputDirectories(FileSystem fileSys,
+                                            Path[] inputDirs
+                                            ) throws IOException {
+    boolean[] result = new boolean[inputDirs.length];
+    for(int i=0; i < inputDirs.length; ++i) {
+      result[i] = fileSys.isDirectory(inputDirs[i]);
+    }
+    return result;
+  }
+
   /** Splits files returned by {#listPaths(FileSystem,JobConf) when
    * they're too big.*/ 
   public FileSplit[] getSplits(FileSystem fs, JobConf job, int numSplits)
"
hadoop,8c10090919467ec30377b9b6a716dd002561b216,"HADOOP-278.  Check for the existence of input directories before starting MapReduce jobs, making it easier to debug this common error.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417256 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-26 17:48:11,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobClient.java b/src/java/org/apache/hadoop/mapred/JobClient.java
index bebaa73..8cbf48f 100644
--- a/src/java/org/apache/hadoop/mapred/JobClient.java
+++ b/src/java/org/apache/hadoop/mapred/JobClient.java
@@ -260,6 +260,17 @@
           job.setWorkingDirectory(fs.getWorkingDirectory());          
         }
 
+        Path[] inputDirs = job.getInputPaths();
+        boolean[] validDirs = 
+          job.getInputFormat().areValidInputDirectories(fs, inputDirs);
+        for(int i=0; i < validDirs.length; ++i) {
+          if (!validDirs[i]) {
+            String msg = ""Input directory "" + inputDirs[i] + "" is invalid."";
+            LOG.error(msg);
+            throw new IOException(msg);
+          }
+        }
+
         // Check the output specification
         job.getOutputFormat().checkOutputSpecs(fs, job);
 
"
hadoop,ab574637003e93f624195d6119062a742f90b0e2,"HADOOP-135.  Fix potential deadlock in JobTracker by acquiring locks in a consistent order.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417249 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-26 17:36:30,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index 17a5ca7..ad33d24 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -255,8 +255,8 @@
                 }
                 
                 synchronized (jobs) {
-                    synchronized (jobInitQueue) {
-                        synchronized (jobsByArrival) {
+                    synchronized (jobsByArrival) {
+                        synchronized (jobInitQueue) {
                             for (Iterator it = jobs.keySet().iterator(); it.hasNext(); ) {
                                 String jobid = (String) it.next();
                                 JobInProgress job = (JobInProgress) jobs.get(jobid);
"
hadoop,3cfaff414153c645d03d3cce23437d8245b91c72,"HADOOP-319.  Fix FileSystem.close() to remove FileSystem instance from the cache.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417245 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-26 17:32:23,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java b/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
index f860304..39ff609 100644
--- a/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
+++ b/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
@@ -196,6 +196,7 @@
     }
 
     public void close() throws IOException {
+        super.close();
         dfs.close();
     }
 
"
hadoop,3cfaff414153c645d03d3cce23437d8245b91c72,"HADOOP-319.  Fix FileSystem.close() to remove FileSystem instance from the cache.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417245 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-26 17:32:23,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FileSystem.java b/src/java/org/apache/hadoop/fs/FileSystem.java
index 5b1a9bf..8bf1e0e 100644
--- a/src/java/org/apache/hadoop/fs/FileSystem.java
+++ b/src/java/org/apache/hadoop/fs/FileSystem.java
@@ -540,7 +540,9 @@
      * No more filesystem operations are needed.  Will
      * release any held locks.
      */
-    public abstract void close() throws IOException;
+    public void close() throws IOException {
+        NAME_TO_FS.remove(getName());
+    }
 
     /**
      * Report a checksum error to the file system.
"
hadoop,3cfaff414153c645d03d3cce23437d8245b91c72,"HADOOP-319.  Fix FileSystem.close() to remove FileSystem instance from the cache.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@417245 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-26 17:32:23,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/LocalFileSystem.java b/src/java/org/apache/hadoop/fs/LocalFileSystem.java
index 858caa5..1824cc0 100644
--- a/src/java/org/apache/hadoop/fs/LocalFileSystem.java
+++ b/src/java/org/apache/hadoop/fs/LocalFileSystem.java
@@ -322,7 +322,9 @@
       throws IOException {
     }
 
-    public void close() throws IOException {}
+    public void close() throws IOException {
+        super.close();
+    }
 
     public String toString() {
         return ""LocalFS"";
"
hadoop,bcfc1de3e5db074eab71b125282ea35e3df1c953,"HADOOP-317.  Fix server logging to correctly log stack traces.  Also
downgrade a few common log messages to debug-level.  Finally, cancel
keys on exceptions and check their validity before using them.


git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@416474 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-22 21:07:12,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/Server.java b/src/java/org/apache/hadoop/ipc/Server.java
index 9d32f81..e43c3ab 100644
--- a/src/java/org/apache/hadoop/ipc/Server.java
+++ b/src/java/org/apache/hadoop/ipc/Server.java
@@ -204,17 +204,24 @@
           
           while (iter.hasNext()) {
             key = (SelectionKey)iter.next();
-            if (key.isAcceptable())
-              doAccept(key);
-            else if (key.isReadable())
-              doRead(key);
             iter.remove();
+            try {
+              if (key.isValid()) {
+                if (key.isAcceptable())
+                  doAccept(key);
+                else if (key.isReadable())
+                  doRead(key);
+              }
+            } catch (IOException e) {
+              key.cancel();
+            }
             key = null;
           }
         } catch (OutOfMemoryError e) {
           // we can run out of memory if we have too many threads
           // log the event and sleep for a minute and give 
           // some thread(s) a chance to finish
+          LOG.warn(""Out of Memory in server select"", e);
           closeCurrentConnection(key, e);
           cleanupConnections(true);
           try { Thread.sleep(60000); } catch (Exception ie) {}
@@ -238,10 +245,6 @@
     }
 
     private void closeCurrentConnection(SelectionKey key, Throwable e) {
-      if (running) {
-        LOG.warn(""selector: "" + e);
-        e.printStackTrace();
-      }
       if (key != null) {
         Connection c = (Connection)key.attachment();
         if (c != null) {
@@ -277,8 +280,6 @@
 
     void doRead(SelectionKey key) {
       int count = 0;
-      if (!key.isValid() || !key.isReadable())
-        return;
       Connection c = (Connection)key.attachment();
       if (c == null) {
         return;  
@@ -288,8 +289,8 @@
       try {
         count = c.readAndProcess();
       } catch (Exception e) {
-        LOG.info(getName() + "": readAndProcess threw exception "" + e + "". Count of bytes read: "" + count);
-        e.printStackTrace();
+        key.cancel();
+        LOG.debug(getName() + "": readAndProcess threw exception "" + e + "". Count of bytes read: "" + count, e);
         count = -1; //so that the (count < 0) block is executed
       }
       if (count < 0) {
@@ -484,7 +485,7 @@
               }
               out.flush();
             } catch (Exception e) {
-              e.printStackTrace();
+              LOG.warn(""handler output error"", e);
               synchronized (connectionList) {
                 if (connectionList.remove(call.connection))
                   numConnections--;
"
hadoop,58afe30c5776639e9c21d5d0ff603a175d41d407,"HADOOP-316.  Fix a potential deadlock in the jobtracker.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@416451 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-22 19:38:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index beab046..17a5ca7 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -113,32 +113,32 @@
             Thread.sleep(TASKTRACKER_EXPIRY_INTERVAL/3);
             long now = System.currentTimeMillis();
             LOG.debug(""Starting launching task sweep"");
-            synchronized (launchingTasks) {
-              Iterator itr = launchingTasks.entrySet().iterator();
-              while (itr.hasNext()) {
-                Map.Entry pair = (Map.Entry) itr.next();
-                String taskId = (String) pair.getKey();
-                long age = now - ((Long) pair.getValue()).longValue();
-                LOG.info(taskId + "" is "" + age + "" ms debug."");
-                if (age > TASKTRACKER_EXPIRY_INTERVAL) {
-                  LOG.info(""Launching task "" + taskId + "" timed out."");
-                  TaskInProgress tip = null;
-                  synchronized (JobTracker.this) {
+            synchronized (JobTracker.this) {
+              synchronized (launchingTasks) {
+                Iterator itr = launchingTasks.entrySet().iterator();
+                while (itr.hasNext()) {
+                  Map.Entry pair = (Map.Entry) itr.next();
+                  String taskId = (String) pair.getKey();
+                  long age = now - ((Long) pair.getValue()).longValue();
+                  LOG.info(taskId + "" is "" + age + "" ms debug."");
+                  if (age > TASKTRACKER_EXPIRY_INTERVAL) {
+                    LOG.info(""Launching task "" + taskId + "" timed out."");
+                    TaskInProgress tip = null;
                     tip = (TaskInProgress) taskidToTIPMap.get(taskId);
+                    if (tip != null) {
+                      JobInProgress job = tip.getJob();
+                      String trackerName = getAssignedTracker(taskId);
+                      TaskTrackerStatus trackerStatus = 
+                        getTaskTracker(trackerName);
+                      job.failedTask(tip, taskId, ""Error launching task"", 
+                                     trackerStatus.getHost(), trackerName);
+                    }
+                    itr.remove();
+                  } else {
+                    // the tasks are sorted by start time, so once we find
+                    // one that we want to keep, we are done for this cycle.
+                    break;
                   }
-                  if (tip != null) {
-                     JobInProgress job = tip.getJob();
-                     String trackerName = getAssignedTracker(taskId);
-                     TaskTrackerStatus trackerStatus = 
-                       getTaskTracker(trackerName);
-                     job.failedTask(tip, taskId, ""Error launching task"", 
-                                    trackerStatus.getHost(), trackerName);
-                  }
-                  itr.remove();
-                } else {
-                  // the tasks are sorted by start time, so once we find
-                  // one that we want to keep, we are done for this cycle.
-                  break;
                 }
               }
             }
"
hadoop,732dcbf4147e79eb9be73f43429d4a3626cbe9fb,"HADOOP-311.  Change DFS client to retry failed reads, so that a single read failure alone will not cause failure of a task.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@416062 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-21 18:35:32,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSClient.java b/src/java/org/apache/hadoop/dfs/DFSClient.java
index d22053c..4669bd0 100644
--- a/src/java/org/apache/hadoop/dfs/DFSClient.java
+++ b/src/java/org/apache/hadoop/dfs/DFSClient.java
@@ -634,14 +634,26 @@
                 throw new IOException(""Stream closed"");
             }
             if (pos < filelen) {
-                if (pos > blockEnd) {
-                    blockSeekTo(pos);
+              int retries = 2;
+              while (retries > 0) {
+                try {
+                  if (pos > blockEnd) {
+                      blockSeekTo(pos);
+                  }
+                  int realLen = Math.min(len, (int) (blockEnd - pos + 1));
+                  int result = blockStream.read(buf, off, realLen);
+                  if (result >= 0) {
+                      pos += result;
+                  }
+                  return result;
+                } catch (IOException e) {
+                  LOG.warn(""DFS Read: "" + StringUtils.stringifyException(e));
+                  blockEnd = -1;
+                  if (--retries == 0) {
+                    throw e;
+                  }
                 }
-                int result = blockStream.read(buf, off, Math.min(len, (int) (blockEnd - pos + 1)));
-                if (result >= 0) {
-                    pos += result;
-                }
-                return result;
+              }
             }
             return -1;
         }
"
hadoop,93e01b6e7d72f7602546654e4effdfb544885c69,"HADOOP-210.  Change RPC server to use a selector instead of a thread per connection.  Contributed by Devaraj Das.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@416055 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-21 18:15:11,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/Client.java b/src/java/org/apache/hadoop/ipc/Client.java
index ad5d165..dafc58b 100644
--- a/src/java/org/apache/hadoop/ipc/Client.java
+++ b/src/java/org/apache/hadoop/ipc/Client.java
@@ -38,6 +38,7 @@
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableUtils;
 import org.apache.hadoop.io.UTF8;
+import org.apache.hadoop.io.DataOutputBuffer;
 
 /** A client for an IPC service.  IPC calls take a single {@link Writable} as a
  * parameter, and return a {@link Writable} as their value.  A service runs on
@@ -196,8 +197,15 @@
             LOG.debug(getName() + "" sending #"" + call.id);
           try {
             writingCall = call;
-            out.writeInt(call.id);
-            call.param.write(out);
+            DataOutputBuffer d = new DataOutputBuffer(); //for serializing the
+                                                         //data to be written
+            d.writeInt(call.id);
+            call.param.write(d);
+            byte[] data = d.getData();
+            int dataLength = d.getLength();
+
+            out.writeInt(dataLength);      //first put the data length
+            out.write(data, 0, dataLength);//write the data
             out.flush();
           } finally {
             writingCall = null;
@@ -208,7 +216,7 @@
         if (error)
           close();                                // close on error
       }
-    }
+    }  
 
     /** Close the connection and remove it from the pool. */
     public void close() {
"
hadoop,93e01b6e7d72f7602546654e4effdfb544885c69,"HADOOP-210.  Change RPC server to use a selector instead of a thread per connection.  Contributed by Devaraj Das.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@416055 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-21 18:15:11,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/Server.java b/src/java/org/apache/hadoop/ipc/Server.java
index 4e122bc..9d32f81 100644
--- a/src/java/org/apache/hadoop/ipc/Server.java
+++ b/src/java/org/apache/hadoop/ipc/Server.java
@@ -20,17 +20,26 @@
 import java.io.EOFException;
 import java.io.DataInputStream;
 import java.io.DataOutputStream;
-import java.io.BufferedInputStream;
 import java.io.BufferedOutputStream;
 import java.io.StringWriter;
 import java.io.PrintWriter;
+import java.io.ByteArrayInputStream;
 
+import java.nio.ByteBuffer;
+import java.nio.channels.SelectionKey;
+import java.nio.channels.Selector;
+import java.nio.channels.ServerSocketChannel;
+import java.nio.channels.SocketChannel;
+import java.nio.BufferUnderflowException;
+
+import java.net.InetSocketAddress;
 import java.net.Socket;
-import java.net.ServerSocket;
-import java.net.SocketException;
-import java.net.SocketTimeoutException;
 
+import java.util.Collections;
 import java.util.LinkedList;
+import java.util.List;
+import java.util.Iterator;
+import java.util.Random;
 
 import org.apache.commons.logging.*;
 
@@ -38,7 +47,8 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.io.UTF8;
+
+import org.mortbay.http.nio.SocketChannelOutputStream;
 
 /** An abstract IPC service.  IPC calls take a single {@link Writable} as a
  * parameter, and return a {@link Writable} as their value.  A service runs on
@@ -65,6 +75,16 @@
   private int handlerCount;                       // number of handler threads
   private int maxQueuedCalls;                     // max number of queued calls
   private Class paramClass;                       // class of call parameters
+  private int maxIdleTime;                        // the maximum idle time after 
+                                                  // which a client may be disconnected
+  private int thresholdIdleConnections;           // the number of idle connections
+                                                  // after which we will start
+                                                  // cleaning up idle 
+                                                  // connections
+  int maxConnectionsToNuke;                       // the max number of 
+                                                  // connections to nuke
+                                                  //during a cleanup
+  
   private Configuration conf;
 
   private int timeout;
@@ -73,6 +93,12 @@
   private LinkedList callQueue = new LinkedList(); // queued calls
   private Object callDequeued = new Object();     // used by wait/notify
 
+  private List connectionList = 
+       Collections.synchronizedList(new LinkedList()); //maintain a list
+                                                       //of client connectionss
+  private Listener listener;
+  private int numConnections = 0;
+  
   /** A call queued for handling. */
   private static class Call {
     private int id;                               // the client's call id
@@ -86,113 +112,323 @@
     }
   }
 
-  /** Listens on the socket, starting new connection threads. */
+  /** Listens on the socket. Creates jobs for the handler threads*/
   private class Listener extends Thread {
-    private ServerSocket socket;
-
+    
+    private ServerSocketChannel acceptChannel = null; //the accept channel
+    private Selector selector = null; //the selector that we use for the server
+    private InetSocketAddress address; //the address we bind at
+    private Random rand = new Random();
+    private long lastCleanupRunTime = 0; //the last time when a cleanup connec-
+                                         //-tion (for idle connections) ran
+    private long cleanupInterval = 10000; //the minimum interval between 
+                                          //two cleanup runs
+    
     public Listener() throws IOException {
-      this.socket = new ServerSocket(port);
-      socket.setSoTimeout(timeout);
-      this.setDaemon(true);
+      address = new InetSocketAddress(port);
+      // Create a new server socket and set to non blocking mode
+      acceptChannel = ServerSocketChannel.open();
+      acceptChannel.configureBlocking(false);
+
+      // Bind the server socket to the local host and port
+      acceptChannel.socket().bind(address);
+      // create a selector;
+      selector= Selector.open();
+
+      // Register accepts on the server socket with the selector.
+      acceptChannel.register(selector, SelectionKey.OP_ACCEPT);
       this.setName(""Server listener on port "" + port);
-    }
-
-    public void run() {
-      LOG.info(getName() + "": starting"");
-      while (running) {
-        Socket acceptedSock = null;
-        try {
-          acceptedSock = socket.accept();
-          new Connection(acceptedSock).start(); // start a new connection
-        } catch (SocketTimeoutException e) {      // ignore timeouts
-        } catch (OutOfMemoryError e) {
-          // we can run out of memory if we have too many threads
-          // log the event and sleep for a minute and give 
-          // some thread(s) a chance to finish
-          LOG.warn(getName() + "" out of memory, sleeping..."", e);          
-          try {
-            acceptedSock.close();
-            Thread.sleep(60000);
-          } catch (InterruptedException ie) { // ignore interrupts
-          } catch (IOException ioe) { // ignore IOexceptions
-          }          
-        }
-        catch (Exception e) {           // log all other exceptions
-          LOG.info(getName() + "" caught: "" + e, e);
-        }        
-      }
-      try {
-        socket.close();
-      } catch (IOException e) {
-        LOG.info(getName() + "": e="" + e);
-      }
-      LOG.info(getName() + "": exiting"");
-    }
-  }
-
-  /** Reads calls from a connection and queues them for handling. */
-  private class Connection extends Thread {
-    private Socket socket;
-    private DataInputStream in;
-    private DataOutputStream out;
-
-    public Connection(Socket socket) throws IOException {
-      this.socket = socket;
-      socket.setSoTimeout(timeout);
-      this.in = new DataInputStream
-        (new BufferedInputStream(socket.getInputStream()));
-      this.out = new DataOutputStream
-        (new BufferedOutputStream(socket.getOutputStream()));
       this.setDaemon(true);
-      this.setName(""Server connection on port "" + port + "" from ""
-                   + socket.getInetAddress().getHostAddress());
+    }
+    /** cleanup connections from connectionList. Choose a random range
+     * to scan and also have a limit on the number of the connections
+     * that will be cleanedup per run. The criteria for cleanup is the time
+     * for which the connection was idle. If 'force' is true then all 
+     * connections will be looked at for the cleanup.
+     */
+    private void cleanupConnections(boolean force) {
+      if (force || numConnections > thresholdIdleConnections) {
+        long currentTime = System.currentTimeMillis();
+        if (!force && (currentTime - lastCleanupRunTime) < cleanupInterval) {
+          return;
+        }
+        int start = 0;
+        int end = numConnections - 1;
+        if (!force) {
+          start = rand.nextInt() % numConnections;
+          end = rand.nextInt() % numConnections;
+          int temp;
+          if (end < start) {
+            temp = start;
+            start = end;
+            end = temp;
+          }
+        }
+        int i = start;
+        int numNuked = 0;
+        while (i <= end) {
+          Connection c;
+          synchronized (connectionList) {
+            try {
+              c = (Connection)connectionList.get(i);
+            } catch (Exception e) {return;}
+          }
+          if (c.timedOut(currentTime)) {
+            synchronized (connectionList) {
+              if (connectionList.remove(c))
+                numConnections--;
+            }
+            try {
+              LOG.info(getName() + "": disconnecting client "" + c.getHostAddress());
+              c.close();
+            } catch (Exception e) {}
+            numNuked++;
+            end--;
+            c = null;
+            if (!force && numNuked == maxConnectionsToNuke) break;
+          }
+          else i++;
+        }
+        lastCleanupRunTime = System.currentTimeMillis();
+      }
     }
 
     public void run() {
       LOG.info(getName() + "": starting"");
       SERVER.set(Server.this);
-      try {
-        while (running) {
-          int id;
-          try {
-            id = in.readInt();                    // try to read an id
-          } catch (SocketTimeoutException e) {
-            continue;
-          }
-        
-          if (LOG.isDebugEnabled())
-            LOG.debug(getName() + "" got #"" + id);
-        
-          Writable param = makeParam();           // read param
-          param.readFields(in);        
-        
-          Call call = new Call(id, param, this);
-        
-          synchronized (callQueue) {
-            callQueue.addLast(call);              // queue the call
-            callQueue.notify();                   // wake up a waiting handler
-          }
-        
-          while (running && callQueue.size() >= maxQueuedCalls) {
-            synchronized (callDequeued) {         // queue is full
-              callDequeued.wait(timeout);         // wait for a dequeue
-            }
-          }
-        }
-      } catch (EOFException eof) {
-          // This is what happens on linux when the other side shuts down
-      } catch (SocketException eof) {
-          // This is what happens on Win32 when the other side shuts down
-      } catch (Exception e) {
-        LOG.info(getName() + "" caught: "" + e, e);
-      } finally {
+      while (running) {
+        SelectionKey key = null;
         try {
-          socket.close();
-        } catch (IOException e) {}
-        LOG.info(getName() + "": exiting"");
+          selector.select();
+          Iterator iter = selector.selectedKeys().iterator();
+          
+          while (iter.hasNext()) {
+            key = (SelectionKey)iter.next();
+            if (key.isAcceptable())
+              doAccept(key);
+            else if (key.isReadable())
+              doRead(key);
+            iter.remove();
+            key = null;
+          }
+        } catch (OutOfMemoryError e) {
+          // we can run out of memory if we have too many threads
+          // log the event and sleep for a minute and give 
+          // some thread(s) a chance to finish
+          closeCurrentConnection(key, e);
+          cleanupConnections(true);
+          try { Thread.sleep(60000); } catch (Exception ie) {}
+        } catch (Exception e) {
+          closeCurrentConnection(key, e);
+        }
+        cleanupConnections(false);
+      }
+      LOG.info(""Stopping "" + this.getName());
+
+      try {
+        if (acceptChannel != null)
+          acceptChannel.close();
+        if (selector != null)
+          selector.close();
+      } catch (IOException e) { }
+
+      selector= null;
+      acceptChannel= null;
+      connectionList = null;
+    }
+
+    private void closeCurrentConnection(SelectionKey key, Throwable e) {
+      if (running) {
+        LOG.warn(""selector: "" + e);
+        e.printStackTrace();
+      }
+      if (key != null) {
+        Connection c = (Connection)key.attachment();
+        if (c != null) {
+          synchronized (connectionList) {
+            if (connectionList.remove(c))
+              numConnections--;
+          }
+          try {
+            LOG.info(getName() + "": disconnecting client "" + c.getHostAddress());
+            c.close();
+          } catch (Exception ex) {}
+          c = null;
+        }
       }
     }
 
+    void doAccept(SelectionKey key) throws IOException,  OutOfMemoryError {
+      Connection c = null;
+      ServerSocketChannel server = (ServerSocketChannel) key.channel();
+      SocketChannel channel = server.accept();
+      channel.configureBlocking(false);
+      SelectionKey readKey = channel.register(selector, SelectionKey.OP_READ);
+      c = new Connection(readKey, channel, System.currentTimeMillis());
+      readKey.attach(c);
+      synchronized (connectionList) {
+        connectionList.add(numConnections, c);
+        numConnections++;
+      }
+      LOG.info(""Server connection on port "" + port + "" from "" + 
+                c.getHostAddress() +
+                "": starting. Number of active connections: "" + numConnections);
+    }
+
+    void doRead(SelectionKey key) {
+      int count = 0;
+      if (!key.isValid() || !key.isReadable())
+        return;
+      Connection c = (Connection)key.attachment();
+      if (c == null) {
+        return;  
+      }
+      c.setLastContact(System.currentTimeMillis());
+      
+      try {
+        count = c.readAndProcess();
+      } catch (Exception e) {
+        LOG.info(getName() + "": readAndProcess threw exception "" + e + "". Count of bytes read: "" + count);
+        e.printStackTrace();
+        count = -1; //so that the (count < 0) block is executed
+      }
+      if (count < 0) {
+        synchronized (connectionList) {
+          if (connectionList.remove(c))
+            numConnections--;
+        }
+        try {
+          LOG.info(getName() + "": disconnecting client "" + 
+                  c.getHostAddress() + "". Number of active connections: ""+
+                  numConnections);
+          c.close();
+        } catch (Exception e) {}
+        c = null;
+      }
+      else {
+        c.setLastContact(System.currentTimeMillis());
+      }
+    }   
+
+    void doStop()
+    {
+        selector.wakeup();
+        Thread.yield();
+    }
+  }
+
+  /** Reads calls from a connection and queues them for handling. */
+  private class Connection {
+    private SocketChannel channel;
+    private SelectionKey key;
+    private ByteBuffer data;
+    private ByteBuffer dataLengthBuffer;
+    private DataOutputStream out;
+    private SocketChannelOutputStream channelOut;
+    private long lastContact;
+    private int dataLength;
+    private Socket socket;
+
+    public Connection(SelectionKey key, SocketChannel channel, 
+    long lastContact) {
+      this.key = key;
+      this.channel = channel;
+      this.lastContact = lastContact;
+      this.data = null;
+      this.dataLengthBuffer = null;
+      this.socket = channel.socket();
+      this.out = new DataOutputStream
+        (new BufferedOutputStream(
+         this.channelOut = new SocketChannelOutputStream(channel, 4096)));
+    }   
+
+    public String getHostAddress() {
+      return socket.getInetAddress().getHostAddress();
+    }
+
+    public void setLastContact(long lastContact) {
+      this.lastContact = lastContact;
+    }
+
+    public long getLastContact() {
+      return lastContact;
+    }
+
+    private boolean timedOut() {
+      if(System.currentTimeMillis() -  lastContact > maxIdleTime)
+        return true;
+      return false;
+    }
+
+    private boolean timedOut(long currentTime) {
+        if(currentTime -  lastContact > maxIdleTime)
+          return true;
+        return false;
+    }
+
+    public int readAndProcess() throws IOException, InterruptedException {
+      int count = -1;
+      if (dataLengthBuffer == null)
+        dataLengthBuffer = ByteBuffer.allocateDirect(4);
+      if (dataLengthBuffer.remaining() > 0) {
+        count = channel.read(dataLengthBuffer);
+        if (count < 0) return count;
+        if (dataLengthBuffer.remaining() == 0) {
+          dataLengthBuffer.flip(); 
+          dataLength = dataLengthBuffer.getInt();
+          data = ByteBuffer.allocateDirect(dataLength);
+        }
+        //return count;
+      }
+      count = channel.read(data);
+      if (data.remaining() == 0) {
+        data.flip();
+        processData();
+        data = dataLengthBuffer = null; 
+      }
+      return count;
+    }
+
+    private void processData() throws  IOException, InterruptedException {
+      byte[] bytes = new byte[dataLength];
+      data.get(bytes);
+      DataInputStream dis = new DataInputStream(new ByteArrayInputStream(bytes));
+      int id = dis.readInt();                    // try to read an id
+        
+      if (LOG.isDebugEnabled())
+        LOG.debug("" got #"" + id);
+            
+      Writable param = makeParam();           // read param
+      param.readFields(dis);        
+        
+      Call call = new Call(id, param, this);
+      synchronized (callQueue) {
+        callQueue.addLast(call);              // queue the call
+        callQueue.notify();                   // wake up a waiting handler
+      }
+        
+      while (running && callQueue.size() >= maxQueuedCalls) {
+        synchronized (callDequeued) {         // queue is full
+          callDequeued.wait(timeout);         // wait for a dequeue
+        }
+      }
+    }
+
+    private void close() throws IOException {
+      data = null;
+      dataLengthBuffer = null;
+      if (!channel.isOpen())
+        return;
+      try {socket.shutdownOutput();} catch(Exception e) {}
+      try {out.close();} catch(Exception e) {}
+      try {channelOut.destroy();} catch(Exception e) {}
+      if (channel.isOpen()) {
+        try {channel.close();} catch(Exception e) {}
+      }
+      try {socket.close();} catch(Exception e) {}
+      try {key.cancel();} catch(Exception e) {}
+      key = null;
+    }
   }
 
   /** Handles queued calls . */
@@ -237,15 +473,24 @@
             
           DataOutputStream out = call.connection.out;
           synchronized (out) {
-            out.writeInt(call.id);                // write call id
-            out.writeBoolean(error!=null);        // write error flag
-            if (error == null) {
-              value.write(out);
-            } else {
-              WritableUtils.writeString(out, errorClass);
-              WritableUtils.writeString(out, error);
+            try {
+              out.writeInt(call.id);                // write call id
+              out.writeBoolean(error!=null);        // write error flag
+              if (error == null) {
+                value.write(out);
+              } else {
+                WritableUtils.writeString(out, errorClass);
+                WritableUtils.writeString(out, error);
+              }
+              out.flush();
+            } catch (Exception e) {
+              e.printStackTrace();
+              synchronized (connectionList) {
+                if (connectionList.remove(call.connection))
+                  numConnections--;
+              }
+              call.connection.close();
             }
-            out.flush();
           }
 
         } catch (Exception e) {
@@ -275,7 +520,10 @@
     this.paramClass = paramClass;
     this.handlerCount = handlerCount;
     this.maxQueuedCalls = handlerCount;
-    this.timeout = conf.getInt(""ipc.client.timeout"",10000); 
+    this.timeout = conf.getInt(""ipc.client.timeout"",10000);
+    this.maxIdleTime = conf.getInt(""ipc.client.maxidletime"", 120000);
+    this.maxConnectionsToNuke = conf.getInt(""ipc.client.kill.max"", 10);
+    this.thresholdIdleConnections = conf.getInt(""ipc.client.idlethreshold"", 4000);
   }
 
   /** Sets the timeout used for network i/o. */
@@ -283,7 +531,7 @@
 
   /** Starts the service.  Must be called before any calls will be handled. */
   public synchronized void start() throws IOException {
-    Listener listener = new Listener();
+    listener = new Listener();
     listener.start();
     
     for (int i = 0; i < handlerCount; i++) {
@@ -298,6 +546,7 @@
   public synchronized void stop() {
     LOG.info(""Stopping server on "" + port);
     running = false;
+    listener.doStop();
     try {
       Thread.sleep(timeout);     //  inexactly wait for pending requests to finish
     } catch (InterruptedException e) {}
"
hadoop,7d29589f3abd22d7a9dfd169dfa2cdded6b477a7,"HADOOP-299.  Fix the tasktracker, permitting multiple jobs to more easily execute at the same time.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@415383 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-19 18:31:59,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index 242569c..b5f487e 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -39,7 +39,7 @@
     static long RETIRE_JOB_CHECK_INTERVAL;
     static float TASK_ALLOC_EPSILON;
     static float PAD_FRACTION;
-    static float MIN_SLOTS_FOR_PADDING;
+    static final int MIN_CLUSTER_SIZE_FOR_PADDING = 3;
 
     /**
      * Used for formatting the id numbers
@@ -405,8 +405,8 @@
         RETIRE_JOB_INTERVAL = conf.getLong(""mapred.jobtracker.retirejob.interval"", 24 * 60 * 60 * 1000);
         RETIRE_JOB_CHECK_INTERVAL = conf.getLong(""mapred.jobtracker.retirejob.check"", 60 * 1000);
         TASK_ALLOC_EPSILON = conf.getFloat(""mapred.jobtracker.taskalloc.loadbalance.epsilon"", 0.2f);
-        PAD_FRACTION = conf.getFloat(""mapred.jobtracker.taskalloc.capacitypad"", 0.1f);
-        MIN_SLOTS_FOR_PADDING = 3 * maxCurrentTasks;
+        PAD_FRACTION = conf.getFloat(""mapred.jobtracker.taskalloc.capacitypad"", 
+                                     0.01f);
 
         // This is a directory of temporary submission files.  We delete it
         // on startup, and can delete any files that we're done with
@@ -643,8 +643,6 @@
         //
         // Compute average map and reduce task numbers across pool
         //
-        int avgMaps = 0;
-        int avgReduces = 0;
         int remainingReduceLoad = 0;
         int remainingMapLoad = 0;
         int numTaskTrackers;
@@ -669,8 +667,6 @@
         }
         
         if (numTaskTrackers > 0) {
-          avgMaps = totalMaps / numTaskTrackers;
-          avgReduces = totalReduces / numTaskTrackers;
           avgMapLoad = remainingMapLoad / numTaskTrackers;
           avgReduceLoad = remainingReduceLoad / numTaskTrackers;
         }
@@ -727,11 +723,12 @@
                     // schedule tasks to the hilt.
                     //
                     totalNeededMaps += job.desiredMaps();
-                    double padding = 0;
-                    if (totalCapacity > MIN_SLOTS_FOR_PADDING) {
-                        padding = Math.min(maxCurrentTasks, totalNeededMaps * PAD_FRACTION);
+                    int padding = 0;
+                    if (numTaskTrackers > MIN_CLUSTER_SIZE_FOR_PADDING) {
+                      padding = Math.min(maxCurrentTasks,
+                                         (int)(totalNeededMaps * PAD_FRACTION));
                     }
-                    if (totalNeededMaps + padding >= totalCapacity) {
+                    if (totalMaps + padding >= totalCapacity) {
                         break;
                     }
                 }
@@ -762,11 +759,13 @@
                     // schedule tasks to the hilt.
                     //
                     totalNeededReduces += job.desiredReduces();
-                    double padding = 0;
-                    if (totalCapacity > MIN_SLOTS_FOR_PADDING) {
-                        padding = Math.min(maxCurrentTasks, totalNeededReduces * PAD_FRACTION);
+                    int padding = 0;
+                    if (numTaskTrackers > MIN_CLUSTER_SIZE_FOR_PADDING) {
+                        padding = 
+                          Math.min(maxCurrentTasks,
+                                   (int) (totalNeededReduces * PAD_FRACTION));
                     }
-                    if (totalNeededReduces + padding >= totalCapacity) {
+                    if (totalReduces + padding >= totalCapacity) {
                         break;
                     }
                 }
"
hadoop,978f1cd65c7a053efeb6ec2620e6827a8dd0d8f6,"Reverting patch for HADOOP-210, which was causing problems.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@414404 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-14 22:41:25,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/Client.java b/src/java/org/apache/hadoop/ipc/Client.java
index dafc58b..ad5d165 100644
--- a/src/java/org/apache/hadoop/ipc/Client.java
+++ b/src/java/org/apache/hadoop/ipc/Client.java
@@ -38,7 +38,6 @@
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableUtils;
 import org.apache.hadoop.io.UTF8;
-import org.apache.hadoop.io.DataOutputBuffer;
 
 /** A client for an IPC service.  IPC calls take a single {@link Writable} as a
  * parameter, and return a {@link Writable} as their value.  A service runs on
@@ -197,15 +196,8 @@
             LOG.debug(getName() + "" sending #"" + call.id);
           try {
             writingCall = call;
-            DataOutputBuffer d = new DataOutputBuffer(); //for serializing the
-                                                         //data to be written
-            d.writeInt(call.id);
-            call.param.write(d);
-            byte[] data = d.getData();
-            int dataLength = d.getLength();
-
-            out.writeInt(dataLength);      //first put the data length
-            out.write(data, 0, dataLength);//write the data
+            out.writeInt(call.id);
+            call.param.write(out);
             out.flush();
           } finally {
             writingCall = null;
@@ -216,7 +208,7 @@
         if (error)
           close();                                // close on error
       }
-    }  
+    }
 
     /** Close the connection and remove it from the pool. */
     public void close() {
"
hadoop,978f1cd65c7a053efeb6ec2620e6827a8dd0d8f6,"Reverting patch for HADOOP-210, which was causing problems.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@414404 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-14 22:41:25,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/Server.java b/src/java/org/apache/hadoop/ipc/Server.java
index d589dbf..4e122bc 100644
--- a/src/java/org/apache/hadoop/ipc/Server.java
+++ b/src/java/org/apache/hadoop/ipc/Server.java
@@ -20,23 +20,17 @@
 import java.io.EOFException;
 import java.io.DataInputStream;
 import java.io.DataOutputStream;
+import java.io.BufferedInputStream;
+import java.io.BufferedOutputStream;
 import java.io.StringWriter;
 import java.io.PrintWriter;
-import java.io.ByteArrayInputStream;
 
-import java.nio.ByteBuffer;
-import java.nio.channels.SelectionKey;
-import java.nio.channels.Selector;
-import java.nio.channels.ServerSocketChannel;
-import java.nio.channels.SocketChannel;
-import java.nio.BufferUnderflowException;
-
-import java.net.InetSocketAddress;
 import java.net.Socket;
+import java.net.ServerSocket;
+import java.net.SocketException;
+import java.net.SocketTimeoutException;
 
 import java.util.LinkedList;
-import java.util.Iterator;
-import java.util.Random;
 
 import org.apache.commons.logging.*;
 
@@ -44,8 +38,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableUtils;
-
-import org.mortbay.http.nio.SocketChannelOutputStream;
+import org.apache.hadoop.io.UTF8;
 
 /** An abstract IPC service.  IPC calls take a single {@link Writable} as a
  * parameter, and return a {@link Writable} as their value.  A service runs on
@@ -72,16 +65,6 @@
   private int handlerCount;                       // number of handler threads
   private int maxQueuedCalls;                     // max number of queued calls
   private Class paramClass;                       // class of call parameters
-  private int maxIdleTime;                        // the maximum idle time after 
-                                                  // which a client may be disconnected
-  private int thresholdIdleConnections;           // the number of idle connections
-                                                  // after which we will start
-                                                  // cleaning up idle 
-                                                  // connections
-  int maxConnectionsToNuke;                       // the max number of 
-                                                  // connections to nuke
-                                                  //during a cleanup
-  
   private Configuration conf;
 
   private int timeout;
@@ -90,12 +73,6 @@
   private LinkedList callQueue = new LinkedList(); // queued calls
   private Object callDequeued = new Object();     // used by wait/notify
 
-  private InetSocketAddress address; //the address we bind at
-  private ServerSocketChannel acceptChannel = null; //the (main) accept channel
-  private Selector selector = null; //the selector that we use for the server
-  private Listener listener;
-  private int numConnections = 0;
-  
   /** A call queued for handling. */
   private static class Call {
     private int id;                               // the client's call id
@@ -109,300 +86,113 @@
     }
   }
 
-  /** Listens on the socket. Creates jobs for the handler threads*/
+  /** Listens on the socket, starting new connection threads. */
   private class Listener extends Thread {
-    
-    private LinkedList connectionList = new LinkedList(); //maintain a list
-                                                       //of client connectionss
-    private Random rand = new Random();
-    private long lastCleanupRunTime = 0; //the last time when a cleanup connec-
-                                         //-tion (for idle connections) ran
-    private int cleanupInterval = 10000; //the minimum interval between 
-                                         //two cleanup runs
-    
-    public Listener() {
-      address = new InetSocketAddress(port);
+    private ServerSocket socket;
+
+    public Listener() throws IOException {
+      this.socket = new ServerSocket(port);
+      socket.setSoTimeout(timeout);
       this.setDaemon(true);
-    }
-    /** cleanup connections from connectionList. Choose a random range
-     * to scan and also have a limit on the number of the connections
-     * that will be cleanedup per run. The criteria for cleanup is the time
-     * for which the connection was idle. If 'force' is true then all 
-     * connections will be looked at for the cleanup.
-     */
-    private void cleanupConnections(boolean force) {
-      if (force || numConnections > thresholdIdleConnections) {
-        long currentTime = System.currentTimeMillis();
-        if (!force && (int)(currentTime - lastCleanupRunTime) < cleanupInterval) {
-          return;
-        }
-        int start = 0;
-        int end = numConnections - 1;
-        if (!force) {
-          start = rand.nextInt() % numConnections;
-          end = rand.nextInt() % numConnections;
-          int temp;
-          if (end < start) {
-            temp = start;
-            start = end;
-            end = temp;
-          }
-        }
-        int i = start;
-        int numNuked = 0;
-        while (i <= end) {
-          Connection c = (Connection)connectionList.get(i);
-          if (c.timedOut(currentTime)) {
-            connectionList.remove(i);
-            try {
-              LOG.info(getName() + "": disconnecting client "" + c.getHostAddress());
-              c.close();
-            } catch (Exception e) {}
-            numNuked++;
-            end--;
-            if (!force && numNuked == maxConnectionsToNuke) break;
-          }
-          else i++;
-        }
-        lastCleanupRunTime = System.currentTimeMillis();
-      }
+      this.setName(""Server listener on port "" + port);
     }
 
     public void run() {
-      SERVER.set(Server.this);
-      
-      try {
-        // Create a new server socket and set to non blocking mode
-        acceptChannel = ServerSocketChannel.open();
-        acceptChannel.configureBlocking(false);
-
-        // Bind the server socket to the local host and port
-        acceptChannel.socket().bind(address);
-
-        // create a selector;
-        selector= Selector.open();
-
-        // Register accepts on the server socket with the selector.
-        acceptChannel.register(selector, SelectionKey.OP_ACCEPT);
-        this.setName(""Server listener on port "" + port);
-          
-        LOG.info(getName() + "": starting"");
-        
-        while (running) {
-          SelectionKey key = null;
-          try {
-            selector.select(timeout);
-            Iterator iter = selector.selectedKeys().iterator();
-            
-            while (iter.hasNext()) {
-              key = (SelectionKey)iter.next();
-              if (key.isAcceptable())
-                doAccept(key);
-              else if (key.isReadable())
-                doRead(key);
-              iter.remove();
-              key = null;
-            }
-          } catch (OutOfMemoryError e) {
-            closeCurrentConnection(key, e);
-            cleanupConnections(true);
-            Thread.sleep(60000);
-          } catch (Exception e) {
-            closeCurrentConnection(key, e);
-          }
-          cleanupConnections(false);
-        }
-      } catch (Exception e) {
-        LOG.fatal(""selector"",e);  
-      }
-      LOG.info(""Stopping "" + this.getName());
-
-      try {
-        if (acceptChannel != null)
-          acceptChannel.close();
-        if (selector != null)
-          selector.close();
-      } catch (IOException e) { }
-
-      selector= null;
-      acceptChannel= null;
-      connectionList = null;
-    }
-
-    private void closeCurrentConnection(SelectionKey key, Throwable e) {
-      if (running) {
-        LOG.warn(""selector: "" + e);
-        e.printStackTrace();
-      }
-      if (key != null) {
-        Connection c = (Connection)key.attachment();
-        if (c != null) {
-          connectionList.remove(c);
-          try {
-            LOG.info(getName() + "": disconnecting client "" + c.getHostAddress());
-            c.close();
-          } catch (Exception ex) {}
-        }
-      }
-    }
-
-    void doAccept(SelectionKey key) throws IOException,  OutOfMemoryError {
-      Connection c = null;
-      ServerSocketChannel server = (ServerSocketChannel) key.channel();
-      SocketChannel channel = server.accept();
-      channel.configureBlocking(false);
-      SelectionKey readKey = channel.register(selector, SelectionKey.OP_READ);
-      c = new Connection(readKey, channel, System.currentTimeMillis());
-      readKey.attach(c);
-      connectionList.addLast(c);
-      numConnections++;
-      LOG.info(""Server connection on port "" + port + "" from "" + 
-                c.getHostAddress() + "": starting"");
-    }
-
-    void doRead(SelectionKey key) {
-      int count = 0;
-      if (!key.isValid() || !key.isReadable())
-        return;
-      Connection c = (Connection)key.attachment();
-      if (c == null) {
-        return;  
-      }
-      c.setLastContact(System.currentTimeMillis());
-      
-      try {
-        count = c.readAndProcess();
-      } catch (Exception e) {
-        LOG.info(getName() + "": readAndProcess threw exception "" + e + "". Count of bytes read: "" + count);
-        count = -1; //so that the (count < 0) block is executed
-      }
-      if (count < 0) {
-        connectionList.remove(c);
+      LOG.info(getName() + "": starting"");
+      while (running) {
+        Socket acceptedSock = null;
         try {
-          LOG.info(getName() + "": disconnecting client "" + c.getHostAddress());
-          c.close();
-        } catch (Exception e) {}
+          acceptedSock = socket.accept();
+          new Connection(acceptedSock).start(); // start a new connection
+        } catch (SocketTimeoutException e) {      // ignore timeouts
+        } catch (OutOfMemoryError e) {
+          // we can run out of memory if we have too many threads
+          // log the event and sleep for a minute and give 
+          // some thread(s) a chance to finish
+          LOG.warn(getName() + "" out of memory, sleeping..."", e);          
+          try {
+            acceptedSock.close();
+            Thread.sleep(60000);
+          } catch (InterruptedException ie) { // ignore interrupts
+          } catch (IOException ioe) { // ignore IOexceptions
+          }          
+        }
+        catch (Exception e) {           // log all other exceptions
+          LOG.info(getName() + "" caught: "" + e, e);
+        }        
       }
-      else {
-        c.setLastContact(System.currentTimeMillis());
+      try {
+        socket.close();
+      } catch (IOException e) {
+        LOG.info(getName() + "": e="" + e);
       }
-    }   
-
-    void doStop()
-    {
-        selector.wakeup();
-        Thread.yield();
+      LOG.info(getName() + "": exiting"");
     }
   }
 
   /** Reads calls from a connection and queues them for handling. */
-  private class Connection {
-    private SocketChannel channel;
-    private SelectionKey key;
-    private ByteBuffer data;
-    private ByteBuffer dataLengthBuffer;
-    private DataOutputStream out;
-    private long lastContact;
-    private int dataLength;
+  private class Connection extends Thread {
     private Socket socket;
+    private DataInputStream in;
+    private DataOutputStream out;
 
-    public Connection(SelectionKey key, SocketChannel channel, 
-    long lastContact) {
-      this.key = key;
-      this.channel = channel;
-      this.lastContact = lastContact;
-      this.data = null;
-      this.dataLengthBuffer = null;
-      this.socket = channel.socket();
+    public Connection(Socket socket) throws IOException {
+      this.socket = socket;
+      socket.setSoTimeout(timeout);
+      this.in = new DataInputStream
+        (new BufferedInputStream(socket.getInputStream()));
       this.out = new DataOutputStream
-        (new SocketChannelOutputStream(channel, 4096));
-    }   
-
-    public String getHostAddress() {
-      return socket.getInetAddress().getHostAddress();
+        (new BufferedOutputStream(socket.getOutputStream()));
+      this.setDaemon(true);
+      this.setName(""Server connection on port "" + port + "" from ""
+                   + socket.getInetAddress().getHostAddress());
     }
 
-    public void setLastContact(long lastContact) {
-      this.lastContact = lastContact;
-    }
-
-    public long getLastContact() {
-      return lastContact;
-    }
-
-    private boolean timedOut() {
-      if(System.currentTimeMillis() -  lastContact > maxIdleTime)
-        return true;
-      return false;
-    }
-
-    private boolean timedOut(long currentTime) {
-        if(currentTime -  lastContact > timeout)
-          return true;
-        return false;
-    }
-
-    public int readAndProcess() throws IOException, InterruptedException {
-      int count = -1;
-      if (dataLengthBuffer == null)
-        dataLengthBuffer = ByteBuffer.allocateDirect(4);
-      if (dataLengthBuffer.remaining() > 0) {
-        count = channel.read(dataLengthBuffer);
-        if (count < 0) return count;
-        if (dataLengthBuffer.remaining() == 0) {
-          dataLengthBuffer.flip(); 
-          dataLength = dataLengthBuffer.getInt();
-          data = ByteBuffer.allocateDirect(dataLength);
+    public void run() {
+      LOG.info(getName() + "": starting"");
+      SERVER.set(Server.this);
+      try {
+        while (running) {
+          int id;
+          try {
+            id = in.readInt();                    // try to read an id
+          } catch (SocketTimeoutException e) {
+            continue;
+          }
+        
+          if (LOG.isDebugEnabled())
+            LOG.debug(getName() + "" got #"" + id);
+        
+          Writable param = makeParam();           // read param
+          param.readFields(in);        
+        
+          Call call = new Call(id, param, this);
+        
+          synchronized (callQueue) {
+            callQueue.addLast(call);              // queue the call
+            callQueue.notify();                   // wake up a waiting handler
+          }
+        
+          while (running && callQueue.size() >= maxQueuedCalls) {
+            synchronized (callDequeued) {         // queue is full
+              callDequeued.wait(timeout);         // wait for a dequeue
+            }
+          }
         }
-        return count;
-      }
-      count = channel.read(data);
-      if (data.remaining() == 0) {
-        data.flip();
-        processData();
-        data = dataLengthBuffer = null; 
-      }
-      return count;
-    }
-
-    private void processData() throws  IOException, InterruptedException {
-      byte[] bytes = new byte[dataLength];
-      data.get(bytes);
-      DataInputStream dis = new DataInputStream(new ByteArrayInputStream(bytes));
-      int id = dis.readInt();                    // try to read an id
-        
-      if (LOG.isDebugEnabled())
-        LOG.debug("" got #"" + id);
-            
-      Writable param = makeParam();           // read param
-      param.readFields(dis);        
-        
-      Call call = new Call(id, param, this);
-      synchronized (callQueue) {
-        callQueue.addLast(call);              // queue the call
-        callQueue.notify();                   // wake up a waiting handler
-      }
-        
-      while (running && callQueue.size() >= maxQueuedCalls) {
-        synchronized (callDequeued) {         // queue is full
-          callDequeued.wait(timeout);         // wait for a dequeue
-        }
+      } catch (EOFException eof) {
+          // This is what happens on linux when the other side shuts down
+      } catch (SocketException eof) {
+          // This is what happens on Win32 when the other side shuts down
+      } catch (Exception e) {
+        LOG.info(getName() + "" caught: "" + e, e);
+      } finally {
+        try {
+          socket.close();
+        } catch (IOException e) {}
+        LOG.info(getName() + "": exiting"");
       }
     }
 
-    private void close() throws IOException {
-      data = null;
-      dataLengthBuffer = null;
-      if (!channel.isOpen())
-        return;
-      socket.shutdownOutput();
-      channel.close();
-      socket.close();
-      channel.close();
-      out.close();
-      key.cancel();
-      numConnections--;
-    }
   }
 
   /** Handles queued calls . */
@@ -455,6 +245,7 @@
               WritableUtils.writeString(out, errorClass);
               WritableUtils.writeString(out, error);
             }
+            out.flush();
           }
 
         } catch (Exception e) {
@@ -484,10 +275,7 @@
     this.paramClass = paramClass;
     this.handlerCount = handlerCount;
     this.maxQueuedCalls = handlerCount;
-    this.timeout = conf.getInt(""ipc.client.timeout"",10000);
-    this.maxIdleTime = conf.getInt(""ipc.client.maxidletime"", 120000);
-    this.maxConnectionsToNuke = conf.getInt(""ipc.client.kill.max"", 10);
-    this.thresholdIdleConnections = conf.getInt(""ipc.client.idlethreshold"", 120000);
+    this.timeout = conf.getInt(""ipc.client.timeout"",10000); 
   }
 
   /** Sets the timeout used for network i/o. */
@@ -495,7 +283,7 @@
 
   /** Starts the service.  Must be called before any calls will be handled. */
   public synchronized void start() throws IOException {
-    listener = new Listener();
+    Listener listener = new Listener();
     listener.start();
     
     for (int i = 0; i < handlerCount; i++) {
@@ -510,7 +298,6 @@
   public synchronized void stop() {
     LOG.info(""Stopping server on "" + port);
     running = false;
-    listener.doStop();
     try {
       Thread.sleep(timeout);     //  inexactly wait for pending requests to finish
     } catch (InterruptedException e) {}
"
hadoop,9b049b9a47d25bf269c6aa09849fcb19a0c3657b,"HADOOP-210.  Change RPC server to use a selector instead of a thread per connection.  This should make it easier to scale to larger clusters.  Contributed by Devaraj Das.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@413958 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-13 19:18:43,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/Client.java b/src/java/org/apache/hadoop/ipc/Client.java
index ad5d165..dafc58b 100644
--- a/src/java/org/apache/hadoop/ipc/Client.java
+++ b/src/java/org/apache/hadoop/ipc/Client.java
@@ -38,6 +38,7 @@
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableUtils;
 import org.apache.hadoop.io.UTF8;
+import org.apache.hadoop.io.DataOutputBuffer;
 
 /** A client for an IPC service.  IPC calls take a single {@link Writable} as a
  * parameter, and return a {@link Writable} as their value.  A service runs on
@@ -196,8 +197,15 @@
             LOG.debug(getName() + "" sending #"" + call.id);
           try {
             writingCall = call;
-            out.writeInt(call.id);
-            call.param.write(out);
+            DataOutputBuffer d = new DataOutputBuffer(); //for serializing the
+                                                         //data to be written
+            d.writeInt(call.id);
+            call.param.write(d);
+            byte[] data = d.getData();
+            int dataLength = d.getLength();
+
+            out.writeInt(dataLength);      //first put the data length
+            out.write(data, 0, dataLength);//write the data
             out.flush();
           } finally {
             writingCall = null;
@@ -208,7 +216,7 @@
         if (error)
           close();                                // close on error
       }
-    }
+    }  
 
     /** Close the connection and remove it from the pool. */
     public void close() {
"
hadoop,9b049b9a47d25bf269c6aa09849fcb19a0c3657b,"HADOOP-210.  Change RPC server to use a selector instead of a thread per connection.  This should make it easier to scale to larger clusters.  Contributed by Devaraj Das.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@413958 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-13 19:18:43,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/Server.java b/src/java/org/apache/hadoop/ipc/Server.java
index 4e122bc..d589dbf 100644
--- a/src/java/org/apache/hadoop/ipc/Server.java
+++ b/src/java/org/apache/hadoop/ipc/Server.java
@@ -20,17 +20,23 @@
 import java.io.EOFException;
 import java.io.DataInputStream;
 import java.io.DataOutputStream;
-import java.io.BufferedInputStream;
-import java.io.BufferedOutputStream;
 import java.io.StringWriter;
 import java.io.PrintWriter;
+import java.io.ByteArrayInputStream;
 
+import java.nio.ByteBuffer;
+import java.nio.channels.SelectionKey;
+import java.nio.channels.Selector;
+import java.nio.channels.ServerSocketChannel;
+import java.nio.channels.SocketChannel;
+import java.nio.BufferUnderflowException;
+
+import java.net.InetSocketAddress;
 import java.net.Socket;
-import java.net.ServerSocket;
-import java.net.SocketException;
-import java.net.SocketTimeoutException;
 
 import java.util.LinkedList;
+import java.util.Iterator;
+import java.util.Random;
 
 import org.apache.commons.logging.*;
 
@@ -38,7 +44,8 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.io.UTF8;
+
+import org.mortbay.http.nio.SocketChannelOutputStream;
 
 /** An abstract IPC service.  IPC calls take a single {@link Writable} as a
  * parameter, and return a {@link Writable} as their value.  A service runs on
@@ -65,6 +72,16 @@
   private int handlerCount;                       // number of handler threads
   private int maxQueuedCalls;                     // max number of queued calls
   private Class paramClass;                       // class of call parameters
+  private int maxIdleTime;                        // the maximum idle time after 
+                                                  // which a client may be disconnected
+  private int thresholdIdleConnections;           // the number of idle connections
+                                                  // after which we will start
+                                                  // cleaning up idle 
+                                                  // connections
+  int maxConnectionsToNuke;                       // the max number of 
+                                                  // connections to nuke
+                                                  //during a cleanup
+  
   private Configuration conf;
 
   private int timeout;
@@ -73,6 +90,12 @@
   private LinkedList callQueue = new LinkedList(); // queued calls
   private Object callDequeued = new Object();     // used by wait/notify
 
+  private InetSocketAddress address; //the address we bind at
+  private ServerSocketChannel acceptChannel = null; //the (main) accept channel
+  private Selector selector = null; //the selector that we use for the server
+  private Listener listener;
+  private int numConnections = 0;
+  
   /** A call queued for handling. */
   private static class Call {
     private int id;                               // the client's call id
@@ -86,113 +109,300 @@
     }
   }
 
-  /** Listens on the socket, starting new connection threads. */
+  /** Listens on the socket. Creates jobs for the handler threads*/
   private class Listener extends Thread {
-    private ServerSocket socket;
-
-    public Listener() throws IOException {
-      this.socket = new ServerSocket(port);
-      socket.setSoTimeout(timeout);
+    
+    private LinkedList connectionList = new LinkedList(); //maintain a list
+                                                       //of client connectionss
+    private Random rand = new Random();
+    private long lastCleanupRunTime = 0; //the last time when a cleanup connec-
+                                         //-tion (for idle connections) ran
+    private int cleanupInterval = 10000; //the minimum interval between 
+                                         //two cleanup runs
+    
+    public Listener() {
+      address = new InetSocketAddress(port);
       this.setDaemon(true);
-      this.setName(""Server listener on port "" + port);
+    }
+    /** cleanup connections from connectionList. Choose a random range
+     * to scan and also have a limit on the number of the connections
+     * that will be cleanedup per run. The criteria for cleanup is the time
+     * for which the connection was idle. If 'force' is true then all 
+     * connections will be looked at for the cleanup.
+     */
+    private void cleanupConnections(boolean force) {
+      if (force || numConnections > thresholdIdleConnections) {
+        long currentTime = System.currentTimeMillis();
+        if (!force && (int)(currentTime - lastCleanupRunTime) < cleanupInterval) {
+          return;
+        }
+        int start = 0;
+        int end = numConnections - 1;
+        if (!force) {
+          start = rand.nextInt() % numConnections;
+          end = rand.nextInt() % numConnections;
+          int temp;
+          if (end < start) {
+            temp = start;
+            start = end;
+            end = temp;
+          }
+        }
+        int i = start;
+        int numNuked = 0;
+        while (i <= end) {
+          Connection c = (Connection)connectionList.get(i);
+          if (c.timedOut(currentTime)) {
+            connectionList.remove(i);
+            try {
+              LOG.info(getName() + "": disconnecting client "" + c.getHostAddress());
+              c.close();
+            } catch (Exception e) {}
+            numNuked++;
+            end--;
+            if (!force && numNuked == maxConnectionsToNuke) break;
+          }
+          else i++;
+        }
+        lastCleanupRunTime = System.currentTimeMillis();
+      }
     }
 
     public void run() {
-      LOG.info(getName() + "": starting"");
-      while (running) {
-        Socket acceptedSock = null;
-        try {
-          acceptedSock = socket.accept();
-          new Connection(acceptedSock).start(); // start a new connection
-        } catch (SocketTimeoutException e) {      // ignore timeouts
-        } catch (OutOfMemoryError e) {
-          // we can run out of memory if we have too many threads
-          // log the event and sleep for a minute and give 
-          // some thread(s) a chance to finish
-          LOG.warn(getName() + "" out of memory, sleeping..."", e);          
-          try {
-            acceptedSock.close();
-            Thread.sleep(60000);
-          } catch (InterruptedException ie) { // ignore interrupts
-          } catch (IOException ioe) { // ignore IOexceptions
-          }          
-        }
-        catch (Exception e) {           // log all other exceptions
-          LOG.info(getName() + "" caught: "" + e, e);
-        }        
-      }
+      SERVER.set(Server.this);
+      
       try {
-        socket.close();
-      } catch (IOException e) {
-        LOG.info(getName() + "": e="" + e);
+        // Create a new server socket and set to non blocking mode
+        acceptChannel = ServerSocketChannel.open();
+        acceptChannel.configureBlocking(false);
+
+        // Bind the server socket to the local host and port
+        acceptChannel.socket().bind(address);
+
+        // create a selector;
+        selector= Selector.open();
+
+        // Register accepts on the server socket with the selector.
+        acceptChannel.register(selector, SelectionKey.OP_ACCEPT);
+        this.setName(""Server listener on port "" + port);
+          
+        LOG.info(getName() + "": starting"");
+        
+        while (running) {
+          SelectionKey key = null;
+          try {
+            selector.select(timeout);
+            Iterator iter = selector.selectedKeys().iterator();
+            
+            while (iter.hasNext()) {
+              key = (SelectionKey)iter.next();
+              if (key.isAcceptable())
+                doAccept(key);
+              else if (key.isReadable())
+                doRead(key);
+              iter.remove();
+              key = null;
+            }
+          } catch (OutOfMemoryError e) {
+            closeCurrentConnection(key, e);
+            cleanupConnections(true);
+            Thread.sleep(60000);
+          } catch (Exception e) {
+            closeCurrentConnection(key, e);
+          }
+          cleanupConnections(false);
+        }
+      } catch (Exception e) {
+        LOG.fatal(""selector"",e);  
       }
-      LOG.info(getName() + "": exiting"");
+      LOG.info(""Stopping "" + this.getName());
+
+      try {
+        if (acceptChannel != null)
+          acceptChannel.close();
+        if (selector != null)
+          selector.close();
+      } catch (IOException e) { }
+
+      selector= null;
+      acceptChannel= null;
+      connectionList = null;
+    }
+
+    private void closeCurrentConnection(SelectionKey key, Throwable e) {
+      if (running) {
+        LOG.warn(""selector: "" + e);
+        e.printStackTrace();
+      }
+      if (key != null) {
+        Connection c = (Connection)key.attachment();
+        if (c != null) {
+          connectionList.remove(c);
+          try {
+            LOG.info(getName() + "": disconnecting client "" + c.getHostAddress());
+            c.close();
+          } catch (Exception ex) {}
+        }
+      }
+    }
+
+    void doAccept(SelectionKey key) throws IOException,  OutOfMemoryError {
+      Connection c = null;
+      ServerSocketChannel server = (ServerSocketChannel) key.channel();
+      SocketChannel channel = server.accept();
+      channel.configureBlocking(false);
+      SelectionKey readKey = channel.register(selector, SelectionKey.OP_READ);
+      c = new Connection(readKey, channel, System.currentTimeMillis());
+      readKey.attach(c);
+      connectionList.addLast(c);
+      numConnections++;
+      LOG.info(""Server connection on port "" + port + "" from "" + 
+                c.getHostAddress() + "": starting"");
+    }
+
+    void doRead(SelectionKey key) {
+      int count = 0;
+      if (!key.isValid() || !key.isReadable())
+        return;
+      Connection c = (Connection)key.attachment();
+      if (c == null) {
+        return;  
+      }
+      c.setLastContact(System.currentTimeMillis());
+      
+      try {
+        count = c.readAndProcess();
+      } catch (Exception e) {
+        LOG.info(getName() + "": readAndProcess threw exception "" + e + "". Count of bytes read: "" + count);
+        count = -1; //so that the (count < 0) block is executed
+      }
+      if (count < 0) {
+        connectionList.remove(c);
+        try {
+          LOG.info(getName() + "": disconnecting client "" + c.getHostAddress());
+          c.close();
+        } catch (Exception e) {}
+      }
+      else {
+        c.setLastContact(System.currentTimeMillis());
+      }
+    }   
+
+    void doStop()
+    {
+        selector.wakeup();
+        Thread.yield();
     }
   }
 
   /** Reads calls from a connection and queues them for handling. */
-  private class Connection extends Thread {
-    private Socket socket;
-    private DataInputStream in;
+  private class Connection {
+    private SocketChannel channel;
+    private SelectionKey key;
+    private ByteBuffer data;
+    private ByteBuffer dataLengthBuffer;
     private DataOutputStream out;
+    private long lastContact;
+    private int dataLength;
+    private Socket socket;
 
-    public Connection(Socket socket) throws IOException {
-      this.socket = socket;
-      socket.setSoTimeout(timeout);
-      this.in = new DataInputStream
-        (new BufferedInputStream(socket.getInputStream()));
+    public Connection(SelectionKey key, SocketChannel channel, 
+    long lastContact) {
+      this.key = key;
+      this.channel = channel;
+      this.lastContact = lastContact;
+      this.data = null;
+      this.dataLengthBuffer = null;
+      this.socket = channel.socket();
       this.out = new DataOutputStream
-        (new BufferedOutputStream(socket.getOutputStream()));
-      this.setDaemon(true);
-      this.setName(""Server connection on port "" + port + "" from ""
-                   + socket.getInetAddress().getHostAddress());
+        (new SocketChannelOutputStream(channel, 4096));
+    }   
+
+    public String getHostAddress() {
+      return socket.getInetAddress().getHostAddress();
     }
 
-    public void run() {
-      LOG.info(getName() + "": starting"");
-      SERVER.set(Server.this);
-      try {
-        while (running) {
-          int id;
-          try {
-            id = in.readInt();                    // try to read an id
-          } catch (SocketTimeoutException e) {
-            continue;
-          }
-        
-          if (LOG.isDebugEnabled())
-            LOG.debug(getName() + "" got #"" + id);
-        
-          Writable param = makeParam();           // read param
-          param.readFields(in);        
-        
-          Call call = new Call(id, param, this);
-        
-          synchronized (callQueue) {
-            callQueue.addLast(call);              // queue the call
-            callQueue.notify();                   // wake up a waiting handler
-          }
-        
-          while (running && callQueue.size() >= maxQueuedCalls) {
-            synchronized (callDequeued) {         // queue is full
-              callDequeued.wait(timeout);         // wait for a dequeue
-            }
-          }
+    public void setLastContact(long lastContact) {
+      this.lastContact = lastContact;
+    }
+
+    public long getLastContact() {
+      return lastContact;
+    }
+
+    private boolean timedOut() {
+      if(System.currentTimeMillis() -  lastContact > maxIdleTime)
+        return true;
+      return false;
+    }
+
+    private boolean timedOut(long currentTime) {
+        if(currentTime -  lastContact > timeout)
+          return true;
+        return false;
+    }
+
+    public int readAndProcess() throws IOException, InterruptedException {
+      int count = -1;
+      if (dataLengthBuffer == null)
+        dataLengthBuffer = ByteBuffer.allocateDirect(4);
+      if (dataLengthBuffer.remaining() > 0) {
+        count = channel.read(dataLengthBuffer);
+        if (count < 0) return count;
+        if (dataLengthBuffer.remaining() == 0) {
+          dataLengthBuffer.flip(); 
+          dataLength = dataLengthBuffer.getInt();
+          data = ByteBuffer.allocateDirect(dataLength);
         }
-      } catch (EOFException eof) {
-          // This is what happens on linux when the other side shuts down
-      } catch (SocketException eof) {
-          // This is what happens on Win32 when the other side shuts down
-      } catch (Exception e) {
-        LOG.info(getName() + "" caught: "" + e, e);
-      } finally {
-        try {
-          socket.close();
-        } catch (IOException e) {}
-        LOG.info(getName() + "": exiting"");
+        return count;
+      }
+      count = channel.read(data);
+      if (data.remaining() == 0) {
+        data.flip();
+        processData();
+        data = dataLengthBuffer = null; 
+      }
+      return count;
+    }
+
+    private void processData() throws  IOException, InterruptedException {
+      byte[] bytes = new byte[dataLength];
+      data.get(bytes);
+      DataInputStream dis = new DataInputStream(new ByteArrayInputStream(bytes));
+      int id = dis.readInt();                    // try to read an id
+        
+      if (LOG.isDebugEnabled())
+        LOG.debug("" got #"" + id);
+            
+      Writable param = makeParam();           // read param
+      param.readFields(dis);        
+        
+      Call call = new Call(id, param, this);
+      synchronized (callQueue) {
+        callQueue.addLast(call);              // queue the call
+        callQueue.notify();                   // wake up a waiting handler
+      }
+        
+      while (running && callQueue.size() >= maxQueuedCalls) {
+        synchronized (callDequeued) {         // queue is full
+          callDequeued.wait(timeout);         // wait for a dequeue
+        }
       }
     }
 
+    private void close() throws IOException {
+      data = null;
+      dataLengthBuffer = null;
+      if (!channel.isOpen())
+        return;
+      socket.shutdownOutput();
+      channel.close();
+      socket.close();
+      channel.close();
+      out.close();
+      key.cancel();
+      numConnections--;
+    }
   }
 
   /** Handles queued calls . */
@@ -245,7 +455,6 @@
               WritableUtils.writeString(out, errorClass);
               WritableUtils.writeString(out, error);
             }
-            out.flush();
           }
 
         } catch (Exception e) {
@@ -275,7 +484,10 @@
     this.paramClass = paramClass;
     this.handlerCount = handlerCount;
     this.maxQueuedCalls = handlerCount;
-    this.timeout = conf.getInt(""ipc.client.timeout"",10000); 
+    this.timeout = conf.getInt(""ipc.client.timeout"",10000);
+    this.maxIdleTime = conf.getInt(""ipc.client.maxidletime"", 120000);
+    this.maxConnectionsToNuke = conf.getInt(""ipc.client.kill.max"", 10);
+    this.thresholdIdleConnections = conf.getInt(""ipc.client.idlethreshold"", 120000);
   }
 
   /** Sets the timeout used for network i/o. */
@@ -283,7 +495,7 @@
 
   /** Starts the service.  Must be called before any calls will be handled. */
   public synchronized void start() throws IOException {
-    Listener listener = new Listener();
+    listener = new Listener();
     listener.start();
     
     for (int i = 0; i < handlerCount; i++) {
@@ -298,6 +510,7 @@
   public synchronized void stop() {
     LOG.info(""Stopping server on "" + port);
     running = false;
+    listener.doStop();
     try {
       Thread.sleep(timeout);     //  inexactly wait for pending requests to finish
     } catch (InterruptedException e) {}
"
hadoop,3ab5cc64ade45df9f785f20612ee500a4a14b877,"HADOOP-294.  Fix a bug where the conditions for retrying after errors in the DFS client were reversed.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@413122 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-09 18:38:31,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSClient.java b/src/java/org/apache/hadoop/dfs/DFSClient.java
index e649ef7..d22053c 100644
--- a/src/java/org/apache/hadoop/dfs/DFSClient.java
+++ b/src/java/org/apache/hadoop/dfs/DFSClient.java
@@ -808,7 +808,7 @@
                     localName, overwrite, replication, blockSize);
               } catch (RemoteException e) {
                 if (--retries == 0 || 
-                    AlreadyBeingCreatedException.class.getName().
+                    !AlreadyBeingCreatedException.class.getName().
                         equals(e.getClassName())) {
                   throw e;
                 } else {
@@ -838,7 +838,7 @@
                                          clientName.toString());
               } catch (RemoteException e) {
                 if (--retries == 0 || 
-                    NotReplicatedYetException.class.getName().
+                    !NotReplicatedYetException.class.getName().
                         equals(e.getClassName())) {
                   throw e;
                 } else {
"
hadoop,3ab5cc64ade45df9f785f20612ee500a4a14b877,"HADOOP-294.  Fix a bug where the conditions for retrying after errors in the DFS client were reversed.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@413122 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-09 18:38:31,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index e22d467..7c22172 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -179,7 +179,8 @@
         } catch( ConnectException se ) {  // namenode has not been started
           LOG.info(""Namenode not available yet, Zzzzz..."");
         } catch( SocketTimeoutException te ) {  // namenode is busy
-          LOG.info(""Namenode "" + te.getLocalizedMessage() );
+          LOG.info(""Problem connecting to Namenode: "" + 
+                   StringUtils.stringifyException(te));
         }
         try {
           Thread.sleep(10 * 1000);
@@ -338,7 +339,8 @@
       } catch( RemoteException re ) {
         String reClass = re.getClassName();
         if( UnregisteredDatanodeException.class.getName().equals( reClass )) {
-          LOG.warn( ""DataNode is shutting down.\n"" + re );
+          LOG.warn( ""DataNode is shutting down: "" + 
+                    StringUtils.stringifyException(re));
           shutdown();
           return;
         }
"
hadoop,095307710d2fa6664445f38b799944b7a53419ea,"HADOOP-289.  Improved exception handling in DFS datanode.  Contributed by Konstantin.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@413096 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-09 17:17:14,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index 64873b0..e22d467 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -105,17 +105,7 @@
              new File(datadir),
              createSocketAddr(conf.get(""fs.default.name"", ""local"")), conf);
         // register datanode
-        while (shouldRun) {
-          try {
-            register();
-            break;
-          } catch (ConnectException ce) {
-            LOG.info(""Namenode not available yet, Zzzzz..."");
-            try {
-              Thread.sleep(10 * 1000);
-            } catch (InterruptedException ie) {}
-          }
-        }
+        register();
     }
 
     /**
@@ -182,7 +172,19 @@
      * @throws IOException
      */
     private void register() throws IOException {
-      dnRegistration = namenode.register( dnRegistration );
+      while (shouldRun) {
+        try {
+          dnRegistration = namenode.register( dnRegistration );
+          break;
+        } catch( ConnectException se ) {  // namenode has not been started
+          LOG.info(""Namenode not available yet, Zzzzz..."");
+        } catch( SocketTimeoutException te ) {  // namenode is busy
+          LOG.info(""Namenode "" + te.getLocalizedMessage() );
+        }
+        try {
+          Thread.sleep(10 * 1000);
+        } catch (InterruptedException ie) {}
+      }
       if( storage.getStorageID().equals("""") ) {
         storage.setStorageID( dnRegistration.getStorageID());
         storage.write();
@@ -203,7 +205,7 @@
     }
 
     void handleDiskError( String errMsgr ) {
-        LOG.warn( ""Shuting down DataNode because ""+errMsgr );
+        LOG.warn( ""DataNode is shutting down.\n"" + errMsgr );
         try {
             namenode.errorReport(
                     dnRegistration, DatanodeProtocol.DISK_ERROR, errMsgr);
@@ -332,9 +334,16 @@
           } // synchronized
         } // while (shouldRun)
       } catch(DiskErrorException e) {
-        handleDiskError(e.getMessage());
+        handleDiskError(e.getLocalizedMessage());
+      } catch( RemoteException re ) {
+        String reClass = re.getClassName();
+        if( UnregisteredDatanodeException.class.getName().equals( reClass )) {
+          LOG.warn( ""DataNode is shutting down.\n"" + re );
+          shutdown();
+          return;
+        }
+        throw re;
       }
-      
     } // offerService
 
     /**
"
hadoop,095307710d2fa6664445f38b799944b7a53419ea,"HADOOP-289.  Improved exception handling in DFS datanode.  Contributed by Konstantin.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@413096 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-09 17:17:14,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index 800372c..2a18033 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -1329,6 +1329,8 @@
      */
     private void proccessOverReplicatedBlock( Block block, short replication ) {
       TreeSet containingNodes = (TreeSet) blocksMap.get(block);
+      if( containingNodes == null )
+        return;
       Vector nonExcess = new Vector();
       for (Iterator it = containingNodes.iterator(); it.hasNext(); ) {
           DatanodeInfo cur = (DatanodeInfo) it.next();
"
hadoop,4e40638f53d1098cac49556d5f8ea8641d81d029,"HADOOP-282.  Fix datanode to retry registration, rather than permanently fail if the namenode is down when it starts.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@412545 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-07 21:09:40,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index 7d38d31..64873b0 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -24,7 +24,6 @@
 
 import java.io.*;
 import java.net.*;
-import java.nio.channels.FileLock;
 import java.util.*;
 
 /**********************************************************
@@ -106,7 +105,17 @@
              new File(datadir),
              createSocketAddr(conf.get(""fs.default.name"", ""local"")), conf);
         // register datanode
-        register();
+        while (shouldRun) {
+          try {
+            register();
+            break;
+          } catch (ConnectException ce) {
+            LOG.info(""Namenode not available yet, Zzzzz..."");
+            try {
+              Thread.sleep(10 * 1000);
+            } catch (InterruptedException ie) {}
+          }
+        }
     }
 
     /**
"
hadoop,c04b1fb6b885ee736d9e3e27979762f898762bfc,"HADOOP-277.  Fix a race condition when creating directories.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@412496 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-07 19:35:46,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FileSystem.java b/src/java/org/apache/hadoop/fs/FileSystem.java
index 45c43ec..5b1a9bf 100644
--- a/src/java/org/apache/hadoop/fs/FileSystem.java
+++ b/src/java/org/apache/hadoop/fs/FileSystem.java
@@ -459,7 +459,8 @@
 
     /**
      * Make the given file and all non-existent parents into
-     * directories.
+     * directories. Has the semantics of Unix 'mkdir -p'.
+     * Existence of the directory hierarchy is not an error.
      */
     public abstract boolean mkdirs(Path f) throws IOException;
 
"
hadoop,c04b1fb6b885ee736d9e3e27979762f898762bfc,"HADOOP-277.  Fix a race condition when creating directories.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@412496 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-07 19:35:46,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/LocalFileSystem.java b/src/java/org/apache/hadoop/fs/LocalFileSystem.java
index 878209e..858caa5 100644
--- a/src/java/org/apache/hadoop/fs/LocalFileSystem.java
+++ b/src/java/org/apache/hadoop/fs/LocalFileSystem.java
@@ -223,11 +223,18 @@
         }
         return results;
     }
-
+    
+    /**
+     * Creates the specified directory hierarchy. Does not
+     * treat existence as an error.
+     */
     public boolean mkdirs(Path f) throws IOException {
-      return pathToFile(f).mkdirs();
+      Path parent = f.getParent();
+      File p2f = pathToFile(f);
+      return (parent == null || mkdirs(parent)) &&
+             (p2f.mkdir() || p2f.isDirectory());
     }
-
+    
     /**
      * Set the working directory to the given directory.
      */
"
hadoop,c08324049e186800a7f38e019e1c80824e80d136,"HADOOP-285.  Fix DFS datanodes to be able to re-join the cluster after the connection to the namenode is lost.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@412494 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-07 19:29:20,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index 9eb163d..7d38d31 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -208,9 +208,7 @@
      * forever calling remote NameNode functions.
      */
     public void offerService() throws Exception {
-      // start dataXceiveServer  
-      dataXceiveServer.start();
-      
+     
       long lastHeartbeat = 0, lastBlockReport = 0;
       LOG.info(""using BLOCKREPORT_INTERVAL of "" + blockReportInterval + ""msec"");
 
@@ -328,11 +326,6 @@
         handleDiskError(e.getMessage());
       }
       
-      // wait for dataXceiveServer to terminate
-      try {
-          this.dataXceiveServer.join();
-      } catch (InterruptedException ie) {
-      }
     } // offerService
 
     /**
@@ -818,6 +811,10 @@
      */
     public void run() {
         LOG.info(""Starting DataNode in: ""+data.data);
+        
+        // start dataXceiveServer
+        dataXceiveServer.start();
+        
         while (shouldRun) {
             try {
                 offerService();
@@ -832,7 +829,14 @@
               }
             }
         }
-      LOG.info(""Finishing DataNode in: ""+data.data);
+        
+        // wait for dataXceiveServer to terminate
+        try {
+            this.dataXceiveServer.join();
+        } catch (InterruptedException ie) {
+        }
+        
+        LOG.info(""Finishing DataNode in: ""+data.data);
     }
 
     /** Start datanode daemons.
"
hadoop,5534a28b6883bb4d0dc75a7f126d1899173a77c5,"HADOOP-240.  Fix DFS mkdirs() to not warn when directories already exist.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@412474 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-07 18:16:03,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSDirectory.java b/src/java/org/apache/hadoop/dfs/FSDirectory.java
index 48a6d9d..4d68404 100644
--- a/src/java/org/apache/hadoop/dfs/FSDirectory.java
+++ b/src/java/org/apache/hadoop/dfs/FSDirectory.java
@@ -143,18 +143,22 @@
          * @param path file path
          * @param newNode INode to be added
          * @return null if the node already exists; inserted INode, otherwise
+         * @throws FileNotFoundException 
          * @author shv
          */
-        INode addNode(String path, INode newNode) {
+        INode addNode(String path, INode newNode) throws FileNotFoundException {
           File target = new File( path );
           // find parent
           Path parent = new Path(path).getParent();
-          if (parent == null)
-            return null;
+          if (parent == null) { // add root
+              return null;
+          }
           INode parentNode = getNode(parent.toString());
-          if (parentNode == null)
-            return null;
-          // check whether the parent already has a node with that name
+          if (parentNode == null) {
+              throw new FileNotFoundException(
+                      ""Parent path does not exist: ""+path);
+          }
+           // check whether the parent already has a node with that name
           String name = newNode.name = target.getName();
           if( parentNode.getChild( name ) != null )
             return null;
@@ -688,11 +692,19 @@
      */
     boolean unprotectedAddFile(UTF8 path, INode newNode) {
       synchronized (rootDir) {
-        int nrBlocks = (newNode.blocks == null) ? 0 : newNode.blocks.length;
-        // Add file->block mapping
-        for (int i = 0; i < nrBlocks; i++)
-            activeBlocks.put(newNode.blocks[i], newNode);
-        return (rootDir.addNode(path.toString(), newNode) != null);
+         try {
+            if( rootDir.addNode(path.toString(), newNode ) != null ) {
+                int nrBlocks = (newNode.blocks == null) ? 0 : newNode.blocks.length;
+                // Add file->block mapping
+                for (int i = 0; i < nrBlocks; i++)
+                    activeBlocks.put(newNode.blocks[i], newNode);
+                return true;
+            } else {
+                return false;
+            }
+        } catch (FileNotFoundException e ) {
+            return false;
+        }
       }
     }
 
@@ -720,23 +732,36 @@
             INode renamedNode = rootDir.getNode(srcStr);
             if (renamedNode == null) {
                 NameNode.stateChangeLog.warn(""DIR* FSDirectory.unprotectedRenameTo: ""
-                        +""failed to rename ""+src+"" to ""+dst+ "" because ""+ src+"" does not exist"" );
+                        +""failed to rename ""+src+"" to ""+dst+ "" because source does not exist"" );
                 return false;
             }
-            renamedNode.removeNode();
             if (isDir(dst)) {
               dstStr += ""/"" + new File(srcStr).getName();
             }
+            if( rootDir.getNode(dstStr.toString()) != null ) {
+                NameNode.stateChangeLog.warn(""DIR* FSDirectory.unprotectedRenameTo: ""
+                        +""failed to rename ""+src+"" to ""+dstStr+ "" because destination exists"" );
+                return false;
+            }
+            renamedNode.removeNode();
+            
             // the renamed node can be reused now
-            if( rootDir.addNode(dstStr, renamedNode ) == null ) {
+            try {
+                if( rootDir.addNode(dstStr, renamedNode ) != null ) {
+                    NameNode.stateChangeLog.debug(""DIR* FSDirectory.unprotectedRenameTo: ""
+                        +src+"" is renamed to ""+dst );
+                    return true;
+                }
+            } catch (FileNotFoundException e ) {
                 NameNode.stateChangeLog.warn(""DIR* FSDirectory.unprotectedRenameTo: ""
                         +""failed to rename ""+src+"" to ""+dst );
-              rootDir.addNode(srcStr, renamedNode); // put it back
-              return false;
+                try {
+                    rootDir.addNode(srcStr, renamedNode); // put it back
+                }catch(FileNotFoundException e2) {                
+                }
             }
-            NameNode.stateChangeLog.debug(""DIR* FSDirectory.unprotectedRenameTo: ""
-                     +src+"" is renamed to ""+dst );
-            return true;
+
+            return false;
         }
     }
 
@@ -977,29 +1002,28 @@
 
         // Now go backwards through list of dirs, creating along
         // the way
-        boolean lastSuccess = false;
         int numElts = v.size();
         for (int i = numElts - 1; i >= 0; i--) {
             String cur = (String) v.elementAt(i);
-            INode inserted = unprotectedMkdir(cur);
-            if (inserted != null) {
-                NameNode.stateChangeLog.debug(""DIR* FSDirectory.mkdirs: ""
+            try {
+               INode inserted = unprotectedMkdir(cur);
+               if (inserted != null) {
+                   NameNode.stateChangeLog.debug(""DIR* FSDirectory.mkdirs: ""
                         +""created directory ""+cur );
-                logEdit(OP_MKDIR, new UTF8(inserted.computeName()), null);
-                lastSuccess = true;
-            } else {
-                lastSuccess = false;
+                   logEdit(OP_MKDIR, new UTF8(inserted.computeName()), null);
+               } // otherwise cur exists, continue
+            } catch (FileNotFoundException e ) {
+                NameNode.stateChangeLog.debug(""DIR* FSDirectory.mkdirs: ""
+                        +""failed to create directory ""+src);
+                return false;
             }
         }
-/*        if( !lastSuccess )
-            NameNode.stateChangeLog.warn(""DIR* FSDirectory.mkdirs: ""
-                    +""failed to create directory ""+src );*/
-        return lastSuccess;
+        return true;
     }
 
     /**
      */
-    INode unprotectedMkdir(String src) {
+    INode unprotectedMkdir(String src) throws FileNotFoundException {
         synchronized (rootDir) {
             return rootDir.addNode(src, new INode(new File(src).getName()));
         }
"
hadoop,5534a28b6883bb4d0dc75a7f126d1899173a77c5,"HADOOP-240.  Fix DFS mkdirs() to not warn when directories already exist.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@412474 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-07 18:16:03,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index 31084e9..800372c 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -1266,7 +1266,7 @@
 
             if (! dir.isValidBlock(b) && ! pendingCreateBlocks.contains(b)) {
                 obsolete.add(b);
-                NameNode.stateChangeLog.info(""BLOCK* NameSystem.processReport: ""
+                NameNode.stateChangeLog.debug(""BLOCK* NameSystem.processReport: ""
                         +""ask ""+nodeID.getName()+"" to delete ""+b.getBlockName() );
             }
         }
@@ -1509,7 +1509,7 @@
                 blockList.append(' ');
                 blockList.append(((Block)invalidateSet.elementAt(i)).getBlockName());
             }
-            NameNode.stateChangeLog.info(""BLOCK* NameSystem.blockToInvalidate: ""
+            NameNode.stateChangeLog.debug(""BLOCK* NameSystem.blockToInvalidate: ""
                    +""ask ""+nodeID.getName()+"" to delete "" + blockList );
         }
         return (Block[]) invalidateSet.toArray(new Block[invalidateSet.size()]);
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/BinaryInputArchive.java b/src/java/org/apache/hadoop/record/BinaryInputArchive.java
index f426345..a2fb59a 100644
--- a/src/java/org/apache/hadoop/record/BinaryInputArchive.java
+++ b/src/java/org/apache/hadoop/record/BinaryInputArchive.java
@@ -25,7 +25,7 @@
 
 /**
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 public class BinaryInputArchive implements InputArchive {
     
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/BinaryOutputArchive.java b/src/java/org/apache/hadoop/record/BinaryOutputArchive.java
index 4d66c26..30a94cb 100644
--- a/src/java/org/apache/hadoop/record/BinaryOutputArchive.java
+++ b/src/java/org/apache/hadoop/record/BinaryOutputArchive.java
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 public class BinaryOutputArchive implements OutputArchive {
     
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/CsvInputArchive.java b/src/java/org/apache/hadoop/record/CsvInputArchive.java
index 66319a4..3b52a17 100644
--- a/src/java/org/apache/hadoop/record/CsvInputArchive.java
+++ b/src/java/org/apache/hadoop/record/CsvInputArchive.java
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 class CsvInputArchive implements InputArchive {
     
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/CsvOutputArchive.java b/src/java/org/apache/hadoop/record/CsvOutputArchive.java
index 75ee8e2..f43b88f 100644
--- a/src/java/org/apache/hadoop/record/CsvOutputArchive.java
+++ b/src/java/org/apache/hadoop/record/CsvOutputArchive.java
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 public class CsvOutputArchive implements OutputArchive {
 
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/InputArchive.java b/src/java/org/apache/hadoop/record/InputArchive.java
index 382cf68..5d14ed7 100644
--- a/src/java/org/apache/hadoop/record/InputArchive.java
+++ b/src/java/org/apache/hadoop/record/InputArchive.java
@@ -24,7 +24,7 @@
 /**
  * Interface that all the Deserializers have to implement.
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 public interface InputArchive {
     public byte readByte(String tag) throws IOException;
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/OutputArchive.java b/src/java/org/apache/hadoop/record/OutputArchive.java
index 3332635..667e2e6 100644
--- a/src/java/org/apache/hadoop/record/OutputArchive.java
+++ b/src/java/org/apache/hadoop/record/OutputArchive.java
@@ -24,7 +24,7 @@
 /**
  * Interface that alll the serializers have to implement.
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 public interface OutputArchive {
     public void writeByte(byte b, String tag) throws IOException;
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/RecordReader.java b/src/java/org/apache/hadoop/record/RecordReader.java
index 992c068..2d2c37c 100644
--- a/src/java/org/apache/hadoop/record/RecordReader.java
+++ b/src/java/org/apache/hadoop/record/RecordReader.java
@@ -30,7 +30,7 @@
  * Front-end interface to deserializers. Also acts as a factory
  * for deserializers.
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 public class RecordReader {
     
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/RecordWriter.java b/src/java/org/apache/hadoop/record/RecordWriter.java
index f59b761..6cbe68b 100644
--- a/src/java/org/apache/hadoop/record/RecordWriter.java
+++ b/src/java/org/apache/hadoop/record/RecordWriter.java
@@ -30,7 +30,7 @@
 /**
  * Front-end for serializers. Also serves as a factory for serializers.
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 public class RecordWriter {
     
@@ -112,4 +112,4 @@
     public void write(Record r) throws IOException {
         r.serialize(archive, """");
     }
-}
\ No newline at end of file
+}
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/Utils.java b/src/java/org/apache/hadoop/record/Utils.java
index 539f077..9f85889 100644
--- a/src/java/org/apache/hadoop/record/Utils.java
+++ b/src/java/org/apache/hadoop/record/Utils.java
@@ -24,7 +24,7 @@
 
 /**
  * Various utility functions for Hadooop record I/O runtime.
- * @author milindb@yahoo-inc.com
+ * @author Milind Bhandarkar
  */
 public class Utils {
     
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/XmlInputArchive.java b/src/java/org/apache/hadoop/record/XmlInputArchive.java
index 0e79573..5ad3104 100644
--- a/src/java/org/apache/hadoop/record/XmlInputArchive.java
+++ b/src/java/org/apache/hadoop/record/XmlInputArchive.java
@@ -29,7 +29,7 @@
 import javax.xml.parsers.SAXParser;
 /**
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 class XmlInputArchive implements InputArchive {
     
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/XmlOutputArchive.java b/src/java/org/apache/hadoop/record/XmlOutputArchive.java
index a38d529..28a2ad7 100644
--- a/src/java/org/apache/hadoop/record/XmlOutputArchive.java
+++ b/src/java/org/apache/hadoop/record/XmlOutputArchive.java
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 class XmlOutputArchive implements OutputArchive {
 
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/CppGenerator.java b/src/java/org/apache/hadoop/record/compiler/CppGenerator.java
index 1cb2f66..4041a34 100644
--- a/src/java/org/apache/hadoop/record/compiler/CppGenerator.java
+++ b/src/java/org/apache/hadoop/record/compiler/CppGenerator.java
@@ -23,22 +23,35 @@
 import java.util.Iterator;
 
 /**
+ * C++ Code generator front-end for Hadoop record I/O.
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 class CppGenerator {
+    private String mFullName;
     private String mName;
     private ArrayList mInclFiles;
     private ArrayList mRecList;
     
-    /** Creates a new instance of CppGenerator */
-    public CppGenerator(String name, ArrayList ilist, ArrayList rlist) {
-        mName = name;
+    /** Creates a new instance of CppGenerator
+     *
+     * @param name possibly full pathname to the file
+     * @param ilist included files (as JFile)
+     * @param rlist List of records defined within this file
+     */
+    CppGenerator(String name, ArrayList ilist, ArrayList rlist) {
+        mFullName = name;
+        mName = (new File(name)).getName();
         mInclFiles = ilist;
         mRecList = rlist;
     }
     
-    public void genCode() throws IOException {
+    /**
+     * Generate C++ code. This method only creates the requested file(s)
+     * and spits-out file-level elements (such as include statements etc.)
+     * record-level code is generated by JRecord.
+     */
+    void genCode() throws IOException {
         FileWriter cc = new FileWriter(mName+"".cc"");
         FileWriter hh = new FileWriter(mName+"".hh"");
         hh.write(""#ifndef __""+mName.toUpperCase().replace('.','_')+""__\n"");
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/JBoolean.java b/src/java/org/apache/hadoop/record/compiler/JBoolean.java
index 385f031..d510a6e 100644
--- a/src/java/org/apache/hadoop/record/compiler/JBoolean.java
+++ b/src/java/org/apache/hadoop/record/compiler/JBoolean.java
@@ -18,7 +18,7 @@
 
 /**
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 public class JBoolean extends JType {
     
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/JBuffer.java b/src/java/org/apache/hadoop/record/compiler/JBuffer.java
index 37a91d9..677b509 100644
--- a/src/java/org/apache/hadoop/record/compiler/JBuffer.java
+++ b/src/java/org/apache/hadoop/record/compiler/JBuffer.java
@@ -18,7 +18,7 @@
 
 /**
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 public class JBuffer extends JCompType {
     
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/JByte.java b/src/java/org/apache/hadoop/record/compiler/JByte.java
index 2564b2f..63a7cfa 100644
--- a/src/java/org/apache/hadoop/record/compiler/JByte.java
+++ b/src/java/org/apache/hadoop/record/compiler/JByte.java
@@ -18,7 +18,7 @@
 
 /**
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 public class JByte extends JType {
     
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/JCompType.java b/src/java/org/apache/hadoop/record/compiler/JCompType.java
index 892066f..9ba52e8 100644
--- a/src/java/org/apache/hadoop/record/compiler/JCompType.java
+++ b/src/java/org/apache/hadoop/record/compiler/JCompType.java
@@ -17,17 +17,19 @@
 package org.apache.hadoop.record.compiler;
 
 /**
+ * Abstract base class for all the ""compound"" types such as ustring,
+ * buffer, vector, map, and record.
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 abstract class JCompType extends JType {
     
     /** Creates a new instance of JCompType */
-    public JCompType(String cppType, String javaType, String suffix, String wrapper) {
+    JCompType(String cppType, String javaType, String suffix, String wrapper) {
         super(cppType, javaType, suffix, wrapper, null);
     }
     
-    public String genCppGetSet(String fname, int fIdx) {
+    String genCppGetSet(String fname, int fIdx) {
         String cgetFunc = ""  virtual const ""+getCppType()+""& get""+fname+""() const {\n"";
         cgetFunc += ""    return m""+fname+"";\n"";
         cgetFunc += ""  }\n"";
@@ -37,15 +39,15 @@
         return cgetFunc + getFunc;
     }
     
-    public String genJavaCompareTo(String fname) {
+    String genJavaCompareTo(String fname) {
         return ""    ret = ""+fname+"".compareTo(peer.""+fname+"");\n"";
     }
     
-    public String genJavaEquals(String fname, String peer) {
+    String genJavaEquals(String fname, String peer) {
         return ""    ret = ""+fname+"".equals(""+peer+"");\n"";
     }
     
-    public String genJavaHashCode(String fname) {
+    String genJavaHashCode(String fname) {
         return ""    ret = ""+fname+"".hashCode();\n"";
     }
 }
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/JDouble.java b/src/java/org/apache/hadoop/record/compiler/JDouble.java
index 06f5446..a606ea1 100644
--- a/src/java/org/apache/hadoop/record/compiler/JDouble.java
+++ b/src/java/org/apache/hadoop/record/compiler/JDouble.java
@@ -18,7 +18,7 @@
 
 /**
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 public class JDouble extends JType {
     
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/JField.java b/src/java/org/apache/hadoop/record/compiler/JField.java
index 660e9f4..1d6248a 100644
--- a/src/java/org/apache/hadoop/record/compiler/JField.java
+++ b/src/java/org/apache/hadoop/record/compiler/JField.java
@@ -18,7 +18,7 @@
 
 /**
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 public class JField {
     private JType mType;
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/JFile.java b/src/java/org/apache/hadoop/record/compiler/JFile.java
index fd18917..71c97f9 100644
--- a/src/java/org/apache/hadoop/record/compiler/JFile.java
+++ b/src/java/org/apache/hadoop/record/compiler/JFile.java
@@ -20,8 +20,11 @@
 import java.util.ArrayList;
 
 /**
+ * Container for the Hadoop Record DDL.
+ * The main components of the file are filename, list of included files,
+ * and records defined in that file.
  *
- * @author milindb@yahoo-inc.com
+ * @author Milind Bhandarkar
  */
 public class JFile {
     
@@ -29,18 +32,27 @@
     private ArrayList mInclFiles;
     private ArrayList mRecords;
     
-    /** Creates a new instance of JFile */
+    /** Creates a new instance of JFile
+     *
+     * @param name possibly full pathname to the file
+     * @param inclFiles included files (as JFile)
+     * @param recList List of records defined within this file
+     */
     public JFile(String name, ArrayList inclFiles, ArrayList recList) {
         mName = name;
         mInclFiles = inclFiles;
         mRecords = recList;
     }
-        
+    
+    /** Strip the other pathname components and return the basename */
     String getName() {
         int idx = mName.lastIndexOf('/');
         return (idx > 0) ? mName.substring(idx) : mName; 
     }
     
+    /** Generate record code in given language. Language should be all
+     *  lowercase.
+     */
     public void genCode(String language) throws IOException {
         if (""c++"".equals(language)) {
             CppGenerator gen = new CppGenerator(mName, mInclFiles, mRecords);
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/JFloat.java b/src/java/org/apache/hadoop/record/compiler/JFloat.java
index ce9fb2f..a90fe04 100644
--- a/src/java/org/apache/hadoop/record/compiler/JFloat.java
+++ b/src/java/org/apache/hadoop/record/compiler/JFloat.java
@@ -18,7 +18,7 @@
 
 /**
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 public class JFloat extends JType {
     
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/JInt.java b/src/java/org/apache/hadoop/record/compiler/JInt.java
index 79764f8..a39e2ec 100644
--- a/src/java/org/apache/hadoop/record/compiler/JInt.java
+++ b/src/java/org/apache/hadoop/record/compiler/JInt.java
@@ -18,7 +18,7 @@
 
 /**
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 public class JInt extends JType {
     
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/JLong.java b/src/java/org/apache/hadoop/record/compiler/JLong.java
index 81e7c6f..8bb590f 100644
--- a/src/java/org/apache/hadoop/record/compiler/JLong.java
+++ b/src/java/org/apache/hadoop/record/compiler/JLong.java
@@ -18,7 +18,7 @@
 
 /**
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 public class JLong extends JType {
     
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/JMap.java b/src/java/org/apache/hadoop/record/compiler/JMap.java
index dbeb74a..d0bd44f 100644
--- a/src/java/org/apache/hadoop/record/compiler/JMap.java
+++ b/src/java/org/apache/hadoop/record/compiler/JMap.java
@@ -18,7 +18,7 @@
 
 /**
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 public class JMap extends JCompType {
    
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/JRecord.java b/src/java/org/apache/hadoop/record/compiler/JRecord.java
index 15626d1..cfc8b70 100644
--- a/src/java/org/apache/hadoop/record/compiler/JRecord.java
+++ b/src/java/org/apache/hadoop/record/compiler/JRecord.java
@@ -24,7 +24,7 @@
 
 /**
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 public class JRecord extends JCompType {
 
@@ -132,7 +132,7 @@
             hh.write(jf.genCppGetSet(fIdx));
         }
         hh.write(""}; // end record ""+getName()+""\n"");
-        for (int i=0; i<ns.length; i++) {
+        for (int i=ns.length-1; i>=0; i--) {
             hh.write(""} // end namespace ""+ns[i]+""\n"");
         }
         cc.write(""void ""+getCppFQName()+""::serialize(::hadoop::OArchive& a_, const char* tag) {\n"");
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/JString.java b/src/java/org/apache/hadoop/record/compiler/JString.java
index 1680f6a..14c561b 100644
--- a/src/java/org/apache/hadoop/record/compiler/JString.java
+++ b/src/java/org/apache/hadoop/record/compiler/JString.java
@@ -18,7 +18,7 @@
 
 /**
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 public class JString extends JCompType {
     
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/JType.java b/src/java/org/apache/hadoop/record/compiler/JType.java
index a8e368a..e90d49a 100644
--- a/src/java/org/apache/hadoop/record/compiler/JType.java
+++ b/src/java/org/apache/hadoop/record/compiler/JType.java
@@ -17,10 +17,11 @@
 package org.apache.hadoop.record.compiler;
 
 /**
- *
- * @author milindb
+ * Abstract Base class for all types supported by Hadoop Record I/O.
+ * 
+ * @author Milind Bhandarkar
  */
-public abstract class JType {
+abstract public class JType {
     
     private String mCppName;
     private String mJavaName;
@@ -31,7 +32,7 @@
     /**
      * Creates a new instance of JType
      */
-    public JType(String cppname, String javaname, String suffix, String wrapper, String unwrap) {
+    JType(String cppname, String javaname, String suffix, String wrapper, String unwrap) {
         mCppName = cppname;
         mJavaName = javaname;
         mMethodSuffix = suffix;
@@ -39,21 +40,21 @@
         mUnwrapMethod = unwrap;
     }
     
-    abstract public String getSignature();
+    abstract String getSignature();
     
-    public String genCppDecl(String fname) {
+    String genCppDecl(String fname) {
         return ""  ""+mCppName+"" m""+fname+"";\n""; 
     }
     
-    public String genJavaDecl (String fname) {
+    String genJavaDecl (String fname) {
         return ""  private ""+mJavaName+"" m""+fname+"";\n"";
     }
     
-    public String genJavaConstructorParam (int fIdx) {
+    String genJavaConstructorParam (int fIdx) {
         return ""        ""+mJavaName+"" m""+fIdx;
     }
     
-    public String genCppGetSet(String fname, int fIdx) {
+    String genCppGetSet(String fname, int fIdx) {
         String getFunc = ""  virtual ""+mCppName+"" get""+fname+""() const {\n"";
         getFunc += ""    return m""+fname+"";\n"";
         getFunc += ""  }\n"";
@@ -63,7 +64,7 @@
         return getFunc+setFunc;
     }
     
-    public String genJavaGetSet(String fname, int fIdx) {
+    String genJavaGetSet(String fname, int fIdx) {
         String getFunc = ""  public ""+mJavaName+"" get""+fname+""() {\n"";
         getFunc += ""    return m""+fname+"";\n"";
         getFunc += ""  }\n"";
@@ -73,31 +74,31 @@
         return getFunc+setFunc;
     }
     
-    public String getCppType() {
+    String getCppType() {
         return mCppName;
     }
     
-    public String getJavaType() {
+    String getJavaType() {
         return mJavaName;
     }
    
-    public String getJavaWrapperType() {
+    String getJavaWrapperType() {
         return mWrapper;
     }
     
-    public String getMethodSuffix() {
+    String getMethodSuffix() {
         return mMethodSuffix;
     }
     
-    public String genJavaWriteMethod(String fname, String tag) {
+    String genJavaWriteMethod(String fname, String tag) {
         return ""    a_.write""+mMethodSuffix+""(""+fname+"",\""""+tag+""\"");\n"";
     }
     
-    public String genJavaReadMethod(String fname, String tag) {
+    String genJavaReadMethod(String fname, String tag) {
         return ""    ""+fname+""=a_.read""+mMethodSuffix+""(\""""+tag+""\"");\n"";
     }
     
-    public String genJavaReadWrapper(String fname, String tag, boolean decl) {
+    String genJavaReadWrapper(String fname, String tag, boolean decl) {
         String ret = """";
         if (decl) {
             ret = ""    ""+mWrapper+"" ""+fname+"";\n"";
@@ -105,19 +106,19 @@
         return ret + ""    ""+fname+""=new ""+mWrapper+""(a_.read""+mMethodSuffix+""(\""""+tag+""\""));\n"";
     }
     
-    public String genJavaWriteWrapper(String fname, String tag) {
+    String genJavaWriteWrapper(String fname, String tag) {
         return ""        a_.write""+mMethodSuffix+""(""+fname+"".""+mUnwrapMethod+""(),\""""+tag+""\"");\n"";
     }
     
-    public String genJavaCompareTo(String fname) {
+    String genJavaCompareTo(String fname) {
         return ""    ret = (""+fname+"" == peer.""+fname+"")? 0 :((""+fname+""<peer.""+fname+"")?-1:1);\n"";
     }
     
-    public String genJavaEquals(String fname, String peer) {
+    String genJavaEquals(String fname, String peer) {
         return ""    ret = (""+fname+""==""+peer+"");\n"";
     }
     
-    public String genJavaHashCode(String fname) {
+    String genJavaHashCode(String fname) {
         return ""    ret = (int)""+fname+"";\n"";
     }
 
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/JVector.java b/src/java/org/apache/hadoop/record/compiler/JVector.java
index 35d752a..0984e45 100644
--- a/src/java/org/apache/hadoop/record/compiler/JVector.java
+++ b/src/java/org/apache/hadoop/record/compiler/JVector.java
@@ -18,7 +18,7 @@
 
 /**
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 public class JVector extends JCompType {
     
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/record/compiler/JavaGenerator.java b/src/java/org/apache/hadoop/record/compiler/JavaGenerator.java
index a1f051e..c13e077 100644
--- a/src/java/org/apache/hadoop/record/compiler/JavaGenerator.java
+++ b/src/java/org/apache/hadoop/record/compiler/JavaGenerator.java
@@ -23,22 +23,32 @@
 import java.util.Iterator;
 
 /**
+ * Java Code generator front-end for Hadoop record I/O.
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 class JavaGenerator {
     private String mName;
     private ArrayList mInclFiles;
     private ArrayList mRecList;
     
-    /** Creates a new instance of JavaGenerator */
+    /** Creates a new instance of JavaGenerator
+     *
+     * @param name possibly full pathname to the file
+     * @param incl included files (as JFile)
+     * @param records List of records defined within this file
+     */
     JavaGenerator(String name, ArrayList incl, ArrayList records) {
         mName = name;
         mInclFiles = incl;
         mRecList = records;
     }
     
-    public void genCode() throws IOException {
+    /**
+     * Generate Java code for records. This method is only a front-end to
+     * JRecord, since one file is generated for each record.
+     */
+    void genCode() throws IOException {
         for (Iterator i = mRecList.iterator(); i.hasNext(); ) {
             JRecord rec = (JRecord) i.next();
             rec.genJavaCode();
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/record/test/FromCpp.java b/src/test/org/apache/hadoop/record/test/FromCpp.java
index b2b0e8a..a38bc19 100644
--- a/src/test/org/apache/hadoop/record/test/FromCpp.java
+++ b/src/test/org/apache/hadoop/record/test/FromCpp.java
@@ -29,7 +29,7 @@
 
 /**
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 public class FromCpp extends TestCase {
     
"
hadoop,858ad78f4acac34c775a9d7cf5e73e3c8029a2eb,"HADOOP-245 and HADOOP-246.  Improvements to record io package.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411909 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 20:44:07,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/record/test/ToCpp.java b/src/test/org/apache/hadoop/record/test/ToCpp.java
index 04e8c61..2963f47 100644
--- a/src/test/org/apache/hadoop/record/test/ToCpp.java
+++ b/src/test/org/apache/hadoop/record/test/ToCpp.java
@@ -29,7 +29,7 @@
 
 /**
  *
- * @author milindb
+ * @author Milind Bhandarkar
  */
 public class ToCpp extends TestCase {
     
"
hadoop,19d973718c05db84b768588e40b6657cf8d1363e,"HADOOP-262.  Fix reduce tasks to report progress while they're waiting for map outputs so that they do not time out.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411901 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-05 19:59:29,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java b/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
index dfbd8b7..75e58ce 100644
--- a/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
+++ b/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
@@ -419,6 +419,7 @@
       // new, just wait for a bit
       try {
         if (numInFlight == 0 && numScheduled == 0) {
+          getTask().reportProgress(getTracker());
           Thread.sleep(5000);
         }
       } catch (InterruptedException e) { } // IGNORE
"
hadoop,e29d298212be80b9d82a92fbde761e8cceab7c4e,"HADOOP-270.  Fix potential deadlock in datanode shutdown.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411288 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-02 20:13:24,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index 390cc90..9eb163d 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -146,7 +146,6 @@
       // initialize data node internal structure
       this.data = new FSDataset(datadir, conf);
       this.dataXceiveServer = new Daemon(new DataXceiveServer(ss));
-      this.dataXceiveServer.start();
 
       long blockReportIntervalBasis =
         conf.getLong(""dfs.blockreport.intervalMsec"", BLOCKREPORT_INTERVAL);
@@ -189,10 +188,6 @@
         this.shouldRun = false;
         ((DataXceiveServer) this.dataXceiveServer.getRunnable()).kill();
         try {
-            this.dataXceiveServer.join();
-        } catch (InterruptedException ie) {
-        }
-        try {
           this.storage.close();
         } catch (IOException ie) {
         }
@@ -213,6 +208,9 @@
      * forever calling remote NameNode functions.
      */
     public void offerService() throws Exception {
+      // start dataXceiveServer  
+      dataXceiveServer.start();
+      
       long lastHeartbeat = 0, lastBlockReport = 0;
       LOG.info(""using BLOCKREPORT_INTERVAL of "" + blockReportInterval + ""msec"");
 
@@ -329,6 +327,12 @@
       } catch(DiskErrorException e) {
         handleDiskError(e.getMessage());
       }
+      
+      // wait for dataXceiveServer to terminate
+      try {
+          this.dataXceiveServer.join();
+      } catch (InterruptedException ie) {
+      }
     } // offerService
 
     /**
"
hadoop,a70e171165df21a9b855035ec866356e7aa536ad,"HADOOP-217.  Fix another illegal access problem when static
initializers are not run.


git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411282 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-02 19:59:33,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/NameNode.java b/src/java/org/apache/hadoop/dfs/NameNode.java
index 4226d51..68026bb 100644
--- a/src/java/org/apache/hadoop/dfs/NameNode.java
+++ b/src/java/org/apache/hadoop/dfs/NameNode.java
@@ -72,6 +72,7 @@
     // creating an instance will do the static initialization of the class
     static {
       new DatanodeRegistration();
+      new Block();
     }
 
     /** Format a new filesystem.  Destroys any filesystem that may already
"
hadoop,5839aa18af0052d1967189e5dfb9c14b694c7e77,"HADOOP-265.  Fix tasktracker to not start when it doesn't have a writable local directory.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411277 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-02 19:52:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java b/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java
index 5dabd23..b073510 100644
--- a/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java
+++ b/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java
@@ -63,6 +63,18 @@
    * files referred to by the JobTracker
    */
   public String getFilesystemName() throws IOException;
+  
+  /**
+   * Report a problem to the job tracker.
+   * @param taskTracker the name of the task tracker
+   * @param errorClass the kind of error (eg. the class that was thrown)
+   * @param errorMessage the human readable error message
+   * @throws IOException if there was a problem in communication or on the
+   *                     remote side
+   */
+  public void reportTaskTrackerError(String taskTracker,
+                                     String errorClass,
+                                     String errorMessage) throws IOException;
 }
 
 
"
hadoop,5839aa18af0052d1967189e5dfb9c14b694c7e77,"HADOOP-265.  Fix tasktracker to not start when it doesn't have a writable local directory.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411277 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-02 19:52:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index e330678..242569c 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -835,6 +835,13 @@
         return fs.getName();
     }
 
+
+    public void reportTaskTrackerError(String taskTracker,
+            String errorClass,
+            String errorMessage) throws IOException {
+        LOG.warn(""Report from "" + taskTracker + "": "" + errorMessage);        
+    }
+
     ////////////////////////////////////////////////////
     // JobSubmissionProtocol
     ////////////////////////////////////////////////////
"
hadoop,5839aa18af0052d1967189e5dfb9c14b694c7e77,"HADOOP-265.  Fix tasktracker to not start when it doesn't have a writable local directory.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411277 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-02 19:52:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 1bc1f0e..ba669bd 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -20,6 +20,7 @@
 import org.apache.hadoop.fs.*;
 import org.apache.hadoop.ipc.*;
 import org.apache.hadoop.util.*;
+import org.apache.hadoop.util.DiskChecker.DiskErrorException;
 
 import java.io.*;
 import java.net.*;
@@ -102,32 +103,6 @@
     }
     
     /**
-     * Start with the local machine name, and the default JobTracker
-     */
-    public TaskTracker(JobConf conf) throws IOException {
-      maxCurrentTasks = conf.getInt(""mapred.tasktracker.tasks.maximum"", 2);
-      this.fConf = conf;
-      this.jobTrackAddr = JobTracker.getAddress(conf);
-      this.taskTimeout = conf.getInt(""mapred.task.timeout"", 10* 60 * 1000);
-      this.mapOutputFile = new MapOutputFile();
-      this.mapOutputFile.setConf(conf);
-      int httpPort = conf.getInt(""tasktracker.http.port"", 50060);
-      StatusHttpServer server = new StatusHttpServer(""task"", httpPort, true);
-      int workerThreads = conf.getInt(""tasktracker.http.threads"", 40);
-      server.setThreads(1, workerThreads);
-      server.start();
-      this.httpPort = server.getPort();
-      // let the jsp pages get to the task tracker, config, and other relevant
-      // objects
-      FileSystem local = FileSystem.getNamed(""local"", conf);
-      server.setAttribute(""task.tracker"", this);
-      server.setAttribute(""local.file.system"", local);
-      server.setAttribute(""conf"", conf);
-      server.setAttribute(""log"", LOG);
-      initialize();
-    }
-
-    /**
      * Do the real constructor work here.  It's in a separate method
      * so we can call it again and ""recycle"" the object after calling
      * close().
@@ -135,6 +110,8 @@
     synchronized void initialize() throws IOException {
         this.localHostname = InetAddress.getLocalHost().getHostName();
 
+        //check local disk
+        checkLocalDirs(this.fConf.getLocalDirs());
         fConf.deleteLocalFiles(SUBDIR);
 
         // Clear out state tables
@@ -219,6 +196,32 @@
     }
 
     /**
+     * Start with the local machine name, and the default JobTracker
+     */
+    public TaskTracker(JobConf conf) throws IOException {
+      maxCurrentTasks = conf.getInt(""mapred.tasktracker.tasks.maximum"", 2);
+      this.fConf = conf;
+      this.jobTrackAddr = JobTracker.getAddress(conf);
+      this.taskTimeout = conf.getInt(""mapred.task.timeout"", 10* 60 * 1000);
+      this.mapOutputFile = new MapOutputFile();
+      this.mapOutputFile.setConf(conf);
+      int httpPort = conf.getInt(""tasktracker.http.port"", 50060);
+      StatusHttpServer server = new StatusHttpServer(""task"", httpPort, true);
+      int workerThreads = conf.getInt(""tasktracker.http.threads"", 40);
+      server.setThreads(1, workerThreads);
+      server.start();
+      this.httpPort = server.getPort();
+      // let the jsp pages get to the task tracker, config, and other relevant
+      // objects
+      FileSystem local = FileSystem.getNamed(""local"", conf);
+      server.setAttribute(""task.tracker"", this);
+      server.setAttribute(""local.file.system"", local);
+      server.setAttribute(""conf"", conf);
+      server.setAttribute(""log"", LOG);
+      initialize();
+    }
+
+    /**
      * The connection to the JobTracker, used by the TaskRunner 
      * for locating remote files.
      */
@@ -287,11 +290,17 @@
             //
             try {
               if (mapTotal < maxCurrentTasks || reduceTotal < maxCurrentTasks) {
+                  checkLocalDirs(fConf.getLocalDirs());
                   Task t = jobClient.pollForNewTask(taskTrackerName);
                   if (t != null) {
                     startNewTask(t);
                   }
               }
+            } catch (DiskErrorException de ) {
+                LOG.warn(""Exiting task tracker because ""+de.getMessage());
+                jobClient.reportTaskTrackerError(taskTrackerName, 
+                        ""DiskErrorException"", de.getMessage());
+                return STALE_STATE;
             } catch (IOException ie) {
               LOG.info(""Problem launching task: "" + 
                        StringUtils.stringifyException(ie));
@@ -914,6 +923,33 @@
     }
     
     /**
+     * Check if the given local directories
+     * (and parent directories, if necessary) can be created.
+     * @param localDirs where the new TaskTracker should keep its local files.
+     * @throws DiskErrorException if all local directories are not writable
+     * @author hairong
+     */
+    private static void checkLocalDirs( String[] localDirs ) 
+            throws DiskErrorException {
+        boolean writable = false;
+        
+        if( localDirs != null ) {
+            for (int i = 0; i < localDirs.length; i++) {
+                try {
+                    DiskChecker.checkDir( new File(localDirs[i]) );
+                    writable = true;
+                } catch( DiskErrorException e ) {
+                    LOG.warn(""Task Tracker local "" + e.getMessage() );
+                }
+            }
+        }
+
+        if( !writable )
+            throw new DiskErrorException( 
+                    ""all local directories are not writable"" );
+    }
+    
+    /**
      * Start the TaskTracker, point toward the indicated JobTracker
      */
     public static void main(String argv[]) throws Exception {
@@ -922,7 +958,12 @@
             System.exit(-1);
         }
 
-        JobConf conf=new JobConf();
-        new TaskTracker(conf).run();
+        try {
+          JobConf conf=new JobConf();
+          new TaskTracker(conf).run();
+        } catch (IOException e) {
+            LOG.warn( ""Can not start task tracker because ""+e.getMessage());
+            System.exit(-1);
+        }
     }
 }
"
hadoop,5839aa18af0052d1967189e5dfb9c14b694c7e77,"HADOOP-265.  Fix tasktracker to not start when it doesn't have a writable local directory.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@411277 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-02 19:52:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/util/DiskChecker.java b/src/java/org/apache/hadoop/util/DiskChecker.java
index edeec10..d94e05d 100644
--- a/src/java/org/apache/hadoop/util/DiskChecker.java
+++ b/src/java/org/apache/hadoop/util/DiskChecker.java
@@ -11,7 +11,7 @@
 public class DiskChecker {
 
     public static class DiskErrorException extends IOException {
-      DiskErrorException(String msg) {
+      public DiskErrorException(String msg) {
         super(msg);
       }
     }
"
hadoop,f2d7007743c2ebe9d2409c9b6ea5134c536d5173,"HADOOP-259.  Fix HTTP transfers of map output to timoout.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@410929 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-01 19:51:50,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/MapOutputLocation.java b/src/java/org/apache/hadoop/mapred/MapOutputLocation.java
index cea23c0..bba2d1f 100644
--- a/src/java/org/apache/hadoop/mapred/MapOutputLocation.java
+++ b/src/java/org/apache/hadoop/mapred/MapOutputLocation.java
@@ -91,15 +91,26 @@
   }
 
   /**
+   * An interface for callbacks when an method makes some progress.
+   * @author Owen O'Malley
+   */
+  public static interface Pingable {
+    void ping();
+  }
+  
+  /**
    * Get the map output into a local file from the remote server.
    * We use the file system so that we generate checksum files on the data.
    * @param fileSys the filesystem to write the file to
    * @param localFilename the filename to write the data into
    * @param reduce the reduce id to get for
+   * @param pingee a status object that wants to know when we make progress
    * @throws IOException when something goes wrong
    */
   public long getFile(FileSystem fileSys, 
-                      Path localFilename, int reduce) throws IOException {
+                      Path localFilename, 
+                      int reduce,
+                      Pingable pingee) throws IOException {
     URL path = new URL(toString() + ""&reduce="" + reduce);
     InputStream input = path.openConnection().getInputStream();
     OutputStream output = fileSys.create(localFilename);
@@ -110,6 +121,9 @@
       while (len > 0) {
         totalBytes += len;
         output.write(buffer, 0 ,len);
+        if (pingee != null) {
+          pingee.ping();
+        }
         len = input.read(buffer);
       }
     } finally {
"
hadoop,f2d7007743c2ebe9d2409c9b6ea5134c536d5173,"HADOOP-259.  Fix HTTP transfers of map output to timoout.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@410929 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-01 19:51:50,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java b/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
index 237bc56..473c2b3 100644
--- a/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
+++ b/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
@@ -78,6 +78,11 @@
    * A reference to the local file system for writing the map outputs to.
    */
   private FileSystem localFileSys;
+
+  /**
+   * The threads for fetching the files.
+   */
+  private MapOutputCopier[] copiers = null;
   
   /**
    * the minimum interval between jobtracker polls
@@ -109,13 +114,76 @@
     public String getHost() { return loc.getHost(); }
     public MapOutputLocation getLocation() { return loc; }
   }
+
+  private static class PingTimer implements MapOutputLocation.Pingable {
+    private long pingTime;
+    
+    public synchronized void reset() {
+      pingTime = 0;
+    }
+    
+    public synchronized long getLastPing() {
+      return pingTime;
+    }
+    
+    public void ping() {
+      synchronized (this) {
+        pingTime = System.currentTimeMillis();
+      }
+    }
+  }
   
   /** Copies map outputs as they become available */
   private class MapOutputCopier extends Thread {
 
+    private PingTimer pingTimer = new PingTimer();
+    private MapOutputLocation currentLocation = null;
+    
     public MapOutputCopier() {
     }
     
+    /**
+     * Get the last time that this copier made progress.
+     * @return the System.currentTimeMillis when this copier last made progress
+     */
+    public long getLastProgressTime() {
+      return pingTimer.getLastPing();
+    }
+    
+    /**
+     * Fail the current file that we are fetching
+     * @return were we currently fetching?
+     */
+    public synchronized boolean fail() {
+      if (currentLocation != null) {
+        finish(-1);
+        return true;
+      } else {
+        return false;
+      }
+    }
+    
+    /**
+     * Get the current map output location.
+     */
+    public synchronized MapOutputLocation getLocation() {
+      return currentLocation;
+    }
+    
+    private synchronized void start(MapOutputLocation loc) {
+      currentLocation = loc;
+    }
+    
+    private synchronized void finish(long size) {
+      if (currentLocation != null) {
+        synchronized (copyResults) {
+          copyResults.add(new CopyResult(currentLocation, size));
+          copyResults.notify();
+        }
+        currentLocation = null;
+      }
+    }
+    
     /** Loop forever and fetch map outputs as they become available.
      * The thread exits when it is interrupted by the {@link ReduceTaskRunner}
      */
@@ -133,27 +201,28 @@
           }
 
           try {
-            size = copyOutput(loc);
+            start(loc);
+            pingTimer.ping();
+            size = copyOutput(loc, pingTimer);
+            pingTimer.reset();
           } catch (IOException e) {
             LOG.warning(reduceTask.getTaskId() + "" copy failed: "" +
                         loc.getMapTaskId() + "" from "" + loc.getHost());
             LOG.warning(StringUtils.stringifyException(e));
           }
-          
-          synchronized (copyResults) {
-            copyResults.add(new CopyResult(loc, size));
-            copyResults.notifyAll();
-          }
+          finish(size);
         }
       } catch (InterruptedException e) { }  // ALL DONE!
     }
 
     /** Copies a a map output from a remote host, using raw RPC. 
-     * @param loc the map output location to be copied
+     * @param currentLocation the map output location to be copied
+     * @param pingee a status object to ping as we make progress
      * @return the size of the copied file
      * @throws IOException if there is an error copying the file
      */
-    private long copyOutput(MapOutputLocation loc)
+    private long copyOutput(MapOutputLocation loc, 
+                            MapOutputLocation.Pingable pingee)
     throws IOException {
 
       String reduceId = reduceTask.getTaskId();
@@ -165,7 +234,7 @@
         Path filename = conf.getLocalPath(reduceId + ""/map_"" +
                                           loc.getMapId() + "".out"");
         long bytes = loc.getFile(localFileSys, filename,
-                                 reduceTask.getPartition());
+                                 reduceTask.getPartition(), pingee);
 
         LOG.info(reduceTask.getTaskId() + "" done copying "" + loc.getMapTaskId() +
                  "" output from "" + loc.getHost() + ""."");
@@ -181,6 +250,46 @@
 
   }
   
+  private class MapCopyLeaseChecker extends Thread {
+    private static final long STALLED_COPY_TIMEOUT = 3 * 60 * 1000;
+    private static final long STALLED_COPY_CHECK = 60 * 1000;
+    private long lastStalledCheck = 0;
+    
+    public void run() {
+      try {
+        while (true) {
+          long currentTime = System.currentTimeMillis();
+          if (currentTime - lastStalledCheck > STALLED_COPY_CHECK) {
+            lastStalledCheck = currentTime;
+            synchronized (copiers) {
+              for(int i=0; i < copiers.length; ++i) {
+                if (copiers[i] == null) {
+                  break;
+                }
+                long lastProgress = copiers[i].getLastProgressTime();
+                if (lastProgress != 0 && 
+                    currentTime - lastProgress > STALLED_COPY_TIMEOUT)  {
+                  LOG.warning(""Map output copy stalled on "" + 
+                              copiers[i].getLocation());
+                  // mark the current file as failed
+                  copiers[i].fail();
+                  // tell the thread to stop
+                  copiers[i].interrupt();
+                  // create a replacement thread
+                  copiers[i] = new MapOutputCopier();
+                  copiers[i].start();
+                }
+              }
+            }
+          } else {
+            Thread.sleep(lastStalledCheck + STALLED_COPY_CHECK - currentTime);
+          }
+        }
+      } catch (InterruptedException ie) {}
+      
+    }
+  }
+
   public ReduceTaskRunner(Task task, TaskTracker tracker, 
                           JobConf conf) throws IOException {
     super(task, tracker, conf);
@@ -218,6 +327,7 @@
     DecimalFormat  mbpsFormat = new DecimalFormat(""0.00"");
     Random         backoff = new Random();
     final Progress copyPhase = getTask().getProgress().phase();
+    MapCopyLeaseChecker leaseChecker = null;
     
     for (int i = 0; i < numOutputs; i++) {
       neededOutputs.add(new Integer(i));
@@ -225,13 +335,15 @@
     }
 
     InterTrackerProtocol jobClient = getTracker().getJobClient();
-    MapOutputCopier[] copiers = new MapOutputCopier[numCopiers];
+    copiers = new MapOutputCopier[numCopiers];
 
     // start all the copying threads
     for (int i=0; i < copiers.length; i++) {
       copiers[i] = new MapOutputCopier();
       copiers[i].start();
     }
+    leaseChecker = new MapCopyLeaseChecker();
+    leaseChecker.start();
     
     // start the clock for bandwidth measurement
     long startTime = System.currentTimeMillis();
@@ -314,34 +426,36 @@
       while (!killed && numInFlight > 0) {
         CopyResult cr = getCopyResult();
         
-        if (cr.getSuccess()) {  // a successful copy
-          numCopied++;
-          bytesTransferred += cr.getSize();
+        if (cr != null) {
+          if (cr.getSuccess()) {  // a successful copy
+            numCopied++;
+            bytesTransferred += cr.getSize();
           
-          long secsSinceStart = (System.currentTimeMillis()-startTime)/1000+1;
-          float mbs = ((float)bytesTransferred)/(1024*1024);
-          float transferRate = mbs/secsSinceStart;
+            long secsSinceStart = (System.currentTimeMillis()-startTime)/1000+1;
+            float mbs = ((float)bytesTransferred)/(1024*1024);
+            float transferRate = mbs/secsSinceStart;
           
-          copyPhase.startNextPhase();
-          copyPhase.setStatus(""copy ("" + numCopied + "" of "" + numOutputs + "" at "" +
-                              mbpsFormat.format(transferRate) +  "" MB/s)"");          
-          getTask().reportProgress(getTracker());
+            copyPhase.startNextPhase();
+            copyPhase.setStatus(""copy ("" + numCopied + "" of "" + numOutputs + 
+                                "" at "" +
+                                mbpsFormat.format(transferRate) +  "" MB/s)"");          
+            getTask().reportProgress(getTracker());
+          } else {
+            // this copy failed, put it back onto neededOutputs
+            neededOutputs.add(new Integer(cr.getMapId()));
+          
+            // wait a random amount of time for next contact
+            currentTime = System.currentTimeMillis();
+            long nextContact = currentTime + 60 * 1000 +
+                               backoff.nextInt(maxBackoff*1000);
+            penaltyBox.put(cr.getHost(), new Long(nextContact));          
+            LOG.warning(reduceTask.getTaskId() + "" adding host "" +
+                        cr.getHost() + "" to penalty box, next contact in "" +
+                        ((nextContact-currentTime)/1000) + "" seconds"");
+          }
+          uniqueHosts.remove(cr.getHost());
+          numInFlight--;
         }
-        else {
-          // this copy failed, put it back onto neededOutputs
-          neededOutputs.add(new Integer(cr.getMapId()));
-          
-          // wait a random amount of time for next contact
-          currentTime = System.currentTimeMillis();
-          long nextContact = currentTime + 60 * 1000 +
-                             backoff.nextInt(maxBackoff*1000);
-          penaltyBox.put(cr.getHost(), new Long(nextContact));          
-          LOG.warning(reduceTask.getTaskId() + "" adding host "" +
-                      cr.getHost() + "" to penalty box, next contact in "" +
-                      ((nextContact-currentTime)/1000) + "" seconds"");
-        }
-        uniqueHosts.remove(cr.getHost());
-        numInFlight--;
         
         // ensure we have enough to keep us busy
         if (numInFlight < lowThreshold && (numOutputs-numCopied) > PROBE_SAMPLE_SIZE) {
@@ -352,9 +466,13 @@
     }
 
     // all done, inform the copiers to exit
-    synchronized (scheduledCopies) {
-     for (int i=0; i < copiers.length; i++) {
-       copiers[i].interrupt();
+    leaseChecker.interrupt();
+    synchronized (copiers) {
+      synchronized (scheduledCopies) {
+        for (int i=0; i < copiers.length; i++) {
+          copiers[i].interrupt();
+          copiers[i] = null;
+        }
       }
     }
     
@@ -363,17 +481,18 @@
   
   
   private CopyResult getCopyResult() {  
-    CopyResult cr = null;
-    
-    synchronized (copyResults) {
-      while (copyResults.isEmpty()) {
+   synchronized (copyResults) {
+      while (!killed && copyResults.isEmpty()) {
         try {
           copyResults.wait();
         } catch (InterruptedException e) { }
       }
-      cr = (CopyResult)copyResults.remove(0);      
+      if (copyResults.isEmpty()) {
+        return null;
+      } else {
+        return (CopyResult) copyResults.remove(0);
+      }
     }    
-    return cr;
   }
   
   /** Queries the job tracker for a set of outputs ready to be copied
@@ -421,4 +540,15 @@
     this.mapOutputFile.removeAll(getTask().getTaskId());
   }
 
+  /**
+   * Kill the child process, but also kick getCopyResult so that it checks
+   * the kill flag.
+   */
+  public void kill() {
+    synchronized (copyResults) {
+      super.kill();
+      copyResults.notify();
+    }
+  }
+
 }
"
hadoop,9ebbf32283a9b92b32bc16f0f33e25a5db3479c6,"HADOOP-264.  Workaround a problem with WritableFactories.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@410910 13f79535-47bb-0310-9956-ffa450edef68
",2006-06-01 18:57:27,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/NameNode.java b/src/java/org/apache/hadoop/dfs/NameNode.java
index 752b41b..ed57bf5 100644
--- a/src/java/org/apache/hadoop/dfs/NameNode.java
+++ b/src/java/org/apache/hadoop/dfs/NameNode.java
@@ -67,6 +67,11 @@
     
     /** only used for testing purposes  */
     private boolean stopRequested = false;
+    // force loading of classes that will be received via RPC
+    // creating an instance will do the static initialization of the class
+    static {
+      new DatanodeRegistration();
+    }
 
     /** Format a new filesystem.  Destroys any filesystem that may already
      * exist at this location.  **/
"
hadoop,5bdda7743eda781a3ddee4bf97bf31b88c94d676,"HADOOP-242.  Improved error message when unable to create a local file.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@410399 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-30 22:56:15,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/conf/Configuration.java b/src/java/org/apache/hadoop/conf/Configuration.java
index 5f69a26..21224fa 100644
--- a/src/java/org/apache/hadoop/conf/Configuration.java
+++ b/src/java/org/apache/hadoop/conf/Configuration.java
@@ -286,10 +286,16 @@
       int index = (hashCode+i & Integer.MAX_VALUE) % dirs.length;
       Path file = new Path(dirs[index], path);
       Path dir = file.getParent();
-      if (fs.exists(dir) || fs.mkdirs(dir)) {
+      if (fs.mkdirs(dir) || fs.exists(dir)) {
         return file;
       }
     }
+    LOG.warning(""Could not make "" + path + 
+                "" in local directories from "" + dirsProp);
+    for(int i=0; i < dirs.length; i++) {
+      int index = (hashCode+i & Integer.MAX_VALUE) % dirs.length;
+      LOG.warning(dirsProp + ""["" + index + ""]="" + dirs[index]);
+    }
     throw new IOException(""No valid local directories in property: ""+dirsProp);
   }
 
"
hadoop,b8434e7a24716c2267df9b3dc8be791d98210904,"HADOOP-163.  Cause datanodes that are unable to either read or write data to exit.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@409773 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-26 22:42:19,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index 79d7f92..b6fa070 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -18,6 +18,7 @@
 import org.apache.hadoop.ipc.*;
 import org.apache.hadoop.conf.*;
 import org.apache.hadoop.util.*;
+import org.apache.hadoop.util.DiskChecker.DiskErrorException;
 
 import java.io.*;
 import java.net.*;
@@ -153,6 +154,16 @@
         }
     }
 
+    void handleDiskError( String errMsgr ) {
+        LOG.warning( ""Shuting down DataNode because ""+errMsgr );
+        try {
+            namenode.errorReport(
+                    localName, DatanodeProtocol.DISK_ERROR, errMsgr);
+        } catch( IOException ignored) {              
+        }
+        shutdown();
+    }
+    
     /**
      * Main loop for the DataNode.  Runs until shutdown,
      * forever calling remote NameNode functions.
@@ -164,96 +175,110 @@
         //
         // Now loop for a long time....
         //
-        while (shouldRun) {
-            long now = System.currentTimeMillis();
 
-            //
-            // Every so often, send heartbeat or block-report
-            //
-            if (now - lastHeartbeat > HEARTBEAT_INTERVAL) {
+        try {
+            while (shouldRun) {
+                long now = System.currentTimeMillis();
+    
                 //
-                // All heartbeat messages include following info:
-                // -- Datanode name
-                // -- data transfer port
-                // -- Total capacity
-                // -- Bytes remaining
+                // Every so often, send heartbeat or block-report
                 //
-                BlockCommand cmd = namenode.sendHeartbeat(localName, 
-                        data.getCapacity(), data.getRemaining(), xmitsInProgress);
-                //LOG.info(""Just sent heartbeat, with name "" + localName);
-                lastHeartbeat = now;
-
-                if (cmd != null && cmd.transferBlocks()) {
+                if (now - lastHeartbeat > HEARTBEAT_INTERVAL) {
                     //
-                    // Send a copy of a block to another datanode
+                    // All heartbeat messages include following info:
+                    // -- Datanode name
+                    // -- data transfer port
+                    // -- Total capacity
+                    // -- Bytes remaining
                     //
-                    Block blocks[] = cmd.getBlocks();
-                    DatanodeInfo xferTargets[][] = cmd.getTargets();
+                    BlockCommand cmd = namenode.sendHeartbeat(localName, 
+                            data.getCapacity(), data.getRemaining(), xmitsInProgress);
+                    //LOG.info(""Just sent heartbeat, with name "" + localName);
+                    lastHeartbeat = now;
+    
+                    if( cmd != null ) {
+                        data.checkDataDir();
+                        if (cmd.transferBlocks()) {
+                            //
+                            // Send a copy of a block to another datanode
+                            //
+                            Block blocks[] = cmd.getBlocks();
+                            DatanodeInfo xferTargets[][] = cmd.getTargets();
                         
-                    for (int i = 0; i < blocks.length; i++) {
-                        if (!data.isValidBlock(blocks[i])) {
-                            String errStr = ""Can't send invalid block "" + blocks[i];
-                            LOG.info(errStr);
-                            namenode.errorReport(localName, errStr);
-                            break;
-                        } else {
-                            if (xferTargets[i].length > 0) {
-                                LOG.info(""Starting thread to transfer block "" + blocks[i] + "" to "" + xferTargets[i]);
-                                new Daemon(new DataTransfer(xferTargets[i], blocks[i])).start();
+                            for (int i = 0; i < blocks.length; i++) {
+                                if (!data.isValidBlock(blocks[i])) {
+                                    String errStr = ""Can't send invalid block "" + blocks[i];
+                                    LOG.info(errStr);
+                                    namenode.errorReport(
+                                        localName, 
+                                        DatanodeProtocol.INVALID_BLOCK, 
+                                        errStr);
+                                    break;
+                                } else {
+                                    if (xferTargets[i].length > 0) {
+                                        LOG.info(""Starting thread to transfer block "" + blocks[i] + "" to "" + xferTargets[i]);
+                                        new Daemon(new DataTransfer(xferTargets[i], blocks[i])).start();
+                                    }
+                                }
                             }
+                         } else if (cmd.invalidateBlocks()) {
+                            //
+                            // Some local block(s) are obsolete and can be 
+                            // safely garbage-collected.
+                            //
+                            data.invalidate(cmd.getBlocks());
+                         }
+                    }
+                }
+                
+                // send block report
+                if (now - lastBlockReport > blockReportInterval) {
+                    // before send block report, check if data directory is healthy
+                    data.checkDataDir();
+                    
+                     //
+                     // Send latest blockinfo report if timer has expired.
+                     // Get back a list of local block(s) that are obsolete
+                     // and can be safely GC'ed.
+                     //
+                     Block toDelete[] = namenode.blockReport(localName, data.getBlockReport());
+                     data.invalidate(toDelete);
+                     lastBlockReport = now;
+                     continue;
+                }
+                
+                // check if there are newly received blocks
+                Block [] blockArray=null;
+                synchronized( receivedBlockList ) {
+                    if (receivedBlockList.size() > 0) {
+                        //
+                        // Send newly-received blockids to namenode
+                        //
+                        blockArray = (Block[]) receivedBlockList.toArray(new Block[receivedBlockList.size()]);
+                        receivedBlockList.removeAllElements();
+                    }
+                }
+                if( blockArray != null ) {
+                    namenode.blockReceived(localName, blockArray);
+                }
+                
+                //
+                // There is no work to do;  sleep until hearbeat timer elapses, 
+                // or work arrives, and then iterate again.
+                //
+                long waitTime = HEARTBEAT_INTERVAL - (System.currentTimeMillis() - lastHeartbeat);
+                synchronized( receivedBlockList ) {
+                    if (waitTime > 0 && receivedBlockList.size() == 0) {
+                        try {
+                            receivedBlockList.wait(waitTime);
+                        } catch (InterruptedException ie) {
                         }
                     }
-                } else if (cmd != null && cmd.invalidateBlocks()) {
-                    //
-                    // Some local block(s) are obsolete and can be 
-                    // safely garbage-collected.
-                    //
-                    data.invalidate(cmd.getBlocks());
-                }
-            }
-            
-            // send block report
-            if (now - lastBlockReport > blockReportInterval) {
-                //
-                // Send latest blockinfo report if timer has expired.
-                // Get back a list of local block(s) that are obsolete
-                // and can be safely GC'ed.
-                //
-                Block toDelete[] = namenode.blockReport(localName, data.getBlockReport());
-                data.invalidate(toDelete);
-                lastBlockReport = now;
-                continue;
-            }
-            
-            // check if there are newly received blocks
-            Block [] blockArray=null;
-            synchronized( receivedBlockList ) {
-                if (receivedBlockList.size() > 0) {
-                    //
-                    // Send newly-received blockids to namenode
-                    //
-                    blockArray = (Block[]) receivedBlockList.toArray(new Block[receivedBlockList.size()]);
-                    receivedBlockList.removeAllElements();
-                }
-            }
-            if( blockArray != null ) {
-                namenode.blockReceived(localName, blockArray);
-            }
-            
-            //
-            // There is no work to do;  sleep until hearbeat timer elapses, 
-            // or work arrives, and then iterate again.
-            //
-            long waitTime = HEARTBEAT_INTERVAL - (System.currentTimeMillis() - lastHeartbeat);
-            synchronized( receivedBlockList ) {
-                if (waitTime > 0 && receivedBlockList.size() == 0) {
-                    try {
-                        receivedBlockList.wait(waitTime);
-                    } catch (InterruptedException ie) {
-                    }
-                }
-            } // synchronized
-        } // while (shouldRun)
+                } // synchronized
+            } // while (shouldRun)
+        } catch(DiskErrorException e) {
+            handleDiskError(e.getMessage());
+        }
     } // offerService
 
     /**
@@ -276,9 +301,14 @@
                 while (shouldListen) {
                     Socket s = ss.accept();
                     //s.setSoTimeout(READ_TIMEOUT);
+                    data.checkDataDir();
                     new Daemon(new DataXceiver(s)).start();
                 }
                 ss.close();
+            } catch (DiskErrorException de ) {
+                String errMsgr = de.getMessage();
+                LOG.warning(""Exiting DataXceiveServer due to ""+ errMsgr );
+                handleDiskError(errMsgr);
             } catch (IOException ie) {
                 LOG.info(""Exiting DataXceiveServer due to "" + ie.toString());
             }
@@ -791,6 +821,7 @@
     }
   }
 
+
   /**
    * Make an instance of DataNode after ensuring that given data directory
    * (and parent directories, if necessary) can be created.
@@ -803,14 +834,14 @@
   static DataNode makeInstanceForDir(String dataDir, Configuration conf) throws IOException {
     DataNode dn = null;
     File data = new File(dataDir);
-    data.mkdirs();
-    if (!data.isDirectory()) {
-      LOG.warning(""Can't start DataNode in non-directory: ""+dataDir);
-      return null;
-    } else {
-      dn = new DataNode(conf, dataDir);
+    try {
+        DiskChecker.checkDir( data );
+        dn = new DataNode(conf, dataDir);
+        return dn;
+    } catch( DiskErrorException e ) {
+        LOG.warning(""Can't start DataNode because "" + e.getMessage() );
+        return null;
     }
-    return dn;
   }
 
   public String toString() {
"
hadoop,b8434e7a24716c2267df9b3dc8be791d98210904,"HADOOP-163.  Cause datanodes that are unable to either read or write data to exit.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@409773 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-26 22:42:19,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DatanodeProtocol.java b/src/java/org/apache/hadoop/dfs/DatanodeProtocol.java
index aa2fee0..e883d3c 100644
--- a/src/java/org/apache/hadoop/dfs/DatanodeProtocol.java
+++ b/src/java/org/apache/hadoop/dfs/DatanodeProtocol.java
@@ -28,6 +28,9 @@
  * @author Michael Cafarella
  **********************************************************************/
 interface DatanodeProtocol {
+    // error code
+    final static int DISK_ERROR = 1;
+    final static int INVALID_BLOCK = 2;
     /**
      * sendHeartbeat() tells the NameNode that the DataNode is still
      * alive and well.  Includes some status info, too. 
@@ -60,5 +63,5 @@
      * errorReport() tells the NameNode about something that has gone
      * awry.  Useful for debugging.
      */
-    public void errorReport(String sender, String msg) throws IOException;
+    public void errorReport(String sender, int errorCode, String msg) throws IOException;
 }
"
hadoop,b8434e7a24716c2267df9b3dc8be791d98210904,"HADOOP-163.  Cause datanodes that are unable to either read or write data to exit.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@409773 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-26 22:42:19,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSDataset.java b/src/java/org/apache/hadoop/dfs/FSDataset.java
index 20ddb56..62cfc57 100644
--- a/src/java/org/apache/hadoop/dfs/FSDataset.java
+++ b/src/java/org/apache/hadoop/dfs/FSDataset.java
@@ -19,6 +19,8 @@
 import java.util.*;
 
 import org.apache.hadoop.fs.*;
+import org.apache.hadoop.util.DiskChecker;
+import org.apache.hadoop.util.DiskChecker.DiskErrorException;
 import org.apache.hadoop.conf.*;
 
 /**************************************************
@@ -167,7 +169,22 @@
             blkid = blkid >> ((15 - halfByteIndex) * 4);
             return (int) ((0x000000000000000F) & blkid);
         }
-
+        
+        /**
+         * check if a data diretory is healthy
+         * @throws DiskErrorException
+         * @author hairong
+         */
+        public void checkDirTree() throws DiskErrorException {
+            DiskChecker.checkDir(dir);
+            
+            if (children != null) {
+                for (int i = 0; i < children.length; i++) {
+                    children[i].checkDirTree();
+                }
+            }
+        }
+        
         public String toString() {
           return ""FSDir{"" +
               ""dir="" + dir +
@@ -422,6 +439,17 @@
         return new File(tmp, b.getBlockName());
     }
 
+    /**
+     * check if a data diretory is healthy
+     * @throws DiskErrorException
+     * @author hairong
+     */
+    void checkDataDir() throws DiskErrorException {
+        dirTree.checkDirTree();
+        DiskChecker.checkDir( tmp );
+    }
+    
+
     public String toString() {
       return ""FSDataset{"" +
         ""dirpath='"" + diskUsage.getDirPath() + ""'"" +
"
hadoop,b8434e7a24716c2267df9b3dc8be791d98210904,"HADOOP-163.  Cause datanodes that are unable to either read or write data to exit.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@409773 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-26 22:42:19,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index 449bdf3..9c47796 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -1026,7 +1026,46 @@
             }
         }
     }
+    
+    /**
+     * remove a datanode info
+     * @param name: datanode name
+     * @author hairong
+     */
 
+    synchronized void rmDataNodeByName( UTF8 name ) {
+        DatanodeInfo nodeInfo = (DatanodeInfo) datanodeMap.get(name);
+        if (nodeInfo != null) {
+            rmDataNode( nodeInfo );
+        } else {
+            NameNode.stateChangeLog.warning(""BLOCK* NameSystem.rmDataNodeByName: ""
+                    + nodeInfo.getName() + "" does not exist"");
+        }
+    }
+    
+    /**
+     * remove a datanode info
+     * @param nodeInfo: datanode info
+     * @author hairong
+     */
+    private synchronized void rmDataNode( DatanodeInfo nodeInfo ) {
+        heartbeats.remove( nodeInfo );
+        synchronized (datanodeMap) {
+            datanodeMap.remove(nodeInfo.getName());
+            NameNode.stateChangeLog.finer(""BLOCK* NameSystem.heartbeatCheck: ""
+                    + nodeInfo.getName() + "" is removed from datanodeMap"");
+        }
+        totalCapacity -= nodeInfo.getCapacity();
+        totalRemaining -= nodeInfo.getRemaining();
+
+        Block deadblocks[] = nodeInfo.getBlocks();
+        if (deadblocks != null) {
+            for (int i = 0; i < deadblocks.length; i++) {
+                removeStoredBlock(deadblocks[i], nodeInfo);
+            }
+        }
+    }
+        
     /**
      * Check if there are any expired heartbeats, and if so,
      * whether any blocks have to be re-replicated.
@@ -1038,27 +1077,9 @@
             while ((heartbeats.size() > 0) &&
                    ((nodeInfo = (DatanodeInfo) heartbeats.first()) != null) &&
                    (nodeInfo.lastUpdate() < System.currentTimeMillis() - EXPIRE_INTERVAL)) {
-                heartbeats.remove(nodeInfo);
                 NameNode.stateChangeLog.info(""BLOCK* NameSystem.heartbeatCheck: ""
                            + ""lost heartbeat from "" + nodeInfo.getName());
-                synchronized (datanodeMap) {
-                    datanodeMap.remove(nodeInfo.getName());
-                    NameNode.stateChangeLog.finer(""BLOCK* NameSystem.heartbeatCheck: ""
-                            + nodeInfo.getName() + "" is removed from datanodeMap"");
-                }
-                totalCapacity -= nodeInfo.getCapacity();
-                totalRemaining -= nodeInfo.getRemaining();
-
-                Block deadblocks[] = nodeInfo.getBlocks();
-                if (deadblocks != null) {
-                    for (int i = 0; i < deadblocks.length; i++) {
-                        removeStoredBlock(deadblocks[i], nodeInfo);
-                    }
-                }
-
-                if (heartbeats.size() > 0) {
-                    nodeInfo = (DatanodeInfo) heartbeats.first();
-                }
+                rmDataNode(nodeInfo);
             }
         }
     }
"
hadoop,b8434e7a24716c2267df9b3dc8be791d98210904,"HADOOP-163.  Cause datanodes that are unable to either read or write data to exit.  Contributed by Hairong.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@409773 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-26 22:42:19,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/NameNode.java b/src/java/org/apache/hadoop/dfs/NameNode.java
index 80b48b7..8aa20b9 100644
--- a/src/java/org/apache/hadoop/dfs/NameNode.java
+++ b/src/java/org/apache/hadoop/dfs/NameNode.java
@@ -445,9 +445,13 @@
 
     /**
      */
-    public void errorReport(String sender, String msg) {
+    public void errorReport(String sender, int errorCode, String msg) {
         // Log error message from datanode
-        //LOG.info(""Report from "" + sender + "": "" + msg);
+        LOG.warning(""Report from "" + sender + "": "" + msg);
+        if( errorCode == DatanodeProtocol.DISK_ERROR ) {
+            namesystem.rmDataNodeByName(new UTF8(sender));            
+        }
+            
     }
 
     /**
"
hadoop,db6e9f2850d31a1301745b4133bb4fb550bab729,"HADOOP-325.  Improve the FileNotFound exceptions thrown by LocalFileSystem to include the name of the file.  Contributed by Benjamin Reed.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@409451 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-25 20:02:40,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/LocalFileSystem.java b/src/java/org/apache/hadoop/fs/LocalFileSystem.java
index 014f924..b9d8f87 100644
--- a/src/java/org/apache/hadoop/fs/LocalFileSystem.java
+++ b/src/java/org/apache/hadoop/fs/LocalFileSystem.java
@@ -120,7 +120,7 @@
     
     public FSInputStream openRaw(Path f) throws IOException {
         if (! exists(f)) {
-            throw new FileNotFoundException(toString());
+            throw new FileNotFoundException(f.toString());
         }
         return new LocalFSFileInputStream(f);
     }
"
hadoop,d04427d63fb92d7a1f808e92fb161f5045f422dc,"HADOOP-251.  Make tasks processes tolerant of failed progress reports to their parent process.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@409265 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-24 23:17:22,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/Task.java b/src/java/org/apache/hadoop/mapred/Task.java
index 1ef7a3f..f1318a3 100644
--- a/src/java/org/apache/hadoop/mapred/Task.java
+++ b/src/java/org/apache/hadoop/mapred/Task.java
@@ -21,9 +21,13 @@
 import org.apache.hadoop.util.*;
 
 import java.io.*;
+import java.util.logging.Logger;
 
 /** Base class for tasks. */
 abstract class Task implements Writable, Configurable {
+  private static final Logger LOG =
+    LogFormatter.getLogger(""org.apache.hadoop.mapred.TaskRunner"");
+
   ////////////////////////////////////////////
   // Fields
   ////////////////////////////////////////////
@@ -101,15 +105,18 @@
     reportProgress(umbilical);
   }
 
-  public void reportProgress(TaskUmbilicalProtocol umbilical)
-    throws IOException {
+  public void reportProgress(TaskUmbilicalProtocol umbilical) {
     long now = System.currentTimeMillis();
     if (now > nextProgressTime)  {
       synchronized (this) {
         nextProgressTime = now + PROGRESS_INTERVAL;
         float progress = taskProgress.get();
         String status = taskProgress.toString();
-        umbilical.progress(getTaskId(), progress, status);
+        try {
+          umbilical.progress(getTaskId(), progress, status);
+        } catch (IOException ie) {
+          LOG.warning(StringUtils.stringifyException(ie));
+        }
       }
     }
   }
"
hadoop,ace6074e06a4fbdcefd27a8fe202435dc323dca2,"HADOOP-241.  Fix problem raised by CopyFiles on Windows.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@409217 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-24 17:55:08,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index afedaa9..449bdf3 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -183,6 +183,12 @@
           try {
             lmthread.join(3000);
           } catch (InterruptedException ie) {
+          } finally {
+              try {
+                dir.close();
+              } catch (IOException ex) {
+                  // do nothing
+              }
           }
         }
     }
"
hadoop,c231872403bc3bb61b7af3133d572d1e98b4c85d,"HADOOP-247.  Fix sort progress to better handle exceptions.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@409024 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-23 23:25:10,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/ReduceTask.java b/src/java/org/apache/hadoop/mapred/ReduceTask.java
index a7ebaca..8d1c325 100644
--- a/src/java/org/apache/hadoop/mapred/ReduceTask.java
+++ b/src/java/org/apache/hadoop/mapred/ReduceTask.java
@@ -229,9 +229,12 @@
               reportProgress(umbilical);
               Thread.sleep(PROGRESS_INTERVAL);
             } catch (InterruptedException e) {
-              continue;
+                return;
             } catch (Throwable e) {
-              return;
+                System.out.println(""Thread Exception in "" +
+                                   ""reporting sort progress\n"" +
+                                   StringUtils.stringifyException(e));
+                continue;
             }
           }
         }
"
hadoop,432fe8789e73c8985728585edf8309315e74e84e,"HADOOP-205.  Incorporate pending tasksinto tasktracker load calculations.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@409014 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-23 22:23:35,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobInProgress.java b/src/java/org/apache/hadoop/mapred/JobInProgress.java
index 70ae932..77069c2 100644
--- a/src/java/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/JobInProgress.java
@@ -42,7 +42,10 @@
     TaskInProgress reduces[] = new TaskInProgress[0];
     int numMapTasks = 0;
     int numReduceTasks = 0;
-
+    int runningMapTasks = 0;
+    int runningReduceTasks = 0;
+    int finishedMapTasks = 0;
+    int finishedReduceTasks = 0;
     JobTracker jobtracker = null;
     HashMap hostToMaps = new HashMap();
 
@@ -201,27 +204,21 @@
         return numMapTasks;
     }
     public int finishedMaps() {
-        int finishedCount = 0;
-        for (int i = 0; i < maps.length; i++) {
-            if (maps[i].isComplete()) {
-                finishedCount++;
-            }
-        }
-        return finishedCount;
+        return finishedMapTasks;
     }
     public int desiredReduces() {
         return numReduceTasks;
     }
-    public int finishedReduces() {
-        int finishedCount = 0;
-        for (int i = 0; i < reduces.length; i++) {
-            if (reduces[i].isComplete()) {
-                finishedCount++;
-            }
-        }
-        return finishedCount;
+    public synchronized int runningMaps() {
+        return runningMapTasks;
     }
-
+    public synchronized int runningReduces() {
+        return runningReduceTasks;
+    }
+    public int finishedReduces() {
+        return finishedReduceTasks;
+    }
+ 
     /**
      * Get the list of map tasks
      * @return the raw array of maps for this job
@@ -377,12 +374,24 @@
         //
         if (cacheTarget >= 0) {
             t = maps[cacheTarget].getTaskToRun(taskTracker, tts, avgProgress);
+            runningMapTasks += 1;
         } else if (stdTarget >= 0) {
             t = maps[stdTarget].getTaskToRun(taskTracker, tts, avgProgress);
-        } else if (specTarget >= 0) {
+            runningMapTasks += 1;
+	} else if (specTarget >= 0) {
+	    //should always be true, but being paranoid
+            boolean isRunning = maps[specTarget].isRunning(); 
             t = maps[specTarget].getTaskToRun(taskTracker, tts, avgProgress);
+            if (!isRunning){
+                runningMapTasks += 1;
+            }
         } else if (failedTarget >= 0) {
+           //should always be false, but being paranoid again
+            boolean isRunning = maps[failedTarget].isRunning(); 
             t = maps[failedTarget].getTaskToRun(taskTracker, tts, avgProgress);
+            if (!isRunning) {
+                runningMapTasks += 1;
+	    }
         }
         return t;
     }
@@ -424,11 +433,21 @@
         
         if (stdTarget >= 0) {
             t = reduces[stdTarget].getTaskToRun(taskTracker, tts, avgProgress);
-        } else if (specTarget >= 0) {
+            runningReduceTasks += 1;
+	} else if (specTarget >= 0) {
+            //should be false
+            boolean isRunning = reduces[specTarget].isRunning();
             t = reduces[specTarget].getTaskToRun(taskTracker, tts, avgProgress);
+            if (!isRunning){
+               runningReduceTasks += 1;
+            }
         } else if (failedTarget >= 0) {
+            boolean isRunning = reduces[failedTarget].isRunning();
             t = reduces[failedTarget].getTaskToRun(taskTracker, tts, 
                                                    avgProgress);
+            if (!isRunning){
+                runningReduceTasks += 1;
+            }
         }
         return t;
     }
@@ -439,10 +458,25 @@
     public synchronized void completedTask(TaskInProgress tip, 
                                            TaskStatus status) {
         String taskid = status.getTaskId();
+        boolean oldDone = tip.isComplete();
         updateTaskStatus(tip, status);
         LOG.info(""Taskid '"" + taskid + ""' has finished successfully."");
         tip.completed(taskid);
-
+        boolean newDone = tip.isComplete();
+        // updating the running/finished map/reduce counts
+        if (oldDone != newDone) {
+            if (newDone) {  
+                if (tip.isMapTask()){
+                    runningMapTasks -= 1;
+                    finishedMapTasks += 1;
+                }
+                else{
+                    runningReduceTasks -= 1;
+                    finishedReduceTasks += 1;
+                }    
+            }
+        }
+        
         //
         // Figure out whether the Job is done
         //
@@ -480,7 +514,8 @@
         if (status.getRunState() != JobStatus.FAILED) {
             this.status = new JobStatus(status.getJobId(), 1.0f, 1.0f, JobStatus.FAILED);
             this.finishTime = System.currentTimeMillis();
-
+            this.runningMapTasks = 0;
+            this.runningReduceTasks = 0;
             //
             // kill all TIPs.
             //
@@ -509,9 +544,31 @@
      */
     public synchronized void failedTask(TaskInProgress tip, String taskid, 
                                         TaskStatus status, String trackerName) {
+        boolean oldStatus = tip.isRunning();
+        boolean oldRun = tip.isComplete();
         tip.failedSubTask(taskid, trackerName);
         updateTaskStatus(tip, status);
-        
+        boolean newStatus = tip.isRunning();
+        boolean newRun = tip.isComplete();
+        //update running  count on task failure.
+        if (oldStatus != newStatus) {
+           if (!newStatus) {
+              if (tip.isMapTask()){
+                  runningMapTasks -= 1;
+              }
+              else{
+                  runningReduceTasks -= 1;
+              }
+           }
+        }
+        // the case when the map was complete but the task tracker went down.
+        if (oldRun != newRun) {
+            if (oldRun){
+                if (tip.isMapTask()){
+                    finishedMapTasks -= 1;
+                }
+            }
+        }
         // After this, try to assign tasks with the one after this, so that
         // the failed task goes to the end of the list.
         if (tip.isMapTask()) {
"
hadoop,432fe8789e73c8985728585edf8309315e74e84e,"HADOOP-205.  Incorporate pending tasksinto tasktracker load calculations.  Contributed by Mahadev.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@409014 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-23 22:23:35,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index 5411d49..430ac28 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -645,15 +645,34 @@
         //
         int avgMaps = 0;
         int avgReduces = 0;
+        int remainingReduceLoad = 0;
+        int remainingMapLoad = 0;
         int numTaskTrackers;
         TaskTrackerStatus tts;
+        int avgMapLoad = 0;
+        int avgReduceLoad = 0;
+	
         synchronized (taskTrackers) {
           numTaskTrackers = taskTrackers.size();
           tts = (TaskTrackerStatus) taskTrackers.get(taskTracker);
         }
+        synchronized(jobsByArrival){
+            for (Iterator it = jobsByArrival.iterator(); it.hasNext(); ) {
+                    JobInProgress job = (JobInProgress) it.next();
+                    if (job.getStatus().getRunState() == JobStatus.RUNNING) {
+                         int totalMapTasks = job.desiredMaps();
+                         int totalReduceTasks = job.desiredReduces();
+                         remainingMapLoad += (totalMapTasks - job.finishedMaps());
+                         remainingReduceLoad += (totalReduceTasks - job.finishedReduces());
+                    }
+            }   
+        }
+        
         if (numTaskTrackers > 0) {
           avgMaps = totalMaps / numTaskTrackers;
           avgReduces = totalReduces / numTaskTrackers;
+          avgMapLoad = remainingMapLoad / numTaskTrackers;
+          avgReduceLoad = remainingReduceLoad / numTaskTrackers;
         }
         int totalCapacity = numTaskTrackers * maxCurrentTasks;
         //
@@ -676,15 +695,18 @@
 
         //
         // We hand a task to the current taskTracker if the given machine 
-        // has a workload that's equal to or less than the averageMaps 
+        // has a workload that's equal to or less than the pendingMaps average.
+        // This way the maps are launched if the TaskTracker has running tasks 
+        // less than the pending average 
         // +/- TASK_ALLOC_EPSILON.  (That epsilon is in place in case
         // there is an odd machine that is failing for some reason but 
         // has not yet been removed from the pool, making capacity seem
         // larger than it really is.)
         //
+       
         synchronized (jobsByArrival) {
             if ((numMaps < maxCurrentTasks) &&
-                (numMaps <= (avgMaps + TASK_ALLOC_EPSILON))) {
+                (numMaps <= avgMapLoad + 1 + TASK_ALLOC_EPSILON)) {
 
                 int totalNeededMaps = 0;
                 for (Iterator it = jobsByArrival.iterator(); it.hasNext(); ) {
@@ -719,7 +741,7 @@
             // Same thing, but for reduce tasks
             //
             if ((numReduces < maxCurrentTasks) &&
-                (numReduces <= (avgReduces + TASK_ALLOC_EPSILON))) {
+                (numReduces <= avgReduceLoad + 1 + TASK_ALLOC_EPSILON)) {
 
                 int totalNeededReduces = 0;
                 for (Iterator it = jobsByArrival.iterator(); it.hasNext(); ) {
"
hadoop,2e6d9e8f2e5e6d800fd2b9457e657053063e269d,"HADOOP-238.  Fix an oops.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@408788 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-22 23:33:59,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index ca682d8..5411d49 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -799,7 +799,7 @@
              result.add(new MapOutputLocation(status.getTaskId(), 
                                               mapTasksNeeded[i],
                                               tracker.getHost(), 
-                                              tracker.getHttpPort()));
+                                              tracker.getPort()));
           }
         }
         return (MapOutputLocation[]) 
"
hadoop,e5b2d15294a5bd0613c8d70885088b70d96cdf66,"HADOOP-161.  Add hashCode() method to Block.  Contributed by Milind.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@407099 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-16 23:00:10,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/Block.java b/src/java/org/apache/hadoop/dfs/Block.java
index d9eed12..495811d 100644
--- a/src/java/org/apache/hadoop/dfs/Block.java
+++ b/src/java/org/apache/hadoop/dfs/Block.java
@@ -132,4 +132,8 @@
         Block b = (Block) o;
         return (this.compareTo(b) == 0);
     }
+    
+    public int hashCode() {
+        return 37 * 17 + (int) (getBlockId()^(getBlockId()>>>32));
+    }
 }
"
hadoop,74add90cac0ad8c9e2da33239a366f06370d7f1e,"HADOOP-219.  Fix a NPE when handling a checksum exception under SequenceFile.Sorter.sort().  With help from Stack.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@406732 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-15 20:13:11,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/SequenceFile.java b/src/java/org/apache/hadoop/io/SequenceFile.java
index d6d7a8b..467979e 100644
--- a/src/java/org/apache/hadoop/io/SequenceFile.java
+++ b/src/java/org/apache/hadoop/io/SequenceFile.java
@@ -235,23 +235,25 @@
     /** Open the named file. */
     public Reader(FileSystem fs, Path file, Configuration conf)
       throws IOException {
-      this(fs, file, conf.getInt(""io.file.buffer.size"", 4096));
-      this.conf = conf;
+      this(fs, file, conf.getInt(""io.file.buffer.size"", 4096), conf);
     }
 
-    private Reader(FileSystem fs, Path name, int bufferSize) throws IOException {
+    private Reader(FileSystem fs, Path name, int bufferSize,
+                   Configuration conf) throws IOException {
       this.fs = fs;
       this.file = name;
       this.in = fs.open(file, bufferSize);
       this.end = fs.getLength(file);
+      this.conf = conf;
       init();
     }
     
-    private Reader(FileSystem fs, Path file, int bufferSize, long start, long length)
-      throws IOException {
+    private Reader(FileSystem fs, Path file, int bufferSize, long start,
+                   long length, Configuration conf) throws IOException {
       this.fs = fs;
       this.file = file;
       this.in = fs.open(file, bufferSize);
+      this.conf = conf;
       seek(start);
       init();
 
@@ -742,7 +744,7 @@
             totalCount+= count;
 
             Reader reader = new Reader(fs, inName, memory/(factor+1),
-                                       in.getPos(), length);
+                                       in.getPos(), length, conf);
             reader.sync = null;                   // disable sync on temp files
 
             MergeStream ms = new MergeStream(reader); // add segment to queue
@@ -801,7 +803,7 @@
         for (int i = 0; i < inFiles.length; i++) {
           Path inFile = inFiles[i];
           MergeStream ms =
-            new MergeStream(new Reader(fs, inFile, memory/(factor+1)));
+            new MergeStream(new Reader(fs, inFile, memory/(factor+1), conf));
           if (ms.next())
             queue.put(ms);
         }
"
hadoop,bea19da3b043a71e0223bce2588782edb7daab88,"HADOOP-200.  Avoid transmitting entire list of map task names to reduce tasks.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@406718 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-15 19:13:33,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java b/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java
index c21f72c..5dabd23 100644
--- a/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java
+++ b/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java
@@ -48,12 +48,14 @@
 
   /** Called by a reduce task to find which map tasks are completed.
    *
-   * @param taskId the reduce task id
-   * @param mapTasksNeeded an array of UTF8 naming map task ids whose output is needed.
+   * @param jobId the job id
+   * @param mapTasksNeeded an array of the mapIds that we need
+   * @param partition the reduce's id
    * @return an array of MapOutputLocation
    */
-  MapOutputLocation[] locateMapOutputs(String taskId, 
-                                       String[][] mapTasksNeeded
+  MapOutputLocation[] locateMapOutputs(String jobId, 
+                                       int[] mapTasksNeeded,
+                                       int partition
                                        ) throws IOException;
 
   /**
"
hadoop,bea19da3b043a71e0223bce2588782edb7daab88,"HADOOP-200.  Avoid transmitting entire list of map task names to reduce tasks.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@406718 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-15 19:13:33,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobInProgress.java b/src/java/org/apache/hadoop/mapred/JobInProgress.java
index 9a45855..70ae932 100644
--- a/src/java/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/JobInProgress.java
@@ -151,7 +151,8 @@
         //
         this.reduces = new TaskInProgress[numReduceTasks];
         for (int i = 0; i < numReduceTasks; i++) {
-            reduces[i] = new TaskInProgress(uniqueString, jobFile, maps, i, 
+            reduces[i] = new TaskInProgress(uniqueString, jobFile, 
+                                            numMapTasks, i, 
                                             jobtracker, conf, this);
         }
 
@@ -582,16 +583,34 @@
       * Return the TaskInProgress that matches the tipid.
       */
     public TaskInProgress getTaskInProgress(String tipid){
-        for (int i = 0; i < maps.length; i++) {
-	    if (tipid.equals(maps[i].getTIPId())){
-                return maps[i];
-	    }               
-	}
-	for (int i = 0; i < reduces.length; i++) {
-	    if (tipid.equals(reduces[i].getTIPId())){
-		return reduces[i];
-            }
-	}
-	return null;
+      for (int i = 0; i < maps.length; i++) {
+        if (tipid.equals(maps[i].getTIPId())){
+          return maps[i];
+        }               
+      }
+      for (int i = 0; i < reduces.length; i++) {
+        if (tipid.equals(reduces[i].getTIPId())){
+          return reduces[i];
+        }
+      }
+      return null;
+    }
+    
+    /**
+     * Find the details of someplace where a map has finished
+     * @param mapId the id of the map
+     * @return the task status of the completed task
+     */
+    public TaskStatus findFinishedMap(int mapId) {
+       TaskInProgress tip = maps[mapId];
+       if (tip.isComplete()) {
+         TaskStatus[] statuses = tip.getTaskStatuses();
+         for(int i=0; i < statuses.length; i++) {
+           if (statuses[i].getRunState() == TaskStatus.SUCCEEDED) {
+             return statuses[i];
+           }
+         }
+       }
+       return null;
     }
 }
"
hadoop,bea19da3b043a71e0223bce2588782edb7daab88,"HADOOP-200.  Avoid transmitting entire list of map task names to reduce tasks.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@406718 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-15 19:13:33,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index 599e96e..ab43d5d 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -783,26 +783,27 @@
      * yet closed, tasks.  This exists so the reduce task thread can locate
      * map task outputs.
      */
-    public synchronized MapOutputLocation[] locateMapOutputs(String taskId, String[][] mapTasksNeeded) {
-        ArrayList v = new ArrayList();
+    public synchronized MapOutputLocation[] 
+             locateMapOutputs(String jobId, int[] mapTasksNeeded, int reduce) {
+        ArrayList result = new ArrayList(mapTasksNeeded.length);
+        JobInProgress job = getJob(jobId);
         for (int i = 0; i < mapTasksNeeded.length; i++) {
-            for (int j = 0; j < mapTasksNeeded[i].length; j++) {
-                TaskInProgress tip = (TaskInProgress) taskidToTIPMap.get(mapTasksNeeded[i][j]);
-                if (tip != null && tip.isComplete(mapTasksNeeded[i][j])) {
-                    String trackerId = (String) taskidToTrackerMap.get(mapTasksNeeded[i][j]);
-                    TaskTrackerStatus tracker;
-                    synchronized (taskTrackers) {
-                      tracker = (TaskTrackerStatus) taskTrackers.get(trackerId);
-                    }
-                    v.add(new MapOutputLocation(mapTasksNeeded[i][j], tracker.getHost(), tracker.getPort()));
-                    break;
-                }
-            }
+          TaskStatus status = job.findFinishedMap(mapTasksNeeded[i]);
+          if (status != null) {
+             String trackerId = 
+               (String) taskidToTrackerMap.get(status.getTaskId());
+             TaskTrackerStatus tracker;
+             synchronized (taskTrackers) {
+               tracker = (TaskTrackerStatus) taskTrackers.get(trackerId);
+             }
+             result.add(new MapOutputLocation(status.getTaskId(), 
+                                              mapTasksNeeded[i],
+                                              tracker.getHost(), 
+                                              tracker.getPort()));
+          }
         }
-        // randomly shuffle results to load-balance map output requests
-        Collections.shuffle(v);
-
-        return (MapOutputLocation[]) v.toArray(new MapOutputLocation[v.size()]);
+        return (MapOutputLocation[]) 
+               result.toArray(new MapOutputLocation[result.size()]);
     }
 
     /**
"
hadoop,bea19da3b043a71e0223bce2588782edb7daab88,"HADOOP-200.  Avoid transmitting entire list of map task names to reduce tasks.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@406718 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-15 19:13:33,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/LocalJobRunner.java b/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
index 9cd63e9..cd604ff 100644
--- a/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
+++ b/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
@@ -96,7 +96,7 @@
         for (int i = 0; i < mapIds.size(); i++) {
           String mapId = (String)mapIds.get(i);
           Path mapOut = this.mapoutputFile.getOutputFile(mapId, 0);
-          Path reduceIn = this.mapoutputFile.getInputFile(mapId, reduceId);
+          Path reduceIn = this.mapoutputFile.getInputFile(i, reduceId);
           localFs.mkdirs(reduceIn.getParent());
           if (!localFs.rename(mapOut, reduceIn))
             throw new IOException(""Couldn't rename "" + mapOut);
@@ -104,11 +104,8 @@
         }
 
         // run a single reduce task
-        String mapDependencies[][] = new String[mapIds.size()][1];
-        for (int i = 0; i < mapIds.size(); i++) {
-            mapDependencies[i][0] = (String) mapIds.get(i);
-        }
-        ReduceTask reduce = new ReduceTask(file, reduceId, mapDependencies,0);
+        ReduceTask reduce = new ReduceTask(profile.getJobId(), file, 
+                                           reduceId, mapIds.size(),0);
         reduce.setConf(job);
         reduce_tasks += 1;
         reduce.run(job, this);
"
hadoop,bea19da3b043a71e0223bce2588782edb7daab88,"HADOOP-200.  Avoid transmitting entire list of map task names to reduce tasks.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@406718 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-15 19:13:33,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/MapOutputFile.java b/src/java/org/apache/hadoop/mapred/MapOutputFile.java
index 22dab38..d85c9c8 100644
--- a/src/java/org/apache/hadoop/mapred/MapOutputFile.java
+++ b/src/java/org/apache/hadoop/mapred/MapOutputFile.java
@@ -39,6 +39,7 @@
 
   private String mapTaskId;
   private String reduceTaskId;
+  private int mapId;
   private int partition;
   
   /** Permits reporting of file copy progress. */
@@ -66,18 +67,10 @@
    * @param mapTaskId a map task id
    * @param reduceTaskId a reduce task id
    */
-  public Path getInputFile(String mapTaskId, String reduceTaskId)
+  public Path getInputFile(int mapId, String reduceTaskId)
     throws IOException {
-    return this.jobConf.getLocalPath(reduceTaskId+""/""+mapTaskId+"".out"");
-  }
-  public Path getInputFile(String mapTaskIds[], String reduceTaskId)
-    throws IOException {
-    for (int i = 0; i < mapTaskIds.length; i++) {
-      Path file = jobConf.getLocalPath(reduceTaskId+""/""+mapTaskIds[i]+"".out"");
-      if (getLocalFs().exists(file))
-        return file;
-    }
-    throw new IOException(""Input file not found!"");
+    // TODO *oom* should use a format here
+    return this.jobConf.getLocalPath(reduceTaskId+""/map_""+mapId+"".out"");
   }
 
   /** Removes all of the files related to a task. */
@@ -97,9 +90,11 @@
   public MapOutputFile() { 
   }
   
-  public MapOutputFile(String mapTaskId, String reduceTaskId, int partition) {
+  public MapOutputFile(String mapTaskId, String reduceTaskId, 
+                       int mapId, int partition) {
     this.mapTaskId = mapTaskId;
     this.reduceTaskId = reduceTaskId;
+    this.mapId = mapId;
     this.partition = partition;
   }
 
@@ -110,6 +105,7 @@
   public void write(DataOutput out) throws IOException {
     UTF8.writeString(out, mapTaskId);
     UTF8.writeString(out, reduceTaskId);
+    out.writeInt(mapId);
     out.writeInt(partition);
     
     Path file = getOutputFile(mapTaskId, partition);
@@ -145,12 +141,13 @@
   public void readFields(DataInput in) throws IOException {
     this.mapTaskId = UTF8.readString(in);
     this.reduceTaskId = UTF8.readString(in);
+    this.mapId = in.readInt();
     this.partition = in.readInt();
 
     ProgressReporter reporter = (ProgressReporter)REPORTERS.get();
 
     // read the length-prefixed file content into a local file
-    Path file = getInputFile(mapTaskId, reduceTaskId);
+    Path file = getInputFile(mapId, reduceTaskId);
     long length = in.readLong();
     float progPerByte = 1.0f / length;
     long unread = length;
"
hadoop,bea19da3b043a71e0223bce2588782edb7daab88,"HADOOP-200.  Avoid transmitting entire list of map task names to reduce tasks.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@406718 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-15 19:13:33,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/MapOutputLocation.java b/src/java/org/apache/hadoop/mapred/MapOutputLocation.java
index 4c1157b..136c786 100644
--- a/src/java/org/apache/hadoop/mapred/MapOutputLocation.java
+++ b/src/java/org/apache/hadoop/mapred/MapOutputLocation.java
@@ -34,6 +34,7 @@
     }
 
   private String mapTaskId;
+  private int mapId;
   private String host;
   private int port;
 
@@ -42,14 +43,24 @@
   }
 
   /** Construct a location. */
-  public MapOutputLocation(String mapTaskId, String host, int port) {
+  public MapOutputLocation(String mapTaskId, int mapId, 
+                           String host, int port) {
     this.mapTaskId = mapTaskId;
+    this.mapId = mapId;
     this.host = host;
     this.port = port;
   }
 
   /** The map task id. */
   public String getMapTaskId() { return mapTaskId; }
+  
+  /**
+   * Get the map's id number.
+   * @return The numeric id for this map
+   */
+  public int getMapId() {
+    return mapId;
+  }
 
   /** The host the task completed on. */
   public String getHost() { return host; }
@@ -59,12 +70,14 @@
 
   public void write(DataOutput out) throws IOException {
     UTF8.writeString(out, mapTaskId);
+    out.writeInt(mapId);
     UTF8.writeString(out, host);
     out.writeInt(port);
   }
 
   public void readFields(DataInput in) throws IOException {
     this.mapTaskId = UTF8.readString(in);
+    this.mapId = in.readInt();
     this.host = UTF8.readString(in);
     this.port = in.readInt();
   }
"
hadoop,bea19da3b043a71e0223bce2588782edb7daab88,"HADOOP-200.  Avoid transmitting entire list of map task names to reduce tasks.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@406718 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-15 19:13:33,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/MapOutputProtocol.java b/src/java/org/apache/hadoop/mapred/MapOutputProtocol.java
index 6ef8aef..39195e2 100644
--- a/src/java/org/apache/hadoop/mapred/MapOutputProtocol.java
+++ b/src/java/org/apache/hadoop/mapred/MapOutputProtocol.java
@@ -26,6 +26,6 @@
 
   /** Returns the output from the named map task destined for this partition.*/
   MapOutputFile getFile(String mapTaskId, String reduceTaskId,
-                        IntWritable partition) throws IOException;
+                        int mapId, int partition) throws IOException;
 
 }
"
hadoop,bea19da3b043a71e0223bce2588782edb7daab88,"HADOOP-200.  Avoid transmitting entire list of map task names to reduce tasks.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@406718 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-15 19:13:33,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/ReduceTask.java b/src/java/org/apache/hadoop/mapred/ReduceTask.java
index 6d87b52..e42f781 100644
--- a/src/java/org/apache/hadoop/mapred/ReduceTask.java
+++ b/src/java/org/apache/hadoop/mapred/ReduceTask.java
@@ -36,7 +36,8 @@
        });
   }
 
-  private String[][] mapTaskIds;
+  private UTF8 jobId = new UTF8();
+  private int numMaps;
   private int partition;
   private boolean sortComplete;
 
@@ -51,10 +52,11 @@
 
   public ReduceTask() {}
 
-  public ReduceTask(String jobFile, String taskId,
-                    String[][] mapTaskIds, int partition) {
+  public ReduceTask(String jobId, String jobFile, String taskId,
+                    int numMaps, int partition) {
     super(jobFile, taskId);
-    this.mapTaskIds = mapTaskIds;
+    this.jobId.set(jobId);
+    this.numMaps = numMaps;
     this.partition = partition;
   }
 
@@ -66,34 +68,30 @@
       return false;
   }
 
-  public String[][] getMapTaskIds() { return mapTaskIds; }
+  /**
+   * Get the job name for this task.
+   * @return the job name
+   */
+  public UTF8 getJobId() {
+    return jobId;
+  }
+  
+  public int getNumMaps() { return numMaps; }
   public int getPartition() { return partition; }
 
   public void write(DataOutput out) throws IOException {
     super.write(out);
 
-    out.writeInt(mapTaskIds.length);              // write mapTaskIds
-    for (int i = 0; i < mapTaskIds.length; i++) {
-        out.writeInt(mapTaskIds[i].length);
-        for (int j = 0; j < mapTaskIds[i].length; j++) {
-            UTF8.writeString(out, mapTaskIds[i][j]);
-        }
-    }
-
+    jobId.write(out);
+    out.writeInt(numMaps);                        // write the number of maps
     out.writeInt(partition);                      // write partition
   }
 
   public void readFields(DataInput in) throws IOException {
     super.readFields(in);
 
-    mapTaskIds = new String[in.readInt()][];        // read mapTaskIds
-    for (int i = 0; i < mapTaskIds.length; i++) {
-        mapTaskIds[i] = new String[in.readInt()];
-        for (int j = 0; j < mapTaskIds[i].length; j++) {
-            mapTaskIds[i][j] = UTF8.readString(in);
-        }
-    }
-
+    jobId.readFields(in);
+    numMaps = in.readInt();
     this.partition = in.readInt();                // read partition
   }
 
@@ -189,15 +187,15 @@
       new SequenceFile.Writer(lfs, file, keyClass, valueClass);
     try {
       // append all input files into a single input file
-      for (int i = 0; i < mapTaskIds.length; i++) {
+      for (int i = 0; i < numMaps; i++) {
         appendPhase.addPhase();                 // one per file
       }
       
       DataOutputBuffer buffer = new DataOutputBuffer();
 
-      for (int i = 0; i < mapTaskIds.length; i++) {
+      for (int i = 0; i < numMaps; i++) {
         Path partFile =
-          this.mapOutputFile.getInputFile(mapTaskIds[i], getTaskId());
+          this.mapOutputFile.getInputFile(i, getTaskId());
         float progPerByte = 1.0f / lfs.getLength(partFile);
         Progress phase = appendPhase.phase();
         phase.setStatus(partFile.toString());
"
hadoop,bea19da3b043a71e0223bce2588782edb7daab88,"HADOOP-200.  Avoid transmitting entire list of map task names to reduce tasks.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@406718 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-15 19:13:33,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java b/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
index f0dbeaf..946fb92 100644
--- a/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
+++ b/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
@@ -15,9 +15,7 @@
  */
 package org.apache.hadoop.mapred;
 
-import org.apache.hadoop.io.*;
 import org.apache.hadoop.ipc.*;
-import org.apache.hadoop.conf.*;
 import org.apache.hadoop.util.*;
 
 import java.io.*;
@@ -27,8 +25,6 @@
 
 /** Runs a reduce task. */
 class ReduceTaskRunner extends TaskRunner {
-  private static final Logger LOG =
-    LogFormatter.getLogger(""org.apache.hadoop.mapred.ReduceTaskRunner"");
   private MapOutputFile mapOutputFile;
 
   public ReduceTaskRunner(Task task, TaskTracker tracker, JobConf conf) {
@@ -41,13 +37,13 @@
   public boolean prepare() throws IOException {
     ReduceTask task = ((ReduceTask)getTask());
     this.mapOutputFile.removeAll(task.getTaskId());    // cleanup from failures
-    String[][] mapTaskIds = task.getMapTaskIds();
+    int numMaps = task.getNumMaps();
     final Progress copyPhase = getTask().getProgress().phase();
 
     // we need input from every map task
-    Vector needed = new Vector();
-    for (int i = 0; i < mapTaskIds.length; i++) {
-      needed.add(mapTaskIds[i]);
+    List needed = new ArrayList(numMaps);
+    for (int i = 0; i < numMaps; i++) {
+      needed.add(new Integer(i));
       copyPhase.addPhase();                       // add sub-phase per file
     }
 
@@ -59,15 +55,17 @@
       // query for a just a random subset of needed segments so that we don't
       // overwhelm jobtracker.  ideally perhaps we could send a more compact
       // representation of all needed, i.e., a bit-vector
-      Collections.shuffle(needed);
       int checkSize = Math.min(10, needed.size());
-      String[][] neededStrings = new String[checkSize][];
+      int[] neededIds = new int[checkSize];
+      Collections.shuffle(needed);
+      ListIterator itr = needed.listIterator();
       for (int i = 0; i < checkSize; i++) {
-          neededStrings[i] = (String[]) needed.elementAt(i);
+        neededIds[i] = ((Integer) itr.next()).intValue();
       }
       MapOutputLocation[] locs = null;
       try {
-        locs = jobClient.locateMapOutputs(task.getTaskId(), neededStrings);
+        locs = jobClient.locateMapOutputs(task.getJobId().toString(), 
+                                          neededIds, task.getPartition());
       } catch (IOException ie) {
         LOG.info(""Problem locating map outputs: "" + 
                  StringUtils.stringifyException(ie));
@@ -112,18 +110,15 @@
           LOG.info(task.getTaskId()+"" Copying ""+loc.getMapTaskId()
                    +"" output from ""+loc.getHost()+""."");
           client.getFile(loc.getMapTaskId(), task.getTaskId(),
-                         new IntWritable(task.getPartition()));
+                         loc.getMapId(),
+                         task.getPartition());
 
           // Success: remove from 'needed'
-          boolean foundit = false;
-          for (Iterator it = needed.iterator(); it.hasNext() && !foundit; ) {
-              String idsForSingleMap[] = (String[]) it.next();
-              for (int j = 0; j < idsForSingleMap.length; j++) {
-                  if (idsForSingleMap[j].equals(loc.getMapTaskId())) {
-                      it.remove();
-                      foundit = true;
-                      break;
-                  }
+          for (Iterator it = needed.iterator(); it.hasNext(); ) {
+              int mapId = ((Integer) it.next()).intValue();
+              if (mapId == loc.getMapId()) {
+                it.remove();
+                break;
               }
           }
           copyPhase.startNextPhase();
"
hadoop,bea19da3b043a71e0223bce2588782edb7daab88,"HADOOP-200.  Avoid transmitting entire list of map task names to reduce tasks.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@406718 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-15 19:13:33,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskInProgress.java b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
index b860c40..33b44bd 100644
--- a/src/java/org/apache/hadoop/mapred/TaskInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
@@ -52,7 +52,7 @@
     // Defines the TIP
     private String jobFile = null;
     private FileSplit split = null;
-    private TaskInProgress predecessors[] = null;
+    private int numMaps;
     private int partition;
     private JobTracker jobtracker;
     private String id;
@@ -95,11 +95,11 @@
      * Constructor for ReduceTask
      */
     public TaskInProgress(String uniqueString, String jobFile, 
-                          TaskInProgress predecessors[], 
+                          int numMaps, 
                           int partition, JobTracker jobtracker, JobConf conf,
                           JobInProgress job) {
         this.jobFile = jobFile;
-        this.predecessors = predecessors;
+        this.numMaps = numMaps;
         this.partition = partition;
         this.jobtracker = jobtracker;
         this.job = job;
@@ -439,11 +439,8 @@
             if (isMapTask()) {
                 t = new MapTask(jobFile, taskid, split);
             } else {
-                String mapIdPredecessors[][] = new String[predecessors.length][];
-                for (int i = 0; i < mapIdPredecessors.length; i++) {
-                    mapIdPredecessors[i] = predecessors[i].getAllPossibleTaskIds();
-                }
-                t = new ReduceTask(jobFile, taskid, mapIdPredecessors, partition);
+                t = new ReduceTask(job.getProfile().getJobId(), jobFile, taskid, 
+                                   numMaps, partition);
             }
             t.setConf(conf);
 
"
hadoop,bea19da3b043a71e0223bce2588782edb7daab88,"HADOOP-200.  Avoid transmitting entire list of map task names to reduce tasks.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@406718 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-15 19:13:33,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index ec27436..730bccc 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -730,9 +730,9 @@
     // MapOutputProtocol
     /////////////////////////////////////////////////////////////////
     public MapOutputFile getFile(String mapTaskId, String reduceTaskId,
-      IntWritable partition) {
-    MapOutputFile mapOutputFile = new MapOutputFile(mapTaskId, reduceTaskId,
-        partition.get());
+                                 int mapId, int partition) {
+    MapOutputFile mapOutputFile = 
+      new MapOutputFile(mapTaskId, reduceTaskId, mapId, partition);
     mapOutputFile.setConf(this.fConf);
     return mapOutputFile;
   }
"
hadoop,c2169ded3b442166e193e8e4d006f68e8c7c2cd6,"HADOOP-180.  Quee task cleanups so that TaskTracker remains responsive.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@406706 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-15 18:42:36,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/RPC.java b/src/java/org/apache/hadoop/ipc/RPC.java
index c5f81b2f..0cffa40 100644
--- a/src/java/org/apache/hadoop/ipc/RPC.java
+++ b/src/java/org/apache/hadoop/ipc/RPC.java
@@ -23,7 +23,7 @@
 import java.lang.reflect.InvocationTargetException;
 
 import java.net.InetSocketAddress;
-import java.util.logging.Logger;
+import java.util.logging.*;
 import java.io.*;
 
 import org.apache.hadoop.io.*;
@@ -146,8 +146,11 @@
 
     public Object invoke(Object proxy, Method method, Object[] args)
       throws Throwable {
+      long startTime = System.currentTimeMillis();
       ObjectWritable value = (ObjectWritable)
         client.call(new Invocation(method, args), address);
+      long callTime = System.currentTimeMillis() - startTime;
+      LOG.fine(""Call: "" + method.getName() + "" "" + callTime);
       return value.get();
     }
   }
@@ -240,7 +243,7 @@
         long startTime = System.currentTimeMillis();
         Object value = method.invoke(instance, call.getParameters());
         long callTime = System.currentTimeMillis() - startTime;
-        LOG.fine(""Call: "" + call.getMethodName() + "" "" + callTime);
+        LOG.fine(""Served: "" + call.getMethodName() + "" "" + callTime);
         if (verbose) log(""Return: ""+value);
 
         return new ObjectWritable(method.getReturnType(), value);
"
hadoop,c2169ded3b442166e193e8e4d006f68e8c7c2cd6,"HADOOP-180.  Quee task cleanups so that TaskTracker remains responsive.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@406706 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-15 18:42:36,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 13130c9..ec27436 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -72,7 +72,33 @@
 
     private int maxCurrentTasks;
     private int failures;
-
+    
+    /**
+     * A list of tips that should be cleaned up.
+     */
+    private BlockingQueue tasksToCleanup = new BlockingQueue();
+    
+    /**
+     * A daemon-thread that pulls tips off the list of things to cleanup.
+     */
+    private Thread taskCleanupThread = 
+      new Thread(new Runnable() {
+        public void run() {
+          while (true) {
+            try {
+              TaskInProgress tip = (TaskInProgress) tasksToCleanup.take();
+              tip.jobHasFinished();
+            } catch (Throwable except) {
+              LOG.warning(StringUtils.stringifyException(except));
+            }
+          }
+        }
+      });
+    {
+      taskCleanupThread.setDaemon(true);
+      taskCleanupThread.start();
+    }
+    
     class MapOutputServer extends RPC.Server {
       private MapOutputServer(int port, int threads) {
         super(TaskTracker.this, fConf, port, threads, false);
@@ -108,11 +134,8 @@
      * so we can call it again and ""recycle"" the object after calling
      * close().
      */
-    void initialize() throws IOException {
+    synchronized void initialize() throws IOException {
         this.localHostname = InetAddress.getLocalHost().getHostName();
-        this.taskTrackerName = ""tracker_"" + localHostname + ""_"" +
-                               (Math.abs(r.nextInt()) % 100000);
-        LOG.info(""Starting tracker "" + taskTrackerName);
 
         fConf.deleteLocalFiles(SUBDIR);
 
@@ -148,6 +171,9 @@
                 this.mapOutputPort++;
             }
         }
+        this.taskTrackerName = ""tracker_"" + 
+                               localHostname + "":"" + taskReportPort;
+        LOG.info(""Starting tracker "" + taskTrackerName);
 
         // Clear out temporary files that might be lying around
         this.mapOutputFile.cleanupStorage();
@@ -323,12 +349,11 @@
             if (toCloseIds != null) {
               synchronized (this) {
                 for (int i = 0; i < toCloseIds.length; i++) {
-                  TaskInProgress tip = (TaskInProgress) tasks.get(toCloseIds[i]);
-                  try {
-                    tip.jobHasFinished();
-                  } catch (IOException ie) {
-                    LOG.info(""problem finishing task: "" +
-                             StringUtils.stringifyException(ie));
+                  Object tip = tasks.get(toCloseIds[i]);
+                  if (tip != null) {
+                    tasksToCleanup.put(tip);
+                  } else {
+                    LOG.info(""Attempt to cleanup unknown tip "" + toCloseIds[i]);
                   }
                 }
               }
@@ -376,7 +401,6 @@
       }
     }
     
-
     /**
      * The server retry loop.  
      * This while-loop attempts to connect to the JobTracker.  It only 
@@ -414,6 +438,51 @@
         }
     }
 
+    /**
+     * This class implements a queue that is put between producer and 
+     * consumer threads. It will grow without bound.
+     * @author Owen O'Malley
+     */
+    static private class BlockingQueue {
+      private List queue;
+      
+      /**
+       * Create an empty queue.
+       */
+      public BlockingQueue() {
+        queue = new ArrayList();
+      }
+       
+      /**
+       * Put the given object at the back of the queue.
+       * @param obj
+       */
+      public void put(Object obj) {
+        synchronized (queue) {
+          queue.add(obj);
+          queue.notify();
+        }
+      }
+      
+      /**
+       * Take the object at the front of the queue.
+       * It blocks until there is an object available.
+       * @return the head of the queue
+       */
+      public Object take() {
+        synchronized (queue) {
+          while (queue.isEmpty()) {
+            try {
+              queue.wait();
+            } catch (InterruptedException ie) {}
+          }
+          Object result = queue.get(0);
+          queue.remove(0);
+          return result;
+        }
+      }
+    }
+    
     ///////////////////////////////////////////////////////
     // TaskInProgress maintains all the info for a Task that
     // lives at this TaskTracker.  It maintains the Task object,
@@ -641,13 +710,19 @@
          * controlling job is all done and the files have been copied
          * away, or the task failed and we don't need the remains.
          */
-        synchronized void cleanup() throws IOException {
-            tasks.remove(task.getTaskId());
-            try {
-                runner.close();
-            } catch (IOException ie) {
+        void cleanup() throws IOException {
+            String taskId = task.getTaskId();
+            LOG.fine(""Cleaning up "" + taskId);
+            synchronized (TaskTracker.this) {
+               tasks.remove(taskId);
+               synchronized (this) {
+                 try {
+                    runner.close();
+                 } catch (Throwable ie) {
+                 }
+               }
             }
-            this.defaultJobConf.deleteLocalFiles(SUBDIR + ""/"" + task.getTaskId());
+            this.defaultJobConf.deleteLocalFiles(SUBDIR + ""/"" + taskId);
         }
     }
 
"
hadoop,2ac11b0dd6c0f0e931722ec1d989488449ad0e05,"HADOOP-146.  Fix DFS to check that newly allocated block id's are not already used.  Contributed by Konstantin.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@405890 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-12 21:41:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/Block.java b/src/java/org/apache/hadoop/dfs/Block.java
index f8b05ca..1f91490 100644
--- a/src/java/org/apache/hadoop/dfs/Block.java
+++ b/src/java/org/apache/hadoop/dfs/Block.java
@@ -35,8 +35,6 @@
          });
     }
 
-    static Random r = new Random();
-
     /**
      */
     public static boolean isBlockFilename(File f) {
@@ -53,7 +51,7 @@
     /**
      */
     public Block() {
-        this.blkid = r.nextLong();
+        this.blkid = 0;
         this.len = 0;
     }
 
"
hadoop,2ac11b0dd6c0f0e931722ec1d989488449ad0e05,"HADOOP-146.  Fix DFS to check that newly allocated block id's are not already used.  Contributed by Konstantin.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@405890 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-12 21:41:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index cd890a8..d45a8fa 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -585,11 +585,16 @@
         return COMPLETE_SUCCESS;
     }
 
+    static Random randBlockId = new Random();
+    
     /**
      * Allocate a block at the given pending filename
      */
     synchronized Block allocateBlock(UTF8 src) {
-        Block b = new Block();
+        Block b = null;
+        do {
+            b = new Block(FSNamesystem.randBlockId.nextLong(), 0);
+        } while (dir.isValidBlock(b));
         FileUnderConstruction v = 
           (FileUnderConstruction) pendingCreates.get(src);
         v.getBlocks().add(b);
"
hadoop,51c6422a6095a3cdf37a6b54318d9c58677a04b3,"HADOOP-207, Fix JDK 1.4 incompatible use of System.getenv().

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@405853 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-12 20:13:19,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/util/LogFormatter.java b/src/java/org/apache/hadoop/util/LogFormatter.java
index cd118df..4b49300 100644
--- a/src/java/org/apache/hadoop/util/LogFormatter.java
+++ b/src/java/org/apache/hadoop/util/LogFormatter.java
@@ -49,10 +49,10 @@
 
   public static String initFileHandler( Configuration conf, String opName )
       throws IOException {
-          String logDir=System.getenv(""HADOOP_LOG_DIR"");
+          String logDir=System.getProperty(""hadoop.log.dir"");
           String userHome=System.getProperty(""user.dir"");
           if( logDir==null ) {
-        	  logDir=System.getenv(""HADOOP_HOME"");
+        	  logDir=System.getProperty(""hadoop.home.dir"");
         	  if(logDir==null) {
         		  logDir=userHome;
         	  } else {
@@ -82,7 +82,9 @@
           	hostname=""localhost"";
           }
           
-          String logFile = logDir+File.separator+""hadoop-""+System.getProperty( ""user.name"" )
+          String id = System.getProperty( ""hadoop.id.str"", 
+                                          System.getProperty(""user.name"") );
+          String logFile = logDir+File.separator+""hadoop-""+id
                +""-""+opName+""-""+hostname+"".log"";
 
           int logFileSize = conf.getInt( ""hadoop.logfile.size"", 10000000 );
"
hadoop,3e6b0660057516764ef2e3a5a3465fa594807db5,"HADOOP-201.  Fix 'bin/hadoop dfs -report'.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@405121 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-08 18:23:01,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSShell.java b/src/java/org/apache/hadoop/dfs/DFSShell.java
index 059df88..04fd761 100644
--- a/src/java/org/apache/hadoop/dfs/DFSShell.java
+++ b/src/java/org/apache/hadoop/dfs/DFSShell.java
@@ -27,6 +27,10 @@
  * @author Mike Cafarella
  **************************************************/
 public class DFSShell {
+
+    // required for unknown reason to make WritableFactories work distributed
+    static { new DatanodeInfo(); }
+
     FileSystem fs;
 
     /**
"
hadoop,3ed02e0cd95780719d6f2c54a919468ea2d91599,"HADOOP-199.  Fix reduce progress.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@405113 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-08 18:08:13,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index 4500b2d..599e96e 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -967,6 +967,8 @@
                     // Tell the job to fail the relevant task
                     job.failedTask(tip, report.getTaskId(), report, 
                                    status.getTrackerName());
+                } else {
+                    job.updateTaskStatus(tip, report);
                 }
             }
         }
"
hadoop,af5832a247abc0b20e429d02f4e3c5a87567dbb5,"HADOOP-182.  Fix problems related to lost task trackers.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@399833 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-04 19:27:36,Doug Cutting,"diff --git a/src/examples/org/apache/hadoop/examples/ExampleDriver.java b/src/examples/org/apache/hadoop/examples/ExampleDriver.java
index 12e7638..5c690b1 100644
--- a/src/examples/org/apache/hadoop/examples/ExampleDriver.java
+++ b/src/examples/org/apache/hadoop/examples/ExampleDriver.java
@@ -16,12 +16,6 @@
 
 package org.apache.hadoop.examples;
 import org.apache.hadoop.util.ProgramDriver;
-import java.lang.reflect.InvocationTargetException;
-import java.lang.reflect.Method;
-import java.util.TreeMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.Map.Entry;
 
 public class ExampleDriver {
   
"
hadoop,af5832a247abc0b20e429d02f4e3c5a87567dbb5,"HADOOP-182.  Fix problems related to lost task trackers.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@399833 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-04 19:27:36,Doug Cutting,"diff --git a/src/examples/org/apache/hadoop/examples/WordCount.java b/src/examples/org/apache/hadoop/examples/WordCount.java
index 63a4cbd..11f1abb 100644
--- a/src/examples/org/apache/hadoop/examples/WordCount.java
+++ b/src/examples/org/apache/hadoop/examples/WordCount.java
@@ -19,7 +19,6 @@
 import java.io.*;
 import java.util.*;
 
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.UTF8;
"
hadoop,af5832a247abc0b20e429d02f4e3c5a87567dbb5,"HADOOP-182.  Fix problems related to lost task trackers.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@399833 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-04 19:27:36,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSck.java b/src/java/org/apache/hadoop/dfs/DFSck.java
index 3279736..dbe1664 100644
--- a/src/java/org/apache/hadoop/dfs/DFSck.java
+++ b/src/java/org/apache/hadoop/dfs/DFSck.java
@@ -333,9 +333,9 @@
       e.printStackTrace();
       success = false;
     } finally {
-      try {in.close(); } catch (Exception e1) {};
-      try {out.close(); } catch (Exception e1) {};
-      try {s.close(); } catch (Exception e1) {};
+      try {in.close(); } catch (Exception e1) {}
+      try {out.close(); } catch (Exception e1) {}
+      try {s.close(); } catch (Exception e1) {}
     }
     if (!success)
       throw new Exception(""Could not copy block data for "" + lblock.getBlock().getBlockName());
"
hadoop,af5832a247abc0b20e429d02f4e3c5a87567dbb5,"HADOOP-182.  Fix problems related to lost task trackers.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@399833 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-04 19:27:36,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java b/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
index 7d63996..98a958a 100644
--- a/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
+++ b/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
@@ -18,7 +18,6 @@
 
 import java.io.*;
 import java.net.*;
-import java.util.*;
 
 import org.apache.hadoop.io.*;
 import org.apache.hadoop.fs.*;
"
hadoop,af5832a247abc0b20e429d02f4e3c5a87567dbb5,"HADOOP-182.  Fix problems related to lost task trackers.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@399833 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-04 19:27:36,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobInProgress.java b/src/java/org/apache/hadoop/mapred/JobInProgress.java
index 0341786..133999a 100644
--- a/src/java/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/JobInProgress.java
@@ -95,7 +95,7 @@
      * Construct the splits, etc.  This is invoked from an async
      * thread so that split-computation doesn't block anyone.
      */
-    public void initTasks() throws IOException {
+    public synchronized void initTasks() throws IOException {
         if (tasksInited) {
             return;
         }
@@ -243,9 +243,12 @@
     ////////////////////////////////////////////////////
     // Status update methods
     ////////////////////////////////////////////////////
-    public void updateTaskStatus(TaskInProgress tip, TaskStatus status) {
+    public synchronized void updateTaskStatus(TaskInProgress tip, 
+                                              TaskStatus status) {
         double oldProgress = tip.getProgress();   // save old progress
         tip.updateStatus(status);                 // update tip
+        LOG.fine(""Taking progress for "" + tip.getTIPId() + "" from "" + 
+                 oldProgress + "" to "" + tip.getProgress());
 
         //
         // Update JobInProgress status
@@ -416,7 +419,10 @@
     /**
      * A taskid assigned to this JobInProgress has reported in successfully.
      */
-    public synchronized void completedTask(TaskInProgress tip, String taskid) {
+    public synchronized void completedTask(TaskInProgress tip, 
+                                           TaskStatus status) {
+        String taskid = status.getTaskId();
+        updateTaskStatus(tip, status);
         LOG.info(""Taskid '"" + taskid + ""' has finished successfully."");
         tip.completed(taskid);
 
@@ -443,7 +449,8 @@
         // If all tasks are complete, then the job is done!
         //
         if (status.getRunState() == JobStatus.RUNNING && allDone) {
-            this.status = new JobStatus(status.getJobId(), 1.0f, 1.0f, JobStatus.SUCCEEDED);
+            this.status = new JobStatus(this.status.getJobId(), 1.0f, 1.0f, 
+                                        JobStatus.SUCCEEDED);
             this.finishTime = System.currentTimeMillis();
             garbageCollect();
         }
@@ -483,8 +490,10 @@
      * we need to schedule reexecution so that downstream reduce tasks can 
      * obtain the map task's output.
      */
-    public void failedTask(TaskInProgress tip, String taskid, String trackerName) {
+    public synchronized void failedTask(TaskInProgress tip, String taskid, 
+                                        TaskStatus status, String trackerName) {
         tip.failedSubTask(taskid, trackerName);
+        updateTaskStatus(tip, status);
         
         // After this, try to assign tasks with the one after this, so that
         // the failed task goes to the end of the list.
@@ -501,8 +510,31 @@
             LOG.info(""Aborting job "" + profile.getJobId());
             kill();
         }
-    }
 
+        jobtracker.removeTaskEntry(taskid);
+ }
+
+    /**
+     * Fail a task with a given reason, but without a status object.
+     * @author Owen O'Malley
+     * @param tip The task's tip
+     * @param taskid The task id
+     * @param reason The reason that the task failed
+     * @param trackerName The task tracker the task failed on
+     */
+    public void failedTask(TaskInProgress tip, String taskid, 
+                           String reason, String hostname, String trackerName) {
+       TaskStatus status = new TaskStatus(taskid,
+                                          tip.isMapTask(),
+                                          0.0f,
+                                          TaskStatus.FAILED,
+                                          reason,
+                                          reason,
+                                          hostname);
+       failedTask(tip, taskid, status, trackerName);
+    }
+       
+                           
     /**
      * The job is dead.  We're now GC'ing it, getting rid of the job
      * from all tables.  Be sure to remove all of this job's tasks
"
hadoop,af5832a247abc0b20e429d02f4e3c5a87567dbb5,"HADOOP-182.  Fix problems related to lost task trackers.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@399833 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-04 19:27:36,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index 6afd607..3145522 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -95,8 +95,6 @@
        * map: task-id (String) -> time-assigned (Long)
        */
       private Map launchingTasks = new LinkedHashMap();
-      private static final String errorMsg = ""Error launching task"";
-      private static final String errorHost = ""n/a"";
       
       public void run() {
         try {
@@ -119,21 +117,9 @@
                     tip = (TaskInProgress) taskidToTIPMap.get(taskId);
                   }
                   if (tip != null) {
-                    synchronized (tip) {
-                      JobInProgress job = tip.getJob();
-                      // record why the job failed, so that the user can
-                      // see the problem
-                      TaskStatus status = 
-                        new TaskStatus(taskId,
-                                       tip.isMapTask(),
-                                       0.0f,
-                                       TaskStatus.FAILED,
-                                       errorMsg,
-                                       errorMsg,
-                                       errorHost);
-                      tip.updateStatus(status);
-                      job.failedTask(tip, taskId, errorHost);
-                    }
+                     JobInProgress job = tip.getJob();
+                     job.failedTask(tip, taskId, ""Error launching task"", 
+                                    ""n/a"", ""n/a"");
                   }
                   itr.remove();
                 } else {
@@ -214,7 +200,8 @@
                                 if (now - newProfile.getLastSeen() > TASKTRACKER_EXPIRY_INTERVAL) {
                                     // Remove completely
                                     updateTaskTrackerStatus(trackerName, null);
-                                    lostTaskTracker(leastRecent.getTrackerName());
+                                    lostTaskTracker(leastRecent.getTrackerName(),
+                                                    leastRecent.getHost());
                                 } else {
                                     // Update time by inserting latest profile
                                     trackerExpiryQueue.add(newProfile);
@@ -582,14 +569,6 @@
     ////////////////////////////////////////////////////
     // InterTrackerProtocol
     ////////////////////////////////////////////////////
-    public void initialize(String taskTrackerName) {
-      synchronized (taskTrackers) {
-        boolean seenBefore = updateTaskTrackerStatus(taskTrackerName, null);
-        if (seenBefore) {
-          lostTaskTracker(taskTrackerName);
-        }
-      }
-    }
 
     /**
      * Update the last recorded status for the given task tracker.
@@ -632,7 +611,7 @@
                 if (initialContact) {
                     // If it's first contact, then clear out any state hanging around
                     if (seenBefore) {
-                        lostTaskTracker(trackerName);
+                        lostTaskTracker(trackerName, trackerStatus.getHost());
                     }
                 } else {
                     // If not first contact, there should be some record of the tracker
@@ -981,13 +960,13 @@
             } else {
                 expireLaunchingTasks.removeTask(taskId);
                 JobInProgress job = tip.getJob();
-                job.updateTaskStatus(tip, report);
 
                 if (report.getRunState() == TaskStatus.SUCCEEDED) {
-                    job.completedTask(tip, report.getTaskId());
+                    job.completedTask(tip, report);
                 } else if (report.getRunState() == TaskStatus.FAILED) {
                     // Tell the job to fail the relevant task
-                    job.failedTask(tip, report.getTaskId(), status.getTrackerName());
+                    job.failedTask(tip, report.getTaskId(), report, 
+                                   status.getTrackerName());
                 }
             }
         }
@@ -998,7 +977,7 @@
      * already been updated.  Just process the contained tasks and any
      * jobs that might be affected.
      */
-    void lostTaskTracker(String trackerName) {
+    void lostTaskTracker(String trackerName, String hostname) {
         LOG.info(""Lost tracker '"" + trackerName + ""'"");
         TreeSet lostTasks = (TreeSet) trackerToTaskMap.get(trackerName);
         trackerToTaskMap.remove(trackerName);
@@ -1008,9 +987,16 @@
                 String taskId = (String) it.next();
                 TaskInProgress tip = (TaskInProgress) taskidToTIPMap.get(taskId);
 
-                // Tell the job to fail the relevant task
-                JobInProgress job = tip.getJob();
-                job.failedTask(tip, taskId, trackerName);
+                // Completed reduce tasks never need to be failed, because 
+                // their outputs go to dfs
+                if (tip.isMapTask() || !tip.isComplete()) {
+                  JobInProgress job = tip.getJob();
+                  // if the job is done, we don't want to change anything
+                  if (job.getStatus().getRunState() == JobStatus.RUNNING) {
+                    job.failedTask(tip, taskId, ""Lost task tracker"", 
+                                   hostname, trackerName);
+                  }
+                }
             }
         }
     }
"
hadoop,af5832a247abc0b20e429d02f4e3c5a87567dbb5,"HADOOP-182.  Fix problems related to lost task trackers.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@399833 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-04 19:27:36,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskInProgress.java b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
index ece9864..e80320f 100644
--- a/src/java/org/apache/hadoop/mapred/TaskInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
@@ -15,11 +15,8 @@
  */
 package org.apache.hadoop.mapred;
 
-import org.apache.hadoop.fs.*;
-import org.apache.hadoop.conf.*;
 import org.apache.hadoop.util.LogFormatter;
 
-import java.io.*;
 import java.text.NumberFormat;
 import java.util.*;
 import java.util.logging.*;
@@ -304,11 +301,6 @@
             kill();
         }
         machinesWhereFailed.add(trackerName);
-
-        // Ask JobTracker to forget about this task
-        jobtracker.removeTaskEntry(taskid);
-
-        recomputeProgress();
     }
 
     /**
"
hadoop,af5832a247abc0b20e429d02f4e3c5a87567dbb5,"HADOOP-182.  Fix problems related to lost task trackers.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@399833 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-04 19:27:36,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index fa33588..91fe69e 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -575,6 +575,7 @@
                     failures += 1;
                   }
                   runstate = TaskStatus.FAILED;
+                  progress = 0.0f;
               }
               
               needCleanup = runstate == TaskStatus.FAILED;
@@ -627,6 +628,7 @@
             if (runstate == TaskStatus.SUCCEEDED) {
               LOG.info(""Reporting output lost:""+task.getTaskId());
               runstate = TaskStatus.FAILED;       // change status to failure
+              progress = 0.0f;
               runningTasks.put(task.getTaskId(), this);
               mapTotal++;
             } else {
"
hadoop,0645a04602e36a04b6e6f06540ddcca9f4cbfa43,"HADOOP-192.  Fix a Java 1.4 incompatibility.  Contributed by David Bowen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@399428 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-03 20:50:56,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index ca21793..f27f73e 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -148,7 +148,7 @@
       public void addNewTask(String taskName) {
         synchronized (launchingTasks) {
           launchingTasks.put(taskName, 
-                             Long.valueOf(System.currentTimeMillis()));
+                             new Long(System.currentTimeMillis()));
         }
       }
       
"
hadoop,b04af0d5bee4c9a95c74fe889f635b4e33af1ccf,"Fix HADOOP-19.  If a child process hangs after it has reported completion its output should not be lost.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@399074 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-02 22:53:31,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index d6b72d8..4d4affa 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -573,7 +573,8 @@
                   }
                   runstate = TaskStatus.FAILED;
               }
-              needCleanup = wasKilled || runstate == TaskStatus.FAILED;
+              
+              needCleanup = runstate == TaskStatus.FAILED;
             }
 
             //
"
hadoop,c9dbe05377158ecd0e859ac8fe9878aeef83927a,"HADOOP-185.  Fix so that, if a task tracker times out making the RPC asking for a new task to run, the job tracker does not think that it is actually running the task returned (but never received).  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@399065 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-02 22:05:08,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index e6894df..ca21793 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -76,6 +76,93 @@
         return tracker;
     }
 
+    /**
+     * A thread to timeout tasks that have been assigned to task trackers,
+     * but that haven't reported back yet.
+     * Note that I included a stop() method, even though there is no place
+     * where JobTrackers are cleaned up.
+     * @author Owen O'Malley
+     */
+    private class ExpireLaunchingTasks implements Runnable {
+      private volatile boolean shouldRun = true;
+      /**
+       * This is a map of the tasks that have been assigned to task trackers,
+       * but that have not yet been seen in a status report.
+       * map: task-id (String) -> time-assigned (Long)
+       */
+      private Map launchingTasks = new LinkedHashMap();
+      private static final String errorMsg = ""Error launching task"";
+      private static final String errorHost = ""n/a"";
+      
+      public void run() {
+        try {
+          while (shouldRun) {
+            // Every 3 minutes check for any tasks that are overdue
+            Thread.sleep(TASKTRACKER_EXPIRY_INTERVAL/3);
+            long now = System.currentTimeMillis();
+            LOG.fine(""Starting launching task sweep"");
+            synchronized (launchingTasks) {
+              Iterator itr = launchingTasks.entrySet().iterator();
+              while (itr.hasNext()) {
+                Map.Entry pair = (Map.Entry) itr.next();
+                String taskId = (String) pair.getKey();
+                long age = now - ((Long) pair.getValue()).longValue();
+                LOG.fine(taskId + "" is "" + age + "" ms old."");
+                if (age > TASKTRACKER_EXPIRY_INTERVAL) {
+                  LOG.info(""Launching task "" + taskId + "" timed out."");
+                  TaskInProgress tip = null;
+                  synchronized (JobTracker.this) {
+                    tip = (TaskInProgress) taskidToTIPMap.get(taskId);
+                  }
+                  if (tip != null) {
+                    synchronized (tip) {
+                      JobInProgress job = tip.getJob();
+                      // record why the job failed, so that the user can
+                      // see the problem
+                      TaskStatus status = 
+                        new TaskStatus(taskId,
+                                       tip.isMapTask(),
+                                       0.0f,
+                                       TaskStatus.FAILED,
+                                       errorMsg,
+                                       errorMsg,
+                                       errorHost);
+                      tip.updateStatus(status);
+                      job.failedTask(tip, taskId, errorHost);
+                    }
+                  }
+                  itr.remove();
+                } else {
+                  // the tasks are sorted by start time, so once we find
+                  // one that we want to keep, we are done for this cycle.
+                  break;
+                }
+              }
+            }
+          }
+        } catch (InterruptedException ie) {
+          // all done
+        }
+      }
+      
+      public void addNewTask(String taskName) {
+        synchronized (launchingTasks) {
+          launchingTasks.put(taskName, 
+                             Long.valueOf(System.currentTimeMillis()));
+        }
+      }
+      
+      public void removeTask(String taskName) {
+        synchronized (launchingTasks) {
+          launchingTasks.remove(taskName);
+        }
+      }
+      
+      public void stop() {
+        shouldRun = false;
+      }
+    }
+    
     ///////////////////////////////////////////////////////
     // Used to expire TaskTrackers that have gone down
     ///////////////////////////////////////////////////////
@@ -277,7 +364,9 @@
     ExpireTrackers expireTrackers = new ExpireTrackers();
     RetireJobs retireJobs = new RetireJobs();
     JobInitThread initJobs = new JobInitThread();
-
+    ExpireLaunchingTasks expireLaunchingTasks = new ExpireLaunchingTasks();
+    Thread expireLaunchingTaskThread = new Thread(expireLaunchingTasks);
+    
     /**
      * It might seem like a bug to maintain a TreeSet of status objects,
      * which can be updated at any time.  But that's not what happens!  We
@@ -346,12 +435,12 @@
         this.port = addr.getPort();
         this.interTrackerServer = RPC.getServer(this, addr.getPort(), 10, false, conf);
         this.interTrackerServer.start();
-	Properties p = System.getProperties();
-	for (Iterator it = p.keySet().iterator(); it.hasNext(); ) {
-	    String key = (String) it.next();
-	    String val = (String) p.getProperty(key);
-	    LOG.info(""Property '"" + key + ""' is "" + val);
-	}
+        Properties p = System.getProperties();
+        for (Iterator it = p.keySet().iterator(); it.hasNext(); ) {
+          String key = (String) it.next();
+          String val = (String) p.getProperty(key);
+          LOG.info(""Property '"" + key + ""' is "" + val);
+        }
 
         this.infoPort = conf.getInt(""mapred.job.tracker.info.port"", 50030);
         this.infoServer = new JobTrackerInfoServer(this, infoPort);
@@ -362,6 +451,7 @@
         new Thread(this.expireTrackers).start();
         new Thread(this.retireJobs).start();
         new Thread(this.initJobs).start();
+        expireLaunchingTaskThread.start();
     }
 
     public static InetSocketAddress getAddress(Configuration conf) {
@@ -622,7 +712,8 @@
 
                     Task t = job.obtainNewMapTask(taskTracker, tts);
                     if (t != null) {
-                        return t;
+                      expireLaunchingTasks.addNewTask(t.getTaskId());
+                      return t;
                     }
 
                     //
@@ -656,7 +747,8 @@
 
                     Task t = job.obtainNewReduceTask(taskTracker, tts);
                     if (t != null) {
-                        return t;
+                      expireLaunchingTasks.addNewTask(t.getTaskId());
+                      return t;
                     }
 
                     //
@@ -878,10 +970,12 @@
         for (Iterator it = status.taskReports(); it.hasNext(); ) {
             TaskStatus report = (TaskStatus) it.next();
             report.setHostname(status.getHost());
-            TaskInProgress tip = (TaskInProgress) taskidToTIPMap.get(report.getTaskId());
+            String taskId = report.getTaskId();
+            TaskInProgress tip = (TaskInProgress) taskidToTIPMap.get(taskId);
             if (tip == null) {
                 LOG.info(""Serious problem.  While updating status, cannot find taskid "" + report.getTaskId());
             } else {
+                expireLaunchingTasks.removeTask(taskId);
                 JobInProgress job = tip.getJob();
                 job.updateTaskStatus(tip, report);
 
"
hadoop,0c0d78d8c820161bdd138f0e7726aca0fc77f0a8,"HADOOP-188.  More fixes to to JobClient, following on HADOOP-174.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@399030 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-02 20:07:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobClient.java b/src/java/org/apache/hadoop/mapred/JobClient.java
index c9951fa..ad251ef 100644
--- a/src/java/org/apache/hadoop/mapred/JobClient.java
+++ b/src/java/org/apache/hadoop/mapred/JobClient.java
@@ -308,27 +308,31 @@
         running = jc.submitJob(job);
         String jobId = running.getJobID();
         LOG.info(""Running job: "" + jobId);
-        while (!running.isComplete()) {
+        while (true) {
           try {
             Thread.sleep(1000);
           } catch (InterruptedException e) {}
           try {
+            if (running.isComplete()) {
+              break;
+            }
             running = jc.getJob(jobId);
+            String report = ("" map "" + Math.round(running.mapProgress()*100)+
+                             ""%  reduce "" + 
+                             Math.round(running.reduceProgress()*100) + ""%"");
+            if (!report.equals(lastReport)) {
+              LOG.info(report);
+              lastReport = report;
+            }
             retries = MAX_RETRIES;
           } catch (IOException ie) {
             if (--retries == 0) {
-              LOG.info(""Final attempt failed, killing job."");
+              LOG.warning(""Final attempt failed, killing job."");
               throw ie;
             }
             LOG.info(""Communication problem with server: "" +
                      StringUtils.stringifyException(ie));
           }
-          String report = null;
-          report = "" map ""+Math.round(running.mapProgress()*100)+""%  reduce "" + Math.round(running.reduceProgress()*100)+""%"";
-          if (!report.equals(lastReport)) {
-            LOG.info(report);
-            lastReport = report;
-          }
         }
         if (!running.isSuccessful()) {
           throw new IOException(""Job failed!"");
"
hadoop,afbcd7e2b790d45570ad6f5b210c29c4fdecee6b,"HADOOP-186.  Better error handling in TaskTracker's top-level loop.  Also improve calculation of time to send next heartbeat.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@398994 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-02 18:15:53,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index ba4a66e..d6b72d8 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -18,7 +18,6 @@
 import org.apache.hadoop.fs.*;
 import org.apache.hadoop.io.*;
 import org.apache.hadoop.ipc.*;
-import org.apache.hadoop.conf.*;
 import org.apache.hadoop.util.*;
 
 import java.io.*;
@@ -215,6 +214,7 @@
      */
     int offerService() throws Exception {
         long lastHeartbeat = 0;
+        this.fs = FileSystem.getNamed(jobClient.getFilesystemName(), this.fConf);
 
         while (running) {
             long now = System.currentTimeMillis();
@@ -227,15 +227,16 @@
                 }
                 continue;
             }
+            lastHeartbeat = now;
 
             //
             // Emit standard hearbeat message to check in with JobTracker
             //
             Vector taskReports = new Vector();
             synchronized (this) {
-                for (Iterator it = runningTasks.keySet().iterator(); it.hasNext(); ) {
-                    String taskid = (String) it.next();
-                    TaskInProgress tip = (TaskInProgress) runningTasks.get(taskid);
+                for (Iterator it = runningTasks.values().iterator(); 
+                     it.hasNext(); ) {
+                    TaskInProgress tip = (TaskInProgress) it.next();
                     TaskStatus status = tip.createStatus();
                     taskReports.add(status);
                     if (status.getRunState() != TaskStatus.RUNNING) {
@@ -252,9 +253,6 @@
             //
             // Xmit the heartbeat
             //
-            if (justStarted) {
-                this.fs = FileSystem.getNamed(jobClient.getFilesystemName(), this.fConf);
-            }
             
             TaskTrackerStatus status = 
               new TaskTrackerStatus(taskTrackerName, localHostname, 
@@ -269,11 +267,16 @@
             //
             // Check if we should create a new Task
             //
-            if (mapTotal < maxCurrentTasks || reduceTotal < maxCurrentTasks) {
-                Task t = jobClient.pollForNewTask(taskTrackerName);
-                if (t != null) {
-                  startNewTask(t);
-                }
+            try {
+              if (mapTotal < maxCurrentTasks || reduceTotal < maxCurrentTasks) {
+                  Task t = jobClient.pollForNewTask(taskTrackerName);
+                  if (t != null) {
+                    startNewTask(t);
+                  }
+              }
+            } catch (IOException ie) {
+              LOG.info(""Problem launching task: "" + 
+                       StringUtils.stringifyException(ie));
             }
 
             //
@@ -292,7 +295,12 @@
                                      "" seconds. Killing."";
                         LOG.info(tip.getTask().getTaskId() + "": "" + msg);
                         tip.reportDiagnosticInfo(msg);
-                        tip.killAndCleanup(true);
+                        try {
+                          tip.killAndCleanup(true);
+                        } catch (IOException ie) {
+                          LOG.info(""Problem cleaning task up: "" +
+                                   StringUtils.stringifyException(ie));
+                        }
                     }
                 }
             }
@@ -307,16 +315,25 @@
             //
             // Check for any Tasks whose job may have ended
             //
+            try {
             String[] toCloseIds = jobClient.pollForTaskWithClosedJob(taskTrackerName);
             if (toCloseIds != null) {
               synchronized (this) {
                 for (int i = 0; i < toCloseIds.length; i++) {
                   TaskInProgress tip = (TaskInProgress) tasks.get(toCloseIds[i]);
-                  tip.jobHasFinished();                        
+                  try {
+                    tip.jobHasFinished();
+                  } catch (IOException ie) {
+                    LOG.info(""problem finishing task: "" +
+                             StringUtils.stringifyException(ie));
+                  }
                 }
               }
             }
-            lastHeartbeat = now;
+            } catch (IOException ie) {
+              LOG.info(""Problem getting closed tasks: "" +
+                       StringUtils.stringifyException(ie));
+            }
         }
 
         return 0;
"
hadoop,c43b39cc22600589b74f2dcfe600714add71a7d2,"HADOOP-183.  If the namendode is restarted with different minimum or maximum replication counts, existing files' replication counts are now automatically adjusted to be within the newly configured bounds.  Contributed by Hairong Kuang.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@398674 13f79535-47bb-0310-9956-ffa450edef68
",2006-05-01 20:00:57,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSDirectory.java b/src/java/org/apache/hadoop/dfs/FSDirectory.java
index 9176212..38cbc48 100644
--- a/src/java/org/apache/hadoop/dfs/FSDirectory.java
+++ b/src/java/org/apache/hadoop/dfs/FSDirectory.java
@@ -403,8 +403,10 @@
                     UTF8 name = new UTF8();
                     name.readFields(in);
                     // version 0 does not support per file replication
-                    if( !(imgVersion >= 0) )
+                    if( !(imgVersion >= 0) ) {
                       replication = in.readShort(); // other versions do
+                      replication = adjustReplication( replication, conf );
+                    }
                     int numBlocks = in.readInt();
                     Block blocks[] = null;
                     if (numBlocks > 0) {
@@ -484,6 +486,7 @@
                           name = (UTF8) writables[0];
                           replication = Short.parseShort(
                                               ((UTF8)writables[1]).toString());
+                          replication = adjustReplication( replication, conf );
                         }
                         // get blocks
                         aw = new ArrayWritable(Block.class);
@@ -501,8 +504,11 @@
                         UTF8 repl = new UTF8();
                         src.readFields(in);
                         repl.readFields(in);
+                        replication=adjustReplication(
+                                fromLogReplication(repl),
+                                conf);
                         unprotectedSetReplication(src.toString(), 
-                                                  fromLogReplication(repl),
+                                                  replication,
                                                   null);
                         break;
                     } 
@@ -541,6 +547,17 @@
         return numEdits;
     }
 
+    private static short adjustReplication( short replication, Configuration conf) {
+        short minReplication = (short)conf.getInt(""dfs.replication.min"", 1);
+        if( replication<minReplication ) {
+            replication = minReplication;
+        }
+        short maxReplication = (short)conf.getInt(""dfs.replication.max"", 512);
+        if( replication>maxReplication ) {
+            replication = maxReplication;
+        }
+        return replication;
+    }
     /**
      * Save the contents of the FS image
      */
"
hadoop,9eb0886585a05409cf249c34b7fabd254e0022c9,"Fix HADOOP-174.  Make job client try up to five times to contact job tracker before aborting a job.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@398014 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-28 21:23:33,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/Client.java b/src/java/org/apache/hadoop/ipc/Client.java
index 4f5c6ad..8e9c264 100644
--- a/src/java/org/apache/hadoop/ipc/Client.java
+++ b/src/java/org/apache/hadoop/ipc/Client.java
@@ -302,7 +302,7 @@
       if (call.error != null) {
         throw call.error;
       } else if (!call.done) {
-        throw new IOException(""timed out waiting for response"");
+        throw new SocketTimeoutException(""timed out waiting for rpc response"");
       } else {
         return call.value;
       }
"
hadoop,9eb0886585a05409cf249c34b7fabd254e0022c9,"Fix HADOOP-174.  Make job client try up to five times to contact job tracker before aborting a job.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@398014 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-28 21:23:33,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobClient.java b/src/java/org/apache/hadoop/mapred/JobClient.java
index 97fa2d5..c9951fa 100644
--- a/src/java/org/apache/hadoop/mapred/JobClient.java
+++ b/src/java/org/apache/hadoop/mapred/JobClient.java
@@ -18,7 +18,7 @@
 import org.apache.hadoop.fs.*;
 import org.apache.hadoop.ipc.*;
 import org.apache.hadoop.conf.*;
-import org.apache.hadoop.util.LogFormatter;
+import org.apache.hadoop.util.*;
 
 import java.io.*;
 import java.net.*;
@@ -302,6 +302,8 @@
       boolean error = true;
       RunningJob running = null;
       String lastReport = null;
+      final int MAX_RETRIES = 5;
+      int retries = MAX_RETRIES;
       try {
         running = jc.submitJob(job);
         String jobId = running.getJobID();
@@ -310,7 +312,17 @@
           try {
             Thread.sleep(1000);
           } catch (InterruptedException e) {}
-          running = jc.getJob(jobId);
+          try {
+            running = jc.getJob(jobId);
+            retries = MAX_RETRIES;
+          } catch (IOException ie) {
+            if (--retries == 0) {
+              LOG.info(""Final attempt failed, killing job."");
+              throw ie;
+            }
+            LOG.info(""Communication problem with server: "" +
+                     StringUtils.stringifyException(ie));
+          }
           String report = null;
           report = "" map ""+Math.round(running.mapProgress()*100)+""%  reduce "" + Math.round(running.reduceProgress()*100)+""%"";
           if (!report.equals(lastReport)) {
"
hadoop,a19c8e80bc3b02c1fbd8dd14618e07f4b2dfba2b,"Fix HADOOP-169.  Don't fail reduce tasks if a call to the jobtracker to locate map outputs fails.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@397321 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-26 22:27:15,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java b/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
index 20a275c..76c290c 100644
--- a/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
+++ b/src/java/org/apache/hadoop/mapred/ReduceTaskRunner.java
@@ -65,10 +65,14 @@
       for (int i = 0; i < checkSize; i++) {
           neededStrings[i] = (String[]) needed.elementAt(i);
       }
-      MapOutputLocation[] locs =
-        jobClient.locateMapOutputs(task.getTaskId(), neededStrings);
-
-      if (locs.length == 0) {
+      MapOutputLocation[] locs = null;
+      try {
+        locs = jobClient.locateMapOutputs(task.getTaskId(), neededStrings);
+      } catch (IOException ie) {
+        LOG.info(""Problem locating map outputs: "" + 
+                 StringUtils.stringifyException(ie));
+      }
+      if (locs == null || locs.length == 0) {
         try {
           if (killed) {
             return false;
"
hadoop,1dafad3980c5dd7f7385862f2a75e3d7b95bee24,"Fix HADOOP-168.  Add IOException to throws of all MapReduce RPC protocol methods.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@397310 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-26 21:43:05,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java b/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java
index 134dbcc..c21f72c 100644
--- a/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java
+++ b/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java
@@ -34,16 +34,17 @@
    * TaskTracker must also indicate whether this is the first interaction
    * (since state refresh)
    */
-  int emitHeartbeat(TaskTrackerStatus status, boolean initialContact);
+  int emitHeartbeat(TaskTrackerStatus status, 
+                    boolean initialContact) throws IOException;
 
   /** Called to get new tasks from from the job tracker for this tracker.*/
-  Task pollForNewTask(String trackerName);
+  Task pollForNewTask(String trackerName) throws IOException;
 
   /** Called to find which tasks that have been run by this tracker should now
    * be closed because their job is complete.  This is used to, e.g., 
    * notify a map task that its output is no longer needed and may 
    * be removed. */
-  String[] pollForTaskWithClosedJob(String trackerName);
+  String[] pollForTaskWithClosedJob(String trackerName) throws IOException;
 
   /** Called by a reduce task to find which map tasks are completed.
    *
@@ -51,7 +52,9 @@
    * @param mapTasksNeeded an array of UTF8 naming map task ids whose output is needed.
    * @return an array of MapOutputLocation
    */
-  MapOutputLocation[] locateMapOutputs(String taskId, String[][] mapTasksNeeded);
+  MapOutputLocation[] locateMapOutputs(String taskId, 
+                                       String[][] mapTasksNeeded
+                                       ) throws IOException;
 
   /**
    * The task tracker calls this once, to discern where it can find
"
hadoop,1dafad3980c5dd7f7385862f2a75e3d7b95bee24,"Fix HADOOP-168.  Add IOException to throws of all MapReduce RPC protocol methods.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@397310 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-26 21:43:05,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobSubmissionProtocol.java b/src/java/org/apache/hadoop/mapred/JobSubmissionProtocol.java
index b371425..978b992 100644
--- a/src/java/org/apache/hadoop/mapred/JobSubmissionProtocol.java
+++ b/src/java/org/apache/hadoop/mapred/JobSubmissionProtocol.java
@@ -34,28 +34,28 @@
      * Get the current status of the cluster
      * @return summary of the state of the cluster
      */
-    public ClusterStatus getClusterStatus();
+    public ClusterStatus getClusterStatus() throws IOException;
     
     /**
      * Kill the indicated job
      */
-    public void killJob(String jobid);
+    public void killJob(String jobid) throws IOException;
 
     /**
      * Grab a handle to a job that is already known to the JobTracker
      */
-    public JobProfile getJobProfile(String jobid);
+    public JobProfile getJobProfile(String jobid) throws IOException;
 
     /**
      * Grab a handle to a job that is already known to the JobTracker
      */
-    public JobStatus getJobStatus(String jobid);
+    public JobStatus getJobStatus(String jobid) throws IOException;
 
     /**
      * Grab a bunch of info on the tasks that make up the job
      */
-    public TaskReport[] getMapTaskReports(String jobid);
-    public TaskReport[] getReduceTaskReports(String jobid);
+    public TaskReport[] getMapTaskReports(String jobid) throws IOException;
+    public TaskReport[] getReduceTaskReports(String jobid) throws IOException;
 
     /**
      * A MapReduce system always operates on a single filesystem.  This 
"
hadoop,dda9f04394ef9334808966d882487a9b171faeb3,"Fix HADOOP-160.  Remove some uneeded synchronization around time-consuming operations in the TaskTracker.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@396959 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-25 19:26:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 6861b7a..04febf9 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -338,6 +338,8 @@
         } else {
           reduceTotal++;
         }
+      }
+      synchronized (tip) {
         try {
           tip.launchTask();
         } catch (Throwable ie) {
@@ -424,7 +426,7 @@
          * Some fields in the Task object need to be made machine-specific.
          * So here, edit the Task's fields appropriately.
          */
-        void localizeTask(Task t) throws IOException {
+        private void localizeTask(Task t) throws IOException {
             this.jobConf.deleteLocalFiles(SUBDIR + ""/"" + task.getTaskId());
             Path localJobFile =
               this.jobConf.getLocalPath(SUBDIR+""/""+t.getTaskId()+""/""+""job.xml"");
@@ -460,7 +462,7 @@
 
         /**
          */
-        public TaskStatus createStatus() {
+        public synchronized TaskStatus createStatus() {
             TaskStatus status = new TaskStatus(task.getTaskId(), task.isMapTask(), progress, runstate, diagnosticInfo.toString(), (stateString == null) ? """" : stateString, """");
             if (diagnosticInfo.length() > 0) {
                 diagnosticInfo = new StringBuffer();
@@ -520,7 +522,7 @@
         /**
          * The task has actually finished running.
          */
-        public synchronized void taskFinished() {
+        public void taskFinished() {
             long start = System.currentTimeMillis();
 
             //
@@ -538,13 +540,17 @@
             // Change state to success or failure, depending on whether
             // task was 'done' before terminating
             //
-            if (done) {
-                runstate = TaskStatus.SUCCEEDED;
-            } else {
-                if (!wasKilled) {
-                  failures += 1;
-                }
-                runstate = TaskStatus.FAILED;
+            boolean needCleanup = false;
+            synchronized (this) {
+              if (done) {
+                  runstate = TaskStatus.SUCCEEDED;
+              } else {
+                  if (!wasKilled) {
+                    failures += 1;
+                  }
+                  runstate = TaskStatus.FAILED;
+              }
+              needCleanup = wasKilled || runstate == TaskStatus.FAILED;
             }
 
             //
@@ -553,7 +559,7 @@
             // if the task succeeded, and its results might be useful
             // later on to downstream job processing.
             //
-            if (wasKilled || runstate == TaskStatus.FAILED) {
+            if (needCleanup) {
                 try {
                     cleanup();
                 } catch (IOException ie) {
@@ -594,10 +600,8 @@
             if (runstate == TaskStatus.SUCCEEDED) {
               LOG.info(""Reporting output lost:""+task.getTaskId());
               runstate = TaskStatus.FAILED;       // change status to failure
-              synchronized (TaskTracker.this) {   // force into next heartbeat
-                runningTasks.put(task.getTaskId(), this);
-                mapTotal++;
-              }
+              runningTasks.put(task.getTaskId(), this);
+              mapTotal++;
             } else {
               LOG.warning(""Output already reported lost:""+task.getTaskId());
             }
@@ -699,8 +703,11 @@
     /**
      * The task is no longer running.  It may not have completed successfully
      */
-    synchronized void reportTaskFinished(String taskid) {
-        TaskInProgress tip = (TaskInProgress) tasks.get(taskid);
+    void reportTaskFinished(String taskid) {
+        TaskInProgress tip;
+        synchronized (this) {
+          tip = (TaskInProgress) tasks.get(taskid);
+        }
         if (tip != null) {
           tip.taskFinished();
         } else {
"
hadoop,da3cf9a45dd2c54db83c6ee17b8113f765f84aaa,"Fix HADOOP-162.  Stop generating ConcurrentModificationExceptions when releasing file locks.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@396721 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-24 23:23:37,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index 7b6505b..b58da07 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -37,6 +37,10 @@
  ***************************************************/
 class FSNamesystem implements FSConstants {
     public static final Logger LOG = LogFormatter.getLogger(""org.apache.hadoop.fs.FSNamesystem"");
+    static {
+      // for debugging the pending Creates problems
+      LOG.setLevel(Level.FINE);
+    }
 
     //
     // Stores the correct file name hierarchy
@@ -391,7 +395,22 @@
                                                    ) throws IOException {
       LOG.info(""abandoning file in progress on "" + src.toString());
       synchronized (leases) {
-        internalReleaseCreate(src, holder);
+        // find the lease
+        Lease lease = (Lease) leases.get(holder);
+        if (lease != null) {
+          // remove the file from the lease
+          if (lease.completedCreate(src)) {
+            // if we found the file in the lease, remove it from pendingCreates
+            internalReleaseCreate(src, holder);
+          } else {
+            LOG.info(""Attempt by "" + holder.toString() + 
+                "" to release someone else's create lock on "" + 
+                src.toString());
+          }
+        } else {
+          LOG.info(""Attempt to release a lock from an unknown lease holder ""
+              + holder.toString() + "" for "" + src.toString());
+        }
       }
     }
 
@@ -818,33 +837,18 @@
      * @param holder The datanode that was creating the file
      */
     private void internalReleaseCreate(UTF8 src, UTF8 holder) {
-      // find the lease
-      Lease lease = (Lease) leases.get(holder);
-      if (lease != null) {
-        // remove the file from the lease
-        if (lease.completedCreate(src)) {
-          // if we found the file in the lease, remove it from pendingCreates
-          FileUnderConstruction v = 
-            (FileUnderConstruction) pendingCreates.remove(src);
-          if (v != null) {
-            LOG.info(""Removing "" + src + "" from pendingCreates for "" + 
-                     holder + "" (failure)"");
-            for (Iterator it2 = v.getBlocks().iterator(); it2.hasNext(); ) {
-              Block b = (Block) it2.next();
-              pendingCreateBlocks.remove(b);
-            }
-          } else {
-            LOG.info(""Attempt to release a create lock on "" + src.toString() +
-                     "" that was not in pendingCreates"");
-          }
-        } else {
-          LOG.info(""Attempt by "" + holder.toString() + 
-                   "" to release someone else's create lock on "" + 
-                   src.toString());
+      FileUnderConstruction v = 
+        (FileUnderConstruction) pendingCreates.remove(src);
+      if (v != null) {
+        LOG.info(""Removing "" + src + "" from pendingCreates for "" + 
+            holder + "" (failure)"");
+        for (Iterator it2 = v.getBlocks().iterator(); it2.hasNext(); ) {
+          Block b = (Block) it2.next();
+          pendingCreateBlocks.remove(b);
         }
       } else {
-        LOG.info(""Attempt to release a lock from an unknown lease holder ""
-                 + holder.toString() + "" for "" + src.toString());
+        LOG.info(""Attempt to release a create lock on "" + src.toString()
+                 + "" that was not in pendingCreates"");
       }
     }
 
"
hadoop,64f8343204e61aaabdbcbf4b48d2b26ac05ac0e9,"Fix for HADOOP-154 (fixed in other relevant places, too).


git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@396706 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-24 22:46:26,Andrzej Bialecki,"diff --git a/src/java/org/apache/hadoop/dfs/DFSck.java b/src/java/org/apache/hadoop/dfs/DFSck.java
index 74c6872..3279736 100644
--- a/src/java/org/apache/hadoop/dfs/DFSck.java
+++ b/src/java/org/apache/hadoop/dfs/DFSck.java
@@ -20,7 +20,6 @@
 import java.io.DataInputStream;
 import java.io.DataOutputStream;
 import java.io.IOException;
-import java.lang.reflect.Method;
 import java.net.InetSocketAddress;
 import java.net.Socket;
 import java.util.ArrayList;
@@ -495,7 +494,9 @@
 
     /** Return the actual replication factor. */
     public float getReplicationFactor() {
-      return (float)(totalBlocks * replication + overReplicatedBlocks - underReplicatedBlocks) / (float)totalBlocks;
+      if (totalBlocks != 0)
+        return (float)(totalBlocks * replication + overReplicatedBlocks - underReplicatedBlocks) / (float)totalBlocks;
+      else return 0.0f;
     }
 
     /** Return the number of under-replicated blocks. Note: missing blocks are not counted here.*/
@@ -560,7 +561,8 @@
       StringBuffer res = new StringBuffer();
       res.append(""Status: "" + (isHealthy() ? ""HEALTHY"" : ""CORRUPT""));
       res.append(""\n Total size:\t"" + totalSize + "" B"");
-      res.append(""\n Total blocks:\t"" + totalBlocks + "" (avg. block size ""
+      res.append(""\n Total blocks:\t"" + totalBlocks);
+      if (totalBlocks > 0) res.append("" (avg. block size ""
               + (totalSize / totalBlocks) + "" B)"");
       res.append(""\n Total dirs:\t"" + totalDirs);
       res.append(""\n Total files:\t"" + totalFiles);
@@ -571,11 +573,13 @@
         res.append(""\n  MISSING SIZE:\t\t"" + missingSize + "" B"");
         res.append(""\n  ********************************"");
       }
-      res.append(""\n Over-replicated blocks:\t"" + overReplicatedBlocks
-              + "" ("" + ((float)(overReplicatedBlocks * 100) / (float)totalBlocks)
+      res.append(""\n Over-replicated blocks:\t"" + overReplicatedBlocks);
+      if (totalBlocks > 0) res.append("" (""
+              + ((float)(overReplicatedBlocks * 100) / (float)totalBlocks)
               + "" %)"");
-      res.append(""\n Under-replicated blocks:\t"" + underReplicatedBlocks
-              + "" ("" + ((float)(underReplicatedBlocks * 100) / (float)totalBlocks)
+      res.append(""\n Under-replicated blocks:\t"" + underReplicatedBlocks);
+      if (totalBlocks > 0) res.append("" (""
+              + ((float)(underReplicatedBlocks * 100) / (float)totalBlocks)
               + "" %)"");
       res.append(""\n Target replication factor:\t"" + replication);
       res.append(""\n Real replication factor:\t"" + getReplicationFactor());
"
hadoop,6dd684dde8d3c4380519d6913f0632ea8263b89c,"Fix HADOOP-157.  Make dfs client wait long enough for locks on abandoned files to expire when creating files, so that when a task that writes to dfs fails, its replacements do not also immediately fail when they try to open the same files.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@396605 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-24 17:02:39,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/ClientProtocol.java b/src/java/org/apache/hadoop/dfs/ClientProtocol.java
index 37497e5..cc11a73 100644
--- a/src/java/org/apache/hadoop/dfs/ClientProtocol.java
+++ b/src/java/org/apache/hadoop/dfs/ClientProtocol.java
@@ -86,7 +86,7 @@
      * A null response means the NameNode could not allocate a block,
      * and that the caller should try again.
      */
-    public LocatedBlock addBlock(String src, String clientMachine) throws IOException;
+    public LocatedBlock addBlock(String src, String clientName) throws IOException;
 
     /**
      * A client that wants to abandon writing to the current file
"
hadoop,6dd684dde8d3c4380519d6913f0632ea8263b89c,"Fix HADOOP-157.  Make dfs client wait long enough for locks on abandoned files to expire when creating files, so that when a task that writes to dfs fails, its replacements do not also immediately fail when they try to open the same files.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@396605 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-24 17:02:39,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSClient.java b/src/java/org/apache/hadoop/dfs/DFSClient.java
index 6bf2975..564ac85 100644
--- a/src/java/org/apache/hadoop/dfs/DFSClient.java
+++ b/src/java/org/apache/hadoop/dfs/DFSClient.java
@@ -353,6 +353,9 @@
                         namenode.renewLease(clientName);
                         lastRenewed = System.currentTimeMillis();
                     } catch (IOException ie) {
+                      String err = StringUtils.stringifyException(ie);
+                      LOG.warning(""Problem renewing lease for "" + clientName +
+                                  "": "" + err);
                     }
                 }
                 try {
@@ -679,31 +682,15 @@
          */
         private synchronized void nextBlockOutputStream() throws IOException {
             boolean retry = false;
-            long start = System.currentTimeMillis();
+            long startTime = System.currentTimeMillis();
             do {
                 retry = false;
                 
-                long localstart = System.currentTimeMillis();
-                boolean blockComplete = false;
-                LocatedBlock lb = null;                
-                while (! blockComplete) {
-                    if (firstTime) {
-                        lb = namenode.create(src.toString(), clientName.toString(), localName, overwrite, replication);
-                    } else {
-                        lb = namenode.addBlock(src.toString(), localName);
-                    }
-
-                    if (lb == null) {
-                        try {
-                            Thread.sleep(400);
-                            if (System.currentTimeMillis() - localstart > 5000) {
-                                LOG.info(""Waiting to find new output block node for "" + (System.currentTimeMillis() - start) + ""ms"");
-                            }
-                        } catch (InterruptedException ie) {
-                        }
-                    } else {
-                        blockComplete = true;
-                    }
+                LocatedBlock lb;
+                if (firstTime) {
+                  lb = locateNewBlock();
+                } else {
+                  lb = locateFollowingBlock(startTime);
                 }
 
                 block = lb.getBlock();
@@ -721,7 +708,7 @@
                 } catch (IOException ie) {
                     // Connection failed.  Let's wait a little bit and retry
                     try {
-                        if (System.currentTimeMillis() - start > 5000) {
+                        if (System.currentTimeMillis() - startTime > 5000) {
                             LOG.info(""Waiting to find target node: "" + target);
                         }
                         Thread.sleep(6000);
@@ -756,6 +743,65 @@
             firstTime = false;
         }
 
+        private LocatedBlock locateNewBlock() throws IOException {     
+          int retries = 3;
+          while (true) {
+            while (true) {
+              try {
+                return namenode.create(src.toString(), clientName.toString(),
+                    localName, overwrite, replication);
+              } catch (RemoteException e) {
+                if (--retries == 0 || 
+                    ""org.apache.hadoop.dfs.NameNode.AlreadyBeingCreatedException"".
+                        equals(e.getClassName())) {
+                  throw e;
+                } else {
+                  // because failed tasks take upto LEASE_PERIOD to
+                  // release their pendingCreates files, if the file
+                  // we want to create is already being created, 
+                  // wait and try again.
+                  LOG.info(StringUtils.stringifyException(e));
+                  try {
+                    Thread.sleep(LEASE_PERIOD);
+                  } catch (InterruptedException ie) {
+                  }
+                }
+              }
+            }
+          }
+        }
+        
+        private LocatedBlock locateFollowingBlock(long start
+                                                  ) throws IOException {     
+          int retries = 5;
+          while (true) {
+            long localstart = System.currentTimeMillis();
+            while (true) {
+              try {
+                return namenode.addBlock(src.toString(), 
+                                         clientName.toString());
+              } catch (RemoteException e) {
+                if (--retries == 0 || 
+                    ""org.apache.hadoop.dfs.NameNode.NotReplicatedYetException"".
+                        equals(e.getClassName())) {
+                  throw e;
+                } else {
+                  LOG.info(StringUtils.stringifyException(e));
+                  if (System.currentTimeMillis() - localstart > 5000) {
+                    LOG.info(""Waiting for replication for "" + 
+                             (System.currentTimeMillis() - localstart)/1000 + 
+                             "" seconds"");
+                  }
+                  try {
+                    Thread.sleep(400);
+                  } catch (InterruptedException ie) {
+                  }
+                }                
+              }
+            }
+          } 
+        }
+
         /**
          * We're referring to the file pos here
          */
"
hadoop,6dd684dde8d3c4380519d6913f0632ea8263b89c,"Fix HADOOP-157.  Make dfs client wait long enough for locks on abandoned files to expire when creating files, so that when a task that writes to dfs fails, its replacements do not also immediately fail when they try to open the same files.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@396605 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-24 17:02:39,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index b8c63d6..7b6505b 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -233,8 +233,9 @@
      * writes data.  Subsequent items in the list must be provided in
      * the connection to the first datanode.
      * @return Return an array that consists of the block, plus a set
-     * of machines, or null if src is invalid for creation (based on
-     * {@link FSDirectory#isValidToCreate(UTF8)}.
+     * of machines
+     * @throws IOException if the filename is invalid
+     *         {@link FSDirectory#isValidToCreate(UTF8)}.
      */
     public synchronized Object[] startFile( UTF8 src, 
                                             UTF8 holder, 
@@ -242,11 +243,12 @@
                                             boolean overwrite,
                                             short replication 
                                           ) throws IOException {
+      try {
         if (pendingCreates.get(src) != null) {
-          LOG.warning(""Cannot create file "" + src + "" for "" + holder +
+          String msg = ""Cannot create file "" + src + "" for "" + holder +
                        "" on "" + clientMachine + 
-                       "" because pendingCreates is non-null."");
-          return null;
+                       "" because pendingCreates is non-null."";
+          throw new NameNode.AlreadyBeingCreatedException(msg);
         }
 
         if( replication > maxReplication )
@@ -261,27 +263,28 @@
             + ""Requested replication "" + replication
             + "" is less than the required minimum "" + minReplication );
         
-        boolean fileValid = dir.isValidToCreate(src);
-        if (overwrite && ! fileValid) {
+        if (!dir.isValidToCreate(src)) {
+          if (overwrite) {
             delete(src);
-            fileValid = true;
-        }
-  
-        if ( ! fileValid) {
-          LOG.warning(""Cannot start file because it is invalid. src="" + src);
-          return null;
+          } else {
+            throw new IOException(""Can't create file "" + src + 
+                                  "", because the filename is invalid."");
+          }
         }
 
         // Get the array of replication targets 
         DatanodeInfo targets[] = chooseTargets(replication, null, clientMachine);
         if (targets.length < this.minReplication) {
-            LOG.warning(""Target-length is "" + targets.length +
-                "", below MIN_REPLICATION ("" + this.minReplication+ "")"");
-            return null;
-        }
+            throw new IOException(""Target-length is "" + targets.length +
+                                  "", below MIN_REPLICATION ("" + 
+                                  minReplication+ "")"");
+       }
 
         // Reserve space for this pending file
-        pendingCreates.put(src, new FileUnderConstruction( replication ));
+        pendingCreates.put(src, 
+                           new FileUnderConstruction(replication, 
+                                                     holder,
+                                                     clientMachine));
         LOG.fine(""Adding "" + src + "" to pendingCreates for "" + holder);
         synchronized (leases) {
             Lease lease = (Lease) leases.get(holder);
@@ -302,6 +305,10 @@
         results[0] = allocateBlock(src);
         results[1] = targets;
         return results;
+      } catch (IOException ie) {
+        LOG.warning(ie.getMessage());
+        throw ie;
+      }
     }
 
     /**
@@ -315,29 +322,42 @@
      * are replicated.  Will return an empty 2-elt array if we want the
      * client to ""try again later"".
      */
-    public synchronized Object[] getAdditionalBlock(UTF8 src, UTF8 clientMachine) {
-        Object results[] = null;
+    public synchronized Object[] getAdditionalBlock(UTF8 src, 
+                                                    UTF8 clientName
+                                                    ) throws IOException {
         FileUnderConstruction pendingFile = 
           (FileUnderConstruction) pendingCreates.get(src);
-        if (dir.getFile(src) == null && pendingFile != null) {
-            results = new Object[2];
-
-            //
-            // If we fail this, bad things happen!
-            //
-            if (checkFileProgress(src)) {
-                // Get the array of replication targets 
-                DatanodeInfo targets[] = chooseTargets(pendingFile.getReplication(), null, clientMachine);
-                if (targets.length < this.minReplication) {
-                    return null;
-                }
-
-                // Create next block
-                results[0] = allocateBlock(src);
-                results[1] = targets;
-            }
+        // make sure that we still have the lease on this file
+        if (pendingFile == null) {
+          throw new NameNode.LeaseExpiredException(""No lease on "" + src);
         }
-        return results;
+        if (!pendingFile.getClientName().equals(clientName)) {
+          throw new NameNode.LeaseExpiredException(""Lease mismatch on "" + src + 
+              "" owned by "" + pendingFile.getClientName() + 
+              "" and appended by "" + clientName);
+        }
+        if (dir.getFile(src) != null) {
+          throw new IOException(""File "" + src + "" created during write"");
+        }
+
+        //
+        // If we fail this, bad things happen!
+        //
+        if (!checkFileProgress(src)) {
+          throw new NameNode.NotReplicatedYetException(""Not replicated yet"");
+        }
+        
+        // Get the array of replication targets 
+        DatanodeInfo targets[] = chooseTargets(pendingFile.getReplication(), 
+            null, pendingFile.getClientMachine());
+        if (targets.length < this.minReplication) {
+          throw new IOException(""File "" + src + "" could only be replicated to "" +
+                                targets.length + "" nodes, instead of "" +
+                                minReplication);
+        }
+        
+        // Create next block
+        return new Object[]{allocateBlock(src), targets};
     }
 
     /**
@@ -347,8 +367,10 @@
         //
         // Remove the block from the pending creates list
         //
-        Vector pendingVector = (Vector) pendingCreates.get(src);
-        if (pendingVector != null) {
+        FileUnderConstruction pendingFile = 
+          (FileUnderConstruction) pendingCreates.get(src);
+        if (pendingFile != null) {
+            Vector pendingVector = pendingFile.getBlocks();
             for (Iterator it = pendingVector.iterator(); it.hasNext(); ) {
                 Block cur = (Block) it.next();
                 if (cur.compareTo(b) == 0) {
@@ -368,7 +390,9 @@
                                                    UTF8 holder
                                                    ) throws IOException {
       LOG.info(""abandoning file in progress on "" + src.toString());
-      internalReleaseCreate(src, holder);
+      synchronized (leases) {
+        internalReleaseCreate(src, holder);
+      }
     }
 
     /**
@@ -387,9 +411,11 @@
             return STILL_WAITING;
         }
         
-        FileUnderConstruction pendingFile = (FileUnderConstruction) pendingCreates.get(src);
-        int nrBlocks = pendingFile.size();
-        Block pendingBlocks[] = (Block[]) pendingFile.toArray(new Block[nrBlocks]);
+        FileUnderConstruction pendingFile = 
+            (FileUnderConstruction) pendingCreates.get(src);
+        Vector blocks = pendingFile.getBlocks();
+        int nrBlocks = blocks.size();
+        Block pendingBlocks[] = (Block[]) blocks.toArray(new Block[nrBlocks]);
 
         //
         // We have the pending blocks, but they won't have
@@ -473,7 +499,7 @@
         Block b = new Block();
         FileUnderConstruction v = 
           (FileUnderConstruction) pendingCreates.get(src);
-        v.add(b);
+        v.getBlocks().add(b);
         pendingCreateBlocks.add(b);
         return b;
     }
@@ -486,7 +512,7 @@
         FileUnderConstruction v = 
           (FileUnderConstruction) pendingCreates.get(src);
 
-        for (Iterator it = v.iterator(); it.hasNext(); ) {
+        for (Iterator it = v.getBlocks().iterator(); it.hasNext(); ) {
             Block b = (Block) it.next();
             TreeSet containingNodes = (TreeSet) blocksMap.get(b);
             if (containingNodes == null || containingNodes.size() < this.minReplication) {
@@ -639,8 +665,8 @@
     class Lease implements Comparable {
         public UTF8 holder;
         public long lastUpdate;
-        TreeSet locks = new TreeSet();
-        TreeSet creates = new TreeSet();
+        private TreeSet locks = new TreeSet();
+        private TreeSet creates = new TreeSet();
 
         public Lease(UTF8 holder) {
             this.holder = holder;
@@ -803,7 +829,7 @@
           if (v != null) {
             LOG.info(""Removing "" + src + "" from pendingCreates for "" + 
                      holder + "" (failure)"");
-            for (Iterator it2 = v.iterator(); it2.hasNext(); ) {
+            for (Iterator it2 = v.getBlocks().iterator(); it2.hasNext(); ) {
               Block b = (Block) it2.next();
               pendingCreateBlocks.remove(b);
             }
@@ -1416,15 +1442,35 @@
      * 
      * @author shv
      */
-    private class FileUnderConstruction extends Vector {
+    private class FileUnderConstruction {
       private short blockReplication; // file replication
+      private Vector blocks;
+      private UTF8 clientName;         // lease holder
+      private UTF8 clientMachine;
       
-      FileUnderConstruction( short replication ) throws IOException {
+      FileUnderConstruction(short replication,
+                            UTF8 clientName,
+                            UTF8 clientMachine) throws IOException {
         this.blockReplication = replication;
+        this.blocks = new Vector();
+        this.clientName = clientName;
+        this.clientMachine = clientMachine;
       }
       
       public short getReplication() {
         return this.blockReplication;
       }
+      
+      public Vector getBlocks() {
+        return blocks;
+      }
+      
+      public UTF8 getClientName() {
+        return clientName;
+      }
+      
+      public UTF8 getClientMachine() {
+        return clientMachine;
+      }
     }
 }
"
hadoop,6dd684dde8d3c4380519d6913f0632ea8263b89c,"Fix HADOOP-157.  Make dfs client wait long enough for locks on abandoned files to expire when creating files, so that when a task that writes to dfs fails, its replacements do not also immediately fail when they try to open the same files.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@396605 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-24 17:02:39,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/NameNode.java b/src/java/org/apache/hadoop/dfs/NameNode.java
index e2b1d4d..d7d85e7 100644
--- a/src/java/org/apache/hadoop/dfs/NameNode.java
+++ b/src/java/org/apache/hadoop/dfs/NameNode.java
@@ -18,7 +18,7 @@
 import org.apache.hadoop.io.*;
 import org.apache.hadoop.ipc.*;
 import org.apache.hadoop.conf.*;
-import org.apache.hadoop.util.LogFormatter;
+import org.apache.hadoop.util.*;
 
 import java.io.*;
 import java.util.logging.*;
@@ -140,6 +140,37 @@
     }
 
     /**
+     * The exception that happens when you ask to create a file that already
+     * is being created, but is not closed yet.
+     * @author Owen O'Malley
+     */
+    public static class AlreadyBeingCreatedException extends IOException {
+      public AlreadyBeingCreatedException(String msg) {
+        super(msg);
+      }
+    }
+    
+    /**
+     * The lease that was being used to create this file has expired.
+     * @author Owen O'Malley
+     */
+    public static class LeaseExpiredException extends IOException {
+      public LeaseExpiredException(String msg) {
+        super(msg);
+      }
+    }
+    
+    /**
+     * The file has not finished being written to enough datanodes yet.
+     * @author Owen O'Malley
+     */
+    public static class NotReplicatedYetException extends IOException {
+      public NotReplicatedYetException(String msg) {
+        super(msg);
+      }
+    }
+    
+    /**
      */
     public LocatedBlock create(String src, 
                                String clientName, 
@@ -152,9 +183,6 @@
                                                 new UTF8(clientMachine), 
                                                 overwrite,
                                                 replication);
-        if (results == null)
-            throw new IOException(""Cannot create file "" + src + "" on client "" + clientName);
-
         Block b = (Block) results[0];
         DatanodeInfo targets[] = (DatanodeInfo[]) results[1];
         return new LocatedBlock(b, targets);
@@ -162,27 +190,14 @@
 
     /**
      */
-    public LocatedBlock addBlock(String src, String clientMachine) throws IOException {
-        int retries = 5;
-        Object results[] = namesystem.getAdditionalBlock(new UTF8(src), new UTF8(clientMachine));
-        while (results != null && results[0] == null && retries > 0) {
-            try {
-                Thread.sleep(100);
-            } catch (InterruptedException ie) {
-            }
-            results = namesystem.getAdditionalBlock(new UTF8(src), new UTF8(clientMachine));
-            retries--;
-        }
-
-        if (results == null) {
-            throw new IOException(""Cannot obtain additional block for file "" + src);
-        } else if (results[0] == null) {
-            return null;
-        } else {
-            Block b = (Block) results[0];
-            DatanodeInfo targets[] = (DatanodeInfo[]) results[1];
-            return new LocatedBlock(b, targets);
-        }
+    public LocatedBlock addBlock(String src, 
+                                 String clientName) throws IOException {
+        UTF8 src8 = new UTF8(src);
+        UTF8 client8 = new UTF8(clientName);
+        Object[] results = namesystem.getAdditionalBlock(src8, client8);
+        Block b = (Block) results[0];
+        DatanodeInfo targets[] = (DatanodeInfo[]) results[1];
+        return new LocatedBlock(b, targets);            
     }
 
     /**
"
hadoop,6dd684dde8d3c4380519d6913f0632ea8263b89c,"Fix HADOOP-157.  Make dfs client wait long enough for locks on abandoned files to expire when creating files, so that when a task that writes to dfs fails, its replacements do not also immediately fail when they try to open the same files.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@396605 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-24 17:02:39,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/Client.java b/src/java/org/apache/hadoop/ipc/Client.java
index ae04325..4f5c6ad 100644
--- a/src/java/org/apache/hadoop/ipc/Client.java
+++ b/src/java/org/apache/hadoop/ipc/Client.java
@@ -29,8 +29,6 @@
 import java.io.FilterInputStream;
 import java.io.FilterOutputStream;
 
-import java.rmi.RemoteException;
-
 import java.util.Hashtable;
 import java.util.logging.Logger;
 import java.util.logging.Level;
@@ -39,6 +37,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.conf.Configurable;
 import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.WritableUtils;
 import org.apache.hadoop.io.UTF8;
 
 /** A client for an IPC service.  IPC calls take a single {@link Writable} as a
@@ -65,7 +64,7 @@
     int id;                                       // call id
     Writable param;                               // parameter
     Writable value;                               // value, null if error
-    String error;                                 // error, null if value
+    RemoteException error;                        // error, null if value
     long lastActivity;                            // time of last i/o
     boolean done;                                 // true when call is done
 
@@ -89,7 +88,7 @@
     }
 
     /** Update lastActivity with the current time. */
-    public synchronized void setResult(Writable value, String error) {
+    public synchronized void setResult(Writable value, RemoteException error) {
       this.value = value;
       this.error = error;
       this.done = true;
@@ -157,9 +156,10 @@
           Call call = (Call)calls.remove(new Integer(id));
           boolean isError = in.readBoolean();     // read if error
           if (isError) {
-            UTF8 utf8 = new UTF8();
-            utf8.readFields(in);                  // read error string
-            call.setResult(null, utf8.toString());
+            RemoteException ex = 
+              new RemoteException(WritableUtils.readString(in),
+                                  WritableUtils.readString(in));
+            call.setResult(null, ex);
           } else {
             Writable value = makeValue();
             try {
@@ -300,7 +300,7 @@
       } while (!call.done && wait > 0);
 
       if (call.error != null) {
-        throw new RemoteException(call.error);
+        throw call.error;
       } else if (!call.done) {
         throw new IOException(""timed out waiting for response"");
       } else {
"
hadoop,6dd684dde8d3c4380519d6913f0632ea8263b89c,"Fix HADOOP-157.  Make dfs client wait long enough for locks on abandoned files to expire when creating files, so that when a task that writes to dfs fails, its replacements do not also immediately fail when they try to open the same files.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@396605 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-24 17:02:39,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/Server.java b/src/java/org/apache/hadoop/ipc/Server.java
index 4ab6ba8..f3e6046 100644
--- a/src/java/org/apache/hadoop/ipc/Server.java
+++ b/src/java/org/apache/hadoop/ipc/Server.java
@@ -38,6 +38,7 @@
 import org.apache.hadoop.conf.Configurable;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.WritableUtils;
 import org.apache.hadoop.io.UTF8;
 
 /** An abstract IPC service.  IPC calls take a single {@link Writable} as a
@@ -210,15 +211,14 @@
             LOG.fine(getName() + "": has #"" + call.id + "" from "" +
                      call.connection.socket.getInetAddress().getHostAddress());
           
+          String errorClass = null;
           String error = null;
           Writable value = null;
           try {
             value = call(call.param);             // make the call
-          } catch (IOException e) {
+          } catch (Throwable e) {
             LOG.log(Level.INFO, getName() + "" call error: "" + e, e);
-            error = getStackTrace(e);
-          } catch (Exception e) {
-            LOG.log(Level.INFO, getName() + "" call error: "" + e, e);
+            errorClass = e.getClass().getName();
             error = getStackTrace(e);
           }
             
@@ -226,9 +226,12 @@
           synchronized (out) {
             out.writeInt(call.id);                // write call id
             out.writeBoolean(error!=null);        // write error flag
-            if (error != null)
-              value = new UTF8(error);
-            value.write(out);                     // write value
+            if (error == null) {
+              value.write(out);
+            } else {
+              WritableUtils.writeString(out, errorClass);
+              WritableUtils.writeString(out, error);
+            }
             out.flush();
           }
 
"
hadoop,6dd684dde8d3c4380519d6913f0632ea8263b89c,"Fix HADOOP-157.  Make dfs client wait long enough for locks on abandoned files to expire when creating files, so that when a task that writes to dfs fails, its replacements do not also immediately fail when they try to open the same files.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@396605 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-24 17:02:39,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskRunner.java b/src/java/org/apache/hadoop/mapred/TaskRunner.java
index 0af86a9..c1adb31 100644
--- a/src/java/org/apache/hadoop/mapred/TaskRunner.java
+++ b/src/java/org/apache/hadoop/mapred/TaskRunner.java
@@ -269,7 +269,7 @@
       logStream(process.getInputStream());        // normally empty
       
       int exit_code = process.waitFor();
-      if (exit_code != 0) {
+      if (!killed && exit_code != 0) {
         throw new IOException(""Task process exit with nonzero status of "" +
                               exit_code + ""."");
       }
"
hadoop,6dd684dde8d3c4380519d6913f0632ea8263b89c,"Fix HADOOP-157.  Make dfs client wait long enough for locks on abandoned files to expire when creating files, so that when a task that writes to dfs fails, its replacements do not also immediately fail when they try to open the same files.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@396605 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-24 17:02:39,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 631a6f5..6861b7a 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -282,11 +282,16 @@
             synchronized (this) {
                 for (Iterator it = runningTasks.values().iterator(); it.hasNext(); ) {
                     TaskInProgress tip = (TaskInProgress) it.next();
+                    long timeSinceLastReport = System.currentTimeMillis() - 
+                                               tip.getLastProgressReport();
                     if ((tip.getRunState() == TaskStatus.RUNNING) &&
-                        (System.currentTimeMillis() - tip.getLastProgressReport() > this.taskTimeout) &&
+                        (timeSinceLastReport > this.taskTimeout) &&
                         !tip.wasKilled) {
-                        LOG.info(""Task "" + tip.getTask().getTaskId() + "" timed out.  Killing."");
-                        tip.reportDiagnosticInfo(""Timed out."");
+                        String msg = ""Task failed to report status for "" +
+                                     (timeSinceLastReport / 1000) + 
+                                     "" seconds. Killing."";
+                        LOG.info(tip.getTask().getTaskId() + "": "" + msg);
+                        tip.reportDiagnosticInfo(msg);
                         tip.killAndCleanup(true);
                     }
                 }
"
hadoop,8ab9d9b4b326ee268bf60dae576cfbbf39e2018b,"Fix HADOOP-69.  NPE when getting hints for a non-existant file chunk.  Contributed by Bryan Pendelton.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@395665 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-20 18:16:28,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index 0ca69b1..b8c63d6 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -617,9 +617,11 @@
             for (int i = startBlock; i <= endBlock; i++) {
                 TreeSet containingNodes = (TreeSet) blocksMap.get(blocks[i]);
                 Vector v = new Vector();
-                for (Iterator it = containingNodes.iterator(); it.hasNext(); ) {
+                if (containingNodes != null) {
+                  for (Iterator it =containingNodes.iterator(); it.hasNext();) {
                     DatanodeInfo cur = (DatanodeInfo) it.next();
                     v.add(cur.getHost());
+                  }
                 }
                 hosts[i-startBlock] = (UTF8[]) v.toArray(new UTF8[v.size()]);
             }
"
hadoop,0794e232d52cd53abc4d450f4feff61298000210,"Fix HADOOP-151.  Close a potential socket leak.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@395444 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-19 23:55:32,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/RPC.java b/src/java/org/apache/hadoop/ipc/RPC.java
index 77c9366..a0d4a5c 100644
--- a/src/java/org/apache/hadoop/ipc/RPC.java
+++ b/src/java/org/apache/hadoop/ipc/RPC.java
@@ -121,25 +121,33 @@
 
   }
 
-  //TODO mb@media-style.com: static client or non-static client?
   private static Client CLIENT;
 
+  private static synchronized Client getClient(Configuration conf) {
+    // Construct & cache client.  The configuration is only used for timeout,
+    // and Clients have connection pools.  So we can either (a) lose some
+    // connection pooling and leak sockets, or (b) use the same timeout for all
+    // configurations.  Since the IPC is usually intended globally, not
+    // per-job, we choose (a).
+    if (CLIENT == null) {
+      CLIENT = new Client(ObjectWritable.class, conf);
+    }
+    return CLIENT;
+  }
+
   private static class Invoker implements InvocationHandler {
     private InetSocketAddress address;
+    private Client client;
 
     public Invoker(InetSocketAddress address, Configuration conf) {
       this.address = address;
-      CLIENT = (Client) conf.getObject(Client.class.getName());
-      if(CLIENT == null) {
-          CLIENT = new Client(ObjectWritable.class, conf);
-          conf.setObject(Client.class.getName(), CLIENT);
-      }
+      this.client = getClient(conf);
     }
 
     public Object invoke(Object proxy, Method method, Object[] args)
       throws Throwable {
       ObjectWritable value = (ObjectWritable)
-        CLIENT.call(new Invocation(method, args), address);
+        client.call(new Invocation(method, args), address);
       return value.get();
     }
   }
@@ -160,12 +168,7 @@
     Invocation[] invocations = new Invocation[params.length];
     for (int i = 0; i < params.length; i++)
       invocations[i] = new Invocation(method, params[i]);
-    CLIENT = (Client) conf.getObject(Client.class.getName());
-    if(CLIENT == null) {
-        CLIENT = new Client(ObjectWritable.class, conf);
-        conf.setObject(Client.class.getName(), CLIENT);
-    }
-    Writable[] wrappedValues = CLIENT.call(invocations, addrs);
+    Writable[] wrappedValues = getClient(conf).call(invocations, addrs);
     
     if (method.getReturnType() == Void.TYPE) {
       return null;
"
hadoop,4e31745424b907c970f9711a9fd80af59dd3c341,"Fix for HADOOP-133.  Retry pings from child to parent, in case of (local) communcation problems.  Also log exit status, so that one can distinguish patricide from other deaths.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@395067 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-18 22:04:09,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/LocalJobRunner.java b/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
index f02b3d6..9cd63e9 100644
--- a/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
+++ b/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
@@ -154,7 +154,9 @@
       // Ignore for now
     }
 
-    public void ping(String taskid) throws IOException {}
+    public boolean ping(String taskid) throws IOException {
+      return true;
+    }
 
     public void done(String taskId) throws IOException {
       int taskIndex = mapIds.indexOf(taskId);
"
hadoop,4e31745424b907c970f9711a9fd80af59dd3c341,"Fix for HADOOP-133.  Retry pings from child to parent, in case of (local) communcation problems.  Also log exit status, so that one can distinguish patricide from other deaths.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@395067 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-18 22:04:09,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskRunner.java b/src/java/org/apache/hadoop/mapred/TaskRunner.java
index 12f3644..0af86a9 100644
--- a/src/java/org/apache/hadoop/mapred/TaskRunner.java
+++ b/src/java/org/apache/hadoop/mapred/TaskRunner.java
@@ -260,7 +260,6 @@
   private void runChild(String[] args, File dir) throws IOException {
     this.process = Runtime.getRuntime().exec(args, null, dir);
     try {
-      StringBuffer errorBuf = new StringBuffer();
       new Thread() {
         public void run() {
           logStream(process.getErrorStream());    // copy log output
@@ -269,8 +268,10 @@
         
       logStream(process.getInputStream());        // normally empty
       
-      if (this.process.waitFor() != 0) {
-        throw new IOException(""Task process exit with nonzero status."");
+      int exit_code = process.waitFor();
+      if (exit_code != 0) {
+        throw new IOException(""Task process exit with nonzero status of "" +
+                              exit_code + ""."");
       }
       
     } catch (InterruptedException e) {
"
hadoop,4e31745424b907c970f9711a9fd80af59dd3c341,"Fix for HADOOP-133.  Retry pings from child to parent, in case of (local) communcation problems.  Also log exit status, so that one can distinguish patricide from other deaths.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@395067 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-18 22:04:09,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 2697686..0089529 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -653,10 +653,8 @@
     }
 
     /** Child checking to see if we're alive.  Normally does nothing.*/
-    public synchronized void ping(String taskid) throws IOException {
-      if (tasks.get(taskid) == null) {
-        throw new IOException(""No such task id.""); // force child exit
-      }
+    public synchronized boolean ping(String taskid) throws IOException {
+      return tasks.get(taskid) != null;
     }
 
     /**
@@ -748,12 +746,23 @@
                                          final String taskid) {
           Thread thread = new Thread(new Runnable() {
               public void run() {
+                final int MAX_RETRIES = 3;
+                int remainingRetries = MAX_RETRIES;
                 while (true) {
                   try {
-                    umbilical.ping(taskid);
+                    if (!umbilical.ping(taskid)) {
+                      LOG.log(Level.WARNING, ""Parent died.  Exiting ""+taskid);
+                      System.exit(66);
+                    }
+                    remainingRetries = MAX_RETRIES;
                   } catch (Throwable t) {
-                    LOG.log(Level.WARNING, ""Parent died.  Exiting ""+taskid, t);
-                    System.exit(1);
+                    String msg = StringUtils.stringifyException(t);
+                    LOG.info(""Ping exception: "" + msg);
+                    remainingRetries -=1;
+                    if (remainingRetries == 0) {
+                      LOG.log(Level.WARNING, ""Last retry, killing ""+taskid);
+                      System.exit(65);
+                    }
                   }
                   try {
                     Thread.sleep(1000);
"
hadoop,4e31745424b907c970f9711a9fd80af59dd3c341,"Fix for HADOOP-133.  Retry pings from child to parent, in case of (local) communcation problems.  Also log exit status, so that one can distinguish patricide from other deaths.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@395067 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-18 22:04:09,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java b/src/java/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java
index 613228b..625bbc4 100644
--- a/src/java/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java
+++ b/src/java/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java
@@ -42,8 +42,10 @@
    */
   void reportDiagnosticInfo(String taskid, String trace) throws IOException;
 
-  /** Periodically called by child to check if parent is still alive. */
-  void ping(String taskid) throws IOException;
+  /** Periodically called by child to check if parent is still alive. 
+   * @return True if the task is known
+   */
+  boolean ping(String taskid) throws IOException;
 
   /** Report that the task is successfully completed.  Failure is assumed if
    * the task process exits without calling this. */
"
hadoop,0f31b06579458e048bf378e7347610a269753e75,"Fix for HADOOP-134.  Don't hang jobs when the tasktracker is misconfigured to use an un-writable local directory.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@395058 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-18 21:49:46,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index 253874c..b7ef321 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -80,14 +80,6 @@
         return new InetSocketAddress(host, port);
     }
 
-    private static String stringifyException(Exception e) {
-      StringWriter stm = new StringWriter();
-      PrintWriter wrt = new PrintWriter(stm);
-      e.printStackTrace(wrt);
-      wrt.close();
-      return stm.toString();
-    }
-
     private static Vector subThreadList = null;
     DatanodeProtocol namenode;
     FSDataset data;
@@ -510,7 +502,7 @@
                 } catch (IOException ie) {
                   if (out2 != null) {
                     LOG.info(""Exception connecting to mirror "" + mirrorNode 
-                             + ""\n"" + stringifyException(ie));
+                             + ""\n"" + StringUtils.stringifyException(ie));
                     try {
                       out2.close();
                       in2.close();
@@ -548,7 +540,7 @@
                         out2.write(buf, 0, bytesRead);
                       } catch (IOException out2e) {
                         LOG.info(""Exception writing to mirror "" + mirrorNode 
-                            + ""\n"" + stringifyException(out2e));
+                            + ""\n"" + StringUtils.stringifyException(out2e));
                         //
                         // If stream-copy fails, continue 
                         // writing to disk.  We shouldn't 
@@ -577,7 +569,7 @@
                       out2.writeLong(len);
                     } catch (IOException ie) {
                       LOG.info(""Exception writing to mirror "" + mirrorNode 
-                          + ""\n"" + stringifyException(ie));
+                          + ""\n"" + StringUtils.stringifyException(ie));
                       try {
                         out2.close();
                         in2.close();
@@ -612,7 +604,7 @@
                   }
                 } catch (IOException ie) {
                   LOG.info(""Exception writing to mirror "" + mirrorNode 
-                      + ""\n"" + stringifyException(ie));
+                      + ""\n"" + StringUtils.stringifyException(ie));
                   try {
                     out2.close();
                     in2.close();
"
hadoop,0f31b06579458e048bf378e7347610a269753e75,"Fix for HADOOP-134.  Don't hang jobs when the tasktracker is misconfigured to use an un-writable local directory.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@395058 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-18 21:49:46,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index e71c4e5..2697686 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -19,7 +19,7 @@
 import org.apache.hadoop.io.*;
 import org.apache.hadoop.ipc.*;
 import org.apache.hadoop.conf.*;
-import org.apache.hadoop.util.LogFormatter;
+import org.apache.hadoop.util.*;
 
 import java.io.*;
 import java.net.*;
@@ -106,9 +106,10 @@
      * close().
      */
     void initialize() throws IOException {
-        this.taskTrackerName = ""tracker_"" + (Math.abs(r.nextInt()) % 100000);
-        LOG.info(""Starting tracker "" + taskTrackerName);
         this.localHostname = InetAddress.getLocalHost().getHostName();
+        this.taskTrackerName = ""tracker_"" + localHostname + ""_"" +
+                               (Math.abs(r.nextInt()) % 100000);
+        LOG.info(""Starting tracker "" + taskTrackerName);
 
         new JobConf(this.fConf).deleteLocalFiles(SUBDIR);
 
@@ -267,17 +268,7 @@
             if (mapTotal < maxCurrentTasks || reduceTotal < maxCurrentTasks) {
                 Task t = jobClient.pollForNewTask(taskTrackerName);
                 if (t != null) {
-                    TaskInProgress tip = new TaskInProgress(t, this.fConf);
-                    synchronized (this) {
-                      tasks.put(t.getTaskId(), tip);
-                      if (t.isMapTask()) {
-                          mapTotal++;
-                      } else {
-                          reduceTotal++;
-                      }
-                      runningTasks.put(t.getTaskId(), tip);
-                    }
-                    tip.launchTask();
+                  startNewTask(t);
                 }
             }
 
@@ -322,6 +313,39 @@
     }
 
     /**
+     * Start a new task.
+     * All exceptions are handled locally, so that we don't mess up the
+     * task tracker.
+     */
+    private void startNewTask(Task t) {
+      TaskInProgress tip = new TaskInProgress(t, this.fConf);
+      synchronized (this) {
+        tasks.put(t.getTaskId(), tip);
+        runningTasks.put(t.getTaskId(), tip);
+        boolean isMap = t.isMapTask();
+        if (isMap) {
+          mapTotal++;
+        } else {
+          reduceTotal++;
+        }
+        try {
+          tip.launchTask();
+        } catch (Throwable ie) {
+          tip.runstate = TaskStatus.FAILED;
+          try {
+            tip.cleanup();
+          } catch (Throwable ie2) {
+            // Ignore it, we are just trying to cleanup.
+          }
+          String error = StringUtils.stringifyException(ie);
+          tip.reportDiagnosticInfo(error);
+          LOG.info(error);
+        }
+      }
+    }
+    
+
+    /**
      * The server retry loop.  
      * This while-loop attempts to connect to the JobTracker.  It only 
      * loops when the old TaskTracker has gone bad (its state is
@@ -377,12 +401,13 @@
 
         /**
          */
-        public TaskInProgress(Task task, Configuration conf) throws IOException {
+        public TaskInProgress(Task task, Configuration conf) {
             this.task = task;
+            this.progress = 0.0f;
+            this.runstate = TaskStatus.UNASSIGNED;
+            stateString = ""initializing"";
             this.lastProgressReport = System.currentTimeMillis();
             this.jobConf = new JobConf(conf);
-            this.jobConf.deleteLocalFiles(SUBDIR + ""/"" + task.getTaskId());
-            localizeTask(task);
         }
 
         /**
@@ -390,6 +415,7 @@
          * So here, edit the Task's fields appropriately.
          */
         void localizeTask(Task t) throws IOException {
+            this.jobConf.deleteLocalFiles(SUBDIR + ""/"" + task.getTaskId());
             Path localJobFile =
               this.jobConf.getLocalPath(SUBDIR+""/""+t.getTaskId()+""/""+""job.xml"");
             Path localJarFile =
@@ -436,9 +462,8 @@
          * Kick off the task execution
          */
         public synchronized void launchTask() throws IOException {
-            this.progress = 0.0f;
+            localizeTask(task);
             this.runstate = TaskStatus.RUNNING;
-            this.diagnosticInfo = new StringBuffer();
             this.runner = task.createRunner(TaskTracker.this);
             this.runner.start();
         }
"
hadoop,7079103f645141f866cfeb8ec4015d0d537c568c,"Fix for HADOOP-139.  Fix a potential deadlock in LocalFileSystem.lock().  Contributed by Igor Bolotin.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@395003 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-18 19:17:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/LocalFileSystem.java b/src/java/org/apache/hadoop/fs/LocalFileSystem.java
index 537a4f7..1ee2ff8 100644
--- a/src/java/org/apache/hadoop/fs/LocalFileSystem.java
+++ b/src/java/org/apache/hadoop/fs/LocalFileSystem.java
@@ -224,45 +224,54 @@
       return workingDir;
     }
     
-    public synchronized void lock(Path p, boolean shared) throws IOException {
-        File f = pathToFile(p);
-        f.createNewFile();
+    public void lock(Path p, boolean shared) throws IOException {
+      File f = pathToFile(p);
+      f.createNewFile();
 
-        FileLock lockObj = null;
-        if (shared) {
-            FileInputStream lockData = new FileInputStream(f);
-            lockObj = lockData.getChannel().lock(0L, Long.MAX_VALUE, shared);
-            sharedLockDataSet.put(f, lockData);
-        } else {
-            FileOutputStream lockData = new FileOutputStream(f);
-            lockObj = lockData.getChannel().lock(0L, Long.MAX_VALUE, shared);
-            nonsharedLockDataSet.put(f, lockData);
+      if (shared) {
+        FileInputStream lockData = new FileInputStream(f);
+        FileLock lockObj =
+          lockData.getChannel().lock(0L, Long.MAX_VALUE, shared);
+        synchronized (this) {
+          sharedLockDataSet.put(f, lockData);
+          lockObjSet.put(f, lockObj);
         }
-        lockObjSet.put(f, lockObj);
+      } else {
+        FileOutputStream lockData = new FileOutputStream(f);
+        FileLock lockObj = lockData.getChannel().lock(0L, Long.MAX_VALUE, shared);
+        synchronized (this) {
+          nonsharedLockDataSet.put(f, lockData);
+          lockObjSet.put(f, lockObj);
+        }
+      }
     }
 
-    public synchronized void release(Path p) throws IOException {
-        File f = pathToFile(p);
-        FileLock lockObj = (FileLock) lockObjSet.get(f);
-        FileInputStream sharedLockData = (FileInputStream) sharedLockDataSet.get(f);
-        FileOutputStream nonsharedLockData = (FileOutputStream) nonsharedLockDataSet.get(f);
+    public void release(Path p) throws IOException {
+      File f = pathToFile(p);
 
-        if (lockObj == null) {
-            throw new IOException(""Given target not held as lock"");
-        }
-        if (sharedLockData == null && nonsharedLockData == null) {
-            throw new IOException(""Given target not held as lock"");
-        }
+      FileLock lockObj;
+      FileInputStream sharedLockData;
+      FileOutputStream nonsharedLockData;
+      synchronized (this) {
+        lockObj = (FileLock) lockObjSet.remove(f);
+        sharedLockData = (FileInputStream) sharedLockDataSet.remove(f);
+        nonsharedLockData = (FileOutputStream) nonsharedLockDataSet.remove(f);
+      }
+ 
+      if (lockObj == null) {
+        throw new IOException(""Given target not held as lock"");
+      }
+      if (sharedLockData == null && nonsharedLockData == null) {
+        throw new IOException(""Given target not held as lock"");
+      }
 
-        lockObj.release();
-        lockObjSet.remove(f);
-        if (sharedLockData != null) {
-            sharedLockData.close();
-            sharedLockDataSet.remove(f);
-        } else {
-            nonsharedLockData.close();
-            nonsharedLockDataSet.remove(f);
-        }
+      lockObj.release();
+
+      if (sharedLockData != null) {
+        sharedLockData.close();
+      } else {
+        nonsharedLockData.close();
+      }
     }
 
     // In the case of the local filesystem, we can just rename the file.
"
hadoop,3512203f6c54033db1d52118edf595e5b5e1335e,"Fix HADOOP-118.  Improved cleanup of abandoned file creations in DFS.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@395000 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-18 18:58:49,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/ClientProtocol.java b/src/java/org/apache/hadoop/dfs/ClientProtocol.java
index 5fdb421..37497e5 100644
--- a/src/java/org/apache/hadoop/dfs/ClientProtocol.java
+++ b/src/java/org/apache/hadoop/dfs/ClientProtocol.java
@@ -95,8 +95,11 @@
      *
      * Any blocks that have been written for the file will be 
      * garbage-collected.
+     * @param src The filename
+     * @param holder The datanode holding the lease
      */
-    public void abandonFileInProgress(String src) throws IOException;
+    public void abandonFileInProgress(String src, 
+                                      String holder) throws IOException;
 
     /**
      * The client is done writing data to the given filename, and would 
"
hadoop,3512203f6c54033db1d52118edf595e5b5e1335e,"Fix HADOOP-118.  Improved cleanup of abandoned file creations in DFS.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@395000 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-18 18:58:49,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSClient.java b/src/java/org/apache/hadoop/dfs/DFSClient.java
index ab773dc..6bf2975 100644
--- a/src/java/org/apache/hadoop/dfs/DFSClient.java
+++ b/src/java/org/apache/hadoop/dfs/DFSClient.java
@@ -48,7 +48,48 @@
     String clientName;
     Daemon leaseChecker;
     private Configuration conf;
+    
+    /**
+     * A map from name -> DFSOutputStream of files that are currently being
+     * written by this client.
+     */
+    private TreeMap pendingCreates = new TreeMap();
+    
+    /**
+     * A class to track the list of DFS clients, so that they can be closed
+     * on exit.
+     * @author Owen O'Malley
+     */
+    private static class ClientFinalizer extends Thread {
+      private List clients = new ArrayList();
 
+      public synchronized void addClient(DFSClient client) {
+        clients.add(client);
+      }
+
+      public synchronized void run() {
+        Iterator itr = clients.iterator();
+        while (itr.hasNext()) {
+          DFSClient client = (DFSClient) itr.next();
+          if (client.running) {
+            try {
+              client.close();
+            } catch (IOException ie) {
+              System.err.println(""Error closing client"");
+              ie.printStackTrace();
+            }
+          }
+        }
+      }
+    }
+
+    // add a cleanup thread
+    private static ClientFinalizer clientFinalizer = new ClientFinalizer();
+    static {
+      Runtime.getRuntime().addShutdownHook(clientFinalizer);
+    }
+
+        
     /** 
      * Create a new DFSClient connected to the given namenode server.
      */
@@ -70,14 +111,40 @@
         this.leaseChecker.start();
     }
 
+    private void checkOpen() throws IOException {
+      if (!running) {
+        IOException result = new IOException(""Filesystem closed"");
+        throw result;
+      }
+    }
+    
     /**
+     * Close the file system, abadoning all of the leases and files being
+     * created.
      */
     public void close() throws IOException {
+      // synchronize in here so that we don't need to change the API
+      synchronized (this) {
+        checkOpen();
+        synchronized (pendingCreates) {
+          Iterator file_itr = pendingCreates.keySet().iterator();
+          while (file_itr.hasNext()) {
+            String name = (String) file_itr.next();
+            try {
+              namenode.abandonFileInProgress(name, clientName);
+            } catch (IOException ie) {
+              System.err.println(""Exception abandoning create lock on "" + name);
+              ie.printStackTrace();
+            }
+          }
+          pendingCreates.clear();
+        }
         this.running = false;
         try {
             leaseChecker.join();
         } catch (InterruptedException ie) {
         }
+      }
     }
 
     /**
@@ -96,7 +163,8 @@
      * work.
      */
     public FSInputStream open(UTF8 src) throws IOException {
-        // Get block info from namenode
+        checkOpen();
+        //    Get block info from namenode
         return new DFSInputStream(src.toString());
     }
 
@@ -129,7 +197,12 @@
                                   boolean overwrite, 
                                   short replication
                                 ) throws IOException {
-        return new DFSOutputStream(src, overwrite, replication);
+      checkOpen();
+      FSOutputStream result = new DFSOutputStream(src, overwrite, replication);
+      synchronized (pendingCreates) {
+        pendingCreates.put(src.toString(), result);
+      }
+      return result;
     }
 
     /**
@@ -137,6 +210,7 @@
      * there.
      */
     public boolean rename(UTF8 src, UTF8 dst) throws IOException {
+        checkOpen();
         return namenode.rename(src.toString(), dst.toString());
     }
 
@@ -145,24 +219,28 @@
      * there.
      */
     public boolean delete(UTF8 src) throws IOException {
+        checkOpen();
         return namenode.delete(src.toString());
     }
 
     /**
      */
     public boolean exists(UTF8 src) throws IOException {
+        checkOpen();
         return namenode.exists(src.toString());
     }
 
     /**
      */
     public boolean isDirectory(UTF8 src) throws IOException {
+        checkOpen();
         return namenode.isDir(src.toString());
     }
 
     /**
      */
     public DFSFileInfo[] listPaths(UTF8 src) throws IOException {
+        checkOpen();
         return namenode.getListing(src.toString());
     }
 
@@ -187,6 +265,7 @@
     /**
      */
     public boolean mkdirs(UTF8 src) throws IOException {
+        checkOpen();
         return namenode.mkdirs(src.toString());
     }
 
@@ -456,6 +535,7 @@
          * Close it down!
          */
         public synchronized void close() throws IOException {
+            checkOpen();
             if (closed) {
                 throw new IOException(""Stream closed"");
             }
@@ -473,6 +553,7 @@
          * Basic read()
          */
         public synchronized int read() throws IOException {
+            checkOpen();
             if (closed) {
                 throw new IOException(""Stream closed"");
             }
@@ -493,6 +574,7 @@
          * Read the entire buffer.
          */
         public synchronized int read(byte buf[], int off, int len) throws IOException {
+            checkOpen();
             if (closed) {
                 throw new IOException(""Stream closed"");
             }
@@ -646,7 +728,8 @@
                     } catch (InterruptedException iex) {
                     }
                     if (firstTime) {
-                        namenode.abandonFileInProgress(src.toString());
+                        namenode.abandonFileInProgress(src.toString(), 
+                                                       clientName);
                     } else {
                         namenode.abandonBlock(block, src.toString());
                     }
@@ -684,6 +767,7 @@
          * Writes the specified byte to this output stream.
          */
         public synchronized void write(int b) throws IOException {
+            checkOpen();
             if (closed) {
                 throw new IOException(""Stream closed"");
             }
@@ -701,6 +785,7 @@
          */
       public synchronized void write(byte b[], int off, int len)
         throws IOException {
+            checkOpen();
             if (closed) {
                 throw new IOException(""Stream closed"");
             }
@@ -724,6 +809,7 @@
          * Flush the buffer, getting a stream to a new block if necessary.
          */
         public synchronized void flush() throws IOException {
+            checkOpen();
             if (closed) {
                 throw new IOException(""Stream closed"");
             }
@@ -854,20 +940,22 @@
          * resources associated with this stream.
          */
         public synchronized void close() throws IOException {
-            if (closed) {
-                throw new IOException(""Stream closed"");
-            }
-
+          checkOpen();
+          if (closed) {
+              throw new IOException(""Stream closed"");
+          }
+          
+          try {
             flush();
             if (filePos == 0 || bytesWrittenToBlock != 0) {
               try {
                 endBlock();
               } catch (IOException e) {
-                namenode.abandonFileInProgress(src.toString());
+                namenode.abandonFileInProgress(src.toString(), clientName);
                 throw e;
               }
             }
-
+            
             backupStream.close();
             backupFile.delete();
 
@@ -880,18 +968,23 @@
             long localstart = System.currentTimeMillis();
             boolean fileComplete = false;
             while (! fileComplete) {
-                fileComplete = namenode.complete(src.toString(), clientName.toString());
-                if (!fileComplete) {
-                    try {
-                        Thread.sleep(400);
-                        if (System.currentTimeMillis() - localstart > 5000) {
-                            LOG.info(""Could not complete file, retrying..."");
-                        }
-                    } catch (InterruptedException ie) {
-                    }
+              fileComplete = namenode.complete(src.toString(), clientName.toString());
+              if (!fileComplete) {
+                try {
+                  Thread.sleep(400);
+                  if (System.currentTimeMillis() - localstart > 5000) {
+                    LOG.info(""Could not complete file, retrying..."");
+                  }
+                } catch (InterruptedException ie) {
                 }
+              }
             }
             closed = true;
+          } finally {
+            synchronized (pendingCreates) {
+              pendingCreates.remove(src.toString());
+            }
+          }
         }
     }
 }
"
hadoop,3512203f6c54033db1d52118edf595e5b5e1335e,"Fix HADOOP-118.  Improved cleanup of abandoned file creations in DFS.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@395000 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-18 18:58:49,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index 98a233e..74b4368 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -280,6 +280,7 @@
 
         // Reserve space for this pending file
         pendingCreates.put(src, new FileUnderConstruction( replication ));
+        LOG.fine(""Adding "" + src + "" to pendingCreates for "" + holder);
         synchronized (leases) {
             Lease lease = (Lease) leases.get(holder);
             if (lease == null) {
@@ -314,7 +315,8 @@
      */
     public synchronized Object[] getAdditionalBlock(UTF8 src, UTF8 clientMachine) {
         Object results[] = null;
-        FileUnderConstruction pendingFile = (FileUnderConstruction) pendingCreates.get(src);
+        FileUnderConstruction pendingFile = 
+          (FileUnderConstruction) pendingCreates.get(src);
         if (dir.getFile(src) == null && pendingFile != null) {
             results = new Object[2];
 
@@ -360,8 +362,11 @@
     /**
      * Abandon the entire file in progress
      */
-    public synchronized void abandonFileInProgress(UTF8 src) throws IOException {
-        internalReleaseCreate(src);
+    public synchronized void abandonFileInProgress(UTF8 src, 
+                                                   UTF8 holder
+                                                   ) throws IOException {
+      LOG.info(""abandoning file in progress on "" + src.toString());
+      internalReleaseCreate(src, holder);
     }
 
     /**
@@ -416,6 +421,8 @@
 
         // The file is no longer pending
         pendingCreates.remove(src);
+        LOG.fine(""Removing "" + src + "" from pendingCreates for "" + holder +
+                 "". (complete)"");
         for (int i = 0; i < nrBlocks; i++) {
             pendingCreateBlocks.remove(pendingBlocks[i]);
         }
@@ -462,7 +469,8 @@
      */
     synchronized Block allocateBlock(UTF8 src) {
         Block b = new Block();
-        FileUnderConstruction v = (FileUnderConstruction) pendingCreates.get(src);
+        FileUnderConstruction v = 
+          (FileUnderConstruction) pendingCreates.get(src);
         v.add(b);
         pendingCreateBlocks.add(b);
         return b;
@@ -473,7 +481,8 @@
      * replicated.  If not, return false.
      */
     synchronized boolean checkFileProgress(UTF8 src) {
-        FileUnderConstruction v = (FileUnderConstruction) pendingCreates.get(src);
+        FileUnderConstruction v = 
+          (FileUnderConstruction) pendingCreates.get(src);
 
         for (Iterator it = v.iterator(); it.hasNext(); ) {
             Block b = (Block) it.next();
@@ -652,8 +661,8 @@
         public void startedCreate(UTF8 src) {
             creates.add(src);
         }
-        public void completedCreate(UTF8 src) {
-            creates.remove(src);
+        public boolean completedCreate(UTF8 src) {
+            return creates.remove(src);
         }
         public boolean hasLocks() {
             return (locks.size() + creates.size()) > 0;
@@ -666,7 +675,7 @@
             locks.clear();
             for (Iterator it = creates.iterator(); it.hasNext(); ) {
                 UTF8 src = (UTF8) it.next();
-                internalReleaseCreate(src);
+                internalReleaseCreate(src, holder);
             }
             creates.clear();
         }
@@ -674,7 +683,8 @@
         /**
          */
         public String toString() {
-            return ""[Lease.  Holder: "" + holder.toString() + "", heldlocks: "" + locks.size() + "", pendingcreates: "" + creates.size() + ""]"";
+            return ""[Lease.  Holder: "" + holder.toString() + "", heldlocks: "" +
+                   locks.size() + "", pendingcreates: "" + creates.size() + ""]"";
         }
 
         /**
@@ -771,12 +781,41 @@
     private int internalReleaseLock(UTF8 src, UTF8 holder) {
         return dir.releaseLock(src, holder);
     }
-    private void internalReleaseCreate(UTF8 src) {
-        FileUnderConstruction v = (FileUnderConstruction) pendingCreates.remove(src);
-        for (Iterator it2 = v.iterator(); it2.hasNext(); ) {
-            Block b = (Block) it2.next();
-            pendingCreateBlocks.remove(b);
+
+    /**
+     * Release a pending file creation lock.
+     * @param src The filename
+     * @param holder The datanode that was creating the file
+     */
+    private void internalReleaseCreate(UTF8 src, UTF8 holder) {
+      // find the lease
+      Lease lease = (Lease) leases.get(holder);
+      if (lease != null) {
+        // remove the file from the lease
+        if (lease.completedCreate(src)) {
+          // if we found the file in the lease, remove it from pendingCreates
+          FileUnderConstruction v = 
+            (FileUnderConstruction) pendingCreates.remove(src);
+          if (v != null) {
+            LOG.info(""Removing "" + src + "" from pendingCreates for "" + 
+                     holder + "" (failure)"");
+            for (Iterator it2 = v.iterator(); it2.hasNext(); ) {
+              Block b = (Block) it2.next();
+              pendingCreateBlocks.remove(b);
+            }
+          } else {
+            LOG.info(""Attempt to release a create lock on "" + src.toString() +
+                     "" that was not in pendingCreates"");
+          }
+        } else {
+          LOG.info(""Attempt by "" + holder.toString() + 
+                   "" to release someone else's create lock on "" + 
+                   src.toString());
         }
+      } else {
+        LOG.info(""Attempt to release a lock from an unknown lease holder ""
+                 + holder.toString() + "" for "" + src.toString());
+      }
     }
 
     /**
"
hadoop,3512203f6c54033db1d52118edf595e5b5e1335e,"Fix HADOOP-118.  Improved cleanup of abandoned file creations in DFS.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@395000 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-18 18:58:49,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/NameNode.java b/src/java/org/apache/hadoop/dfs/NameNode.java
index baee359..e2b1d4d 100644
--- a/src/java/org/apache/hadoop/dfs/NameNode.java
+++ b/src/java/org/apache/hadoop/dfs/NameNode.java
@@ -208,8 +208,9 @@
     }
     /**
      */
-    public void abandonFileInProgress(String src) throws IOException {
-        namesystem.abandonFileInProgress(new UTF8(src));
+    public void abandonFileInProgress(String src, 
+                                      String holder) throws IOException {
+        namesystem.abandonFileInProgress(new UTF8(src), new UTF8(holder));
     }
     /**
      */
"
hadoop,3512203f6c54033db1d52118edf595e5b5e1335e,"Fix HADOOP-118.  Improved cleanup of abandoned file creations in DFS.  Contributed by Owen.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@395000 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-18 18:58:49,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobClient.java b/src/java/org/apache/hadoop/mapred/JobClient.java
index 95e15fe..775dee8 100644
--- a/src/java/org/apache/hadoop/mapred/JobClient.java
+++ b/src/java/org/apache/hadoop/mapred/JobClient.java
@@ -196,10 +196,6 @@
     /**
      */
     public synchronized void close() throws IOException {
-        if (fs != null) {
-            fs.close();
-            fs = null;
-        }
     }
 
     /**
"
hadoop,ebe15c4f0c5f84324c9a43a9c7e59889522e3eee,"Fix for HADOOP-92.  Show information about all attempts to run each task in the web ui.  Contributed by Mahadev konar.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@393641 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-12 23:04:33,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobInProgress.java b/src/java/org/apache/hadoop/mapred/JobInProgress.java
index c5e7fc0..53c7b43e 100644
--- a/src/java/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/JobInProgress.java
@@ -490,4 +490,21 @@
         LOG.warning(""Error cleaning up ""+profile.getJobId()+"": ""+e);
       }
     }
+
+    /**
+      * Return the TaskInProgress that matches the tipid.
+      */
+    public TaskInProgress getTaskInProgress(String tipid){
+        for (int i = 0; i < maps.length; i++) {
+	    if (tipid.equals(maps[i].getTIPId())){
+                return maps[i];
+	    }               
+	}
+	for (int i = 0; i < reduces.length; i++) {
+	    if (tipid.equals(reduces[i].getTIPId())){
+		return reduces[i];
+            }
+	}
+	return null;
+    }
 }
"
hadoop,ebe15c4f0c5f84324c9a43a9c7e59889522e3eee,"Fix for HADOOP-92.  Show information about all attempts to run each task in the web ui.  Contributed by Mahadev konar.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@393641 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-12 23:04:33,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index 4d2935e..445b199 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -825,6 +825,19 @@
         }
     }
 
+    /** Get all the TaskStatuses from the tipid. */
+    TaskStatus[] getTaskStatuses(String jobid, String tipid){
+	JobInProgress job = (JobInProgress) jobs.get(jobid);
+	if (job == null){
+	    return new TaskStatus[0];
+	}
+	TaskInProgress tip = (TaskInProgress) job.getTaskInProgress(tipid);
+	if (tip == null){
+	    return new TaskStatus[0];
+	}
+	return tip.getTaskStatuses();
+    }
+
     ///////////////////////////////////////////////////////////////
     // JobTracker methods
     ///////////////////////////////////////////////////////////////
@@ -851,6 +864,7 @@
     void updateTaskStatuses(TaskTrackerStatus status) {
         for (Iterator it = status.taskReports(); it.hasNext(); ) {
             TaskStatus report = (TaskStatus) it.next();
+            report.setHostname(status.getHost());
             TaskInProgress tip = (TaskInProgress) taskidToTIPMap.get(report.getTaskId());
             if (tip == null) {
                 LOG.info(""Serious problem.  While updating status, cannot find taskid "" + report.getTaskId());
"
hadoop,ebe15c4f0c5f84324c9a43a9c7e59889522e3eee,"Fix for HADOOP-92.  Show information about all attempts to run each task in the web ui.  Contributed by Mahadev konar.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@393641 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-12 23:04:33,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskInProgress.java b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
index fe6d5da..0782ae8 100644
--- a/src/java/org/apache/hadoop/mapred/TaskInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
@@ -307,6 +307,13 @@
     }
 
     /**
+     * Get the Status of the tasks managed by this TIP
+     */
+    public TaskStatus[] getTaskStatuses() {
+	return (TaskStatus[])taskStatuses.values().toArray(new TaskStatus[taskStatuses.size()]);
+    }
+
+     /**
      * The TIP's been ordered kill()ed.
      */
     public void kill() {
"
hadoop,ebe15c4f0c5f84324c9a43a9c7e59889522e3eee,"Fix for HADOOP-92.  Show information about all attempts to run each task in the web ui.  Contributed by Mahadev konar.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@393641 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-12 23:04:33,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskStatus.java b/src/java/org/apache/hadoop/mapred/TaskStatus.java
index 377f5a7..3fa7d0e 100644
--- a/src/java/org/apache/hadoop/mapred/TaskStatus.java
+++ b/src/java/org/apache/hadoop/mapred/TaskStatus.java
@@ -37,16 +37,20 @@
     private int runState;
     private String diagnosticInfo;
     private String stateString;
+    private String hostname;
 
     public TaskStatus() {}
 
-    public TaskStatus(String taskid, boolean isMap, float progress, int runState, String diagnosticInfo, String stateString) {
+    public TaskStatus(String taskid, boolean isMap, float progress,
+                      int runState, String diagnosticInfo,
+                      String stateString, String hostname) {
         this.taskid = taskid;
         this.isMap = isMap;
         this.progress = progress;
         this.runState = runState;
         this.diagnosticInfo = diagnosticInfo;
         this.stateString = stateString;
+	this.hostname = hostname;
     }
     
     public String getTaskId() { return taskid; }
@@ -54,6 +58,8 @@
     public float getProgress() { return progress; }
     public void setProgress(float progress) { this.progress = progress; } 
     public int getRunState() { return runState; }
+    public String getHostname() {return hostname;}
+    public void setHostname(String host) { this.hostname = host;}
     public void setRunState(int runState) { this.runState = runState; }
     public String getDiagnosticInfo() { return diagnosticInfo; }
     public void setDiagnosticInfo(String info) { this.diagnosticInfo = info; }
"
hadoop,ebe15c4f0c5f84324c9a43a9c7e59889522e3eee,"Fix for HADOOP-92.  Show information about all attempts to run each task in the web ui.  Contributed by Mahadev konar.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@393641 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-12 23:04:33,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index f4da938..03e1fb6 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -422,7 +422,7 @@
         /**
          */
         public TaskStatus createStatus() {
-            TaskStatus status = new TaskStatus(task.getTaskId(), task.isMapTask(), progress, runstate, diagnosticInfo.toString(), (stateString == null) ? """" : stateString);
+            TaskStatus status = new TaskStatus(task.getTaskId(), task.isMapTask(), progress, runstate, diagnosticInfo.toString(), (stateString == null) ? """" : stateString, """");
             if (diagnosticInfo.length() > 0) {
                 diagnosticInfo = new StringBuffer();
             }
"
hadoop,dbccfa12b1199dcf5d03468927d29a69fa504bff,"Fix for HADOOP-126.  'bin/hadoop dfs -cp' now correctly handles .crc files.  This also consolidates a lot of file copying code.  Contributed by Konstantin Shvachko.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@393025 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-10 19:11:54,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSShell.java b/src/java/org/apache/hadoop/dfs/DFSShell.java
index c5ceb22..88a2736 100644
--- a/src/java/org/apache/hadoop/dfs/DFSShell.java
+++ b/src/java/org/apache/hadoop/dfs/DFSShell.java
@@ -137,11 +137,7 @@
      * Copy an DFS file
      */
     public void copy(String srcf, String dstf, Configuration conf) throws IOException {
-        if (FileUtil.copyContents(fs, new File(srcf), new File(dstf), true, conf)) {
-            System.out.println(""Copied "" + srcf + "" to "" + dstf);
-        } else {
-            System.out.println(""Copy failed"");
-        }
+      DistributedFileSystem.doCopy(fs, new File(srcf), fs, new File(dstf), true, conf);
     }
 
     /**
"
hadoop,dbccfa12b1199dcf5d03468927d29a69fa504bff,"Fix for HADOOP-126.  'bin/hadoop dfs -cp' now correctly handles .crc files.  This also consolidates a lot of file copying code.  Contributed by Konstantin Shvachko.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@393025 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-10 19:11:54,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java b/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
index e25d75b..ab5bb8f 100644
--- a/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
+++ b/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
@@ -157,30 +157,39 @@
     }
 
     private void doFromLocalFile(File src, File dst, boolean deleteSource) throws IOException {
-        if (exists(dst)) {
-            if (! isDirectory(dst)) {
+        FileSystem localFs = getNamed(""local"", getConf());
+        doCopy( localFs, src, this, dst, deleteSource, getConf() );
+    }
+
+    public static void doCopy(FileSystem srcFS, 
+                        File src, 
+                        FileSystem dstFS, 
+                        File dst, 
+                        boolean deleteSource,
+                        Configuration conf
+                       ) throws IOException {
+        if (dstFS.exists(dst)) {
+            if (! dstFS.isDirectory(dst)) {
                 throw new IOException(""Target "" + dst + "" already exists"");
             } else {
                 dst = new File(dst, src.getName());
-                if (exists(dst)) {
+                if (dstFS.exists(dst)) {
                     throw new IOException(""Target "" + dst + "" already exists"");
                 }
             }
         }
 
-        FileSystem localFs = getNamed(""local"", getConf());
-
-        if (localFs.isDirectory(src)) {
-            mkdirs(dst);
-            File contents[] = localFs.listFiles(src);
+        if (srcFS.isDirectory(src)) {
+            dstFS.mkdirs(dst);
+            File contents[] = srcFS.listFiles(src);
             for (int i = 0; i < contents.length; i++) {
-                doFromLocalFile(contents[i], new File(dst, contents[i].getName()), deleteSource);
+                doCopy( srcFS, contents[i], dstFS, new File(dst, contents[i].getName()), deleteSource, conf);
             }
         } else {
-            byte buf[] = new byte[getConf().getInt(""io.file.buffer.size"", 4096)];
-            InputStream in = localFs.open(src);
+            byte buf[] = new byte[conf.getInt(""io.file.buffer.size"", 4096)];
+            InputStream in = srcFS.open(src);
             try {
-                OutputStream out = create(dst);
+                OutputStream out = dstFS.create(dst);
                 try {
                     int bytesRead = in.read(buf);
                     while (bytesRead >= 0) {
@@ -195,48 +204,13 @@
             } 
         }
         if (deleteSource)
-            localFs.delete(src);
+          srcFS.delete(src);
     }
 
     public void copyToLocalFile(File src, File dst) throws IOException {
-        if (dst.exists()) {
-            if (! dst.isDirectory()) {
-                throw new IOException(""Target "" + dst + "" already exists"");
-            } else {
-                dst = new File(dst, src.getName());
-                if (dst.exists()) {
-                    throw new IOException(""Target "" + dst + "" already exists"");
-                }
-            }
-        }
         dst = dst.getCanonicalFile();
-
         FileSystem localFs = getNamed(""local"", getConf());
-
-        if (isDirectory(src)) {
-            localFs.mkdirs(dst);
-            File contents[] = listFiles(src);
-            for (int i = 0; i < contents.length; i++) {
-                copyToLocalFile(contents[i], new File(dst, contents[i].getName()));
-            }
-        } else {
-            byte buf[] = new byte[getConf().getInt(""io.file.buffer.size"", 4096)];
-            InputStream in = open(src);
-            try {
-                OutputStream out = localFs.create(dst);
-                try {
-                    int bytesRead = in.read(buf);
-                    while (bytesRead >= 0) {
-                        out.write(buf, 0, bytesRead);
-                        bytesRead = in.read(buf);
-                    }
-                } finally {
-                    out.close();
-                }
-            } finally {
-                in.close();
-            } 
-        }
+        doCopy( this, src, localFs, dst, false, getConf() );
     }
 
     public File startLocalOutput(File fsOutputFile, File tmpLocalFile) throws IOException {
"
hadoop,b54cc2875d1d815ccc00e9f6df5a522c2e785f14,"Fix for HADOOP-125.  Absolute paths are tricky on Windows.  For Hadoop's purposes, consider things that start with a slash to be absolute.  Also, Hadoop should not change the JVM's CWD.  All files are now correctly cleaned up for a Nutch crawl, in either local or psuedo-distributed mode.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@392451 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-07 23:27:42,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/LocalFileSystem.java b/src/java/org/apache/hadoop/fs/LocalFileSystem.java
index bf2b23a..3ee64c1 100644
--- a/src/java/org/apache/hadoop/fs/LocalFileSystem.java
+++ b/src/java/org/apache/hadoop/fs/LocalFileSystem.java
@@ -29,7 +29,8 @@
  * @author Mike Cafarella
  *****************************************************************/
 public class LocalFileSystem extends FileSystem {
-    private File workingDir = new File(System.getProperty(""user.dir""));
+    private File workingDir
+      = new File(System.getProperty(""user.dir"")).getAbsoluteFile();
     TreeMap sharedLockDataSet = new TreeMap();
     TreeMap nonsharedLockDataSet = new TreeMap();
     TreeMap lockObjSet = new TreeMap();
@@ -156,7 +157,7 @@
       if (isAbsolute(f)) {
         return f;
       } else {
-        return new File(workingDir, f.toString());
+        return new File(workingDir, f.toString()).getAbsoluteFile();
       }
     }
     
@@ -200,7 +201,9 @@
     }
 
     public boolean isAbsolute(File f) {
-      return f.isAbsolute();
+      return f.isAbsolute() ||
+        f.getPath().startsWith(""/"") ||
+        f.getPath().startsWith(""\\"");
     }
 
     public long getLength(File f) throws IOException {
@@ -226,7 +229,6 @@
      */
     public void setWorkingDirectory(File new_dir) {
       workingDir = makeAbsolute(new_dir);
-      System.setProperty(""user.dir"", workingDir.toString());
     }
     
     public File getWorkingDirectory() {
"
hadoop,b54cc2875d1d815ccc00e9f6df5a522c2e785f14,"Fix for HADOOP-125.  Absolute paths are tricky on Windows.  For Hadoop's purposes, consider things that start with a slash to be absolute.  Also, Hadoop should not change the JVM's CWD.  All files are now correctly cleaned up for a Nutch crawl, in either local or psuedo-distributed mode.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@392451 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-07 23:27:42,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/LocalJobRunner.java b/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
index 809354d..5f36c29 100644
--- a/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
+++ b/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
@@ -92,7 +92,6 @@
         job.setNumReduceTasks(1);                 // force a single reduce task
         for (int i = 0; i < splits.length; i++) {
           mapIds.add(""map_"" + newId());
-          setWorkingDirectory(job, fs);
           MapTask map = new MapTask(file, (String)mapIds.get(i), splits[i]);
           map.setConf(job);
           map_tasks += 1;
"
hadoop,ddfb5e4949ecd3cc62f431d92b857882d23bb355,"Fix HADOOP-117.  Correctly remove mapred temp files.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@391425 13f79535-47bb-0310-9956-ffa450edef68
",2006-04-04 22:04:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobConf.java b/src/java/org/apache/hadoop/mapred/JobConf.java
index fc9a9c9..5b86570 100644
--- a/src/java/org/apache/hadoop/mapred/JobConf.java
+++ b/src/java/org/apache/hadoop/mapred/JobConf.java
@@ -124,7 +124,7 @@
   /** Constructs a local file name.  Files are distributed among configured
    * local directories.*/
   public File getLocalFile(String subdir, String name) throws IOException {
-    return getFile(""mapred.local.dir"", name + File.separator + subdir);
+    return getFile(""mapred.local.dir"", subdir + File.separator + name);
   }
 
   public void setInputDir(File dir) { set(""mapred.input.dir"", dir); }
"
hadoop,3f5b4c0626af8329a7bb5f78f0fd1114d8eece00,"Fix for HADOOP-102.  Contributed by Konstantin.


git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@390290 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-31 00:37:11,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index f81c346..d7b6f46 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -1310,7 +1310,7 @@
             //
             for (Iterator it = targetList.iterator(); it.hasNext(); ) {
                 DatanodeInfo node = (DatanodeInfo) it.next();
-                if ((node.getRemaining() > BLOCK_SIZE * MIN_BLOCKS_FOR_WRITE)) {
+                if (node.getRemaining() > BLOCK_SIZE * MIN_BLOCKS_FOR_WRITE) {
                     return node;
                 }
             }
@@ -1322,7 +1322,7 @@
             //
             for (Iterator it = targetList.iterator(); it.hasNext(); ) {
                 DatanodeInfo node = (DatanodeInfo) it.next();
-                if (node.getRemaining() > BLOCK_SIZE * MIN_BLOCKS_FOR_WRITE) {
+                if (node.getRemaining() > BLOCK_SIZE) {
                     return node;
                 }
             }
"
hadoop,d98cef1ab3e2bc6a3fc9ba5673c1723133e7df7e,"Fix HADOOP-100.  Be more consistent about synchronization of access to taskTracker collection.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@390287 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-30 23:51:53,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index e5abadc..4d2935e 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -261,7 +261,7 @@
     //
     int totalMaps = 0;
     int totalReduces = 0;
-    TreeMap taskTrackers = new TreeMap();
+    private TreeMap taskTrackers = new TreeMap();
     Vector jobInitQueue = new Vector();
     ExpireTrackers expireTrackers = new ExpireTrackers();
     RetireJobs retireJobs = new RetireJobs();
@@ -464,10 +464,14 @@
         return v;
     }
     public Collection taskTrackers() {
+      synchronized (taskTrackers) {
         return taskTrackers.values();
+      }
     }
     public TaskTrackerStatus getTaskTracker(String trackerID) {
+      synchronized (taskTrackers) {
         return (TaskTrackerStatus) taskTrackers.get(trackerID);
+      }
     }
 
     ////////////////////////////////////////////////////
@@ -557,16 +561,20 @@
         //
         int avgMaps = 0;
         int avgReduces = 0;
-        if (taskTrackers.size() > 0) {
-            avgMaps = totalMaps / taskTrackers.size();
-            avgReduces = totalReduces / taskTrackers.size();
+        int numTaskTrackers;
+        TaskTrackerStatus tts;
+        synchronized (taskTrackers) {
+          numTaskTrackers = taskTrackers.size();
+          tts = (TaskTrackerStatus) taskTrackers.get(taskTracker);
         }
-        int totalCapacity = taskTrackers.size() * maxCurrentTasks;
-
+        if (numTaskTrackers > 0) {
+          avgMaps = totalMaps / numTaskTrackers;
+          avgReduces = totalReduces / numTaskTrackers;
+        }
+        int totalCapacity = numTaskTrackers * maxCurrentTasks;
         //
         // Get map + reduce counts for the current tracker.
         //
-        TaskTrackerStatus tts = (TaskTrackerStatus) taskTrackers.get(taskTracker);
         if (tts == null) {
           LOG.warning(""Unknown task tracker polling; ignoring: "" + taskTracker);
           return null;
@@ -694,7 +702,10 @@
                 TaskInProgress tip = (TaskInProgress) taskidToTIPMap.get(mapTasksNeeded[i][j]);
                 if (tip != null && tip.isComplete(mapTasksNeeded[i][j])) {
                     String trackerId = (String) taskidToTrackerMap.get(mapTasksNeeded[i][j]);
-                    TaskTrackerStatus tracker = (TaskTrackerStatus) taskTrackers.get(trackerId);
+                    TaskTrackerStatus tracker;
+                    synchronized (taskTrackers) {
+                      tracker = (TaskTrackerStatus) taskTrackers.get(trackerId);
+                    }
                     v.add(new MapOutputLocation(mapTasksNeeded[i][j], tracker.getHost(), tracker.getPort()));
                     break;
                 }
@@ -745,10 +756,12 @@
     }
 
     public synchronized ClusterStatus getClusterStatus() {
-        return new ClusterStatus(taskTrackers.size(),
-                                 totalMaps,
-                                 totalReduces,
-                                 maxCurrentTasks);
+        synchronized (taskTrackers) {
+          return new ClusterStatus(taskTrackers.size(),
+                                   totalMaps,
+                                   totalReduces,
+                                   maxCurrentTasks);          
+        }
     }
     
     public synchronized void killJob(String jobid) {
"
hadoop,03c953c0dc5bb90b0a3ddddd881ac1a224e8d40b,"Fix for HADOOP-107.  As they were written, dfs blocks were both trickled to a datanode and tee'd to a temp file (in case the connection to the datanode failed).  Now they're only written to the temp file, with no connection to the datanode made until the block is complete.  This reduces the number of long-lived mostly-idle connections to datanodes, which was causing problems.  It also simplifies the DFSClient code significantly.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@390262 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-30 22:33:27,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSClient.java b/src/java/org/apache/hadoop/dfs/DFSClient.java
index 92cb009..f1d2f50 100644
--- a/src/java/org/apache/hadoop/dfs/DFSClient.java
+++ b/src/java/org/apache/hadoop/dfs/DFSClient.java
@@ -526,9 +526,8 @@
         private int pos = 0;
 
         private UTF8 src;
-        boolean closingDown = false;
         private boolean overwrite;
-        private boolean blockStreamWorking;
+        private boolean firstTime = true;
         private DataOutputStream blockStream;
         private DataInputStream blockReplyStream;
         private File backupFile;
@@ -543,13 +542,8 @@
         public DFSOutputStream(UTF8 src, boolean overwrite) throws IOException {
             this.src = src;
             this.overwrite = overwrite;
-            this.blockStream = null;
-            this.blockReplyStream = null;
-            this.blockStreamWorking = false;
             this.backupFile = newBackupFile();
-
-            this.backupStream = new BufferedOutputStream(new FileOutputStream(backupFile));
-            nextBlockOutputStream(true);
+            this.backupStream = new FileOutputStream(backupFile);
         }
 
         private File newBackupFile() throws IOException {
@@ -565,13 +559,7 @@
          * This happens when a file is created and each time a new block is allocated.
          * Must get block ID and the IDs of the destinations from the namenode.
          */
-        private synchronized void nextBlockOutputStream(boolean firstTime) throws IOException {
-            if (! firstTime && blockStreamWorking) {
-                blockStream.flush();
-                s.close();
-                blockStreamWorking = false;
-            }
-
+        private synchronized void nextBlockOutputStream() throws IOException {
             boolean retry = false;
             long start = System.currentTimeMillis();
             do {
@@ -644,8 +632,8 @@
                 bytesWrittenToBlock = 0;
                 blockStream = out;
                 blockReplyStream = new DataInputStream(new BufferedInputStream(s.getInputStream()));
-                blockStreamWorking = true;
             } while (retry);
+            firstTime = false;
         }
 
         /**
@@ -708,7 +696,6 @@
             }
             if (bytesWrittenToBlock == BLOCK_SIZE) {
                 endBlock();
-                nextBlockOutputStream(false);
             }
             flushData(pos);
         }
@@ -720,19 +707,7 @@
         private synchronized void flushData(int maxPos) throws IOException {
             int workingPos = Math.min(pos, maxPos);
             
-            if (workingPos > 0 || 
-                (workingPos == 0 && closingDown)) {
-                //
-                // To the blockStream, write length, then bytes
-                //
-                if (blockStreamWorking) {
-                    try {
-                        blockStream.writeLong(workingPos);
-                        blockStream.write(outBuf, 0, workingPos);
-                    } catch (IOException ie) {
-                        handleSocketException(ie);
-                    }
-                }
+            if (workingPos > 0) {
                 //
                 // To the local block backup, write just the bytes
                 //
@@ -751,43 +726,27 @@
          * We're done writing to the current block.
          */
         private synchronized void endBlock() throws IOException {
-            boolean mustRecover = ! blockStreamWorking;
-
-            //
-            // A zero-length set of data indicates the end of the block
-            //
-            if (blockStreamWorking) {
-                try {
-                    internalClose();
-                } catch (IOException ie) {
-                    handleSocketException(ie);
-                    mustRecover = true;
-                } finally {
-                    blockStreamWorking = false;
-                }
-            }
-
             //
             // Done with local copy
             //
             backupStream.close();
 
             //
-            // If necessary, recover from a failed datanode connection.
+            // Send it to datanode
             //
+            boolean mustRecover = true;
             while (mustRecover) {
-                nextBlockOutputStream(false);
+                nextBlockOutputStream();
                 InputStream in = new FileInputStream(backupFile);
                 try {
                     byte buf[] = new byte[BUFFER_SIZE];
                     int bytesRead = in.read(buf);
-                    while (bytesRead >= 0) {
+                    while (bytesRead > 0) {
                         blockStream.writeLong((long) bytesRead);
                         blockStream.write(buf, 0, bytesRead);
                         bytesRead = in.read(buf);
                     }
                     internalClose();
-                    LOG.info(""Recovered from failed datanode connection"");
                     mustRecover = false;
                 } catch (IOException ie) {
                     handleSocketException(ie);
@@ -801,12 +760,12 @@
             //
             backupFile.delete();
             backupFile = newBackupFile();
-            backupStream = new BufferedOutputStream(new FileOutputStream(backupFile));
+            backupStream = new FileOutputStream(backupFile);
+            bytesWrittenToBlock = 0;
         }
 
         /**
-         * Close down stream to remote datanode.  Called from two places
-         * in endBlock();
+         * Close down stream to remote datanode.
          */
         private synchronized void internalClose() throws IOException {
             blockStream.writeLong(0);
@@ -823,16 +782,19 @@
             namenode.reportWrittenBlock(lb);
 
             s.close();
+            s = null;
         }
 
         private void handleSocketException(IOException ie) throws IOException {
           LOG.log(Level.WARNING, ""Error while writing."", ie);
           try {
-            s.close();
+            if (s != null) {
+              s.close();
+              s = null;
+            }
           } catch (IOException ie2) {
             LOG.log(Level.WARNING, ""Error closing socket."", ie2);
           }
-          blockStreamWorking = false;
           namenode.abandonBlock(block, src.toString());
         }
 
@@ -845,16 +807,17 @@
                 throw new IOException(""Stream closed"");
             }
 
-            closingDown = true;
             flush();
-            endBlock();
+            if (filePos == 0 || bytesWrittenToBlock != 0) {
+              endBlock();
+            }
 
             backupStream.close();
             backupFile.delete();
 
-            if (blockStreamWorking) {
+            if (s != null) {
                 s.close();
-                blockStreamWorking = false;
+                s = null;
             }
             super.close();
 
@@ -873,7 +836,6 @@
                 }
             }
             closed = true;
-            closingDown = false;
         }
     }
 }
"
hadoop,7d0429d4d228d34f0928a9a63a9247173c5e3c18,"Fix for HADOOP-110.  Reuse keys and values when mapping.  Contributed by Owen O'Malley.


git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@390232 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-30 20:14:36,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/MapRunner.java b/src/java/org/apache/hadoop/mapred/MapRunner.java
index ef800a2..f7f9783 100644
--- a/src/java/org/apache/hadoop/mapred/MapRunner.java
+++ b/src/java/org/apache/hadoop/mapred/MapRunner.java
@@ -39,16 +39,11 @@
                   Reporter reporter)
     throws IOException {
     try {
-      while (true) {
-        // allocate new key & value instances
-        WritableComparable key =
-          (WritableComparable)job.newInstance(inputKeyClass);
-        Writable value = (Writable)job.newInstance(inputValueClass);
-
-        // read next key & value
-        if (!input.next(key, value))
-          return;
-
+      // allocate key & value instances that are re-used for all entries
+      WritableComparable key =
+        (WritableComparable)job.newInstance(inputKeyClass);
+      Writable value = (Writable)job.newInstance(inputValueClass);
+      while (input.next(key, value)) {
         // map pair to output
         mapper.map(key, value, output, reporter);
       }
"
hadoop,ec87d7e0e37ee1ed821bb2ee4cd24c8f0ebafa14,"Fix for HADOOP-2.  The combiner now clones keys and values, so mappers may now safely reuse emitted keys and values.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@390231 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-30 20:10:36,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/WritableUtils.java b/src/java/org/apache/hadoop/io/WritableUtils.java
index 8bfc654..a32d1d4 100644
--- a/src/java/org/apache/hadoop/io/WritableUtils.java
+++ b/src/java/org/apache/hadoop/io/WritableUtils.java
@@ -17,7 +17,7 @@
 package org.apache.hadoop.io;
 
 import java.io.*;
-
+import org.apache.hadoop.mapred.JobConf;
 import java.util.zip.GZIPInputStream;
 import java.util.zip.GZIPOutputStream;
 
@@ -189,5 +189,46 @@
     System.out.println();
   }
 
-
+  /**
+   * A pair of input/output buffers that we use to clone writables.
+   */
+  private static class CopyInCopyOutBuffer {
+    DataOutputBuffer outBuffer = new DataOutputBuffer();
+    DataInputBuffer inBuffer = new DataInputBuffer();
+    /**
+     * Move the data from the output buffer to the input buffer.
+     */
+    void moveData() {
+      inBuffer.reset(outBuffer.getData(), outBuffer.getLength());
+    }
+  }
+  
+  /**
+   * Allocate a buffer for each thread that tries to clone objects.
+   */
+  private static ThreadLocal cloneBuffers = new ThreadLocal() {
+    protected synchronized Object initialValue() {
+      return new CopyInCopyOutBuffer();
+    }
+  };
+  
+  /**
+   * Make a copy of a writable object using serialization to a buffer.
+   * @param orig The object to copy
+   * @return The copied object
+   */
+  public static Writable clone(Writable orig, JobConf conf) {
+    try {
+      Writable newInst = (Writable)conf.newInstance(orig.getClass());
+      CopyInCopyOutBuffer buffer = (CopyInCopyOutBuffer)cloneBuffers.get();
+      buffer.outBuffer.reset();
+      orig.write(buffer.outBuffer);
+      buffer.moveData();
+      newInst.readFields(buffer.inBuffer);
+      return newInst;
+    } catch (IOException e) {
+      throw new RuntimeException(""Error writing/reading clone buffer"", e);
+    }
+  }
+  
 }
"
hadoop,ec87d7e0e37ee1ed821bb2ee4cd24c8f0ebafa14,"Fix for HADOOP-2.  The combiner now clones keys and values, so mappers may now safely reuse emitted keys and values.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@390231 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-30 20:10:36,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/CombiningCollector.java b/src/java/org/apache/hadoop/mapred/CombiningCollector.java
index df19c07..b10b8fb 100644
--- a/src/java/org/apache/hadoop/mapred/CombiningCollector.java
+++ b/src/java/org/apache/hadoop/mapred/CombiningCollector.java
@@ -51,12 +51,16 @@
 
     // buffer new value in map
     ArrayList values = (ArrayList)keyToValues.get(key);
-    if (values == null) {                         // no values yet for this key
-      values = new ArrayList(1);                  // make a new list
-      values.add(value);                          // add this value
-      keyToValues.put(key, values);               // add to map
+    Writable valueClone = WritableUtils.clone(value, job);
+    if (values == null) {
+      // this is a new key, so create a new list
+      values = new ArrayList(1);
+      values.add(valueClone);
+      Writable keyClone = WritableUtils.clone(key, job);
+      keyToValues.put(keyClone, values);
     } else {
-      values.add(value);                          // other values: just add new
+      // other values for this key, so just add.
+      values.add(valueClone);
     }
 
     count++;
"
hadoop,ecf45fd810566c5561886e2a8bc1f1bf0fe878f5,"Fix for HADOOP-112.  All operations on local files are now performed
through a LocalFileSystem.  In particular, listing the local
directory, which was causing this bug, when CRC files were included
in the listing.  Now they are correctly excluded.


git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@390218 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-30 19:09:46,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java b/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
index 8b01e4f..287c641 100644
--- a/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
+++ b/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
@@ -155,15 +155,17 @@
             }
         }
 
-        if (src.isDirectory()) {
+        FileSystem localFs = getNamed(""local"", getConf());
+
+        if (localFs.isDirectory(src)) {
             mkdirs(dst);
-            File contents[] = src.listFiles();
+            File contents[] = localFs.listFiles(src);
             for (int i = 0; i < contents.length; i++) {
                 doFromLocalFile(contents[i], new File(dst, contents[i].getName()), deleteSource);
             }
         } else {
             byte buf[] = new byte[getConf().getInt(""io.file.buffer.size"", 4096)];
-            InputStream in = new BufferedInputStream(new FileInputStream(src));
+            InputStream in = localFs.open(src);
             try {
                 OutputStream out = create(dst);
                 try {
@@ -180,7 +182,7 @@
             } 
         }
         if (deleteSource)
-            src.delete();
+            localFs.delete(src);
     }
 
     public void copyToLocalFile(File src, File dst) throws IOException {
@@ -196,8 +198,10 @@
         }
         dst = dst.getCanonicalFile();
 
+        FileSystem localFs = getNamed(""local"", getConf());
+
         if (isDirectory(src)) {
-            dst.mkdirs();
+            localFs.mkdirs(dst);
             File contents[] = listFiles(src);
             for (int i = 0; i < contents.length; i++) {
                 copyToLocalFile(contents[i], new File(dst, contents[i].getName()));
@@ -206,7 +210,7 @@
             byte buf[] = new byte[getConf().getInt(""io.file.buffer.size"", 4096)];
             InputStream in = open(src);
             try {
-                OutputStream out = FileSystem.getNamed(""local"", getConf()).create(dst);
+                OutputStream out = localFs.create(dst);
                 try {
                     int bytesRead = in.read(buf);
                     while (bytesRead >= 0) {
"
hadoop,7018745fac6b56df381b0063ed29c339589e4b2a,"Fix for HADOOP-84.  Improve error and log messages when block cannot be obtained by including file name and offset.  Also removed a few unused variables.  Contributed by Konstantin Shvachko.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@390213 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-30 18:46:35,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSClient.java b/src/java/org/apache/hadoop/dfs/DFSClient.java
index 79feec5..92cb009 100644
--- a/src/java/org/apache/hadoop/dfs/DFSClient.java
+++ b/src/java/org/apache/hadoop/dfs/DFSClient.java
@@ -261,7 +261,6 @@
 
         private String src;
         private DataInputStream blockStream;
-        private DataOutputStream partnerStream;
         private Block blocks[] = null;
         private DatanodeInfo nodes[][] = null;
         private long pos = 0;
@@ -274,7 +273,6 @@
             this.src = src;
             openInfo();
             this.blockStream = null;
-            this.partnerStream = null;
             for (int i = 0; i < blocks.length; i++) {
                 this.filelen += blocks[i].getNumBytes();
             }
@@ -358,11 +356,13 @@
                     chosenNode = bestNode(nodes[targetBlock], deadNodes);
                     targetAddr = DataNode.createSocketAddr(chosenNode.getName().toString());
                 } catch (IOException ie) {
+                    String blockInfo =
+                      blocks[targetBlock]+"" file=""+src+"" offset=""+target;
                     if (failures >= MAX_BLOCK_ACQUIRE_FAILURES) {
-                        throw new IOException(""Could not obtain block "" + blocks[targetBlock]);
+                        throw new IOException(""Could not obtain block: "" + blockInfo);
                     }
                     if (nodes[targetBlock] == null || nodes[targetBlock].length == 0) {
-                        LOG.info(""No node available for block "" + blocks[targetBlock]);
+                        LOG.info(""No node available for block: "" + blockInfo);
                     }
                     LOG.info(""Could not obtain block from any node:  "" + ie);
                     try {
@@ -404,7 +404,6 @@
                     this.pos = target;
                     this.blockEnd = targetBlockEnd;
                     this.blockStream = in;
-                    this.partnerStream = out;
                 } catch (IOException ex) {
                     // Put chosen node into dead list, continue
                     LOG.info(""Failed to connect to "" + targetAddr + "":"" + ex);
@@ -535,7 +534,6 @@
         private File backupFile;
         private OutputStream backupStream;
         private Block block;
-        private DatanodeInfo targets[]; 
         private long filePos = 0;
         private int bytesWrittenToBlock = 0;
 
"
hadoop,73be419f0070aee61d3d074cfeb5cb83461d1edf,"Fix for HADOOP-19.  A namenode must now be formatted before it may be used.  Attempts to start a namenode in an unformatted directory will fail, rather than automatically creating a new, empty filesystem, causing existing datanodes to delete all blocks.  Thus a mis-configured dfs.data.dir should no longer cause data loss.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@389566 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-28 18:14:02,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSDirectory.java b/src/java/org/apache/hadoop/dfs/FSDirectory.java
index 980cf79..8bfcebc 100644
--- a/src/java/org/apache/hadoop/dfs/FSDirectory.java
+++ b/src/java/org/apache/hadoop/dfs/FSDirectory.java
@@ -245,14 +245,11 @@
     DataOutputStream editlog = null;
     boolean ready = false;
 
-    /**
-     * Create a FileSystem directory, and load its info
-     * from the indicated place.
-     */
+    /** Access an existing dfs name directory. */
     public FSDirectory(File dir) throws IOException {
         File fullimage = new File(dir, ""image"");
         if (! fullimage.exists()) {
-            fullimage.mkdirs();
+          throw new IOException(""NameNode not formatted: "" + dir);
         }
         File edits = new File(dir, ""edits"");
         if (loadFSImage(fullimage, edits)) {
@@ -266,6 +263,19 @@
         }
     }
 
+    /** Create a new dfs name directory.  Caution: this destroys all files
+     * in this filesystem. */
+    public static void format(File dir) throws IOException {
+        File image = new File(dir, ""image"");
+        File edits = new File(dir, ""edits"");
+
+        if (!((!image.exists() || image.delete()) &&
+              (!edits.exists() || edits.delete()) &&
+              image.mkdirs())) {
+          throw new IOException(""Unable to format: ""+dir);
+        }
+    }
+
     /**
      * Shutdown the filestore
      */
"
hadoop,73be419f0070aee61d3d074cfeb5cb83461d1edf,"Fix for HADOOP-19.  A namenode must now be formatted before it may be used.  Attempts to start a namenode in an unformatted directory will fail, rather than automatically creating a new, empty filesystem, causing existing datanodes to delete all blocks.  Thus a mis-configured dfs.data.dir should no longer cause data loss.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@389566 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-28 18:14:02,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/NameNode.java b/src/java/org/apache/hadoop/dfs/NameNode.java
index 244b260..eebcc95 100644
--- a/src/java/org/apache/hadoop/dfs/NameNode.java
+++ b/src/java/org/apache/hadoop/dfs/NameNode.java
@@ -65,12 +65,17 @@
     /** only used for testing purposes  */
     private boolean stopRequested = false;
 
+    /** Format a new filesystem.  Destroys any filesystem that may already
+     * exist at this location.  **/
+    public static void format(Configuration conf) throws IOException {
+      FSDirectory.format(getDir(conf));
+    }
+
     /**
      * Create a NameNode at the default location
      */
     public NameNode(Configuration conf) throws IOException {
-        this(new File(conf.get(""dfs.name.dir"",
-                                          ""/tmp/hadoop/dfs/name"")),
+        this(getDir(conf),
              DataNode.createSocketAddr
              (conf.get(""fs.default.name"", ""local"")).getPort(), conf);
     }
@@ -85,6 +90,11 @@
         this.server.start();
     }
 
+    /** Return the configured directory where name data is stored. */
+    private static File getDir(Configuration conf) {
+      return new File(conf.get(""dfs.name.dir"", ""/tmp/hadoop/dfs/name""));
+    }
+
     /**
      * Wait for service to finish.
      * (Normally, it runs forever.)
@@ -364,8 +374,24 @@
 
     /**
      */
-    public static void main(String argv[]) throws IOException, InterruptedException {
-        NameNode namenode = new NameNode(new Configuration());
+    public static void main(String argv[]) throws Exception {
+        Configuration conf = new Configuration();
+
+        if (argv.length == 1 && argv[0].equals(""-format"")) {
+          File dir = getDir(conf);
+          if (dir.exists()) {
+            System.err.print(""Re-format filesystem in "" + dir +"" ? (Y or N) "");
+            if (!(System.in.read() == 'Y')) {
+              System.err.println(""Format aborted."");
+              System.exit(1);
+            }
+          }
+          format(conf);
+          System.err.println(""Formatted ""+dir);
+          System.exit(0);
+        }
+
+        NameNode namenode = new NameNode(conf);
         namenode.join();
     }
 }
"
hadoop,73be419f0070aee61d3d074cfeb5cb83461d1edf,"Fix for HADOOP-19.  A namenode must now be formatted before it may be used.  Attempts to start a namenode in an unformatted directory will fail, rather than automatically creating a new, empty filesystem, causing existing datanodes to delete all blocks.  Thus a mis-configured dfs.data.dir should no longer cause data loss.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@389566 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-28 18:14:02,Doug Cutting,"diff --git a/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java b/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java
index bd3ec6b..034be8a 100644
--- a/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java
+++ b/src/test/org/apache/hadoop/dfs/MiniDFSCluster.java
@@ -83,7 +83,7 @@
   /**
    * Create the config and start up the servers.
    */
-  public MiniDFSCluster(int namenodePort, Configuration conf) {
+  public MiniDFSCluster(int namenodePort, Configuration conf) throws IOException {
     this.conf = conf;
     conf.set(""fs.default.name"", 
              ""localhost:""+ Integer.toString(namenodePort));
@@ -95,6 +95,7 @@
     // this timeout seems to control the minimum time for the test, so
     // set it down at 5 seconds.
     conf.setInt(""ipc.client.timeout"", 5000);
+    NameNode.format(conf);
     nameNode = new NameNodeRunner();
     nameNodeThread = new Thread(nameNode);
     nameNodeThread.start();
"
hadoop,2487ad9876013af7e38e7ecffebc36f3df84d481,"Fix for HADOOP-98.  Keep more accurate task counts.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@388254 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-23 20:07:56,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index 83aae7f..e5abadc 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -101,6 +101,7 @@
 
                             // Remove profile from head of queue
                             trackerExpiryQueue.remove(leastRecent);
+                            String trackerName = leastRecent.getTrackerName();
 
                             // Figure out if last-seen time should be updated, or if tracker is dead
                             TaskTrackerStatus newProfile = (TaskTrackerStatus) taskTrackers.get(leastRecent.getTrackerName());
@@ -110,12 +111,7 @@
                             if (newProfile != null) {
                                 if (now - newProfile.getLastSeen() > TASKTRACKER_EXPIRY_INTERVAL) {
                                     // Remove completely
-
-                                    TaskTrackerStatus oldStatus = (TaskTrackerStatus) taskTrackers.remove(leastRecent.getTrackerName());
-                                    if (oldStatus != null) {
-                                        totalMaps -= oldStatus.countMapTasks();
-                                        totalReduces -= oldStatus.countReduceTasks();
-                                    }
+                                    updateTaskTrackerStatus(trackerName, null);
                                     lostTaskTracker(leastRecent.getTrackerName());
                                 } else {
                                     // Update time by inserting latest profile
@@ -478,16 +474,42 @@
     // InterTrackerProtocol
     ////////////////////////////////////////////////////
     public void initialize(String taskTrackerName) {
-        if (taskTrackers.get(taskTrackerName) != null) {
-            TaskTrackerStatus oldStatus = (TaskTrackerStatus) taskTrackers.remove(taskTrackerName);
-            totalMaps -= oldStatus.countMapTasks();
-            totalReduces -= oldStatus.countReduceTasks();
-
-            lostTaskTracker(taskTrackerName);
+      synchronized (taskTrackers) {
+        boolean seenBefore = updateTaskTrackerStatus(taskTrackerName, null);
+        if (seenBefore) {
+          lostTaskTracker(taskTrackerName);
         }
+      }
     }
 
     /**
+     * Update the last recorded status for the given task tracker.
+     * It assumes that the taskTrackers are locked on entry.
+     * @author Owen O'Malley
+     * @param trackerName The name of the tracker
+     * @param status The new status for the task tracker
+     * @return Was an old status found?
+     */
+    private boolean updateTaskTrackerStatus(String trackerName,
+                                            TaskTrackerStatus status) {
+      TaskTrackerStatus oldStatus = 
+        (TaskTrackerStatus) taskTrackers.get(trackerName);
+      if (oldStatus != null) {
+        totalMaps -= oldStatus.countMapTasks();
+        totalReduces -= oldStatus.countReduceTasks();
+        if (status == null) {
+          taskTrackers.remove(trackerName);
+        }
+      }
+      if (status != null) {
+        totalMaps += status.countMapTasks();
+        totalReduces += status.countReduceTasks();
+        taskTrackers.put(trackerName, status);
+      }
+      return oldStatus != null;
+    }
+    
+    /**
      * Process incoming heartbeat messages from the task trackers.
      */
     public synchronized int emitHeartbeat(TaskTrackerStatus trackerStatus, boolean initialContact) {
@@ -496,26 +518,20 @@
 
         synchronized (taskTrackers) {
             synchronized (trackerExpiryQueue) {
+                boolean seenBefore = updateTaskTrackerStatus(trackerName,
+                                                             trackerStatus);
                 if (initialContact) {
                     // If it's first contact, then clear out any state hanging around
-                    if (taskTrackers.get(trackerName) != null) {
-                        TaskTrackerStatus oldStatus = (TaskTrackerStatus) taskTrackers.remove(trackerName);
-                        totalMaps -= oldStatus.countMapTasks();
-                        totalReduces -= oldStatus.countReduceTasks();
+                    if (seenBefore) {
                         lostTaskTracker(trackerName);
                     }
                 } else {
                     // If not first contact, there should be some record of the tracker
-                    if (taskTrackers.get(trackerName) == null) {
+                    if (!seenBefore) {
                         return InterTrackerProtocol.UNKNOWN_TASKTRACKER;
                     }
                 }
 
-                // Store latest state.  If first contact, then save current
-                // state in expiry queue
-                totalMaps += trackerStatus.countMapTasks();
-                totalReduces += trackerStatus.countReduceTasks();
-                taskTrackers.put(trackerName, trackerStatus);
                 if (initialContact) {
                     trackerExpiryQueue.add(trackerStatus);
                 }
"
hadoop,8cdc95ec9e14b11aa5138d53d3908c53625428bc,"Fix for HADOOP-52.  Add username and working-directory to FileSystem and JobConf and use these to resolve relative paths.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@388229 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-23 18:38:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java b/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
index 19b3b89..603111b 100644
--- a/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
+++ b/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
@@ -32,10 +32,9 @@
  * @author Mike Cafarella
  *****************************************************************/
 public class DistributedFileSystem extends FileSystem {
-    private static final String HOME_DIR =
-      ""/user/"" + System.getProperty(""user.name"") + ""/"";
+    private File workingDir = 
+      new File(""/user/"", System.getProperty(""user.name""));
 
-    private Random r = new Random();
     private String name;
 
     DFSClient dfs;
@@ -50,11 +49,24 @@
 
     public String getName() { return name; }
 
-    private UTF8 getPath(File file) {
-      String path = getDFSPath(file);
-      if (!path.startsWith(DFSFile.DFS_FILE_SEPARATOR)) {
-        path = getDFSPath(new File(HOME_DIR, path)); // make absolute
+    public File getWorkingDirectory() {
+      return workingDir;
+    }
+    
+    private File makeAbsolute(File f) {
+      if (f.isAbsolute()) {
+        return f;
+      } else {
+        return new File(workingDir, f.toString());
       }
+    }
+    
+    public void setWorkingDirectory(File dir) {
+      workingDir = makeAbsolute(dir);
+    }
+    
+    private UTF8 getPath(File file) {
+      String path = getDFSPath(makeAbsolute(file));
       return new UTF8(path);
     }
 
"
hadoop,8cdc95ec9e14b11aa5138d53d3908c53625428bc,"Fix for HADOOP-52.  Add username and working-directory to FileSystem and JobConf and use these to resolve relative paths.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@388229 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-23 18:38:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FileSystem.java b/src/java/org/apache/hadoop/fs/FileSystem.java
index 7966e9e..92b17cb 100644
--- a/src/java/org/apache/hadoop/fs/FileSystem.java
+++ b/src/java/org/apache/hadoop/fs/FileSystem.java
@@ -295,6 +295,19 @@
     }
 
     /**
+     * Set the current working directory for the given file system.
+     * All relative paths will be resolved relative to it.
+     * @param new_dir
+     */
+    public abstract void setWorkingDirectory(File new_dir);
+    
+    /**
+     * Get the current working directory for the given file system
+     * @return the directory pathname
+     */
+    public abstract File getWorkingDirectory();
+    
+    /**
      * Make the given file and all non-existent parents into
      * directories.
      */
"
hadoop,8cdc95ec9e14b11aa5138d53d3908c53625428bc,"Fix for HADOOP-52.  Add username and working-directory to FileSystem and JobConf and use these to resolve relative paths.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@388229 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-23 18:38:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/LocalFileSystem.java b/src/java/org/apache/hadoop/fs/LocalFileSystem.java
index eeb7739..eb24042 100644
--- a/src/java/org/apache/hadoop/fs/LocalFileSystem.java
+++ b/src/java/org/apache/hadoop/fs/LocalFileSystem.java
@@ -22,7 +22,6 @@
 
 import org.apache.hadoop.fs.DF;
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.UTF8;
 
 /****************************************************************
  * Implement the FileSystem API for the native filesystem.
@@ -30,6 +29,7 @@
  * @author Mike Cafarella
  *****************************************************************/
 public class LocalFileSystem extends FileSystem {
+    private File workingDir = new File(System.getProperty(""user.dir""));
     TreeMap sharedLockDataSet = new TreeMap();
     TreeMap nonsharedLockDataSet = new TreeMap();
     TreeMap lockObjSet = new TreeMap();
@@ -109,6 +109,7 @@
     }
     
     public FSInputStream openRaw(File f) throws IOException {
+        f = makeAbsolute(f);
         if (! f.exists()) {
             throw new FileNotFoundException(f.toString());
         }
@@ -151,8 +152,17 @@
       }
     }
 
+    private File makeAbsolute(File f) {
+      if (f.isAbsolute()) {
+        return f;
+      } else {
+        return new File(workingDir, f.toString());
+      }
+    }
+    
     public FSOutputStream createRaw(File f, boolean overwrite)
       throws IOException {
+        f = makeAbsolute(f);
         if (f.exists() && ! overwrite) {
             throw new IOException(""File already exists:""+f);
         }
@@ -164,6 +174,8 @@
     }
 
     public boolean renameRaw(File src, File dst) throws IOException {
+        src = makeAbsolute(src);
+        dst = makeAbsolute(dst);
         if (useCopyForRename) {
             FileUtil.copyContents(this, src, dst, true, getConf());
             return fullyDelete(src);
@@ -171,32 +183,54 @@
     }
 
     public boolean deleteRaw(File f) throws IOException {
+        f = makeAbsolute(f);
         if (f.isFile()) {
             return f.delete();
         } else return fullyDelete(f);
     }
 
     public boolean exists(File f) throws IOException {
+        f = makeAbsolute(f);
         return f.exists();
     }
 
     public boolean isDirectory(File f) throws IOException {
+        f = makeAbsolute(f);
         return f.isDirectory();
     }
 
     public long getLength(File f) throws IOException {
+        f = makeAbsolute(f);
         return f.length();
     }
 
     public File[] listFilesRaw(File f) throws IOException {
+        f = makeAbsolute(f);
         return f.listFiles();
     }
 
     public void mkdirs(File f) throws IOException {
+        f = makeAbsolute(f);
         f.mkdirs();
     }
 
+    /**
+     * Set the working directory to the given directory.
+     * Sets both a local variable and the system property.
+     * Note that the system property is only used if the application explictly
+     * calls java.io.File.getAbsolutePath().
+     */
+    public void setWorkingDirectory(File new_dir) {
+      workingDir = makeAbsolute(new_dir);
+      System.setProperty(""user.dir"", workingDir.toString());
+    }
+    
+    public File getWorkingDirectory() {
+      return workingDir;
+    }
+    
     public synchronized void lock(File f, boolean shared) throws IOException {
+        f = makeAbsolute(f);
         f.createNewFile();
 
         FileLock lockObj = null;
@@ -213,6 +247,7 @@
     }
 
     public synchronized void release(File f) throws IOException {
+        f = makeAbsolute(f);
         FileLock lockObj = (FileLock) lockObjSet.get(f);
         FileInputStream sharedLockData = (FileInputStream) sharedLockDataSet.get(f);
         FileOutputStream nonsharedLockData = (FileOutputStream) nonsharedLockDataSet.get(f);
@@ -238,6 +273,8 @@
     // In the case of the local filesystem, we can just rename the file.
     public void moveFromLocalFile(File src, File dst) throws IOException {
         if (! src.equals(dst)) {
+            src = makeAbsolute(src);
+            dst = makeAbsolute(dst);
             if (useCopyForRename) {
                 FileUtil.copyContents(this, src, dst, true, getConf());
                 fullyDelete(src);
@@ -248,6 +285,8 @@
     // Similar to moveFromLocalFile(), except the source is kept intact.
     public void copyFromLocalFile(File src, File dst) throws IOException {
         if (! src.equals(dst)) {
+            src = makeAbsolute(src);
+            dst = makeAbsolute(dst);
             FileUtil.copyContents(this, src, dst, true, getConf());
         }
     }
@@ -255,13 +294,15 @@
     // We can't delete the src file in this case.  Too bad.
     public void copyToLocalFile(File src, File dst) throws IOException {
         if (! src.equals(dst)) {
+            src = makeAbsolute(src);
+            dst = makeAbsolute(dst);
             FileUtil.copyContents(this, src, dst, true, getConf());
         }
     }
 
     // We can write output directly to the final location
     public File startLocalOutput(File fsOutputFile, File tmpLocalFile) throws IOException {
-        return fsOutputFile;
+        return makeAbsolute(fsOutputFile);
     }
 
     // It's in the right place - nothing to do.
@@ -270,7 +311,7 @@
 
     // We can read directly from the real local fs.
     public File startLocalInput(File fsInputFile, File tmpLocalFile) throws IOException {
-        return fsInputFile;
+        return makeAbsolute(fsInputFile);
     }
 
     // We're done reading.  Nothing to clean up.
@@ -292,6 +333,7 @@
      * @throws IOException
      */
     private boolean fullyDelete(File dir) throws IOException {
+        dir = makeAbsolute(dir);
         File contents[] = dir.listFiles();
         if (contents != null) {
             for (int i = 0; i < contents.length; i++) {
@@ -315,7 +357,7 @@
                                       long start, long length, int crc) {
       try {
         // canonicalize f   
-        f = f.getCanonicalFile();
+        f = makeAbsolute(f).getCanonicalFile();
       
         // find highest writable parent dir of f on the same device
         String device = new DF(f.toString()).getMount();
"
hadoop,8cdc95ec9e14b11aa5138d53d3908c53625428bc,"Fix for HADOOP-52.  Add username and working-directory to FileSystem and JobConf and use these to resolve relative paths.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@388229 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-23 18:38:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/InputFormatBase.java b/src/java/org/apache/hadoop/mapred/InputFormatBase.java
index fb0bc31..1f4231c 100644
--- a/src/java/org/apache/hadoop/mapred/InputFormatBase.java
+++ b/src/java/org/apache/hadoop/mapred/InputFormatBase.java
@@ -59,9 +59,15 @@
   protected File[] listFiles(FileSystem fs, JobConf job)
     throws IOException {
     File[] dirs = job.getInputDirs();
+    String workDir = job.getWorkingDirectory();
     String subdir = job.get(""mapred.input.subdir"");
     ArrayList result = new ArrayList();
     for (int i = 0; i < dirs.length; i++) {
+      // if it is relative, make it absolute using the directory from the 
+      // JobConf
+      if (workDir != null && !dirs[i].isAbsolute()) {
+        dirs[i] = new File(workDir, dirs[i].toString());
+      }
       File[] dir = fs.listFiles(dirs[i]);
       if (dir != null) {
         for (int j = 0; j < dir.length; j++) {
"
hadoop,8cdc95ec9e14b11aa5138d53d3908c53625428bc,"Fix for HADOOP-52.  Add username and working-directory to FileSystem and JobConf and use these to resolve relative paths.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@388229 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-23 18:38:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobClient.java b/src/java/org/apache/hadoop/mapred/JobClient.java
index 1d1a4cc..9af95b7 100644
--- a/src/java/org/apache/hadoop/mapred/JobClient.java
+++ b/src/java/org/apache/hadoop/mapred/JobClient.java
@@ -247,7 +247,14 @@
           getFs().copyFromLocalFile(new File(originalJarPath), submitJarFile);
         }
 
-        FileSystem fs = getFs();
+        FileSystem fileSys = getFs();
+
+        // Set the user's name and working directory
+        String user = System.getProperty(""user.name"");
+        job.setUser(user != null ? user : ""Dr Who"");
+        if (job.getWorkingDirectory() == null) {
+          job.setWorkingDirectory(fileSys.getWorkingDirectory().toString());          
+        }
 
         // Ensure that the output directory is set and not already there
         File outDir = job.getOutputDir();
@@ -260,7 +267,7 @@
         }
 
         // Write job file to JobTracker's fs        
-        FSDataOutputStream out = fs.create(submitJobFile);
+        FSDataOutputStream out = fileSys.create(submitJobFile);
         try {
           job.write(out);
         } finally {
"
hadoop,8cdc95ec9e14b11aa5138d53d3908c53625428bc,"Fix for HADOOP-52.  Add username and working-directory to FileSystem and JobConf and use these to resolve relative paths.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@388229 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-23 18:38:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobConf.java b/src/java/org/apache/hadoop/mapred/JobConf.java
index 81b8cc9..0476936 100644
--- a/src/java/org/apache/hadoop/mapred/JobConf.java
+++ b/src/java/org/apache/hadoop/mapred/JobConf.java
@@ -143,6 +143,38 @@
     return result;
   }
 
+  /**
+   * Get the reported username for this job.
+   * @return the username
+   */
+  public String getUser() {
+    return get(""user.name"");
+  }
+  
+  /**
+   * Set the reported username for this job.
+   * @param user the username
+   */
+  public void setUser(String user) {
+    set(""user.name"", user);
+  }
+  
+  /**
+   * Set the current working directory for the default file system
+   * @param dir the new current working directory
+   */
+  public void setWorkingDirectory(String dir) {
+    set(""mapred.working.dir"", dir);
+  }
+  
+  /**
+   * Get the current working directory for the default file system.
+   * @return the directory name
+   */
+  public String getWorkingDirectory() {
+    return get(""mapred.working.dir""); 
+  }
+  
   public File getOutputDir() { 
     String name = get(""mapred.output.dir"");
     return name == null ? null: new File(name);
"
hadoop,8cdc95ec9e14b11aa5138d53d3908c53625428bc,"Fix for HADOOP-52.  Add username and working-directory to FileSystem and JobConf and use these to resolve relative paths.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@388229 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-23 18:38:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobInProgress.java b/src/java/org/apache/hadoop/mapred/JobInProgress.java
index 6bb460c..5844f85 100644
--- a/src/java/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/JobInProgress.java
@@ -62,7 +62,6 @@
         String jobid = ""job_"" + jobtracker.createUniqueId();
         String url = ""http://"" + jobtracker.getJobTrackerMachine() + "":"" + jobtracker.getInfoPort() + ""/jobdetails.jsp?jobid="" + jobid;
         this.jobtracker = jobtracker;
-        this.profile = new JobProfile(jobid, jobFile, url);
         this.status = new JobStatus(jobid, 0.0f, 0.0f, JobStatus.PREP);
         this.startTime = System.currentTimeMillis();
 
@@ -75,6 +74,7 @@
         fs.copyToLocalFile(new File(jobFile), localJobFile);
 
         conf = new JobConf(localJobFile);
+        this.profile = new JobProfile(conf.getUser(), jobid, jobFile, url);
 
         String jarFile = conf.getJar();
         if (jarFile != null) {
"
hadoop,8cdc95ec9e14b11aa5138d53d3908c53625428bc,"Fix for HADOOP-52.  Add username and working-directory to FileSystem and JobConf and use these to resolve relative paths.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@388229 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-23 18:38:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobProfile.java b/src/java/org/apache/hadoop/mapred/JobProfile.java
index 562dd1f..380ea1e 100644
--- a/src/java/org/apache/hadoop/mapred/JobProfile.java
+++ b/src/java/org/apache/hadoop/mapred/JobProfile.java
@@ -36,6 +36,7 @@
          });
     }
 
+    String user;
     String jobid;
     String jobFile;
     String url;
@@ -47,13 +48,21 @@
 
     /**
      */
-    public JobProfile(String jobid, String jobFile, String url) {
+    public JobProfile(String user, String jobid, String jobFile, String url) {
+        this.user = user;
         this.jobid = jobid;
         this.jobFile = jobFile;
         this.url = url;
     }
 
     /**
+     * Get the user id.
+     */
+    public String getUser() {
+      return user;
+    }
+    
+    /**
      */
     public String getJobId() {
         return jobid;
@@ -83,11 +92,13 @@
         UTF8.writeString(out, jobid);
         UTF8.writeString(out, jobFile);
         UTF8.writeString(out, url);
+        UTF8.writeString(out, user);
     }
     public void readFields(DataInput in) throws IOException {
         this.jobid = UTF8.readString(in);
         this.jobFile = UTF8.readString(in);
         this.url = UTF8.readString(in);
+        this.user = UTF8.readString(in);
     }
 }
 
"
hadoop,8cdc95ec9e14b11aa5138d53d3908c53625428bc,"Fix for HADOOP-52.  Add username and working-directory to FileSystem and JobConf and use these to resolve relative paths.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@388229 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-23 18:38:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobSubmissionProtocol.java b/src/java/org/apache/hadoop/mapred/JobSubmissionProtocol.java
index 8ee4563..b371425 100644
--- a/src/java/org/apache/hadoop/mapred/JobSubmissionProtocol.java
+++ b/src/java/org/apache/hadoop/mapred/JobSubmissionProtocol.java
@@ -17,7 +17,6 @@
 package org.apache.hadoop.mapred;
 
 import java.io.*;
-import java.util.*;
 
 /** 
  * Protocol that a JobClient and the central JobTracker use to communicate.  The
"
hadoop,8cdc95ec9e14b11aa5138d53d3908c53625428bc,"Fix for HADOOP-52.  Add username and working-directory to FileSystem and JobConf and use these to resolve relative paths.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@388229 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-23 18:38:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/LocalJobRunner.java b/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
index 010cfc6..8385fc1 100644
--- a/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
+++ b/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
@@ -44,6 +44,7 @@
     private JobStatus status = new JobStatus();
     private ArrayList mapIds = new ArrayList();
     private MapOutputFile mapoutputFile;
+    private JobProfile profile;
 
     public Job(String file, Configuration conf) throws IOException {
       this.file = file;
@@ -54,8 +55,8 @@
       File localFile = new JobConf(conf).getLocalFile(""localRunner"", id+"".xml"");
       fs.copyToLocalFile(new File(file), localFile);
       this.job = new JobConf(localFile);
-      
-      
+      profile = new JobProfile(job.getUser(), id, file, 
+                               ""http://localhost:8080/"");
       this.status.jobid = id;
       this.status.runState = JobStatus.RUNNING;
 
@@ -64,15 +65,30 @@
       this.start();
     }
 
+    JobProfile getProfile() {
+      return profile;
+    }
+    
+    private void setWorkingDirectory(JobConf conf, FileSystem fs) {
+      String dir = conf.getWorkingDirectory();
+      if (dir != null) {
+        fs.setWorkingDirectory(new File(dir));
+      }
+    }
+    
     public void run() {
       try {
         // split input into minimum number of splits
-        FileSplit[] splits = job.getInputFormat().getSplits(fs, job, 1);
+        FileSplit[] splits;
+        setWorkingDirectory(job, fs);
+        splits = job.getInputFormat().getSplits(fs, job, 1);
 
+        
         // run a map task for each split
         job.setNumReduceTasks(1);                 // force a single reduce task
         for (int i = 0; i < splits.length; i++) {
           mapIds.add(""map_"" + newId());
+          setWorkingDirectory(job, fs);
           MapTask map = new MapTask(file, (String)mapIds.get(i), splits[i]);
           map.setConf(job);
           map_tasks += 1;
@@ -97,10 +113,9 @@
         for (int i = 0; i < mapIds.size(); i++) {
             mapDependencies[i][0] = (String) mapIds.get(i);
         }
-        ReduceTask reduce =
-          new ReduceTask(file, reduceId,
-                         mapDependencies,
-                         0);
+        setWorkingDirectory(job, fs);
+        ReduceTask reduce = new ReduceTask(file, reduceId,
+            mapDependencies,0);
         reduce.setConf(job);
         reduce_tasks += 1;
         reduce.run(job, this);
@@ -172,7 +187,7 @@
 
   public JobProfile getJobProfile(String id) {
     Job job = (Job)jobs.get(id);
-    return new JobProfile(id, job.file, ""http://localhost:8080/"");
+    return job.getProfile();
   }
 
   public TaskReport[] getMapTaskReports(String id) {
"
hadoop,8cdc95ec9e14b11aa5138d53d3908c53625428bc,"Fix for HADOOP-52.  Add username and working-directory to FileSystem and JobConf and use these to resolve relative paths.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@388229 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-23 18:38:47,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 8197e69..f4da938 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -700,6 +700,12 @@
           startPinging(umbilical, taskid);        // start pinging parent
 
           try {
+              // If the user set a working directory, use it
+              String workDir = job.getWorkingDirectory();
+              if (workDir != null) {
+                FileSystem file_sys = FileSystem.get(job);
+                file_sys.setWorkingDirectory(new File(workDir));
+              }
               task.run(job, umbilical);           // run the task
           } catch (FSError e) {
             LOG.log(Level.SEVERE, ""FSError from child"", e);
"
hadoop,f65655705b88d25e24a9b12b41cfeb3c49e322d3,"Fix for HADOOP-83.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@388225 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-23 18:12:28,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSClient.java b/src/java/org/apache/hadoop/dfs/DFSClient.java
index 5494a6a..79feec5 100644
--- a/src/java/org/apache/hadoop/dfs/DFSClient.java
+++ b/src/java/org/apache/hadoop/dfs/DFSClient.java
@@ -40,7 +40,7 @@
  ********************************************************/
 class DFSClient implements FSConstants {
     public static final Logger LOG = LogFormatter.getLogger(""org.apache.hadoop.fs.DFSClient"");
-    static int MAX_BLOCK_ACQUIRE_FAILURES = 10;
+    static int MAX_BLOCK_ACQUIRE_FAILURES = 3;
     ClientProtocol namenode;
     String localName;
     boolean running = true;
@@ -358,17 +358,15 @@
                     chosenNode = bestNode(nodes[targetBlock], deadNodes);
                     targetAddr = DataNode.createSocketAddr(chosenNode.getName().toString());
                 } catch (IOException ie) {
-                    /**
                     if (failures >= MAX_BLOCK_ACQUIRE_FAILURES) {
                         throw new IOException(""Could not obtain block "" + blocks[targetBlock]);
                     }
-                    **/
                     if (nodes[targetBlock] == null || nodes[targetBlock].length == 0) {
                         LOG.info(""No node available for block "" + blocks[targetBlock]);
                     }
                     LOG.info(""Could not obtain block from any node:  "" + ie);
                     try {
-                        Thread.sleep(10000);
+                        Thread.sleep(3000);
                     } catch (InterruptedException iex) {
                     }
                     deadNodes.clear();
"
hadoop,f6db106b479e674ec98317e9d71916a7bb9a07f7,"Fix for HADOOP-3.  Don't permit jobs to write to a pre-existing output directory.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@387928 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-22 20:14:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobClient.java b/src/java/org/apache/hadoop/mapred/JobClient.java
index 46009e9..1d1a4cc 100644
--- a/src/java/org/apache/hadoop/mapred/JobClient.java
+++ b/src/java/org/apache/hadoop/mapred/JobClient.java
@@ -247,8 +247,20 @@
           getFs().copyFromLocalFile(new File(originalJarPath), submitJarFile);
         }
 
-        // Write job file to JobTracker's fs
-        FSDataOutputStream out = getFs().create(submitJobFile);
+        FileSystem fs = getFs();
+
+        // Ensure that the output directory is set and not already there
+        File outDir = job.getOutputDir();
+        if (outDir == null && job.getNumReduceTasks() != 0) {
+            throw new IOException(""Output directory not set in JobConf."");
+        }
+        if (outDir != null && fs.exists(outDir)) {
+            throw new IOException(""Output directory "" + outDir + 
+                                  "" already exists."");
+        }
+
+        // Write job file to JobTracker's fs        
+        FSDataOutputStream out = fs.create(submitJobFile);
         try {
           job.write(out);
         } finally {
@@ -347,7 +359,6 @@
         }
 
         // Process args
-        String jobTrackerStr = argv[0];
         String jobTrackerSpec = null;
         String submitJobFile = null;
         String jobid = null;
"
hadoop,f6db106b479e674ec98317e9d71916a7bb9a07f7,"Fix for HADOOP-3.  Don't permit jobs to write to a pre-existing output directory.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@387928 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-22 20:14:06,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobConf.java b/src/java/org/apache/hadoop/mapred/JobConf.java
index 10c3df2..81b8cc9 100644
--- a/src/java/org/apache/hadoop/mapred/JobConf.java
+++ b/src/java/org/apache/hadoop/mapred/JobConf.java
@@ -143,7 +143,11 @@
     return result;
   }
 
-  public File getOutputDir() { return new File(get(""mapred.output.dir"")); }
+  public File getOutputDir() { 
+    String name = get(""mapred.output.dir"");
+    return name == null ? null: new File(name);
+  }
+
   public void setOutputDir(File dir) { set(""mapred.output.dir"", dir); }
 
   public InputFormat getInputFormat() {
"
hadoop,9b0f9645d3bf163549ceaf3f6675af25e647d889,"Fix for HADOOP-97.  Improve error handling.  Contributed by Konstantin Shvachko.


git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@387636 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-21 22:01:27,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSShell.java b/src/java/org/apache/hadoop/dfs/DFSShell.java
index 0731ba3..35031ae 100644
--- a/src/java/org/apache/hadoop/dfs/DFSShell.java
+++ b/src/java/org/apache/hadoop/dfs/DFSShell.java
@@ -66,9 +66,8 @@
     }
 
     void cat(String srcf) throws IOException {
-      FSDataInputStream in = null;
+      FSDataInputStream in = fs.open(new File(srcf));
       try {
-        in = fs.open(new File(srcf));
         DataInputStream din = new DataInputStream(new BufferedInputStream(in));
         String line;
         while((line = din.readLine()) != null) {
@@ -242,10 +241,10 @@
         Configuration conf = new Configuration();
         int i = 0;
         FileSystem fs = FileSystem.parseArgs(argv, i, conf);
+        String cmd = argv[i++];
         try {
             DFSShell tc = new DFSShell(fs);
 
-            String cmd = argv[i++];
             if (""-put"".equals(cmd) || ""-copyFromLocal"".equals(cmd)) {
                 tc.copyFromLocal(new File(argv[i++]), argv[i++]);
             } else if (""-moveFromLocal"".equals(cmd)) {
@@ -277,6 +276,9 @@
                 tc.report();
             }
             System.exit(0);
+        } catch (IOException e ) {
+          System.err.println( cmd.substring(1) + "": "" + e.getLocalizedMessage() );  
+          System.exit(-1);
         } finally {
             fs.close();
         }
"
hadoop,7b5ab5946f2340ca9840d69d9064229502102774,"Fix for HADOOP-93.  Convert min split size from int to long, and permit its specification in the config.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@387587 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-21 17:37:45,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/InputFormatBase.java b/src/java/org/apache/hadoop/mapred/InputFormatBase.java
index 73bb021..fb0bc31 100644
--- a/src/java/org/apache/hadoop/mapred/InputFormatBase.java
+++ b/src/java/org/apache/hadoop/mapred/InputFormatBase.java
@@ -33,9 +33,9 @@
 
   private static final double SPLIT_SLOP = 0.1;   // 10% slop
 
-  private int minSplitSize = 1;
+  private long minSplitSize = 1;
 
-  protected void setMinSplitSize(int minSplitSize) {
+  protected void setMinSplitSize(long minSplitSize) {
     this.minSplitSize = minSplitSize;
   }
 
@@ -112,8 +112,11 @@
       bytesPerSplit = fsBlockSize;
     }
 
-    if (bytesPerSplit < minSplitSize) {           // no smaller than min size
-      bytesPerSplit = minSplitSize;
+    long configuredMinSplitSize = job.getLong(""mapred.min.split.size"", 0);
+    if( configuredMinSplitSize < minSplitSize )
+    	configuredMinSplitSize = minSplitSize;
+    if (bytesPerSplit < configuredMinSplitSize) { // no smaller than min size
+      bytesPerSplit = configuredMinSplitSize;
     }
 
     long maxPerSplit = bytesPerSplit + (long)(bytesPerSplit*SPLIT_SLOP);
@@ -135,7 +138,9 @@
       if (bytesRemaining != 0) {
         splits.add(new FileSplit(file, length-bytesRemaining, bytesRemaining));
       }
+      //LOG.info( ""Generating splits for "" + i + ""th file: "" + file.getName() );
     }
+    //LOG.info( ""Total # of splits: "" + splits.size() );
     return (FileSplit[])splits.toArray(new FileSplit[splits.size()]);
   }
 
"
hadoop,74d3933823419393b84aeaaf7ebff85a39426cfe,"Fix for HADOOP-86.  Errors while reading map output now cause map task to fail and be re-executed.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@387279 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-20 19:08:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/MapOutputFile.java b/src/java/org/apache/hadoop/mapred/MapOutputFile.java
index cf9a034..ee83381 100644
--- a/src/java/org/apache/hadoop/mapred/MapOutputFile.java
+++ b/src/java/org/apache/hadoop/mapred/MapOutputFile.java
@@ -23,6 +23,8 @@
 import org.apache.hadoop.io.*;
 import org.apache.hadoop.fs.*;
 import org.apache.hadoop.conf.*;
+import org.apache.hadoop.ipc.Server;
+import org.apache.hadoop.mapred.TaskTracker.MapOutputServer;
 
 /** A local file to be transferred via the {@link MapOutputProtocol}. */ 
 class MapOutputFile implements Writable, Configurable {
@@ -106,16 +108,15 @@
     UTF8.writeString(out, reduceTaskId);
     out.writeInt(partition);
     
-    // write the length-prefixed file content to the wire
     File file = getOutputFile(mapTaskId, partition);
-    out.writeLong(file.length());
-
     FSDataInputStream in = null;
     try {
+      // write the length-prefixed file content to the wire
+      out.writeLong(file.length());
       in = FileSystem.getNamed(""local"", this.jobConf).open(file);
-    } catch (IOException e) {
-      // log a SEVERE exception in order to cause TaskTracker to exit
+    } catch (FileNotFoundException e) {
       TaskTracker.LOG.log(Level.SEVERE, ""Can't open map output:"" + file, e);
+      ((MapOutputServer)Server.get()).getTaskTracker().mapOutputLost(mapTaskId);
       throw e;
     }
     try {
@@ -127,8 +128,8 @@
         try {
           l = in.read(buffer);
         } catch (IOException e) {
-          // log a SEVERE exception in order to cause TaskTracker to exit
           TaskTracker.LOG.log(Level.SEVERE,""Can't read map output:"" + file, e);
+          ((MapOutputServer)Server.get()).getTaskTracker().mapOutputLost(mapTaskId);
           throw e;
         }
       }
"
hadoop,74d3933823419393b84aeaaf7ebff85a39426cfe,"Fix for HADOOP-86.  Errors while reading map output now cause map task to fail and be re-executed.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@387279 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-20 19:08:07,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTracker.java b/src/java/org/apache/hadoop/mapred/TaskTracker.java
index 75496e1..8197e69 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTracker.java
@@ -70,6 +70,15 @@
 
     private int maxCurrentTasks;
 
+    class MapOutputServer extends RPC.Server {
+      private MapOutputServer(int port, int threads) {
+        super(TaskTracker.this, fConf, port, threads, false);
+      }
+      public TaskTracker getTaskTracker() {
+        return TaskTracker.this;
+      }
+    }
+
     /**
      * Start with the local machine name, and the default JobTracker
      */
@@ -127,7 +136,7 @@
         }
         while (true) {
             try {
-                this.mapOutputServer = RPC.getServer(this, this.mapOutputPort, maxCurrentTasks, false, this.fConf);
+                this.mapOutputServer = new MapOutputServer(mapOutputPort, maxCurrentTasks);
                 this.mapOutputServer.start();
                 break;
             } catch (BindException e) {
@@ -305,11 +314,6 @@
               }
             }
             lastHeartbeat = now;
-
-            if (LogFormatter.hasLoggedSevere()) {
-              LOG.info(""Severe problem detected.  TaskTracker exiting."");
-              return STALE_STATE;
-            }
         }
 
         return 0;
@@ -539,6 +543,22 @@
         }
 
         /**
+         * The map output has been lost.
+         */
+        public synchronized void mapOutputLost() throws IOException {
+            if (runstate == TaskStatus.SUCCEEDED) {
+              LOG.info(""Reporting output lost:""+task.getTaskId());
+              runstate = TaskStatus.FAILED;       // change status to failure
+              synchronized (TaskTracker.this) {   // force into next heartbeat
+                runningTasks.put(task.getTaskId(), this);
+                mapTotal++;
+              }
+            } else {
+              LOG.warning(""Output already reported lost:""+task.getTaskId());
+            }
+        }
+
+        /**
          * We no longer need anything from this task.  Either the 
          * controlling job is all done and the files have been copied
          * away, or the task failed and we don't need the remains.
@@ -645,6 +665,18 @@
         }
     }
 
+    /**
+     * A completed map task's output has been lost.
+     */
+    public synchronized void mapOutputLost(String taskid) throws IOException {
+        TaskInProgress tip = (TaskInProgress) tasks.get(taskid);
+        if (tip != null) {
+          tip.mapOutputLost();
+        } else {
+          LOG.warning(""Unknown child with bad map output: ""+taskid+"". Ignored."");
+        }
+    }
+
     /** 
      * The main() for child processes. 
      */
"
hadoop,db9cbb3c4e1251c0c3b2ee7db4f4c18d706ee72b,"Fix for HADOOP-82.  Completes count should never be less than zero.  Contributed by Michael Stack.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@386226 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-16 00:04:38,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskInProgress.java b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
index 292e23d..7a690a8 100644
--- a/src/java/org/apache/hadoop/mapred/TaskInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
@@ -268,7 +268,9 @@
             status.setRunState(TaskStatus.FAILED);
         }
         this.recentTasks.remove(taskid);
-        this.completes--;
+        if (this.completes > 0) {
+            this.completes--;
+        }
 
         numTaskFailures++;
         if (numTaskFailures >= MAX_TASK_FAILURES) {
"
hadoop,98a20a93b42627afe691ead1467f06c3da4a998d,"Fix for HADOOP-81.  Job-specific parameters should be read from the job-specific configuration, not the daemon's.  This permits speculative execution, number of map & reduce tasks, etc. to be settable in the job.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@386224 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-16 00:01:42,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobConf.java b/src/java/org/apache/hadoop/mapred/JobConf.java
index 0a7fe3f..10c3df2 100644
--- a/src/java/org/apache/hadoop/mapred/JobConf.java
+++ b/src/java/org/apache/hadoop/mapred/JobConf.java
@@ -244,6 +244,22 @@
     setClass(""mapred.combiner.class"", theClass, Reducer.class);
   }
   
+  /**
+   * Should speculative execution be used for this job?
+   * @return Defaults to true
+   */
+  public boolean getSpeculativeExecution() { 
+    return getBoolean(""mapred.speculative.execution"", true);
+  }
+  
+  /**
+   * Turn on or off speculative execution for this job.
+   * In general, it should be turned off for map jobs that have side effects.
+   */
+  public void setSpeculativeExecution(boolean new_val) {
+    setBoolean(""mapred.speculative.execution"", new_val);
+  }
+  
   public int getNumMapTasks() { return getInt(""mapred.map.tasks"", 1); }
   public void setNumMapTasks(int n) { setInt(""mapred.map.tasks"", n); }
 
"
hadoop,98a20a93b42627afe691ead1467f06c3da4a998d,"Fix for HADOOP-81.  Job-specific parameters should be read from the job-specific configuration, not the daemon's.  This permits speculative execution, number of map & reduce tasks, etc. to be settable in the job.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@386224 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-16 00:01:42,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobInProgress.java b/src/java/org/apache/hadoop/mapred/JobInProgress.java
index 870add3..6bb460c 100644
--- a/src/java/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/JobInProgress.java
@@ -50,43 +50,46 @@
     long finishTime;
     String deleteUponCompletion = null;
 
-    Configuration conf;
+    private JobConf conf;
     boolean tasksInited = false;
 
     /**
      * Create a JobInProgress with the given job file, plus a handle
      * to the tracker.
      */
-    public JobInProgress(String jobFile, JobTracker jobtracker, Configuration conf) throws IOException {
+    public JobInProgress(String jobFile, JobTracker jobtracker, 
+                         Configuration default_conf) throws IOException {
         String jobid = ""job_"" + jobtracker.createUniqueId();
         String url = ""http://"" + jobtracker.getJobTrackerMachine() + "":"" + jobtracker.getInfoPort() + ""/jobdetails.jsp?jobid="" + jobid;
-        this.conf = conf;
         this.jobtracker = jobtracker;
         this.profile = new JobProfile(jobid, jobFile, url);
         this.status = new JobStatus(jobid, 0.0f, 0.0f, JobStatus.PREP);
         this.startTime = System.currentTimeMillis();
 
-        this.localJobFile = new JobConf(conf).getLocalFile(JobTracker.SUBDIR, jobid + "".xml"");
-        this.localJarFile = new JobConf(conf).getLocalFile(JobTracker.SUBDIR, jobid + "".jar"");
-        FileSystem fs = FileSystem.get(conf);
+        JobConf default_job_conf = new JobConf(default_conf);
+        this.localJobFile = default_job_conf.getLocalFile(JobTracker.SUBDIR, 
+            jobid + "".xml"");
+        this.localJarFile = default_job_conf.getLocalFile(JobTracker.SUBDIR, 
+            jobid + "".jar"");
+        FileSystem fs = FileSystem.get(default_conf);
         fs.copyToLocalFile(new File(jobFile), localJobFile);
 
-        JobConf jd = new JobConf(localJobFile);
+        conf = new JobConf(localJobFile);
 
-        String jarFile = jd.getJar();
+        String jarFile = conf.getJar();
         if (jarFile != null) {
           fs.copyToLocalFile(new File(jarFile), localJarFile);
-          jd.setJar(localJarFile.getCanonicalPath());
+          conf.setJar(localJarFile.getCanonicalPath());
         }
 
-        this.numMapTasks = jd.getNumMapTasks();
-        this.numReduceTasks = jd.getNumReduceTasks();
+        this.numMapTasks = conf.getNumMapTasks();
+        this.numReduceTasks = conf.getNumReduceTasks();
 
         //
         // If a jobFile is in the systemDir, we can delete it (and
         // its JAR) upon completion
         //
-        File systemDir = jd.getSystemDir();
+        File systemDir = conf.getSystemDir();
         if (jobFile.startsWith(systemDir.getPath())) {
             this.deleteUponCompletion = jobFile;
         }
"
hadoop,98a20a93b42627afe691ead1467f06c3da4a998d,"Fix for HADOOP-81.  Job-specific parameters should be read from the job-specific configuration, not the daemon's.  This permits speculative execution, number of map & reduce tasks, etc. to be settable in the job.  Contributed by Owen O'Malley.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@386224 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-16 00:01:42,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskInProgress.java b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
index 7e0c794..292e23d 100644
--- a/src/java/org/apache/hadoop/mapred/TaskInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
@@ -47,37 +47,37 @@
     public static final Logger LOG = LogFormatter.getLogger(""org.apache.hadoop.mapred.TaskInProgress"");
 
     // Defines the TIP
-    String jobFile = null;
-    FileSplit split = null;
-    String hints[][] = null;
-    TaskInProgress predecessors[] = null;
-    int partition;
-    JobTracker jobtracker;
-    String id;
-    String totalTaskIds[];
-    JobInProgress job;
+    private String jobFile = null;
+    private FileSplit split = null;
+    private String hints[][] = null;
+    private TaskInProgress predecessors[] = null;
+    private int partition;
+    private JobTracker jobtracker;
+    private String id;
+    private String totalTaskIds[];
+    private JobInProgress job;
 
     // Status of the TIP
-    int numTaskFailures = 0;
-    double progress = 0;
-    String state = """";
-    long startTime = 0;
-    int completes = 0;
-    boolean failed = false;
-    TreeSet usableTaskIds = new TreeSet();
-    TreeSet recentTasks = new TreeSet();
-    Configuration conf;
+    private int numTaskFailures = 0;
+    private double progress = 0;
+    private String state = """";
+    private long startTime = 0;
+    private int completes = 0;
+    private boolean failed = false;
+    private TreeSet usableTaskIds = new TreeSet();
+    private TreeSet recentTasks = new TreeSet();
+    private JobConf conf;
     
-    TreeMap taskDiagnosticData = new TreeMap();
-    TreeMap taskStatuses = new TreeMap();
+    private TreeMap taskDiagnosticData = new TreeMap();
+    private TreeMap taskStatuses = new TreeMap();
 
-    TreeSet machinesWhereFailed = new TreeSet();
-    TreeSet tasksReportedClosed = new TreeSet();
+    private TreeSet machinesWhereFailed = new TreeSet();
+    private TreeSet tasksReportedClosed = new TreeSet();
 
     /**
      * Constructor for MapTask
      */
-    public TaskInProgress(String jobFile, FileSplit split, JobTracker jobtracker, Configuration conf, JobInProgress job) {
+    public TaskInProgress(String jobFile, FileSplit split, JobTracker jobtracker, JobConf conf, JobInProgress job) {
         this.jobFile = jobFile;
         this.split = split;
         this.jobtracker = jobtracker;
@@ -89,7 +89,7 @@
     /**
      * Constructor for ReduceTask
      */
-    public TaskInProgress(String jobFile, TaskInProgress predecessors[], int partition, JobTracker jobtracker, Configuration conf, JobInProgress job) {
+    public TaskInProgress(String jobFile, TaskInProgress predecessors[], int partition, JobTracker jobtracker, JobConf conf, JobInProgress job) {
         this.jobFile = jobFile;
         this.predecessors = predecessors;
         this.partition = partition;
@@ -408,7 +408,7 @@
         //
         if (isMapTask() &&
             recentTasks.size() <= MAX_TASK_EXECS &&
-            conf.getBoolean(""mapred.speculative.execution"", true) &&
+            conf.getSpeculativeExecution() &&
             (averageProgress - progress >= SPECULATIVE_GAP) &&
             (System.currentTimeMillis() - startTime >= SPECULATIVE_LAG)) {
             return true;
"
hadoop,00cea5a675cb1c33b32f63d2b0d50df68da61c10,"Fix for HADOOP-66.  Delete dfs temp files on JVM exit.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@386220 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-15 23:54:37,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSClient.java b/src/java/org/apache/hadoop/dfs/DFSClient.java
index 6cbb34e..5494a6a 100644
--- a/src/java/org/apache/hadoop/dfs/DFSClient.java
+++ b/src/java/org/apache/hadoop/dfs/DFSClient.java
@@ -557,9 +557,11 @@
         }
 
         private File newBackupFile() throws IOException {
-          return conf.getFile(""dfs.data.dir"",
-                              ""tmp""+File.separator+
-                              ""client-""+Math.abs(r.nextLong()));
+          File result = conf.getFile(""dfs.data.dir"",
+                                     ""tmp""+File.separator+
+                                     ""client-""+Math.abs(r.nextLong()));
+          result.deleteOnExit();
+          return result;
         }
 
         /**
"
hadoop,177ef6436fec3ab75b7f87abba5cb33d45c69209,"Fix for HADOOP-78.  Stream buffering was mistakenly disabled in writes
by the RPC client.  Identified & fixed by Owen O'Malley.


git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@385696 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-13 23:17:09,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/Client.java b/src/java/org/apache/hadoop/ipc/Client.java
index e8c0b14..dbbef5e 100644
--- a/src/java/org/apache/hadoop/ipc/Client.java
+++ b/src/java/org/apache/hadoop/ipc/Client.java
@@ -126,7 +126,7 @@
         (new BufferedOutputStream
          (new FilterOutputStream(socket.getOutputStream()) {
              public void write(byte[] buf, int o, int len) throws IOException {
-               super.write(buf, o, len);
+               out.write(buf, o, len);
                if (writingCall != null) {
                  writingCall.touch();
                }
"
hadoop,820a7cba012283a6fd720b5588cca847ffddad5f,"Fix for HADOOP-77.  Fixes some NPEs.  Contributed by Stefan.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@385655 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-13 20:39:22,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobInProgress.java b/src/java/org/apache/hadoop/mapred/JobInProgress.java
index 09f9c5f..870add3 100644
--- a/src/java/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/JobInProgress.java
@@ -38,8 +38,8 @@
     File localJobFile = null;
     File localJarFile = null;
 
-    TaskInProgress maps[] = null;
-    TaskInProgress reduces[] = null;
+    TaskInProgress maps[] = new TaskInProgress[0];
+    TaskInProgress reduces[] = new TaskInProgress[0];
     int numMapTasks = 0;
     int numReduceTasks = 0;
 
"
hadoop,7dd5fc4a7dab46fd21811acc81d05bc7c4e65dab,"Fix for HADOOP-66.  DFS blocks are no longer written to local temp files.  If a connection to a datanode fails then an exception is now thrown, rather than trying to re-connect to another datanode.  Timeouts were also removed from datanode connections, since these caused a lot of failed connections.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@384385 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-09 00:17:27,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSClient.java b/src/java/org/apache/hadoop/dfs/DFSClient.java
index c0256d6..f46af26 100644
--- a/src/java/org/apache/hadoop/dfs/DFSClient.java
+++ b/src/java/org/apache/hadoop/dfs/DFSClient.java
@@ -377,7 +377,7 @@
                 }
                 try {
                     s = new Socket(targetAddr.getAddress(), targetAddr.getPort());
-                    s.setSoTimeout(READ_TIMEOUT);
+                    //s.setSoTimeout(READ_TIMEOUT);
 
                     //
                     // Xmit header info to datanode
@@ -528,11 +528,8 @@
         private UTF8 src;
         boolean closingDown = false;
         private boolean overwrite;
-        private boolean blockStreamWorking;
         private DataOutputStream blockStream;
         private DataInputStream blockReplyStream;
-        private File backupFile;
-        private OutputStream backupStream;
         private Block block;
         private DatanodeInfo targets[]; 
         private long filePos = 0;
@@ -546,9 +543,7 @@
             this.overwrite = overwrite;
             this.blockStream = null;
             this.blockReplyStream = null;
-            this.blockStreamWorking = false;
-            this.backupFile = File.createTempFile(""dfsout"", ""bak"");
-            this.backupStream = new BufferedOutputStream(new FileOutputStream(backupFile));
+
             nextBlockOutputStream(true);
         }
 
@@ -558,12 +553,6 @@
          * Must get block ID and the IDs of the destinations from the namenode.
          */
         private synchronized void nextBlockOutputStream(boolean firstTime) throws IOException {
-            if (! firstTime && blockStreamWorking) {
-                blockStream.close();
-                blockReplyStream.close();
-                blockStreamWorking = false;
-            }
-
             boolean retry = false;
             long start = System.currentTimeMillis();
             do {
@@ -602,7 +591,7 @@
                 Socket s = null;
                 try {
                     s = new Socket(target.getAddress(), target.getPort());
-                    s.setSoTimeout(READ_TIMEOUT);
+                    //s.setSoTimeout(READ_TIMEOUT);
                 } catch (IOException ie) {
                     // Connection failed.  Let's wait a little bit and retry
                     try {
@@ -636,7 +625,6 @@
                 bytesWrittenToBlock = 0;
                 blockStream = out;
                 blockReplyStream = new DataInputStream(new BufferedInputStream(s.getInputStream()));
-                blockStreamWorking = true;
             } while (retry);
         }
 
@@ -717,27 +705,21 @@
                 //
                 // To the blockStream, write length, then bytes
                 //
-                if (blockStreamWorking) {
-                    try {
-                        blockStream.writeLong(workingPos);
-                        blockStream.write(outBuf, 0, workingPos);
-                    } catch (IOException ie) {
-                        try {
-                            blockStream.close();
-                        } catch (IOException ie2) {
-                        }
-                        try {
-                            blockReplyStream.close();
-                        } catch (IOException ie2) {
-                        }
-                        namenode.abandonBlock(block, src.toString());
-                        blockStreamWorking = false;
-                    }
+                try {
+                  blockStream.writeLong(workingPos);
+                  blockStream.write(outBuf, 0, workingPos);
+                } catch (IOException ie) {
+                  try {
+                    blockStream.close();
+                  } catch (IOException ie2) {
+                  }
+                  try {
+                    blockReplyStream.close();
+                  } catch (IOException ie2) {
+                  }
+                  namenode.abandonBlock(block, src.toString());
+                  throw ie;
                 }
-                //
-                // To the local block backup, write just the bytes
-                //
-                backupStream.write(outBuf, 0, workingPos);
 
                 //
                 // Track position
@@ -752,79 +734,20 @@
          * We're done writing to the current block.
          */
         private synchronized void endBlock() throws IOException {
-            boolean mustRecover = ! blockStreamWorking;
-
-            //
-            // A zero-length set of data indicates the end of the block
-            //
-            if (blockStreamWorking) {
-                try {
-                    internalClose();
-                } catch (IOException ie) {
-                    try {
-                        blockStream.close();
-                    } catch (IOException ie2) {
-                    }
-                    try {
-                        blockReplyStream.close();
-                    } catch (IOException ie2) {
-                    }
-                    namenode.abandonBlock(block, src.toString());
-                    mustRecover = true;
-                } finally {
-                    blockStreamWorking = false;
-                }
+            try {
+              internalClose();
+            } catch (IOException ie) {
+              namenode.abandonBlock(block, src.toString());
+              throw ie;
             }
-
-            //
-            // Done with local copy
-            //
-            backupStream.close();
-
-            //
-            // If necessary, recover from a failed datanode connection.
-            //
-            while (mustRecover) {
-                nextBlockOutputStream(false);
-                InputStream in = new FileInputStream(backupFile);
-                try {
-                    byte buf[] = new byte[BUFFER_SIZE];
-                    int bytesRead = in.read(buf);
-                    while (bytesRead >= 0) {
-                        blockStream.writeLong((long) bytesRead);
-                        blockStream.write(buf, 0, bytesRead);
-                        bytesRead = in.read(buf);
-                    }
-                    internalClose();
-                    LOG.info(""Recovered from failed datanode connection"");
-                    mustRecover = false;
-                } catch (IOException ie) {
-                    try {
-                        blockStream.close();
-                    } catch (IOException ie2) {
-                    }
-                    try {
-                        blockReplyStream.close();
-                    } catch (IOException ie2) {
-                    }
-                    namenode.abandonBlock(block, src.toString());
-                    blockStreamWorking = false;
-                }
-            }
-
-            //
-            // Delete local backup, start new one
-            //
-            backupFile.delete();
-            backupFile = File.createTempFile(""dfsout"", ""bak"");
-            backupStream = new BufferedOutputStream(new FileOutputStream(backupFile));
         }
 
         /**
-         * Close down stream to remote datanode.  Called from two places
-         * in endBlock();
+         * Close down stream to remote datanode.
          */
         private synchronized void internalClose() throws IOException {
+          try {
+            // A zero-length set of data indicates the end of the block
             blockStream.writeLong(0);
             blockStream.flush();
 
@@ -838,8 +761,16 @@
             lb.readFields(blockReplyStream);
             namenode.reportWrittenBlock(lb);
 
-            blockStream.close();
-            blockReplyStream.close();
+          } finally {
+            try {
+              blockStream.close();
+            } catch (IOException ie2) {
+            }
+            try {
+              blockReplyStream.close();
+            } catch (IOException ie2) {
+            }
+          }
         }
 
         /**
@@ -855,14 +786,9 @@
             flush();
             endBlock();
 
-            backupStream.close();
-            backupFile.delete();
+            blockStream.close();                
+            blockReplyStream.close();
 
-            if (blockStreamWorking) {
-                blockStream.close();                
-                blockReplyStream.close();
-                blockStreamWorking = false;
-            }
             super.close();
 
             long localstart = System.currentTimeMillis();
"
hadoop,7dd5fc4a7dab46fd21811acc81d05bc7c4e65dab,"Fix for HADOOP-66.  DFS blocks are no longer written to local temp files.  If a connection to a datanode fails then an exception is now thrown, rather than trying to re-connect to another datanode.  Timeouts were also removed from datanode connections, since these caused a lot of failed connections.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@384385 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-09 00:17:27,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index 611ecc0..2ce0a01 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -286,7 +286,7 @@
             try {
                 while (shouldListen) {
                     Socket s = ss.accept();
-                    s.setSoTimeout(READ_TIMEOUT);
+                    //s.setSoTimeout(READ_TIMEOUT);
                     new Daemon(new DataXceiver(s)).start();
                 }
                 ss.close();
@@ -368,10 +368,10 @@
                                     // Connect to backup machine
                                     mirrorTarget = createSocketAddr(targets[1].getName().toString());
                                     try {
-                                        Socket s = new Socket(mirrorTarget.getAddress(), mirrorTarget.getPort());
-                                        s.setSoTimeout(READ_TIMEOUT);
-                                        out2 = new DataOutputStream(new BufferedOutputStream(s.getOutputStream()));
-                                        in2 = new DataInputStream(new BufferedInputStream(s.getInputStream()));
+                                        Socket s2 = new Socket(mirrorTarget.getAddress(), mirrorTarget.getPort());
+                                        //s2.setSoTimeout(READ_TIMEOUT);
+                                        out2 = new DataOutputStream(new BufferedOutputStream(s2.getOutputStream()));
+                                        in2 = new DataInputStream(new BufferedInputStream(s2.getInputStream()));
 
                                         // Write connection header
                                         out2.write(OP_WRITE_BLOCK);
@@ -507,6 +507,7 @@
                             mirrors.add(curTarget);
                             LocatedBlock newLB = new LocatedBlock(b, (DatanodeInfo[]) mirrors.toArray(new DatanodeInfo[mirrors.size()]));
                             newLB.write(reply);
+                            reply.flush();
                         } finally {
                             reply.close();
                         }
"
hadoop,7dd5fc4a7dab46fd21811acc81d05bc7c4e65dab,"Fix for HADOOP-66.  DFS blocks are no longer written to local temp files.  If a connection to a datanode fails then an exception is now thrown, rather than trying to re-connect to another datanode.  Timeouts were also removed from datanode connections, since these caused a lot of failed connections.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@384385 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-09 00:17:27,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index d08b4bf..5734049 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -1277,15 +1277,14 @@
         //
         // Build list of machines we can actually choose from
         //
-        long latestRemaining = 0;
         Vector targetList = new Vector();
         for (Iterator it = datanodeMap.values().iterator(); it.hasNext(); ) {
             DatanodeInfo node = (DatanodeInfo) it.next();
             if (! forbiddenMachines.contains(node.getHost())) {
                 targetList.add(node);
-                latestRemaining += node.getRemaining();
             }
         }
+        Collections.shuffle(targetList);
 
         //
         // Now pick one
@@ -1309,12 +1308,9 @@
             //
             // Otherwise, choose node according to target capacity
             //
-            double target = Math.abs(r.nextDouble()) * latestRemaining;
             for (Iterator it = targetList.iterator(); it.hasNext(); ) {
                 DatanodeInfo node = (DatanodeInfo) it.next();
-                target -= node.getRemaining();
-                if ((node.getRemaining() > BLOCK_SIZE * MIN_BLOCKS_FOR_WRITE) &&
-                    (target <= 0)) {
+                if ((node.getRemaining() > BLOCK_SIZE * MIN_BLOCKS_FOR_WRITE)) {
                     return node;
                 }
             }
"
hadoop,7dd5fc4a7dab46fd21811acc81d05bc7c4e65dab,"Fix for HADOOP-66.  DFS blocks are no longer written to local temp files.  If a connection to a datanode fails then an exception is now thrown, rather than trying to re-connect to another datanode.  Timeouts were also removed from datanode connections, since these caused a lot of failed connections.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@384385 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-09 00:17:27,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/NameNode.java b/src/java/org/apache/hadoop/dfs/NameNode.java
index 244b260..ce067b9 100644
--- a/src/java/org/apache/hadoop/dfs/NameNode.java
+++ b/src/java/org/apache/hadoop/dfs/NameNode.java
@@ -184,9 +184,7 @@
      * The client needs to give up on the block.
      */
     public void abandonBlock(Block b, String src) throws IOException {
-        if (! namesystem.abandonBlock(b, new UTF8(src))) {
-            throw new IOException(""Cannot abandon block during write to "" + src);
-        }
+        namesystem.abandonBlock(b, new UTF8(src));
     }
     /**
      */
"
hadoop,7fd5df0c88934a1c5a2c3962b8ee7efec57b5ac6,"Fix for HADOOP-57.  Permit listing of ""/"" in dfs.  With help from Mahadev konar.


git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@384019 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-07 21:22:09,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java b/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
index 0880838..19b3b89 100644
--- a/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
+++ b/src/java/org/apache/hadoop/dfs/DistributedFileSystem.java
@@ -266,6 +266,9 @@
         path.append(DFSFile.DFS_FILE_SEPARATOR);
         path.append(l.get(i));
       }
+      if (f.isAbsolute() && path.length() == 0) {
+        path.append(DFSFile.DFS_FILE_SEPARATOR);
+      }
       return path.toString();
     }
 
"
hadoop,49601bd8c10b9439796c52dffcc322a56c1bcb33,"Fix for HADOOP-16.  Splitting and other job planning is now performed in a separate thread.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@382545 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-02 23:05:43,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSClient.java b/src/java/org/apache/hadoop/dfs/DFSClient.java
index 0993af5..e8b3f0b 100644
--- a/src/java/org/apache/hadoop/dfs/DFSClient.java
+++ b/src/java/org/apache/hadoop/dfs/DFSClient.java
@@ -585,7 +585,7 @@
                     // Connection failed.  Let's wait a little bit and retry
                     try {
                         if (System.currentTimeMillis() - start > 5000) {
-                            LOG.info(""Waiting to find target node"");
+                            LOG.info(""Waiting to find target node: "" + target);
                         }
                         Thread.sleep(6000);
                     } catch (InterruptedException iex) {
"
hadoop,49601bd8c10b9439796c52dffcc322a56c1bcb33,"Fix for HADOOP-16.  Splitting and other job planning is now performed in a separate thread.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@382545 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-02 23:05:43,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/FSNamesystem.java b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
index 8b3347c..c39ce48 100644
--- a/src/java/org/apache/hadoop/dfs/FSNamesystem.java
+++ b/src/java/org/apache/hadoop/dfs/FSNamesystem.java
@@ -550,6 +550,10 @@
         int endBlock = -1;
         Block blocks[] = dir.getFile(src);
 
+        if (blocks == null) {                     // no blocks
+            return new UTF8[0][];
+        }
+
         //
         // First, figure out where the range falls in
         // the blocklist.
@@ -579,7 +583,7 @@
         if (startBlock < 0 || endBlock < 0) {
             return new UTF8[0][];
         } else {
-            UTF8 hosts[][] = new UTF8[endBlock - startBlock + 1][];
+            UTF8 hosts[][] = new UTF8[(endBlock - startBlock) + 1][];
             for (int i = startBlock; i <= endBlock; i++) {
                 TreeSet containingNodes = (TreeSet) blocksMap.get(blocks[i]);
                 Vector v = new Vector();
@@ -587,7 +591,7 @@
                     DatanodeInfo cur = (DatanodeInfo) it.next();
                     v.add(cur.getHost());
                 }
-                hosts[i] = (UTF8[]) v.toArray(new UTF8[v.size()]);
+                hosts[i-startBlock] = (UTF8[]) v.toArray(new UTF8[v.size()]);
             }
             return hosts;
         }
"
hadoop,49601bd8c10b9439796c52dffcc322a56c1bcb33,"Fix for HADOOP-16.  Splitting and other job planning is now performed in a separate thread.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@382545 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-02 23:05:43,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobInProgress.java b/src/java/org/apache/hadoop/mapred/JobInProgress.java
index 05161de..307a3ac 100644
--- a/src/java/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/JobInProgress.java
@@ -44,6 +44,7 @@
     int numReduceTasks = 0;
 
     JobTracker jobtracker = null;
+    TreeMap cachedHints = new TreeMap();
 
     long startTime;
     long finishTime;
@@ -62,7 +63,7 @@
         this.conf = conf;
         this.jobtracker = jobtracker;
         this.profile = new JobProfile(jobid, jobFile, url);
-        this.status = new JobStatus(jobid, 0.0f, 0.0f, JobStatus.RUNNING);
+        this.status = new JobStatus(jobid, 0.0f, 0.0f, JobStatus.PREP);
         this.startTime = System.currentTimeMillis();
 
         this.localJobFile = new JobConf(conf).getLocalFile(JobTracker.SUBDIR, jobid + "".xml"");
@@ -92,9 +93,10 @@
     }
 
     /**
-     * Construct the splits, etc
+     * Construct the splits, etc.  This is invoked from an async
+     * thread so that split-computation doesn't block anyone.
      */
-    void initTasks() throws IOException {
+    public void initTasks() throws IOException {
         if (tasksInited) {
             return;
         }
@@ -153,9 +155,33 @@
             reduces[i] = new TaskInProgress(jobFile, maps, i, jobtracker, conf, this);
         }
 
+        //
+        // Obtain some tasktracker-cache information for the map task splits.
+        //
+        for (int i = 0; i < maps.length; i++) {
+            String hints[][] = fs.getFileCacheHints(splits[i].getFile(), splits[i].getStart(), splits[i].getLength());
+            cachedHints.put(maps[i].getTIPId(), hints);
+        }
+
+        this.status = new JobStatus(status.getJobId(), 0.0f, 0.0f, JobStatus.RUNNING);
         tasksInited = true;
     }
 
+    /**
+     * This is called by TaskInProgress objects.  The JobInProgress
+     * prefetches and caches a lot of these hints.  If the hint is
+     * not available, then we pass it through to the filesystem.
+     */
+    String[][] getFileCacheHints(String tipID, File f, long start, long len) throws IOException {
+        String results[][] = (String[][]) cachedHints.get(tipID);
+        if (tipID == null) {
+            FileSystem fs = FileSystem.get(conf);
+            results = fs.getFileCacheHints(f, start, len);
+            cachedHints.put(tipID, results);
+        }
+        return results;
+    }
+
     /////////////////////////////////////////////////////
     // Accessors for the JobInProgress
     /////////////////////////////////////////////////////
@@ -252,12 +278,8 @@
      */
     public Task obtainNewMapTask(String taskTracker, TaskTrackerStatus tts) {
         if (! tasksInited) {
-            try {
-                initTasks();
-            } catch (IOException ie) {
-                ie.printStackTrace();
-                LOG.info(""Cannot create task split for "" + profile.getJobId());
-            }
+            LOG.info(""Cannot create task split for "" + profile.getJobId());
+            return null;
         }
 
         Task t = null;
@@ -271,30 +293,62 @@
         // we call obtainNewMapTask() really fast, twice in a row.
         // There's not enough time for the ""recentTasks""
         //
+
+        //
+        // Compute avg progress through the map tasks
+        //
+        for (int i = 0; i < maps.length; i++) {        
+            totalProgress += maps[i].getProgress();
+        }
+        double avgProgress = totalProgress / maps.length;
+
+        //
+        // See if there is a split over a block that is stored on
+        // the TaskTracker checking in.  That means the block
+        // doesn't have to be transmitted from another node.
+        //
         for (int i = 0; i < maps.length; i++) {
             if (maps[i].hasTaskWithCacheHit(taskTracker, tts)) {
                 if (cacheTarget < 0) {
                     cacheTarget = i;
                     break;
                 }
-            } else if (maps[i].hasTask()) {
-                if (stdTarget < 0) {
-                    stdTarget = i;
-                    break;
-                }
             }
-            totalProgress += maps[i].getProgress();
         }
-        double avgProgress = totalProgress / maps.length;
 
-        for (int i = 0; i < maps.length; i++) {        
-            if (maps[i].hasSpeculativeTask(avgProgress)) {
-                if (specTarget < 0) {
-                    specTarget = i;
+        //
+        // If there's no cached target, see if there's
+        // a std. task to run.
+        //
+        if (cacheTarget < 0) {
+            for (int i = 0; i < maps.length; i++) {
+                if (maps[i].hasTask()) {
+                    if (stdTarget < 0) {
+                        stdTarget = i;
+                        break;
+                    }
                 }
             }
         }
-        
+
+        //
+        // If no cached-target and no std target, see if
+        // there's a speculative task to run.
+        //
+        if (cacheTarget < 0 && stdTarget < 0) {
+            for (int i = 0; i < maps.length; i++) {        
+                if (maps[i].hasSpeculativeTask(avgProgress)) {
+                    if (specTarget < 0) {
+                        specTarget = i;
+                        break;
+                    }
+                }
+            }
+        }
+
+        //
+        // Run whatever we found
+        //
         if (cacheTarget >= 0) {
             t = maps[cacheTarget].getTaskToRun(taskTracker, tts, avgProgress);
         } else if (stdTarget >= 0) {
@@ -312,12 +366,8 @@
      */
     public Task obtainNewReduceTask(String taskTracker, TaskTrackerStatus tts) {
         if (! tasksInited) {
-            try {
-                initTasks();
-            } catch (IOException ie) {
-                ie.printStackTrace();
-                LOG.info(""Cannot create task split for "" + profile.getJobId());
-            }
+            LOG.info(""Cannot create task split for "" + profile.getJobId());
+            return null;
         }
 
         Task t = null;
"
hadoop,49601bd8c10b9439796c52dffcc322a56c1bcb33,"Fix for HADOOP-16.  Splitting and other job planning is now performed in a separate thread.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@382545 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-02 23:05:43,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobStatus.java b/src/java/org/apache/hadoop/mapred/JobStatus.java
index d0c9642..5a27ee4 100644
--- a/src/java/org/apache/hadoop/mapred/JobStatus.java
+++ b/src/java/org/apache/hadoop/mapred/JobStatus.java
@@ -39,6 +39,7 @@
     public static final int RUNNING = 1;
     public static final int SUCCEEDED = 2;
     public static final int FAILED = 3;
+    public static final int PREP = 4;
 
     String jobid;
     float mapProgress;
"
hadoop,49601bd8c10b9439796c52dffcc322a56c1bcb33,"Fix for HADOOP-16.  Splitting and other job planning is now performed in a separate thread.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@382545 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-02 23:05:43,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index 9196317..6283ebe 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -33,6 +33,7 @@
  * @author Mike Cafarella
  *******************************************************/
 public class JobTracker implements MRConstants, InterTrackerProtocol, JobSubmissionProtocol {
+    static long JOBINIT_SLEEP_INTERVAL = 2000;
     static long RETIRE_JOB_INTERVAL;
     static long RETIRE_JOB_CHECK_INTERVAL;
     static float TASK_ALLOC_EPSILON;
@@ -156,14 +157,21 @@
                 }
                 
                 synchronized (jobs) {
-                    for (Iterator it = jobs.keySet().iterator(); it.hasNext(); ) {
-                        String jobid = (String) it.next();
-                        JobInProgress job = (JobInProgress) jobs.get(jobid);
+                    synchronized (jobInitQueue) {
+                        synchronized (jobsByArrival) {
+                            for (Iterator it = jobs.keySet().iterator(); it.hasNext(); ) {
+                                String jobid = (String) it.next();
+                                JobInProgress job = (JobInProgress) jobs.get(jobid);
 
-                        if (job.getStatus().getRunState() != JobStatus.RUNNING &&
-                            (job.getFinishTime() + RETIRE_JOB_INTERVAL < System.currentTimeMillis())) {
-                            it.remove();
-                            jobsByArrival.remove(job);
+                                if (job.getStatus().getRunState() != JobStatus.RUNNING &&
+                                    job.getStatus().getRunState() != JobStatus.PREP &&
+                                    (job.getFinishTime() + RETIRE_JOB_INTERVAL < System.currentTimeMillis())) {
+                                    it.remove();
+                            
+                                    jobInitQueue.remove(job);
+                                    jobsByArrival.remove(job);
+                                }
+                            }
                         }
                     }
                 }
@@ -175,6 +183,43 @@
     }
 
     /////////////////////////////////////////////////////////////////
+    //  Used to init new jobs that have just been created
+    /////////////////////////////////////////////////////////////////
+    class JobInitThread implements Runnable {
+        boolean shouldRun = true;
+        public JobInitThread() {
+        }
+        public void run() {
+            while (shouldRun) {
+                JobInProgress job = null;
+                synchronized (jobInitQueue) {
+                    if (jobInitQueue.size() > 0) {
+                        job = (JobInProgress) jobInitQueue.elementAt(0);
+                        jobInitQueue.remove(job);
+                    } else {
+                        try {
+                            jobInitQueue.wait(JOBINIT_SLEEP_INTERVAL);
+                        } catch (InterruptedException iex) {
+                        }
+                    }
+                }
+                try {
+                    if (job != null) {
+                        job.initTasks();
+                    }
+                } catch (Exception e) {
+                    LOG.log(Level.WARNING, ""job init failed"", e);
+                    job.kill();
+                }
+            }
+        }
+        public void stopIniter() {
+            shouldRun = false;
+        }
+    }
+
+
+    /////////////////////////////////////////////////////////////////
     // The real JobTracker
     ////////////////////////////////////////////////////////////////
     int port;
@@ -221,8 +266,10 @@
     int totalMaps = 0;
     int totalReduces = 0;
     TreeMap taskTrackers = new TreeMap();
+    Vector jobInitQueue = new Vector();
     ExpireTrackers expireTrackers = new ExpireTrackers();
     RetireJobs retireJobs = new RetireJobs();
+    JobInitThread initJobs = new JobInitThread();
 
     /**
      * It might seem like a bug to maintain a TreeSet of status objects,
@@ -307,6 +354,7 @@
 
         new Thread(this.expireTrackers).start();
         new Thread(this.retireJobs).start();
+        new Thread(this.initJobs).start();
     }
 
     public static InetSocketAddress getAddress(Configuration conf) {
@@ -521,67 +569,69 @@
         // has not yet been removed from the pool, making capacity seem
         // larger than it really is.)
         //
-        if ((numMaps < maxCurrentTasks) &&
-            (numMaps <= (avgMaps + TASK_ALLOC_EPSILON))) {
+        synchronized (jobsByArrival) {
+            if ((numMaps < maxCurrentTasks) &&
+                (numMaps <= (avgMaps + TASK_ALLOC_EPSILON))) {
 
-            int totalNeededMaps = 0;
-            for (Iterator it = jobsByArrival.iterator(); it.hasNext(); ) {
-                JobInProgress job = (JobInProgress) it.next();
-                if (job.getStatus().getRunState() != JobStatus.RUNNING) {
-                    continue;
-                }
+                int totalNeededMaps = 0;
+                for (Iterator it = jobsByArrival.iterator(); it.hasNext(); ) {
+                    JobInProgress job = (JobInProgress) it.next();
+                    if (job.getStatus().getRunState() != JobStatus.RUNNING) {
+                        continue;
+                    }
 
-                Task t = job.obtainNewMapTask(taskTracker, tts);
-                if (t != null) {
-                    return t;
-                }
+                    Task t = job.obtainNewMapTask(taskTracker, tts);
+                    if (t != null) {
+                        return t;
+                    }
 
-                //
-                // Beyond the highest-priority task, reserve a little 
-                // room for failures and speculative executions; don't 
-                // schedule tasks to the hilt.
-                //
-                totalNeededMaps += job.desiredMaps();
-                double padding = 0;
-                if (totalCapacity > MIN_SLOTS_FOR_PADDING) {
-                    padding = Math.min(maxCurrentTasks, totalNeededMaps * PAD_FRACTION);
-                }
-                if (totalNeededMaps + padding >= totalCapacity) {
-                    break;
+                    //
+                    // Beyond the highest-priority task, reserve a little 
+                    // room for failures and speculative executions; don't 
+                    // schedule tasks to the hilt.
+                    //
+                    totalNeededMaps += job.desiredMaps();
+                    double padding = 0;
+                    if (totalCapacity > MIN_SLOTS_FOR_PADDING) {
+                        padding = Math.min(maxCurrentTasks, totalNeededMaps * PAD_FRACTION);
+                    }
+                    if (totalNeededMaps + padding >= totalCapacity) {
+                        break;
+                    }
                 }
             }
-        }
 
-        //
-        // Same thing, but for reduce tasks
-        //
-        if ((numReduces < maxCurrentTasks) &&
-            (numReduces <= (avgReduces + TASK_ALLOC_EPSILON))) {
+            //
+            // Same thing, but for reduce tasks
+            //
+            if ((numReduces < maxCurrentTasks) &&
+                (numReduces <= (avgReduces + TASK_ALLOC_EPSILON))) {
 
-            int totalNeededReduces = 0;
-            for (Iterator it = jobsByArrival.iterator(); it.hasNext(); ) {
-                JobInProgress job = (JobInProgress) it.next();
-                if (job.getStatus().getRunState() != JobStatus.RUNNING) {
-                    continue;
-                }
+                int totalNeededReduces = 0;
+                for (Iterator it = jobsByArrival.iterator(); it.hasNext(); ) {
+                    JobInProgress job = (JobInProgress) it.next();
+                    if (job.getStatus().getRunState() != JobStatus.RUNNING) {
+                        continue;
+                    }
 
-                Task t = job.obtainNewReduceTask(taskTracker, tts);
-                if (t != null) {
-                    return t;
-                }
+                    Task t = job.obtainNewReduceTask(taskTracker, tts);
+                    if (t != null) {
+                        return t;
+                    }
 
-                //
-                // Beyond the highest-priority task, reserve a little 
-                // room for failures and speculative executions; don't 
-                // schedule tasks to the hilt.
-                //
-                totalNeededReduces += job.desiredReduces();
-                double padding = 0;
-                if (totalCapacity > MIN_SLOTS_FOR_PADDING) {
-                    padding = Math.min(maxCurrentTasks, totalNeededReduces * PAD_FRACTION);
-                }
-                if (totalNeededReduces + padding >= totalCapacity) {
-                    break;
+                    //
+                    // Beyond the highest-priority task, reserve a little 
+                    // room for failures and speculative executions; don't 
+                    // schedule tasks to the hilt.
+                    //
+                    totalNeededReduces += job.desiredReduces();
+                    double padding = 0;
+                    if (totalCapacity > MIN_SLOTS_FOR_PADDING) {
+                        padding = Math.min(maxCurrentTasks, totalNeededReduces * PAD_FRACTION);
+                    }
+                    if (totalNeededReduces + padding >= totalCapacity) {
+                        break;
+                    }
                 }
             }
         }
@@ -645,9 +695,31 @@
     ////////////////////////////////////////////////////
     // JobSubmissionProtocol
     ////////////////////////////////////////////////////
+    /**
+     * JobTracker.submitJob() kicks off a new job.  
+     *
+     * Create a 'JobInProgress' object, which contains both JobProfile
+     * and JobStatus.  Those two sub-objects are sometimes shipped outside
+     * of the JobTracker.  But JobInProgress adds info that's useful for
+     * the JobTracker alone.
+     *
+     * We add the JIP to the jobInitQueue, which is processed 
+     * asynchronously to handle split-computation and build up
+     * the right TaskTracker/Block mapping.
+     */
     public synchronized JobStatus submitJob(String jobFile) throws IOException {
         totalSubmissions++;
-        JobInProgress job = createJob(jobFile);
+        JobInProgress job = new JobInProgress(jobFile, this, this.conf);
+        synchronized (jobs) {
+            synchronized (jobsByArrival) {
+                synchronized (jobInitQueue) {
+                    jobs.put(job.getProfile().getJobId(), job);
+                    jobsByArrival.add(job);
+                    jobInitQueue.add(job);
+                    jobInitQueue.notifyAll();
+                }
+            }
+        }
         return job.getStatus();
     }
 
@@ -732,25 +804,6 @@
         return """" + Integer.toString(Math.abs(r.nextInt()),36);
     }
 
-    /**
-     * JobProfile createJob() kicks off a new job.  
-     * This function creates a job profile and also decomposes it into
-     * tasks.  The tasks are added to the unassignedTasks structure.  
-     * (The precise structure will change as we get more sophisticated about 
-     * task allocation.)
-     *
-     * Create a 'JobInProgress' object, which contains both JobProfile
-     * and JobStatus.  Those two sub-objects are sometimes shipped outside
-     * of the JobTracker.  But JobInProgress adds info that's useful for
-     * the JobTracker alone.
-     */
-    JobInProgress createJob(String jobFile) throws IOException {
-        JobInProgress job = new JobInProgress(jobFile, this, this.conf);
-        jobs.put(job.getProfile().getJobId(), job);
-        jobsByArrival.add(job);
-        return job;
-    }
-
     ////////////////////////////////////////////////////
     // Methods to track all the TaskTrackers
     ////////////////////////////////////////////////////
"
hadoop,49601bd8c10b9439796c52dffcc322a56c1bcb33,"Fix for HADOOP-16.  Splitting and other job planning is now performed in a separate thread.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@382545 13f79535-47bb-0310-9956-ffa450edef68
",2006-03-02 23:05:43,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskInProgress.java b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
index fb18df4..62d6dd2 100644
--- a/src/java/org/apache/hadoop/mapred/TaskInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
@@ -360,8 +360,7 @@
             try {
                 if (isMapTask()) {
                     if (hints == null) {
-                        FileSystem fs = FileSystem.get(conf);
-                        hints = fs.getFileCacheHints(split.getFile(), split.getStart(), split.getLength());
+                        hints = job.getFileCacheHints(getTIPId(), split.getFile(), split.getStart(), split.getLength());
                     }
                     if (hints != null) {
                         for (int i = 0; i < hints.length; i++) {
"
hadoop,0940edcbcf4642c1561245b452b6cab2b42bc17f,"Fix for HADOOP-40.  Buffer size was ignored.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@380825 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-24 21:37:18,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FSDataOutputStream.java b/src/java/org/apache/hadoop/fs/FSDataOutputStream.java
index c9fe9ac..6e9b0c6 100644
--- a/src/java/org/apache/hadoop/fs/FSDataOutputStream.java
+++ b/src/java/org/apache/hadoop/fs/FSDataOutputStream.java
@@ -122,19 +122,20 @@
   }
 
   public FSDataOutputStream(FileSystem fs, File file,
-                             boolean overwrite, Configuration conf)
+                            boolean overwrite, Configuration conf,
+                            int bufferSize)
     throws IOException {
     super(new Buffer(new PositionCache(new Summer(fs, file, overwrite, conf)),
-            conf.getInt(""io.file.buffer.size"", 4096)));
+                     bufferSize));
   }
 
   /** Construct without checksums. */
-  public FSDataOutputStream(FSOutputStream out, Configuration conf) throws IOException {
+  private FSDataOutputStream(FSOutputStream out, Configuration conf) throws IOException {
     this(out, conf.getInt(""io.file.buffer.size"", 4096));
   }
 
   /** Construct without checksums. */
-  public FSDataOutputStream(FSOutputStream out, int bufferSize)
+  private FSDataOutputStream(FSOutputStream out, int bufferSize)
     throws IOException {
     super(new Buffer(new PositionCache(out), bufferSize));
   }
"
hadoop,0940edcbcf4642c1561245b452b6cab2b42bc17f,"Fix for HADOOP-40.  Buffer size was ignored.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@380825 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-24 21:37:18,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FileSystem.java b/src/java/org/apache/hadoop/fs/FileSystem.java
index e2109fc..7966e9e 100644
--- a/src/java/org/apache/hadoop/fs/FileSystem.java
+++ b/src/java/org/apache/hadoop/fs/FileSystem.java
@@ -177,7 +177,7 @@
      */
     public FSDataOutputStream create(File f, boolean overwrite,
                                       int bufferSize) throws IOException {
-      return new FSDataOutputStream(this, f, overwrite, getConf());
+      return new FSDataOutputStream(this, f, overwrite, getConf(), bufferSize);
     }
 
     /** Opens an OutputStream at the indicated File.
"
hadoop,80254201e5c5efa0aebc58811b6b1cfcb8cf5ea1,"Fix for HADOOP-40.  Buffer position was not maintained correctly.  Contributed by Konstantin Shvachko.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@380042 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-23 05:16:12,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/fs/FSDataInputStream.java b/src/java/org/apache/hadoop/fs/FSDataInputStream.java
index 3911fcd..7685d05 100644
--- a/src/java/org/apache/hadoop/fs/FSDataInputStream.java
+++ b/src/java/org/apache/hadoop/fs/FSDataInputStream.java
@@ -154,8 +154,9 @@
     // This is the only read() method called by BufferedInputStream, so we trap
     // calls to it in order to cache the position.
     public int read(byte b[], int off, int len) throws IOException {
-      int result = in.read(b, off, len);
-      position += result;
+      int result;
+      if( (result = in.read(b, off, len)) > 0 )
+        position += result;
       return result;
     }
 
"
hadoop,6a533c06ffa9965853c724c34b1722b9decbb244,"Fix for HADOOP-12.  The JobTracker now loads the InputFormat from the job's jar file.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@376474 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-09 23:00:29,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobInProgress.java b/src/java/org/apache/hadoop/mapred/JobInProgress.java
index ef2a4be..c75b3fd 100644
--- a/src/java/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/JobInProgress.java
@@ -20,6 +20,7 @@
 import org.apache.hadoop.util.LogFormatter;
 
 import java.io.*;
+import java.net.*;
 import java.util.*;
 import java.util.logging.*;
 
@@ -35,6 +36,7 @@
     JobProfile profile;
     JobStatus status;
     File localJobFile = null;
+    File localJarFile = null;
 
     TaskInProgress maps[] = null;
     TaskInProgress reduces[] = null;
@@ -64,10 +66,18 @@
         this.startTime = System.currentTimeMillis();
 
         this.localJobFile = new JobConf(conf).getLocalFile(JobTracker.SUBDIR, jobid + "".xml"");
+        this.localJarFile = new JobConf(conf).getLocalFile(JobTracker.SUBDIR, jobid + "".jar"");
         FileSystem fs = FileSystem.get(conf);
         fs.copyToLocalFile(new File(jobFile), localJobFile);
 
         JobConf jd = new JobConf(localJobFile);
+
+        String jarFile = jd.getJar();
+        if (jarFile != null) {
+          fs.copyToLocalFile(new File(jarFile), localJarFile);
+          jd.setJar(localJarFile.getCanonicalPath());
+        }
+
         this.numMapTasks = jd.getNumMapTasks();
         this.numReduceTasks = jd.getNumReduceTasks();
 
@@ -97,8 +107,22 @@
 
         JobConf jd = new JobConf(localJobFile);
         FileSystem fs = FileSystem.get(conf);
-        FileSplit[] splits =
-            jd.getInputFormat().getSplits(fs, jd, numMapTasks);
+        String ifClassName = jd.get(""mapred.input.format.class"");
+        InputFormat inputFormat;
+        if (ifClassName != null && localJarFile != null) {
+          try {
+            ClassLoader loader =
+              new URLClassLoader(new URL[]{ localJarFile.toURL() });
+            Class inputFormatClass = loader.loadClass(ifClassName);
+            inputFormat = (InputFormat)inputFormatClass.newInstance();
+          } catch (Exception e) {
+            throw new IOException(e.toString());
+          }
+        } else {
+          inputFormat = jd.getInputFormat();
+        }
+
+        FileSplit[] splits = inputFormat.getSplits(fs, jd, numMapTasks);
 
         //
         // sort splits by decreasing length, to reduce job's tail
@@ -417,6 +441,10 @@
             localJobFile.delete();
             localJobFile = null;
         }
+        if (localJarFile != null) {
+            localJarFile.delete();
+            localJarFile = null;
+        }
 
         //
         // If the job file was in the temporary system directory,
"
hadoop,7814c7d3f6daefd4f88349ed07e6f557be422f84,"Fix HADOOP-28.  Jsp pages are now pre-compiled to servlets that can access package-private classes.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@376055 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-08 20:39:16,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTrackerInfoServer.java b/src/java/org/apache/hadoop/mapred/JobTrackerInfoServer.java
index 9cb2128..fdd37e2 100644
--- a/src/java/org/apache/hadoop/mapred/JobTrackerInfoServer.java
+++ b/src/java/org/apache/hadoop/mapred/JobTrackerInfoServer.java
@@ -61,8 +61,8 @@
 	    } catch (UnsupportedEncodingException e) {
 	    }
 	}
-	File jobtracker = new File(path, ""jobtracker"");
-        WebApplicationContext context = server.addWebApplication(null, ""/"", jobtracker.getCanonicalPath());
+        WebApplicationContext context =
+          server.addWebApplication(null,""/"",new File(path).getCanonicalPath());
 
         SocketListener socketListener = new SocketListener();
         socketListener.setPort(port);
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/ClientProtocol.java b/src/java/org/apache/hadoop/dfs/ClientProtocol.java
index e497d05..b66e110 100644
--- a/src/java/org/apache/hadoop/dfs/ClientProtocol.java
+++ b/src/java/org/apache/hadoop/dfs/ClientProtocol.java
@@ -16,7 +16,6 @@
 package org.apache.hadoop.dfs;
 
 import java.io.*;
-import org.apache.hadoop.io.*;
 
 /**********************************************************************
  * Protocol that an DFS client uses to communicate with the NameNode.
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DF.java b/src/java/org/apache/hadoop/dfs/DF.java
index bb46573..15681f8 100644
--- a/src/java/org/apache/hadoop/dfs/DF.java
+++ b/src/java/org/apache/hadoop/dfs/DF.java
@@ -15,13 +15,11 @@
  */
 package org.apache.hadoop.dfs;
 
-import java.io.File;
 import java.io.IOException;
 import java.io.InputStreamReader;
 import java.io.BufferedReader;
 
 import java.util.StringTokenizer;
-import java.util.Iterator;
 
 /** Filesystem disk space usage statistics.  Uses the unix 'df' program.
  * Tested on Linux, FreeBSD and Cygwin. */
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DFSShell.java b/src/java/org/apache/hadoop/dfs/DFSShell.java
index 73d1178..a771b0d 100644
--- a/src/java/org/apache/hadoop/dfs/DFSShell.java
+++ b/src/java/org/apache/hadoop/dfs/DFSShell.java
@@ -16,11 +16,8 @@
 package org.apache.hadoop.dfs;
 
 import java.io.*;
-import java.net.*;
 import java.util.*;
 
-import org.apache.hadoop.io.*;
-import org.apache.hadoop.ipc.*;
 import org.apache.hadoop.conf.*;
 import org.apache.hadoop.fs.*;
 
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DataNode.java b/src/java/org/apache/hadoop/dfs/DataNode.java
index f986aa4..dc995a1 100644
--- a/src/java/org/apache/hadoop/dfs/DataNode.java
+++ b/src/java/org/apache/hadoop/dfs/DataNode.java
@@ -15,7 +15,6 @@
  */
 package org.apache.hadoop.dfs;
 
-import org.apache.hadoop.io.*;
 import org.apache.hadoop.ipc.*;
 import org.apache.hadoop.conf.*;
 import org.apache.hadoop.util.*;
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DatanodeInfo.java b/src/java/org/apache/hadoop/dfs/DatanodeInfo.java
index dc4ff09..f1c4194 100644
--- a/src/java/org/apache/hadoop/dfs/DatanodeInfo.java
+++ b/src/java/org/apache/hadoop/dfs/DatanodeInfo.java
@@ -16,7 +16,6 @@
 package org.apache.hadoop.dfs;
 
 import org.apache.hadoop.io.*;
-import org.apache.hadoop.conf.*;
 
 import java.io.*;
 import java.util.*;
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/DatanodeProtocol.java b/src/java/org/apache/hadoop/dfs/DatanodeProtocol.java
index c8d9e7e..6f50eed 100644
--- a/src/java/org/apache/hadoop/dfs/DatanodeProtocol.java
+++ b/src/java/org/apache/hadoop/dfs/DatanodeProtocol.java
@@ -17,7 +17,6 @@
 package org.apache.hadoop.dfs;
 
 import java.io.*;
-import org.apache.hadoop.io.*;
 
 /**********************************************************************
  * Protocol that an DFS datanode uses to communicate with the NameNode.
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/dfs/NameNode.java b/src/java/org/apache/hadoop/dfs/NameNode.java
index 239496c..59c9e28 100644
--- a/src/java/org/apache/hadoop/dfs/NameNode.java
+++ b/src/java/org/apache/hadoop/dfs/NameNode.java
@@ -21,8 +21,6 @@
 import org.apache.hadoop.util.LogFormatter;
 
 import java.io.*;
-import java.net.*;
-import java.util.*;
 import java.util.logging.*;
 
 /**********************************************************
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/MapFile.java b/src/java/org/apache/hadoop/io/MapFile.java
index 3e2f9c3..c2fb099 100644
--- a/src/java/org/apache/hadoop/io/MapFile.java
+++ b/src/java/org/apache/hadoop/io/MapFile.java
@@ -17,7 +17,6 @@
 package org.apache.hadoop.io;
 
 import java.io.*;
-import java.util.Arrays;
 import org.apache.hadoop.fs.*;
 import org.apache.hadoop.conf.*;
 
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/ObjectWritable.java b/src/java/org/apache/hadoop/io/ObjectWritable.java
index 928d154..bf8b5c2 100644
--- a/src/java/org/apache/hadoop/io/ObjectWritable.java
+++ b/src/java/org/apache/hadoop/io/ObjectWritable.java
@@ -16,11 +16,7 @@
 
 package org.apache.hadoop.io;
 
-import java.lang.reflect.Proxy;
-import java.lang.reflect.Method;
 import java.lang.reflect.Array;
-import java.lang.reflect.InvocationHandler;
-import java.lang.reflect.InvocationTargetException;
 
 import java.io.*;
 import java.util.*;
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/io/SequenceFile.java b/src/java/org/apache/hadoop/io/SequenceFile.java
index 82a4d33..f992412 100644
--- a/src/java/org/apache/hadoop/io/SequenceFile.java
+++ b/src/java/org/apache/hadoop/io/SequenceFile.java
@@ -20,7 +20,6 @@
 import java.util.*;
 import java.util.zip.*;
 import java.util.logging.*;
-import java.nio.channels.*;
 import java.net.InetAddress;
 import java.rmi.server.UID;
 import java.security.MessageDigest;
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/ipc/RPC.java b/src/java/org/apache/hadoop/ipc/RPC.java
index cc34b5a..a71196a 100644
--- a/src/java/org/apache/hadoop/ipc/RPC.java
+++ b/src/java/org/apache/hadoop/ipc/RPC.java
@@ -25,7 +25,6 @@
 import java.net.InetSocketAddress;
 import java.util.logging.Logger;
 import java.io.*;
-import java.util.*;
 
 import org.apache.hadoop.io.*;
 import org.apache.hadoop.conf.*;
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/CombiningCollector.java b/src/java/org/apache/hadoop/mapred/CombiningCollector.java
index 973a5b8..7b5fa27 100644
--- a/src/java/org/apache/hadoop/mapred/CombiningCollector.java
+++ b/src/java/org/apache/hadoop/mapred/CombiningCollector.java
@@ -17,11 +17,9 @@
 package org.apache.hadoop.mapred;
 
 import java.io.*;
-import java.net.*;
 import java.util.*;
 
 import org.apache.hadoop.io.*;
-import org.apache.hadoop.conf.*;
 
 /** Implements partial value reduction during mapping.  This can minimize the
  * size of intermediate data.  Buffers a list of values for each unique key,
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/InputFormat.java b/src/java/org/apache/hadoop/mapred/InputFormat.java
index ecf34fb..f3f610e 100644
--- a/src/java/org/apache/hadoop/mapred/InputFormat.java
+++ b/src/java/org/apache/hadoop/mapred/InputFormat.java
@@ -17,7 +17,6 @@
 package org.apache.hadoop.mapred;
 
 import java.io.IOException;
-import java.io.File;
 
 import org.apache.hadoop.fs.FileSystem;
 
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/InputFormatBase.java b/src/java/org/apache/hadoop/mapred/InputFormatBase.java
index cfbd7be..e1e9c57 100644
--- a/src/java/org/apache/hadoop/mapred/InputFormatBase.java
+++ b/src/java/org/apache/hadoop/mapred/InputFormatBase.java
@@ -19,7 +19,6 @@
 import java.io.IOException;
 import java.io.File;
 
-import java.util.Arrays;
 import java.util.ArrayList;
 import java.util.logging.Logger;
 
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java b/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java
index 763d735..19bc566 100644
--- a/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java
+++ b/src/java/org/apache/hadoop/mapred/InterTrackerProtocol.java
@@ -17,7 +17,6 @@
 package org.apache.hadoop.mapred;
 
 import java.io.*;
-import org.apache.hadoop.io.*;
 
 /** 
  * Protocol that a TaskTracker and the central JobTracker use to communicate.
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobClient.java b/src/java/org/apache/hadoop/mapred/JobClient.java
index 0fe2845..47179c2 100644
--- a/src/java/org/apache/hadoop/mapred/JobClient.java
+++ b/src/java/org/apache/hadoop/mapred/JobClient.java
@@ -15,7 +15,6 @@
  */
 package org.apache.hadoop.mapred;
 
-import org.apache.hadoop.io.*;
 import org.apache.hadoop.fs.*;
 import org.apache.hadoop.ipc.*;
 import org.apache.hadoop.conf.*;
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobConf.java b/src/java/org/apache/hadoop/mapred/JobConf.java
index f59967a..d450d18 100644
--- a/src/java/org/apache/hadoop/mapred/JobConf.java
+++ b/src/java/org/apache/hadoop/mapred/JobConf.java
@@ -19,19 +19,11 @@
 
 import java.io.IOException;
 import java.io.File;
-import java.io.InputStream;
-import java.io.FileNotFoundException;
-import java.net.URL;
 
-import java.util.Properties;
-import java.util.jar.JarFile;
-import java.util.jar.JarEntry;
 import java.util.StringTokenizer;
 import java.util.ArrayList;
-import java.util.List;
 import java.util.Collections;
 
-import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.FileUtil;
 import org.apache.hadoop.conf.Configuration;
 
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobInProgress.java b/src/java/org/apache/hadoop/mapred/JobInProgress.java
index 03696dc..3987453 100644
--- a/src/java/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/JobInProgress.java
@@ -15,14 +15,11 @@
  */
 package org.apache.hadoop.mapred;
 
-import org.apache.hadoop.io.*;
 import org.apache.hadoop.fs.*;
-import org.apache.hadoop.ipc.*;
 import org.apache.hadoop.conf.*;
 import org.apache.hadoop.util.LogFormatter;
 
 import java.io.*;
-import java.net.*;
 import java.util.*;
 import java.util.logging.*;
 
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobProfile.java b/src/java/org/apache/hadoop/mapred/JobProfile.java
index c646042..a0a96b6 100644
--- a/src/java/org/apache/hadoop/mapred/JobProfile.java
+++ b/src/java/org/apache/hadoop/mapred/JobProfile.java
@@ -19,7 +19,6 @@
 
 import java.io.*;
 import java.net.*;
-import java.util.*;
 
 /**************************************************
  * A JobProfile is a MapReduce primitive.  Tracks a job,
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobStatus.java b/src/java/org/apache/hadoop/mapred/JobStatus.java
index 63d9eb6..e2f65f4 100644
--- a/src/java/org/apache/hadoop/mapred/JobStatus.java
+++ b/src/java/org/apache/hadoop/mapred/JobStatus.java
@@ -18,8 +18,6 @@
 import org.apache.hadoop.io.*;
 
 import java.io.*;
-import java.net.*;
-import java.util.*;
 
 /**************************************************
  * Describes the current status of a job.  This is
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobSubmissionProtocol.java b/src/java/org/apache/hadoop/mapred/JobSubmissionProtocol.java
index 965616b..910a272 100644
--- a/src/java/org/apache/hadoop/mapred/JobSubmissionProtocol.java
+++ b/src/java/org/apache/hadoop/mapred/JobSubmissionProtocol.java
@@ -18,7 +18,6 @@
 
 import java.io.*;
 import java.util.*;
-import org.apache.hadoop.io.*;
 
 /** 
  * Protocol that a JobClient and the central JobTracker use to communicate.  The
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTracker.java b/src/java/org/apache/hadoop/mapred/JobTracker.java
index fc9a0e7..6173fa7 100644
--- a/src/java/org/apache/hadoop/mapred/JobTracker.java
+++ b/src/java/org/apache/hadoop/mapred/JobTracker.java
@@ -16,7 +16,6 @@
 package org.apache.hadoop.mapred;
 
 
-import org.apache.hadoop.io.*;
 import org.apache.hadoop.fs.*;
 import org.apache.hadoop.ipc.*;
 import org.apache.hadoop.conf.*;
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/JobTrackerInfoServer.java b/src/java/org/apache/hadoop/mapred/JobTrackerInfoServer.java
index 837a3ab..cb363d5 100644
--- a/src/java/org/apache/hadoop/mapred/JobTrackerInfoServer.java
+++ b/src/java/org/apache/hadoop/mapred/JobTrackerInfoServer.java
@@ -15,23 +15,12 @@
  */
 package org.apache.hadoop.mapred;
 
-import org.apache.hadoop.io.*;
-import org.apache.hadoop.ipc.*;
-import org.apache.hadoop.conf.*;
-
-import org.mortbay.util.*;
 import org.mortbay.http.*;
 import org.mortbay.http.handler.*;
 import org.mortbay.jetty.servlet.*;
-import org.mortbay.jetty.*;
 
 import java.io.*;
 import java.net.*;
-import java.util.*;
-import java.util.logging.*;
-
-import javax.servlet.*;
-import javax.servlet.http.*;
 
 /*******************************************************
  * JobTrackerInfoServer provides stats about the JobTracker
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/LocalJobRunner.java b/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
index 99a5326..a65010b 100644
--- a/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
+++ b/src/java/org/apache/hadoop/mapred/LocalJobRunner.java
@@ -20,7 +20,6 @@
 import java.util.*;
 import java.util.logging.*;
 
-import org.apache.hadoop.io.*;
 import org.apache.hadoop.fs.*;
 import org.apache.hadoop.conf.*;
 import org.apache.hadoop.util.LogFormatter;
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/MapOutputLocation.java b/src/java/org/apache/hadoop/mapred/MapOutputLocation.java
index 22e9a86..8ce2ccf 100644
--- a/src/java/org/apache/hadoop/mapred/MapOutputLocation.java
+++ b/src/java/org/apache/hadoop/mapred/MapOutputLocation.java
@@ -20,7 +20,6 @@
 
 import java.io.*;
 import org.apache.hadoop.io.*;
-import org.apache.hadoop.conf.*;
 
 /** The location of a map output file, as passed to a reduce task via the
  * {@link InterTrackerProtocol}. */ 
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/MapRunnable.java b/src/java/org/apache/hadoop/mapred/MapRunnable.java
index 2099da4..3ffb927 100644
--- a/src/java/org/apache/hadoop/mapred/MapRunnable.java
+++ b/src/java/org/apache/hadoop/mapred/MapRunnable.java
@@ -18,9 +18,6 @@
 
 import java.io.IOException;
 
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.io.WritableComparable;
-
 /** Expert: Permits greater control of map processing. For example,
  * implementations might perform multi-threaded, asynchronous mappings. */
 public interface MapRunnable extends JobConfigurable {
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/MapTask.java b/src/java/org/apache/hadoop/mapred/MapTask.java
index 888b386..4b623b0 100644
--- a/src/java/org/apache/hadoop/mapred/MapTask.java
+++ b/src/java/org/apache/hadoop/mapred/MapTask.java
@@ -17,8 +17,6 @@
 package org.apache.hadoop.mapred;
 
 import java.io.*;
-import java.net.*;
-import java.util.*;
 
 import org.apache.hadoop.io.*;
 import org.apache.hadoop.conf.Configuration;
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/MapTaskRunner.java b/src/java/org/apache/hadoop/mapred/MapTaskRunner.java
index df03ff9..1ebfba8 100644
--- a/src/java/org/apache/hadoop/mapred/MapTaskRunner.java
+++ b/src/java/org/apache/hadoop/mapred/MapTaskRunner.java
@@ -15,14 +15,9 @@
  */
 package org.apache.hadoop.mapred;
 
-import org.apache.hadoop.io.*;
-import org.apache.hadoop.ipc.*;
 import org.apache.hadoop.conf.*;
 
 import java.io.*;
-import java.net.*;
-import java.util.*;
-import java.util.logging.*;
 
 /** Runs a map task. */
 class MapTaskRunner extends TaskRunner {
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/OutputCollector.java b/src/java/org/apache/hadoop/mapred/OutputCollector.java
index 17b7fd0..969e8d3 100644
--- a/src/java/org/apache/hadoop/mapred/OutputCollector.java
+++ b/src/java/org/apache/hadoop/mapred/OutputCollector.java
@@ -17,7 +17,6 @@
 package org.apache.hadoop.mapred;
 
 import java.io.IOException;
-import java.io.DataInput;
 
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableComparable;
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/OutputFormat.java b/src/java/org/apache/hadoop/mapred/OutputFormat.java
index 43ea456..195afa1 100644
--- a/src/java/org/apache/hadoop/mapred/OutputFormat.java
+++ b/src/java/org/apache/hadoop/mapred/OutputFormat.java
@@ -17,7 +17,6 @@
 package org.apache.hadoop.mapred;
 
 import java.io.IOException;
-import java.io.File;
 
 import org.apache.hadoop.fs.FileSystem;
 
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/RecordReader.java b/src/java/org/apache/hadoop/mapred/RecordReader.java
index 90bcc97..02d97e1 100644
--- a/src/java/org/apache/hadoop/mapred/RecordReader.java
+++ b/src/java/org/apache/hadoop/mapred/RecordReader.java
@@ -19,7 +19,6 @@
 import java.io.IOException;
 import java.io.DataInput;
 
-import org.apache.hadoop.io.WritableComparable;
 import org.apache.hadoop.io.Writable;
 
 /** Reads key/value pairs from an input file {@link FileSplit}.
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/ReduceTask.java b/src/java/org/apache/hadoop/mapred/ReduceTask.java
index dd3b1ff..cfe85e8 100644
--- a/src/java/org/apache/hadoop/mapred/ReduceTask.java
+++ b/src/java/org/apache/hadoop/mapred/ReduceTask.java
@@ -22,7 +22,6 @@
 import org.apache.hadoop.util.*;
 
 import java.io.*;
-import java.net.*;
 import java.util.*;
 import java.text.*;
 
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/RunningJob.java b/src/java/org/apache/hadoop/mapred/RunningJob.java
index 07012d0..c6d8069 100644
--- a/src/java/org/apache/hadoop/mapred/RunningJob.java
+++ b/src/java/org/apache/hadoop/mapred/RunningJob.java
@@ -16,10 +16,7 @@
 
 package org.apache.hadoop.mapred;
 
-import org.apache.hadoop.io.*;
-
 import java.io.*;
-import java.net.*;
 
 /** 
  * Includes details on a running MapReduce job.  A client can
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/SequenceFileInputFormat.java b/src/java/org/apache/hadoop/mapred/SequenceFileInputFormat.java
index 78951ba..77e7ae3 100644
--- a/src/java/org/apache/hadoop/mapred/SequenceFileInputFormat.java
+++ b/src/java/org/apache/hadoop/mapred/SequenceFileInputFormat.java
@@ -23,10 +23,6 @@
 
 import org.apache.hadoop.io.SequenceFile;
 import org.apache.hadoop.io.MapFile;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.io.WritableComparable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.UTF8;
 
 /** An {@link InputFormat} for {@link SequenceFile}s. */
 public class SequenceFileInputFormat extends InputFormatBase {
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java b/src/java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java
index 3fdaf25..c266704 100644
--- a/src/java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java
+++ b/src/java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java
@@ -22,7 +22,6 @@
 
 import org.apache.hadoop.fs.FileSystem;
 
-import org.apache.hadoop.io.MapFile;
 import org.apache.hadoop.io.SequenceFile;
 import org.apache.hadoop.io.WritableComparable;
 import org.apache.hadoop.io.Writable;
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/SequenceFileRecordReader.java b/src/java/org/apache/hadoop/mapred/SequenceFileRecordReader.java
index 230ce78..d6fcf0d 100644
--- a/src/java/org/apache/hadoop/mapred/SequenceFileRecordReader.java
+++ b/src/java/org/apache/hadoop/mapred/SequenceFileRecordReader.java
@@ -17,15 +17,11 @@
 package org.apache.hadoop.mapred;
 
 import java.io.IOException;
-import java.io.File;
 
 import org.apache.hadoop.fs.FileSystem;
 
 import org.apache.hadoop.io.SequenceFile;
 import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.io.WritableComparable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.UTF8;
 import org.apache.hadoop.conf.Configuration;
 
 /** An {@link RecordReader} for {@link SequenceFile}s. */
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/Task.java b/src/java/org/apache/hadoop/mapred/Task.java
index 8f70fd0..cad3a77 100644
--- a/src/java/org/apache/hadoop/mapred/Task.java
+++ b/src/java/org/apache/hadoop/mapred/Task.java
@@ -21,8 +21,6 @@
 import org.apache.hadoop.util.*;
 
 import java.io.*;
-import java.net.*;
-import java.util.*;
 
 /** Base class for tasks. */
 public abstract class Task implements Writable, Configurable {
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskInProgress.java b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
index fb1c3b4..66264b6 100644
--- a/src/java/org/apache/hadoop/mapred/TaskInProgress.java
+++ b/src/java/org/apache/hadoop/mapred/TaskInProgress.java
@@ -15,14 +15,11 @@
  */
 package org.apache.hadoop.mapred;
 
-import org.apache.hadoop.io.*;
 import org.apache.hadoop.fs.*;
-import org.apache.hadoop.ipc.*;
 import org.apache.hadoop.conf.*;
 import org.apache.hadoop.util.LogFormatter;
 
 import java.io.*;
-import java.net.*;
 import java.util.*;
 import java.util.logging.*;
 
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskRunner.java b/src/java/org/apache/hadoop/mapred/TaskRunner.java
index d978fdb..394f18a 100644
--- a/src/java/org/apache/hadoop/mapred/TaskRunner.java
+++ b/src/java/org/apache/hadoop/mapred/TaskRunner.java
@@ -15,15 +15,11 @@
  */
 package org.apache.hadoop.mapred;
 
-import org.apache.hadoop.io.*;
-import org.apache.hadoop.ipc.*;
 import org.apache.hadoop.conf.*;
 import org.apache.hadoop.util.LogFormatter;
 import org.apache.hadoop.fs.*;
 
 import java.io.*;
-import java.net.*;
-import java.util.*;
 import java.util.logging.*;
 
 /** Base class that runs a task in a separate process.  Tasks are run in a
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskStatus.java b/src/java/org/apache/hadoop/mapred/TaskStatus.java
index 40d4397..b66eacb 100644
--- a/src/java/org/apache/hadoop/mapred/TaskStatus.java
+++ b/src/java/org/apache/hadoop/mapred/TaskStatus.java
@@ -18,8 +18,6 @@
 import org.apache.hadoop.io.*;
 
 import java.io.*;
-import java.net.*;
-import java.util.*;
 
 /**************************************************
  * Describes the current status of a task.  This is
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskTrackerStatus.java b/src/java/org/apache/hadoop/mapred/TaskTrackerStatus.java
index 4ef7eb1..a3c8ca0 100644
--- a/src/java/org/apache/hadoop/mapred/TaskTrackerStatus.java
+++ b/src/java/org/apache/hadoop/mapred/TaskTrackerStatus.java
@@ -18,7 +18,6 @@
 import org.apache.hadoop.io.*;
 
 import java.io.*;
-import java.net.*;
 import java.util.*;
 
 /**************************************************
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java b/src/java/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java
index f736927..9f8cc06 100644
--- a/src/java/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java
+++ b/src/java/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java
@@ -18,8 +18,6 @@
 
 import java.io.IOException;
 
-import org.apache.hadoop.io.*;
-
 /** Protocol that task child process uses to contact its parent process.  The
  * parent is a daemon which which polls the central master for a new map or
  * reduce task and runs it as a child process.  All communication between child
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/TextInputFormat.java b/src/java/org/apache/hadoop/mapred/TextInputFormat.java
index d75b6a6..b03742e 100644
--- a/src/java/org/apache/hadoop/mapred/TextInputFormat.java
+++ b/src/java/org/apache/hadoop/mapred/TextInputFormat.java
@@ -17,13 +17,11 @@
 package org.apache.hadoop.mapred;
 
 import java.io.IOException;
-import java.io.File;
 
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.FSDataInputStream;
 
 import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.io.WritableComparable;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.UTF8;
 
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/demo/Grep.java b/src/java/org/apache/hadoop/mapred/demo/Grep.java
index 0abbdf3..f9a964f 100644
--- a/src/java/org/apache/hadoop/mapred/demo/Grep.java
+++ b/src/java/org/apache/hadoop/mapred/demo/Grep.java
@@ -17,18 +17,15 @@
 
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.RunningJob;
 import org.apache.hadoop.mapred.SequenceFileOutputFormat;
 import org.apache.hadoop.mapred.SequenceFileInputFormat;
 
 import org.apache.hadoop.mapred.lib.RegexMapper;
 import org.apache.hadoop.mapred.lib.InverseMapper;
 import org.apache.hadoop.mapred.lib.LongSumReducer;
-import org.apache.hadoop.mapred.lib.IdentityReducer;
 
 import org.apache.hadoop.io.UTF8;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.WritableComparator;
 
 import org.apache.hadoop.conf.Configuration;
 
"
hadoop,f8b13ed6eea50c7d8b316f872cfb6d806735fa77,"Fix for HADOOP-22: remove unused imports.  By Sami Siren.

git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk@375343 13f79535-47bb-0310-9956-ffa450edef68
",2006-02-06 19:49:21,Doug Cutting,"diff --git a/src/java/org/apache/hadoop/mapred/lib/LongSumReducer.java b/src/java/org/apache/hadoop/mapred/lib/LongSumReducer.java
index de73a6c..3e202f8 100644
--- a/src/java/org/apache/hadoop/mapred/lib/LongSumReducer.java
+++ b/src/java/org/apache/hadoop/mapred/lib/LongSumReducer.java
@@ -24,8 +24,6 @@
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.Reporter;
 
-
-import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableComparable;
 import org.apache.hadoop.io.LongWritable;
 
"
