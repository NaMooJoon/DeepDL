{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (2607854817.py, line 63)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [2], line 63\u001b[1;36m\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from transformer import Encoder, Decoder\n",
    "\n",
    "num_layers = 6\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "dff = 2048\n",
    "\n",
    "learning_rate = tf.keras.optimzers.schedule.ExponentialDecay(0.1, 4000, 0.99)\n",
    "optimizer = tf.optimizers.legacy.SGD(learning_rate, 0.5)\n",
    "optimizer.clipnorm = 5\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(reduction='sum_over_batch_size')\n",
    "metrics = tf.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "\n",
    "class DeepDLTransformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "    super(DeepDLTransformer, self).__init__()\n",
    "    \n",
    "    self.central_encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
    "                                   input_vocab_size, pe_input, rate)\n",
    "    self.context_encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
    "                                   input_vocab_size, pe_input, rate)\n",
    "    self.attention_layer = tf.keras.layers.MultiHeadAttention(num_heads, \n",
    "                                                              d_model // num_heads)\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
    "                           target_vocab_size, pe_target, rate)\n",
    "    self.linear_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "  def call(self, inputs, training, mask=None):\n",
    "    cen_enc_in, con_enc_in, dec_in = inputs\n",
    "    cen_enc_padding_mask = self.create_padding_mask(cen_enc_in)\n",
    "    con_enc_padding_mask = self.create_padding_mask(con_enc_in) \n",
    "    \n",
    "    cen_enc_out = self.central_encoder(cen_enc_in, \n",
    "                                       training, cen_enc_padding_mask)  # (batch_size, cen_in_seq_len, d_model)\n",
    "    con_enc_out = self.context_encoder(con_enc_in, \n",
    "                                       training, con_enc_padding_mask)  # (batch_size, con_in_seq_len, d_model)\n",
    "    \n",
    "    attn_out = self.attention_layer(cen_enc_out, con_enc_out, \n",
    "                                    con_enc_padding_mask, False, \n",
    "                                    training, False)  # (batchsize, cen_in_seq_len, d_model) \n",
    "    dec_out, attn_w_dict = self.decoder(dec_in, attn_out, \n",
    "                                        training, cen_enc_padding_mask)  # dec_output.shape == (batch_size, dec_in_seq_len, d_model)\n",
    "    linear_out = self.linear_layer(dec_out)  # (batch_size, dec_in_seq_len, target_vocab_size)\n",
    "    out = tf.math.softmax(linear_out, axis=2)\n",
    "\n",
    "    return out, attn_w_dict\n",
    "\n",
    "  def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.logical_not(tf.math.equal(seq, 0)), dtype=tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "  \n",
    "\n",
    "class DeepDL(tf.Module): \n",
    "  \n",
    "  def __init__(self, deedl_transformer):\n",
    "    \n",
    "  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7rc1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
